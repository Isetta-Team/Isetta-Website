{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Isetta Engine \u00b6 We built a game engine in 3 months. Is that crazy? We've been told so. But there's no way to learn like being pushed straight into the deep end! We are a student project from Carnegie Mellon's Entertainment Technology Center. The aim of the project is to help demystify the game engine development process for ourselves and any other novice developers who have interest in engine development but like us were too intimidated of where to start. What is There to See? \u00b6 The Engine \u00b6 We made a game engine! It's called the Isetta Engine, and we did it to learn about game engine development. The Isetta Engine is open-sourced here for anyone to see and criticize ! The Compendium \u00b6 We documented our whole development process and compiled what we learned on this website. The best place to look for something specific about engine development is our compendium . The Blogs \u00b6 If you want to read our week-by-week progress, you can start at Week 0 in our weekly blog. The Interviews \u00b6 We interviewed industry professionals as part of the project, and you can find all of those interviews publically hosted on our website ! We also have a book published , if you're interested in getting a physical or PDF version of these interviews. The Developers \u00b6 If you're interested in using our engine (we appreciate your curiosity!), you can find our starter documentation on our website as well. The Documentary \u00b6 We made a short documentary, too! It covers our journey as developers from more of the human side, and it was made by our producer and resident creative, Jared. Here's a trailer: The full documentary can be seen on our YouTube page . Isetta Timeline \u00b6","title":"Home"},{"location":"#welcome-to-isetta-engine","text":"We built a game engine in 3 months. Is that crazy? We've been told so. But there's no way to learn like being pushed straight into the deep end! We are a student project from Carnegie Mellon's Entertainment Technology Center. The aim of the project is to help demystify the game engine development process for ourselves and any other novice developers who have interest in engine development but like us were too intimidated of where to start.","title":"Welcome to Isetta Engine"},{"location":"#what-is-there-to-see","text":"","title":"What is There to See?"},{"location":"#the-engine","text":"We made a game engine! It's called the Isetta Engine, and we did it to learn about game engine development. The Isetta Engine is open-sourced here for anyone to see and criticize !","title":"The Engine"},{"location":"#the-compendium","text":"We documented our whole development process and compiled what we learned on this website. The best place to look for something specific about engine development is our compendium .","title":"The Compendium"},{"location":"#the-blogs","text":"If you want to read our week-by-week progress, you can start at Week 0 in our weekly blog.","title":"The Blogs"},{"location":"#the-interviews","text":"We interviewed industry professionals as part of the project, and you can find all of those interviews publically hosted on our website ! We also have a book published , if you're interested in getting a physical or PDF version of these interviews.","title":"The Interviews"},{"location":"#the-developers","text":"If you're interested in using our engine (we appreciate your curiosity!), you can find our starter documentation on our website as well.","title":"The Developers"},{"location":"#the-documentary","text":"We made a short documentary, too! It covers our journey as developers from more of the human side, and it was made by our producer and resident creative, Jared. Here's a trailer: The full documentary can be seen on our YouTube page .","title":"The Documentary"},{"location":"#isetta-timeline","text":"","title":"Isetta Timeline"},{"location":"about/","text":"Mission Statement \u00b6 Game engine development is a very wide field in the industry, but also a very inaccessible one. Budding engineers are advised to just jump into development to learn, and it's for this reason that the Isetta Engine project came to be. Our team built a game engine specialized for the twin-stick shooter genre. Every step of the way, we documented our process through a dedicated blog. Readers can hear from us personally and get an in-depth look at our journey to create the engine. In addition, we supplemented our content through regular interviews with industry professionals who have built their own engines before. Between our own hands-on process and the sage advice from veteran engineers, we hope to give newcomers a clearer representation of the engine-building process. This project spanned from August to December of 2018, and all of our progress and code will be accessible via the Git repo . Disclaimer \u00b6 Although the project is aimed at helping novice developers, this is not to be used as a sole source of learning engine development. Being new engine developers ourselves, we can't guarantee the way we developed the engine is correct, which is why the interviews will help the project remain grounded. This means others who are learning can use what we've done as a guide and not necessarily the ground truth. The blogs won't be a walkthrough/tutorial/step-by-step instructions on how to develop an engine. This also isn't a project about learning C++, programming patterns (although we did use them), or design principles (which we tried to adhere to). From our understanding engine development can be messy, and although we tried to do our best, we had limited time to develop so we commented, tested, and designed as much as we could without losing sight of the goal.","title":"About"},{"location":"about/#mission-statement","text":"Game engine development is a very wide field in the industry, but also a very inaccessible one. Budding engineers are advised to just jump into development to learn, and it's for this reason that the Isetta Engine project came to be. Our team built a game engine specialized for the twin-stick shooter genre. Every step of the way, we documented our process through a dedicated blog. Readers can hear from us personally and get an in-depth look at our journey to create the engine. In addition, we supplemented our content through regular interviews with industry professionals who have built their own engines before. Between our own hands-on process and the sage advice from veteran engineers, we hope to give newcomers a clearer representation of the engine-building process. This project spanned from August to December of 2018, and all of our progress and code will be accessible via the Git repo .","title":"Mission Statement"},{"location":"about/#disclaimer","text":"Although the project is aimed at helping novice developers, this is not to be used as a sole source of learning engine development. Being new engine developers ourselves, we can't guarantee the way we developed the engine is correct, which is why the interviews will help the project remain grounded. This means others who are learning can use what we've done as a guide and not necessarily the ground truth. The blogs won't be a walkthrough/tutorial/step-by-step instructions on how to develop an engine. This also isn't a project about learning C++, programming patterns (although we did use them), or design principles (which we tried to adhere to). From our understanding engine development can be messy, and although we tried to do our best, we had limited time to develop so we commented, tested, and designed as much as we could without losing sight of the goal.","title":"Disclaimer"},{"location":"faq/","text":"FAQ \u00b6 Here is a collection of questions we have often received from our faculty and followers of our project: About the Project \u00b6 Why \"Isetta\"? \u00b6 In our engine name brainstorming process, one idea we kept coming back to was the \"Urkel Engine\", based off Steve Urkel, the loveable loser from Family Matters. We thought his catchphrase \"Did I do that?\" matched our pratfalls that would likely happen throughout the semester. Unfortunately, nobody outside our team liked the name. We then remembered that on the show, Urkel had a tiny rickety car called an Isetta, the image of which was perfect for our engine. What is the point of this project? \u00b6 The Isetta Engine is meant to provide an in-depth look at engine creation for newcomers who want to build their own engine but are not sure what that entails. Will this be paid for or free or open source? \u00b6 Open source all the way! We want as many others to benefit from our experience as possible. Is there a single aspect of engine development that you will be focusing on? \u00b6 Our goal is to show everything required to build a game engine, so our plan is to keep all of it simple and rudimentary rather than emphasize one particular part. How is this different from other series like Handmade Hero or reading Game Engine Architecture \u00b6 During our interview with Casey Muratori , he talked about how good engine programming should be approached like a boundary value problem with the shooting method, with the sides of the boundary being (1) high-level architecture and (2) the real-world, actual solution. So to build the right engine for your needs, you need to improve a bit upon the architecture, then improve a bit on your actual solution, ad infinitum. We're not collaborating with Casey, or other resources, beyond interviewing and reading, but we think this analogy explains the relationship of their work with ours: the professionals are taking their experience as game engine programmers and working down (trying to reach new engine programmers), and we're building our first engine and are working up (trying to take new engine programmers along with us). About the Engine \u00b6 What platforms are you targeting? \u00b6 Our plan is to keep the release to 64-Bit Windows systems. Because of our tight 3-month deadline, we do not think a multiplatform release would be within scope. What language are you using and why? \u00b6 We are using purely C++ for the engine, and possibly python/lua as the scripting language. And the main reason why we chose C++ is that it's the 3A game industry standard language. Are you building everything from scratch? \u00b6 Not everything. Some parts of the engine we will be building from scratch and others we will be importing from 3 rd parties. You can see our architecture for a full overview. Are you going to build an abstraction layer on top of the low level APIs? \u00b6 Yes, that's what the modules are for. What does the Horde3D engine provide? \u00b6 Horde3D is what we will be using for our rendering engine. It does not handle input and GUI. We are using GLFW for input and imgui for GUI. Is your architecture the best/ideal architecture? \u00b6 No, probably not. We are novices so any decision we make may be right, may be wrong. We will probably know better at the end of the journey. We think going through the process of developing an engine, even if the architecture isn't the best, is worthwhile and worth following. Will this be the definitive Twin-Stick Shooter engine? \u00b6 Nope, definitely not. We don't expect others to use this engine to develop any game (we probably wouldn't). It will be a janky engine, that will run a game, and we will learn a lot from. Is your engine multithreaded? \u00b6 No, definitely not. Even though we understand that parrallism is the future of game engines and Unity is recently pushing the ECS system to utilize the computational power of multi-core CPUs, we are afraid that we won't make a decent multithreaded game engine in 3 months. We are only including multithreaded for making something similar to Unity's Coroutines and in our file system using Windows' API. However, if you are following along and don't have such harsh time constraints, we encourage you to make your engine multi-threaded, and most of our decision & thought process will still be applicable.","title":"FAQ"},{"location":"faq/#faq","text":"Here is a collection of questions we have often received from our faculty and followers of our project:","title":"FAQ"},{"location":"faq/#about-the-project","text":"","title":"About the Project"},{"location":"faq/#why-isetta","text":"In our engine name brainstorming process, one idea we kept coming back to was the \"Urkel Engine\", based off Steve Urkel, the loveable loser from Family Matters. We thought his catchphrase \"Did I do that?\" matched our pratfalls that would likely happen throughout the semester. Unfortunately, nobody outside our team liked the name. We then remembered that on the show, Urkel had a tiny rickety car called an Isetta, the image of which was perfect for our engine.","title":"Why \"Isetta\"?"},{"location":"faq/#what-is-the-point-of-this-project","text":"The Isetta Engine is meant to provide an in-depth look at engine creation for newcomers who want to build their own engine but are not sure what that entails.","title":"What is the point of this project?"},{"location":"faq/#will-this-be-paid-for-or-free-or-open-source","text":"Open source all the way! We want as many others to benefit from our experience as possible.","title":"Will this be paid for or free or open source?"},{"location":"faq/#is-there-a-single-aspect-of-engine-development-that-you-will-be-focusing-on","text":"Our goal is to show everything required to build a game engine, so our plan is to keep all of it simple and rudimentary rather than emphasize one particular part.","title":"Is there a single aspect of engine development that you will be focusing on?"},{"location":"faq/#how-is-this-different-from-other-series-like-handmade-hero-or-reading-game-engine-architecture","text":"During our interview with Casey Muratori , he talked about how good engine programming should be approached like a boundary value problem with the shooting method, with the sides of the boundary being (1) high-level architecture and (2) the real-world, actual solution. So to build the right engine for your needs, you need to improve a bit upon the architecture, then improve a bit on your actual solution, ad infinitum. We're not collaborating with Casey, or other resources, beyond interviewing and reading, but we think this analogy explains the relationship of their work with ours: the professionals are taking their experience as game engine programmers and working down (trying to reach new engine programmers), and we're building our first engine and are working up (trying to take new engine programmers along with us).","title":"How is this different from other series like Handmade Hero or reading Game Engine Architecture"},{"location":"faq/#about-the-engine","text":"","title":"About the Engine"},{"location":"faq/#what-platforms-are-you-targeting","text":"Our plan is to keep the release to 64-Bit Windows systems. Because of our tight 3-month deadline, we do not think a multiplatform release would be within scope.","title":"What platforms are you targeting?"},{"location":"faq/#what-language-are-you-using-and-why","text":"We are using purely C++ for the engine, and possibly python/lua as the scripting language. And the main reason why we chose C++ is that it's the 3A game industry standard language.","title":"What language are you using and why?"},{"location":"faq/#are-you-building-everything-from-scratch","text":"Not everything. Some parts of the engine we will be building from scratch and others we will be importing from 3 rd parties. You can see our architecture for a full overview.","title":"Are you building everything from scratch?"},{"location":"faq/#are-you-going-to-build-an-abstraction-layer-on-top-of-the-low-level-apis","text":"Yes, that's what the modules are for.","title":"Are you going to build an abstraction layer on top of the low level APIs?"},{"location":"faq/#what-does-the-horde3d-engine-provide","text":"Horde3D is what we will be using for our rendering engine. It does not handle input and GUI. We are using GLFW for input and imgui for GUI.","title":"What does the Horde3D engine provide?"},{"location":"faq/#is-your-architecture-the-bestideal-architecture","text":"No, probably not. We are novices so any decision we make may be right, may be wrong. We will probably know better at the end of the journey. We think going through the process of developing an engine, even if the architecture isn't the best, is worthwhile and worth following.","title":"Is your architecture the best/ideal architecture?"},{"location":"faq/#will-this-be-the-definitive-twin-stick-shooter-engine","text":"Nope, definitely not. We don't expect others to use this engine to develop any game (we probably wouldn't). It will be a janky engine, that will run a game, and we will learn a lot from.","title":"Will this be the definitive Twin-Stick Shooter engine?"},{"location":"faq/#is-your-engine-multithreaded","text":"No, definitely not. Even though we understand that parrallism is the future of game engines and Unity is recently pushing the ECS system to utilize the computational power of multi-core CPUs, we are afraid that we won't make a decent multithreaded game engine in 3 months. We are only including multithreaded for making something similar to Unity's Coroutines and in our file system using Windows' API. However, if you are following along and don't have such harsh time constraints, we encourage you to make your engine multi-threaded, and most of our decision & thought process will still be applicable.","title":"Is your engine multithreaded?"},{"location":"gallery/","text":"Gallery \u00b6 This pages shows some advanced usages of markdown Extension - Admonition \u00b6 Notes Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Collapsible notes Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Info Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Extension - CodeHilite \u00b6 // Simple C++ program to display \"Hello World\" #include <iostream> using namespace std ; int main () { cout << \"Hello World\" ; return 0 ; } PyMdown \u00b6 Some inline code cout << \"Hello World\" ;","title":"Markdown gallery"},{"location":"gallery/#gallery","text":"This pages shows some advanced usages of markdown","title":"Gallery"},{"location":"gallery/#extension-admonition","text":"Notes Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Collapsible notes Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Info Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Extension - Admonition"},{"location":"gallery/#extension-codehilite","text":"// Simple C++ program to display \"Hello World\" #include <iostream> using namespace std ; int main () { cout << \"Hello World\" ; return 0 ; }","title":"Extension - CodeHilite"},{"location":"gallery/#pymdown","text":"Some inline code cout << \"Hello World\" ;","title":"PyMdown"},{"location":"glossary/","text":"Glossary of All Footnote Terms \u00b6 Linter - a tool that flags code for errors, bugs, stylistic errors, and suspicious constructs. \u21a9 Construct On-Demand Technique - in the context of the singleton pattern , this means that the single instance is only constructed when first requested. We can utilize this nature to control initialization order by manually invoking the getter for the singletons in the right order. \u21a9 Entity-Component-System (ECS) - an architectural pattern that follows composition over inheritance principle and is mostly used in games. \u21a9 Microsoft Foundation Class Library - an object-oriented C++ library that contains useful macros for exceptions, run-time type identification, serialization, and more. DLL - a dynamic-linked library which is Microsoft shared library concept which can be transported around easier than a project and contains information about the compiled project. \u21a9 Wrapper - a class that \"wraps\" around another class to hide/change/add functionality. This is usually done when using other people's libraries to ensure only your features are available. \u21a9 Volume-casting - when a volume's path is traced along a line/curve to test collisions with other objects. \u21a9 PhysX - NVIDIA's real-time physics engine used by most commercially available game engines such as Unity, Unreal, and Lumberyard. \u21a9 Variable-render - refers to the fact that rendering will be updated as fast as the CPU/GPU can allow, not being slowed by a frame rate. All other modules will then be updated with a fixed timestep since some of them dependent on the timestep and can become non-deterministic with a variable timestep. \u21a9 Level Scene Graph - the scene graph corresponding to a level, similar to a level configuration file. It will contain the information (transformation, behavior, whether it is static, etc.) about the starting game objects. \u21a9 Persistent Game Data - Jason Gregory refers to this as \"LSR\" data, Load-and-Stay-Resident, as seen in Game Engine Architecture_section 15.4.2 \u21a9 Defragmentation - Fragmentation is when a lot of memory allocations or files take up noncontinuous chunks of space, leaving awkward bubbles that can't be used by anyone. So defragmentation is the process of reordering those objects so that we can clear up a cleaner, bigger stretch of free space or memory. \u21a9 Runtime or Dynamic Polymorphism - when an overridden class or method is determined at runtime as opposed to compile time. This allows for us to change some behavior of our program depending on the data type it's operating with. \u21a9 Casey Muratori - a game engine developer and creator of Handmade Hero , a web series documenting his efforts in building a game engine from scratch. We interviewed him as part of this project, and his interview can currently be found here . \u21a9 MonoBehaviour - the base class within Unity that all components which attach to GameObjects must derive from, it has methods for start, update, and destroy. \u21a9 ScriptableObjects - scripts which cannot be attached to GameObjects but still store (serialize) user data. \u21a9 Animation-blended System - a graph of multiple animations and transitions from an animation to another, i.e., a idle animation to a walking animation, and the blend system is how the animations are \"mixed\" together. It extrapolates from the starting animation to the ending animation. \u21a9 Core - refers to a CPU in a multi-core processor, it is one of the processing units in the single computing component that read and execute machine instructions. \u21a9 Waterfall Schedules - a linear schedule where each subsequent item is dependent on the previous components being completed, it is less iterative and flexible because the flow is usually mono-directional. \u21a9 Node-based - when an interface is visual with components, \"boxes\", that are connected to each other with outputs connected to inputs. A node-based shader system means a shader is edited through nodes. \u21a9 Scriptable Render Pipeline - a system in Unity that allows the game developer to configure and control the graphics and rendering process via high-level scripting. \u21a9 OpenAL - an audio library used for games, although it contains the word open it actually isn't open-sourced. Its open-source counterpart is OpenALSoft. \u21a9 Panda3D - a game engine, a framework for 3D rendering and game development for Python and C++ programs. It was originally developed by Disney and expanded by past ETC projects. \u21a9 Core Render Loop - the loop where the rendering function is called. The way the rendering occurs/is called varies from engine to engine, but is usually performed at the end of the main game loop. \u21a9 Unreal's Blueprint Visual Scripting System - the node-based scripting in the Unreal Engine used for gameplay scripting. \u21a9 Job Control - the control of multiple tasks on a computer system that may be \"in-flight\" at the same time. It requires proper allocation of resources and locked access to prevent deadlocks and failures. \u21a9 Cross Product - the 3D math operation where the input is two vectors and the output is one vector that's perpendicular to both input vectors. However, the direction of the output vector depends on whether the space is defined as left handed or right handed. \u21a9 Depth Peeling - a method of order-independent transparency when rendering 3D geometry. It determines what should be drawn on top by rendering multiple passes of a scene and comparing depths. \u21a9 Unified Modeling Language (UML) - used to visually represent a software system with its actors and roles so that a programmer can better understand and design said system. Sometimes, UML diagrams can end up as a \"disaster situation\". \u21a9 Build Engineer - the engineer in charge of the infrastructure that builds a software application, as well as testing and troubleshooting code for before the software's release. \u21a9 CMake - a cross-platform, open-source application for managing the build process of software in a compiler-independent way. \u21a9 Ninja - a small build system that is designed to run builds as fast as possible. \u21a9 Engine abstraction - the part of the engine code which depends on the hardware/software platform that the engine runs on and will be different on each platform. For example, the code that talks to the operating system on macOS will be different from that on Windows. Engine developers usually tackle this problem by having an abstraction layer on top of operating system code. So the code above that layer still looks the same when you swap out the underlying operating system. \u21a9 Platform-dependent - code refers to application code that is dependent on one operating system, and typically won't run on multiple. \u21a9 OpenGL - short for Open Graphics Library - a cross-language, cross-platform application programming interface (API) for rendering 2D and 3D vector graphics. The API is typically used to interact with a graphics processing unit (GPU), to achieve hardware-accelerated rendering. It's the underlying rendering library for many modern game engines. \u21a9 SWF - short for Small Web Format, is an Adobe Flash file format used for multimedia, vector graphics and ActionScript. SWF files can contain animations or applets of varying degrees of interactivity and function. \u21a9 Spine - a 2D skeletal animation software for video games by Esoteric Software. \u21a9 Box2D - an open source C++ engine for simulating rigid bodies in 2D. Box2D is developed by Erin Catto and has the zlib license. \u21a9 FMOD - a a cross platform audio engine and authoring tool used throughout the game industry. It was used by over 2,000 games in the last 15 years. \u21a9 Physics Solver - a physics engine for games usually consists of two parts: collision detection and collision resolution, and solver refers to the resolution part. Collision detection detects what objects collide with each other first, and then the solver determines their correct physical response, like position, rotation, velocity, etc. \u21a9 BINK - the defacto video codec for games created by Rad Tools. \u21a9 JAI - a language being developed by Jonathan Blow and his team at Thekla to address some of the issues game developers have with the current industry standard, C++. \u21a9 FLA - the file format for projects created by Adobe Animate, and can contain graphics, video, text, audio, and more. They are often saved as SWF files to be used on the web. \u21a9 Fill Rate - the number of pixels a video card can read/write to the screen per second. \u21a9 Backface Culling - the technique of performing visibility checks on a mesh to not render the back face (face not facing the camera). \u21a9 Stencil Buffer - an additional depth buffer to the depth and color buffers. \u21a9 Depth Culling - the process of deciding which elements to render based on the distance from the camera and if it is being hidden by another element. \u21a9 Run-length Encoding - a form of lossless data compression where data is stored as a single data value and count. \u21a9 Free list - a memory management data structure that uses a linked list which points to successive free regions of memory that can be utilized for allocation individually. \u21a9 STL - standard library which is the C++ library containing most of the needed data structures. It is known to not be best for performance, however, will save us time not implementing them. \u21a9 GLFW - a library utility for creating windows and receiving input from the window. \u21a9 std::function - a container for lambda functions, see cppreference. We have renamed std::function to Action in our aliases for simpler calling. \u21a9 OSI - Open System Interconnection and is a standard for networking layers. \u21a9 Packet - formatted data that is sent over a network. Virtual objects are serialized and broken into these small chunks before being sent, and different protocols expect different formats for packets. \u21a9 Client-server model - in networking where there is one central server that all other machines (the \"clients\") connect to. This server is typically the authority on all important and possibly conflicting information. \u21a9 Peer-to-peer model - in networking is where every machine to one another, which requires more bandwidth per client and more complex data authority handling but avoids needing a dedicated server. Peer-to-peer is generally harder to implement than client-server. \u21a9 Handshake in networking is an automated negotiation process for creating a connection between two machines, typically a client and a server. The process requires the machines to exchange special packets before the connection can be established. \u21a9 Ring buffer - (or circular buffer ) is a FIFO data structure which is broadly used for transmitting data between asynchronous processes. See more: \u21a9 Localhost - an address in networking specifically, 127.0.0.1. Packets that are sent here are not technically sent anywhere, they're just sent up to the next layer for processing. \u21a9 Packet queue - a queue of packets, which are small chunks of an original, bigger message. These are sent out in-batch because of packet size limitations over internet networks. \u21a9 Deterministic Behaviour - a process whose resulting state is determined by the initial state and inputs. It is heavily reliant on having a fixed-time so each step is performing the same amount of \"work\". \u21a9 Data locality - essentially accessing data in as nearby of code as possible to utilize caches most effectively. Robert Nystrom covers it really well in this chapter from Game Programming Patterns . \u21a9 Branching - hen the processor needs to evaluate something in order to determine what code to run next. It tends to be very wasteful of processing time because of the typical instruction pipeline on a computer. Window handle - a GLFW construct can be passed to objects and functions to allow them to hook into a particular window from the operating system. \u21a9 Pixel Stream - a stream of pixel data, typically as four floating point numbers or integers representing red, green, blue, and opacity/alpha. \u21a9 Messaging buffer - a stream of pixel data, typically as four floating point numbers or integers representing red, green, blue, and opacity/alpha. \u21a9 Packet queue - a queue of packets, which are small chunks of an original, bigger message. These are sent out in-batch because of packet size limitations over internet networks. \u21a9 Two-Level Segregate Fit (TLSF) - a memory allocation scheme meant for use in video games because of its fast and efficient memory usage. \u21a9 Regression Testing - the process of testing changes in software to make sure functionality is not broken when updating the software. \u21a9 Qt - a framework for creating retained GUI applications. \u21a9 Viewport - the \"window\" which the camera will render content to the screen, it is specified with an (x,y) offset from the top-left in OpenGL and a width and height. \u21a9 Vertex arrays and buffers - hold the vertex information such as vertex positions, normals, color, etc and are stored within the OpenGL state. \u21a9 Element Buffers - hold additional information regarding the vertices, specifically what index the pertinent information is located within the vertex array. \u21a9 Culling - the early rejection of objects being passed through the render pipeline, because they don't contribute to the final image. \u21a9 Model-view Matrix - the matrix which transforms a position in local space to world space, then to camera space. \u21a9 Transpose - when the entries on the diagonals are flipped about the center diagonal. \u21a9 Vertex Shader - a graphics program that alters information associated to the vertices, it is one of the first stages in the graphics pipeline. \u21a9 Homogenous Coordinates - differentiate points from vectors by expanding the traditional Vector3 to a Vector4 and placing a 0 in the 4 th element for vectors and 1 in the 4 th element for points. \u21a9 Frustum - the portion of the world which is viewable by a camera. It is typically shaped like a pyramid with near and far planes clipping the volume. What is rendered is the volume between the 2 planes. \u21a9 Pivot - the local position of the model which is the zero position. When transforming the model in the world space, all changes are relative to this point. An offset pivot is when the pivot is placed in a position that isn't about the model, for example offset in X=100 from the model. \u21a9 .geo files - Horde3D's processed file for model and animations, optimized for more efficient rendering. The file is processed through the Horde3DUtil library and done prior to runtime. \u21a9 Union - a special class type in C++ that can hold only one of its non-static data members at a time. Similar to a struct, you can declare multiple variables in a union, but only one is available at the same time. Another distinction is that the size of a struct is the sum of all of its members, but the size of a union is the size of the biggest member. The way the author understands it is that union gives you different ways to interpret the same memory values. \u21a9 Object Composition - in Object Oriented Programming, this is a way to combine simple objects or data types into more complex ones. The Component pattern in Game Programming Patterns book describes this in detail. \u21a9 Runtime type information (RTTI) - a language feature that exposes information about an object's data type at runtime. For example, if you want to get the type name of some object as string, you would need RTTI. \u21a9 Macros - a way of automatically substituting text for some other during the compiling process. In C++, they are defined as #define TEXT_IN_CODE TEXT_TO_COPY OVER. For example, if you define #define SPEED 5 and write mySpeed = SPEED, SPEED will be substituted by 5 during compile time and the compiler will actually see mySpeed = 5. \u21a9 Oracle - a computer technology corporation headquartered in Redwood Shores, California, who acquired Java from Sun Microsystems and is now maintaining it. \u21a9 Test Harnesses - a test framework which can ensure the progression of the software. In the Isetta Engine case, they will act as sample levels to demo features of the engine and as versioning happens to ensure old features aren't broken on accident. \u21a9 Heuristic - a technique for solving a problem more quickly, often utilized in optimization problems for computer science. \u21a9 Object Factory - a programming pattern where you decouple object creation and destruction with the actual usage of the object. An object factory typically creates objects based on passed parameters, which you would then receive and use. \u21a9 Typecasting - also known as type conversion, this is a method of changing one data type to another. It helps ensure that variables are processed correctly by functions, but can also be dangerous with the way they handle the conversion. \u21a9 Boilerplate Code - a section of code that has to be included in many places with little to no alteration. \u21a9 Function Templates - functions definitions that, at compile time, generate individual function definitions for any types used with the template. This can be very useful for cutting out duplicate code, but greatly increases compile time and code size. \u21a9 BitSquid - more modernly known as Autodesk Stingray, is a discontinued game engine from Stockholm, Sweden. End of sale was announced for January 7, 2018, and afterward it became a plugin for Autodesk 3DS Max known as 3DS Max Interactive. \u21a9 IMGUI - stands for immediate mode GUI which is a code-driven GUI system where on each rendering frame the application needs to issue the draw commands of the GUI (the GUI is not stored for multiple frames) \u21a9 Scaleform - a vector graphics rendering engine used to display Adobe Flash-based user interfaces and HUDs for video games. \u21a9 Retained GUI - also known as canvas/scene graph, is where GUI is registered once and is displayed, \"retained\", on screen until it removes itself from rendering. \u21a9 Frostbite - EA's proprietary game engine used across most of their studios. \u21a9 Heads-up Display (HUD) - overlay on the screen that presents important information to the player. \u21a9 Intellisense - an intelligent code completion feature in Microsoft Visual Studio that is capable of detailing information about the code and objects that the programmer is working with while coding. \u21a9 Retro Compatibility - also known as backwards compatibility, is when a system is setup such that it works with legacy code/input. \u21a9 Thread-safe code - only manipulates shared data structures in a manner that ensures that all threads behave properly and fulfill their design specifications without unintended interaction. \u21a9 Std::iterator - a C++ type that can be used to iterate through collections of elements based on that collection. \u21a9 Path Tracing - a realistic lighting algorithm that simulates light bouncing around a scene. It uses the Monte Carlo method to give a faithful rendition of the global illumination of the scene. \u21a9 The Demoscene - an international computer art subculture focused on producing demos, which are self-contained, sometimes extremely small, audio-visual computer programs. \u21a9 RenderWare - a game engine by Criterion Software that launched in 1993 and continued to regularly support games through 2010. It was known for providing an off-the-shelf solution to the difficulties of PS2 graphics programming. \u21a9 Autodesk 3ds Max - formerly 3D Studio and 3D Studio Max, is a professional 3D computer graphics program for making 3D animations, models, games, and images. \u21a9 FBX - a proprietary file format owned by Autodesk that is mostly commonly used for 3D model and animation data within the games industry. \u21a9 GlTF(GL Transmission Format) - a royalty-free file format for 3D scenes and models using the JSON standard. \u21a9 Open Dynamic Engine (ODE) - a free and open source physics engine written in C/C++ that can do both rigid body dynamics simulation and collision detection. \u21a9 Cocoa - Apple's native object-oriented API for macOS. \u21a9 UNIX - a family of multitasking, multiuser operating systems that derive from the original AT&T Unix, originally developed at Ken Thompson, Dennis Ritchie, and others at Bell Labs. It's main comparable is Microsoft's DOS, which is mono-task and monouser. \u21a9","title":"Glossary"},{"location":"glossary/#glossary-of-all-footnote-terms","text":"Linter - a tool that flags code for errors, bugs, stylistic errors, and suspicious constructs. \u21a9 Construct On-Demand Technique - in the context of the singleton pattern , this means that the single instance is only constructed when first requested. We can utilize this nature to control initialization order by manually invoking the getter for the singletons in the right order. \u21a9 Entity-Component-System (ECS) - an architectural pattern that follows composition over inheritance principle and is mostly used in games. \u21a9 Microsoft Foundation Class Library - an object-oriented C++ library that contains useful macros for exceptions, run-time type identification, serialization, and more. DLL - a dynamic-linked library which is Microsoft shared library concept which can be transported around easier than a project and contains information about the compiled project. \u21a9 Wrapper - a class that \"wraps\" around another class to hide/change/add functionality. This is usually done when using other people's libraries to ensure only your features are available. \u21a9 Volume-casting - when a volume's path is traced along a line/curve to test collisions with other objects. \u21a9 PhysX - NVIDIA's real-time physics engine used by most commercially available game engines such as Unity, Unreal, and Lumberyard. \u21a9 Variable-render - refers to the fact that rendering will be updated as fast as the CPU/GPU can allow, not being slowed by a frame rate. All other modules will then be updated with a fixed timestep since some of them dependent on the timestep and can become non-deterministic with a variable timestep. \u21a9 Level Scene Graph - the scene graph corresponding to a level, similar to a level configuration file. It will contain the information (transformation, behavior, whether it is static, etc.) about the starting game objects. \u21a9 Persistent Game Data - Jason Gregory refers to this as \"LSR\" data, Load-and-Stay-Resident, as seen in Game Engine Architecture_section 15.4.2 \u21a9 Defragmentation - Fragmentation is when a lot of memory allocations or files take up noncontinuous chunks of space, leaving awkward bubbles that can't be used by anyone. So defragmentation is the process of reordering those objects so that we can clear up a cleaner, bigger stretch of free space or memory. \u21a9 Runtime or Dynamic Polymorphism - when an overridden class or method is determined at runtime as opposed to compile time. This allows for us to change some behavior of our program depending on the data type it's operating with. \u21a9 Casey Muratori - a game engine developer and creator of Handmade Hero , a web series documenting his efforts in building a game engine from scratch. We interviewed him as part of this project, and his interview can currently be found here . \u21a9 MonoBehaviour - the base class within Unity that all components which attach to GameObjects must derive from, it has methods for start, update, and destroy. \u21a9 ScriptableObjects - scripts which cannot be attached to GameObjects but still store (serialize) user data. \u21a9 Animation-blended System - a graph of multiple animations and transitions from an animation to another, i.e., a idle animation to a walking animation, and the blend system is how the animations are \"mixed\" together. It extrapolates from the starting animation to the ending animation. \u21a9 Core - refers to a CPU in a multi-core processor, it is one of the processing units in the single computing component that read and execute machine instructions. \u21a9 Waterfall Schedules - a linear schedule where each subsequent item is dependent on the previous components being completed, it is less iterative and flexible because the flow is usually mono-directional. \u21a9 Node-based - when an interface is visual with components, \"boxes\", that are connected to each other with outputs connected to inputs. A node-based shader system means a shader is edited through nodes. \u21a9 Scriptable Render Pipeline - a system in Unity that allows the game developer to configure and control the graphics and rendering process via high-level scripting. \u21a9 OpenAL - an audio library used for games, although it contains the word open it actually isn't open-sourced. Its open-source counterpart is OpenALSoft. \u21a9 Panda3D - a game engine, a framework for 3D rendering and game development for Python and C++ programs. It was originally developed by Disney and expanded by past ETC projects. \u21a9 Core Render Loop - the loop where the rendering function is called. The way the rendering occurs/is called varies from engine to engine, but is usually performed at the end of the main game loop. \u21a9 Unreal's Blueprint Visual Scripting System - the node-based scripting in the Unreal Engine used for gameplay scripting. \u21a9 Job Control - the control of multiple tasks on a computer system that may be \"in-flight\" at the same time. It requires proper allocation of resources and locked access to prevent deadlocks and failures. \u21a9 Cross Product - the 3D math operation where the input is two vectors and the output is one vector that's perpendicular to both input vectors. However, the direction of the output vector depends on whether the space is defined as left handed or right handed. \u21a9 Depth Peeling - a method of order-independent transparency when rendering 3D geometry. It determines what should be drawn on top by rendering multiple passes of a scene and comparing depths. \u21a9 Unified Modeling Language (UML) - used to visually represent a software system with its actors and roles so that a programmer can better understand and design said system. Sometimes, UML diagrams can end up as a \"disaster situation\". \u21a9 Build Engineer - the engineer in charge of the infrastructure that builds a software application, as well as testing and troubleshooting code for before the software's release. \u21a9 CMake - a cross-platform, open-source application for managing the build process of software in a compiler-independent way. \u21a9 Ninja - a small build system that is designed to run builds as fast as possible. \u21a9 Engine abstraction - the part of the engine code which depends on the hardware/software platform that the engine runs on and will be different on each platform. For example, the code that talks to the operating system on macOS will be different from that on Windows. Engine developers usually tackle this problem by having an abstraction layer on top of operating system code. So the code above that layer still looks the same when you swap out the underlying operating system. \u21a9 Platform-dependent - code refers to application code that is dependent on one operating system, and typically won't run on multiple. \u21a9 OpenGL - short for Open Graphics Library - a cross-language, cross-platform application programming interface (API) for rendering 2D and 3D vector graphics. The API is typically used to interact with a graphics processing unit (GPU), to achieve hardware-accelerated rendering. It's the underlying rendering library for many modern game engines. \u21a9 SWF - short for Small Web Format, is an Adobe Flash file format used for multimedia, vector graphics and ActionScript. SWF files can contain animations or applets of varying degrees of interactivity and function. \u21a9 Spine - a 2D skeletal animation software for video games by Esoteric Software. \u21a9 Box2D - an open source C++ engine for simulating rigid bodies in 2D. Box2D is developed by Erin Catto and has the zlib license. \u21a9 FMOD - a a cross platform audio engine and authoring tool used throughout the game industry. It was used by over 2,000 games in the last 15 years. \u21a9 Physics Solver - a physics engine for games usually consists of two parts: collision detection and collision resolution, and solver refers to the resolution part. Collision detection detects what objects collide with each other first, and then the solver determines their correct physical response, like position, rotation, velocity, etc. \u21a9 BINK - the defacto video codec for games created by Rad Tools. \u21a9 JAI - a language being developed by Jonathan Blow and his team at Thekla to address some of the issues game developers have with the current industry standard, C++. \u21a9 FLA - the file format for projects created by Adobe Animate, and can contain graphics, video, text, audio, and more. They are often saved as SWF files to be used on the web. \u21a9 Fill Rate - the number of pixels a video card can read/write to the screen per second. \u21a9 Backface Culling - the technique of performing visibility checks on a mesh to not render the back face (face not facing the camera). \u21a9 Stencil Buffer - an additional depth buffer to the depth and color buffers. \u21a9 Depth Culling - the process of deciding which elements to render based on the distance from the camera and if it is being hidden by another element. \u21a9 Run-length Encoding - a form of lossless data compression where data is stored as a single data value and count. \u21a9 Free list - a memory management data structure that uses a linked list which points to successive free regions of memory that can be utilized for allocation individually. \u21a9 STL - standard library which is the C++ library containing most of the needed data structures. It is known to not be best for performance, however, will save us time not implementing them. \u21a9 GLFW - a library utility for creating windows and receiving input from the window. \u21a9 std::function - a container for lambda functions, see cppreference. We have renamed std::function to Action in our aliases for simpler calling. \u21a9 OSI - Open System Interconnection and is a standard for networking layers. \u21a9 Packet - formatted data that is sent over a network. Virtual objects are serialized and broken into these small chunks before being sent, and different protocols expect different formats for packets. \u21a9 Client-server model - in networking where there is one central server that all other machines (the \"clients\") connect to. This server is typically the authority on all important and possibly conflicting information. \u21a9 Peer-to-peer model - in networking is where every machine to one another, which requires more bandwidth per client and more complex data authority handling but avoids needing a dedicated server. Peer-to-peer is generally harder to implement than client-server. \u21a9 Handshake in networking is an automated negotiation process for creating a connection between two machines, typically a client and a server. The process requires the machines to exchange special packets before the connection can be established. \u21a9 Ring buffer - (or circular buffer ) is a FIFO data structure which is broadly used for transmitting data between asynchronous processes. See more: \u21a9 Localhost - an address in networking specifically, 127.0.0.1. Packets that are sent here are not technically sent anywhere, they're just sent up to the next layer for processing. \u21a9 Packet queue - a queue of packets, which are small chunks of an original, bigger message. These are sent out in-batch because of packet size limitations over internet networks. \u21a9 Deterministic Behaviour - a process whose resulting state is determined by the initial state and inputs. It is heavily reliant on having a fixed-time so each step is performing the same amount of \"work\". \u21a9 Data locality - essentially accessing data in as nearby of code as possible to utilize caches most effectively. Robert Nystrom covers it really well in this chapter from Game Programming Patterns . \u21a9 Branching - hen the processor needs to evaluate something in order to determine what code to run next. It tends to be very wasteful of processing time because of the typical instruction pipeline on a computer. Window handle - a GLFW construct can be passed to objects and functions to allow them to hook into a particular window from the operating system. \u21a9 Pixel Stream - a stream of pixel data, typically as four floating point numbers or integers representing red, green, blue, and opacity/alpha. \u21a9 Messaging buffer - a stream of pixel data, typically as four floating point numbers or integers representing red, green, blue, and opacity/alpha. \u21a9 Packet queue - a queue of packets, which are small chunks of an original, bigger message. These are sent out in-batch because of packet size limitations over internet networks. \u21a9 Two-Level Segregate Fit (TLSF) - a memory allocation scheme meant for use in video games because of its fast and efficient memory usage. \u21a9 Regression Testing - the process of testing changes in software to make sure functionality is not broken when updating the software. \u21a9 Qt - a framework for creating retained GUI applications. \u21a9 Viewport - the \"window\" which the camera will render content to the screen, it is specified with an (x,y) offset from the top-left in OpenGL and a width and height. \u21a9 Vertex arrays and buffers - hold the vertex information such as vertex positions, normals, color, etc and are stored within the OpenGL state. \u21a9 Element Buffers - hold additional information regarding the vertices, specifically what index the pertinent information is located within the vertex array. \u21a9 Culling - the early rejection of objects being passed through the render pipeline, because they don't contribute to the final image. \u21a9 Model-view Matrix - the matrix which transforms a position in local space to world space, then to camera space. \u21a9 Transpose - when the entries on the diagonals are flipped about the center diagonal. \u21a9 Vertex Shader - a graphics program that alters information associated to the vertices, it is one of the first stages in the graphics pipeline. \u21a9 Homogenous Coordinates - differentiate points from vectors by expanding the traditional Vector3 to a Vector4 and placing a 0 in the 4 th element for vectors and 1 in the 4 th element for points. \u21a9 Frustum - the portion of the world which is viewable by a camera. It is typically shaped like a pyramid with near and far planes clipping the volume. What is rendered is the volume between the 2 planes. \u21a9 Pivot - the local position of the model which is the zero position. When transforming the model in the world space, all changes are relative to this point. An offset pivot is when the pivot is placed in a position that isn't about the model, for example offset in X=100 from the model. \u21a9 .geo files - Horde3D's processed file for model and animations, optimized for more efficient rendering. The file is processed through the Horde3DUtil library and done prior to runtime. \u21a9 Union - a special class type in C++ that can hold only one of its non-static data members at a time. Similar to a struct, you can declare multiple variables in a union, but only one is available at the same time. Another distinction is that the size of a struct is the sum of all of its members, but the size of a union is the size of the biggest member. The way the author understands it is that union gives you different ways to interpret the same memory values. \u21a9 Object Composition - in Object Oriented Programming, this is a way to combine simple objects or data types into more complex ones. The Component pattern in Game Programming Patterns book describes this in detail. \u21a9 Runtime type information (RTTI) - a language feature that exposes information about an object's data type at runtime. For example, if you want to get the type name of some object as string, you would need RTTI. \u21a9 Macros - a way of automatically substituting text for some other during the compiling process. In C++, they are defined as #define TEXT_IN_CODE TEXT_TO_COPY OVER. For example, if you define #define SPEED 5 and write mySpeed = SPEED, SPEED will be substituted by 5 during compile time and the compiler will actually see mySpeed = 5. \u21a9 Oracle - a computer technology corporation headquartered in Redwood Shores, California, who acquired Java from Sun Microsystems and is now maintaining it. \u21a9 Test Harnesses - a test framework which can ensure the progression of the software. In the Isetta Engine case, they will act as sample levels to demo features of the engine and as versioning happens to ensure old features aren't broken on accident. \u21a9 Heuristic - a technique for solving a problem more quickly, often utilized in optimization problems for computer science. \u21a9 Object Factory - a programming pattern where you decouple object creation and destruction with the actual usage of the object. An object factory typically creates objects based on passed parameters, which you would then receive and use. \u21a9 Typecasting - also known as type conversion, this is a method of changing one data type to another. It helps ensure that variables are processed correctly by functions, but can also be dangerous with the way they handle the conversion. \u21a9 Boilerplate Code - a section of code that has to be included in many places with little to no alteration. \u21a9 Function Templates - functions definitions that, at compile time, generate individual function definitions for any types used with the template. This can be very useful for cutting out duplicate code, but greatly increases compile time and code size. \u21a9 BitSquid - more modernly known as Autodesk Stingray, is a discontinued game engine from Stockholm, Sweden. End of sale was announced for January 7, 2018, and afterward it became a plugin for Autodesk 3DS Max known as 3DS Max Interactive. \u21a9 IMGUI - stands for immediate mode GUI which is a code-driven GUI system where on each rendering frame the application needs to issue the draw commands of the GUI (the GUI is not stored for multiple frames) \u21a9 Scaleform - a vector graphics rendering engine used to display Adobe Flash-based user interfaces and HUDs for video games. \u21a9 Retained GUI - also known as canvas/scene graph, is where GUI is registered once and is displayed, \"retained\", on screen until it removes itself from rendering. \u21a9 Frostbite - EA's proprietary game engine used across most of their studios. \u21a9 Heads-up Display (HUD) - overlay on the screen that presents important information to the player. \u21a9 Intellisense - an intelligent code completion feature in Microsoft Visual Studio that is capable of detailing information about the code and objects that the programmer is working with while coding. \u21a9 Retro Compatibility - also known as backwards compatibility, is when a system is setup such that it works with legacy code/input. \u21a9 Thread-safe code - only manipulates shared data structures in a manner that ensures that all threads behave properly and fulfill their design specifications without unintended interaction. \u21a9 Std::iterator - a C++ type that can be used to iterate through collections of elements based on that collection. \u21a9 Path Tracing - a realistic lighting algorithm that simulates light bouncing around a scene. It uses the Monte Carlo method to give a faithful rendition of the global illumination of the scene. \u21a9 The Demoscene - an international computer art subculture focused on producing demos, which are self-contained, sometimes extremely small, audio-visual computer programs. \u21a9 RenderWare - a game engine by Criterion Software that launched in 1993 and continued to regularly support games through 2010. It was known for providing an off-the-shelf solution to the difficulties of PS2 graphics programming. \u21a9 Autodesk 3ds Max - formerly 3D Studio and 3D Studio Max, is a professional 3D computer graphics program for making 3D animations, models, games, and images. \u21a9 FBX - a proprietary file format owned by Autodesk that is mostly commonly used for 3D model and animation data within the games industry. \u21a9 GlTF(GL Transmission Format) - a royalty-free file format for 3D scenes and models using the JSON standard. \u21a9 Open Dynamic Engine (ODE) - a free and open source physics engine written in C/C++ that can do both rigid body dynamics simulation and collision detection. \u21a9 Cocoa - Apple's native object-oriented API for macOS. \u21a9 UNIX - a family of multitasking, multiuser operating systems that derive from the original AT&T Unix, originally developed at Ken Thompson, Dennis Ritchie, and others at Bell Labs. It's main comparable is Microsoft's DOS, which is mono-task and monouser. \u21a9","title":"Glossary of All Footnote Terms"},{"location":"resources/","text":"Resources \u00b6 This page is always updating to include more resources Useful articles/resources we used during the development of Isetta 3D Math \u00b6 Game Engine Architecture 2 nd edition - Chapter 4: 3D Math for Games Essential Mathematics for Games & Interactive Applications by James Van Verth & Lars Bishop Geometric Tools for Computer Graphics by Philip Schneider & David Eberly Builds \u00b6 An Introduction to Modern CMake Cache \u00b6 Gallery of Processor Cache Effects Scott Meyers - CPU Caches and Why You care Latency Numbers Every Programmer Should Know Collision Detection \u00b6 General Introduction Real-Time Collision Detection by Christer Ericson Toptal - Collision Detection for Solid Objects Ryan Schmidt (Duke University) - Everything you ever wanted to know about collision detection Coarse Phase (Broad Phase) Comenius University Slides - Broad Phase Collision Detection Randy Gual - Dynamic AABB Tree Allen Chou - Broadphase - Dynamic AABB Tree AABB Collision Detection Introductory Guide to AABB Tree Collision Detection Fine Phase & Others Gamasutra - Simple Intersection Tests for Games Correct Box Sphere Intersection StackExchange - Simple 3D OBB-Collision Separating Axis Theorem for OBB Randy Gual - Deriving OBB to OBB Intersection Capsule-Capsule Collision in Games gamedev.net - Capsule-Box Intersection Distance between 3D Lines & Segments Raycasting Ray-Capsule Test Sample Code on Github Ray-OBB Intersection Test Picking with custom Ray-OBB function Details of Ray/OBB Intersection Paper by Wildfire Games GJK Casey Muratori - Implementing GJK GJK Sample Code on Github Config File (Engine Config) \u00b6 Create A Simple Configuration Parser Implementing a CVAR System C++11/14 How to parse a simple config file A Small Class to Read INI File CryEngine Using Console and Config Files ICVar Documentation Lumberyard Console Variable Tutorial ISystem.h , IConsole.h , XConsole.h , XConsole.cpp , XConsoleVariable.h , XConsoleVariable.cpp , EntityCVars.h , EntityCBars.cpp , ICfgFile.h , CfgFile.h , CfgFile.cpp Unreal Console Variables in C++ Data-Oriented vs Object-Oriented \u00b6 Entity Component Systems & Data Oriented Design (Aras Pranckevicius) CppCon 2014: Mike Acton \"Data-Oriented Design and C++\" CppCon 2015: Vittorio Romeo \u201cImplementation of a component-based entity system in modern C++\u201d Dice Introduction to Data-Oriented Design Stingray's Practical Examples in Data Oriented Design Data-Oriented Demo: SOA, composition Jonathan Blow Data Structures \u00b6 Ring Buffers Everything about unordered_map unordered_map cppreference Data Structures in Games Game Engine Architecture 2 nd edition - Chapter 5.3: Containers Why inheritance is viewed poorly DLL \u00b6 Windows Walkthrough: Creating and Using a Dynamic Link Library Inline Functions with dllimport/dllexport Multiple Classes in DLL File How to export a DLL from Visual Studio 2017 C++ Project ECS \u00b6 Nomad Game Engine: Part 2\u200a\u2014\u200aECS Engine Loop \u00b6 Game Programming Pattern - Game Loop Gaffer On Games - Fix Your Timestep Filesystem \u00b6 Windows with C++ - The Evolution of Threads and IO in Windows StackOverflow Understanding Multithread Reading C++11 Async Write C++ without Windows Streambuf - What is it? ReadFile Example WriteFile with IO Completion Ports WriteFile Example IO Completion Port with ReadFile Example Microsoft Docs ReadFile WriteFile Async Read | ReadFileEx IO Completion Ports PostQueuedCompletionStatu CreateThread End of File Example Multithreading Performance Understanding Async IO Operations Graphics \u00b6 Horde3D Some How-To Code Procedural Generated Geometry Forum: Horde3D Overlay How to Draw Primitives Basic OpenGL functions with Horde3D Raw OpenGL OpenGL LearnOpenGL Hello Window Hello Triangle Transformations OpenGL Error Codes GUI \u00b6 Retained Mode Versus Immediate Mode Dear ImGui ImGui in 3D IMGUI for GameDev tools Why Qt and not IMGUI Memory \u00b6 Introduction Game Engine Architecture 2 nd edition - Chapter 3.2.5.1: Alignment and Packing Game Engine Architecture 2 nd edition - Chapter 5.2: Memory Management Gamesutra - Writing a Game Engine from Scratch Part2: Memory : Also covers modern CPU memory access patterns. Are we out of memory? Implementation Randy Gaul's Game Programming Blog - Memory Management : \"Anything that has a very clear and non-variable lifespan should be able to be allocated on a stack.\" IBM - Building your own memory manager for C/C++ projects : A step by step guide on implementing some allocators. ISO C++ FAQ: Is there a way to force new to allocate memory from a specific memory area? : Some discussion on placement new , new , and delete , and what you should know if you are going to manage object's lifetime on your own. GitHub - Memory Allocators Example Use the Memory Windows in the Visual Studio Debugger : The memory windows is very important when debugging memory allocators. You can see the memory layout byte by byte. Memory Patterns in Visual Studio : Another utility for debugging memory allocator. Visual Studio uses certain patterns to mark the state of memory chunks. C++ Casting, or: \"Oh No, They Broke Malloc!\" : On different types of casts ( static_cast , reinterpret_cast , const_cast , dynamic_cast , C-style casts) in C++ and when you should use them. reinterpret_cast is especially important as we will need to cast uintptr_t to actual pointers a lot. When should static_cast, dynamic_cast, const_cast and reinterpret_cast be used? ISO C++ FAQ on Memory Management How does delete[] \u201cknow\u201d the size of the operand array? Networking \u00b6 Gaffer On Games - Game Networking The TRIBES Engine Networking Model or How to Make the Internet Rock for Multi\u00adplayer Games The DOOM III Network Architecture Multiplayer Game Programming Ch. 6, Network Topologies and Sample Games covers how to build out an application layer for your game networking UDP Sockets Rutgers - Programming with UDP sockets Sample BroadcastSender.c and BroadcastReceiver.c Serialization / Reflection \u00b6 A C++ 11 Reflection and Serialization library on Github A practical guide to C++ Serialization (use Boost) Serialization and Unserialization (ISO C++) Boost Serialization (Requirements and Other Solutions) C++ Meta-Serialization s11n - Serialization Library Eternity - Serialization Library Cereal - Serialization Library Sound \u00b6 Unreal Audio System Overview Demystifying Audio Middleware FMOD API Overview Game Engine Architecture 2 nd edition - Chapter 13: Audio String Hashing \u00b6 A StringID Library on Github Practical Hash IDs Minial perfect hashing for game assets Preprocessed strings for asset ids Hash Tables - Introduction Scene Graph \u00b6 Transform: Game Engine Architecture 2 nd edition - Chapter 4: 3D Math for Games Mathematics for 3D Game Programming and Computer Graphics 3 rd edition - Chapter 2-4 Godot Engine - Using transforms Unity API - transform Unreal\u2019s FTransform class YouTube - Gimbal Lock Explained Visual Studio \u00b6 Common Macros for Build Commands Misc. \u00b6 In-house Engine Development: Process Tips In-house Engine Development: Technical Tips Deriving objects from stings String Interning - Useful Properties & Github Repo Fast C++ Logging Library Stanford CS101 - Bits and Bytes Microsoft Docs - Walkthrough: Create and use your own Dynamic Link Library (C++) Misc. C++ \u00b6 LearnC++ \u2014 Overloading the comparison operators Cppreference - Array Initialization Cppreference - Priority Queue Cppreference - Parameter pack Cppreference - Constraints and concepts StackOverflow - Why is inline required on static inline variables? StackOverflow - How are C++11 lambdas represented and passed? Variadic Templates Eli Bendersky's Blog - Variadic templates in C++ Bartek's coding blog - Factory With Self-Registering Types : We referenced this article we designing our level auto registration system.","title":"Resources"},{"location":"resources/#resources","text":"This page is always updating to include more resources Useful articles/resources we used during the development of Isetta","title":"Resources"},{"location":"resources/#3d-math","text":"Game Engine Architecture 2 nd edition - Chapter 4: 3D Math for Games Essential Mathematics for Games & Interactive Applications by James Van Verth & Lars Bishop Geometric Tools for Computer Graphics by Philip Schneider & David Eberly","title":"3D Math"},{"location":"resources/#builds","text":"An Introduction to Modern CMake","title":"Builds"},{"location":"resources/#cache","text":"Gallery of Processor Cache Effects Scott Meyers - CPU Caches and Why You care Latency Numbers Every Programmer Should Know","title":"Cache"},{"location":"resources/#collision-detection","text":"General Introduction Real-Time Collision Detection by Christer Ericson Toptal - Collision Detection for Solid Objects Ryan Schmidt (Duke University) - Everything you ever wanted to know about collision detection Coarse Phase (Broad Phase) Comenius University Slides - Broad Phase Collision Detection Randy Gual - Dynamic AABB Tree Allen Chou - Broadphase - Dynamic AABB Tree AABB Collision Detection Introductory Guide to AABB Tree Collision Detection Fine Phase & Others Gamasutra - Simple Intersection Tests for Games Correct Box Sphere Intersection StackExchange - Simple 3D OBB-Collision Separating Axis Theorem for OBB Randy Gual - Deriving OBB to OBB Intersection Capsule-Capsule Collision in Games gamedev.net - Capsule-Box Intersection Distance between 3D Lines & Segments Raycasting Ray-Capsule Test Sample Code on Github Ray-OBB Intersection Test Picking with custom Ray-OBB function Details of Ray/OBB Intersection Paper by Wildfire Games GJK Casey Muratori - Implementing GJK GJK Sample Code on Github","title":"Collision Detection"},{"location":"resources/#config-file-engine-config","text":"Create A Simple Configuration Parser Implementing a CVAR System C++11/14 How to parse a simple config file A Small Class to Read INI File CryEngine Using Console and Config Files ICVar Documentation Lumberyard Console Variable Tutorial ISystem.h , IConsole.h , XConsole.h , XConsole.cpp , XConsoleVariable.h , XConsoleVariable.cpp , EntityCVars.h , EntityCBars.cpp , ICfgFile.h , CfgFile.h , CfgFile.cpp Unreal Console Variables in C++","title":"Config File (Engine Config)"},{"location":"resources/#data-oriented-vs-object-oriented","text":"Entity Component Systems & Data Oriented Design (Aras Pranckevicius) CppCon 2014: Mike Acton \"Data-Oriented Design and C++\" CppCon 2015: Vittorio Romeo \u201cImplementation of a component-based entity system in modern C++\u201d Dice Introduction to Data-Oriented Design Stingray's Practical Examples in Data Oriented Design Data-Oriented Demo: SOA, composition Jonathan Blow","title":"Data-Oriented vs Object-Oriented"},{"location":"resources/#data-structures","text":"Ring Buffers Everything about unordered_map unordered_map cppreference Data Structures in Games Game Engine Architecture 2 nd edition - Chapter 5.3: Containers Why inheritance is viewed poorly","title":"Data Structures"},{"location":"resources/#dll","text":"Windows Walkthrough: Creating and Using a Dynamic Link Library Inline Functions with dllimport/dllexport Multiple Classes in DLL File How to export a DLL from Visual Studio 2017 C++ Project","title":"DLL"},{"location":"resources/#ecs","text":"Nomad Game Engine: Part 2\u200a\u2014\u200aECS","title":"ECS"},{"location":"resources/#engine-loop","text":"Game Programming Pattern - Game Loop Gaffer On Games - Fix Your Timestep","title":"Engine Loop"},{"location":"resources/#filesystem","text":"Windows with C++ - The Evolution of Threads and IO in Windows StackOverflow Understanding Multithread Reading C++11 Async Write C++ without Windows Streambuf - What is it? ReadFile Example WriteFile with IO Completion Ports WriteFile Example IO Completion Port with ReadFile Example Microsoft Docs ReadFile WriteFile Async Read | ReadFileEx IO Completion Ports PostQueuedCompletionStatu CreateThread End of File Example Multithreading Performance Understanding Async IO Operations","title":"Filesystem"},{"location":"resources/#graphics","text":"Horde3D Some How-To Code Procedural Generated Geometry Forum: Horde3D Overlay How to Draw Primitives Basic OpenGL functions with Horde3D Raw OpenGL OpenGL LearnOpenGL Hello Window Hello Triangle Transformations OpenGL Error Codes","title":"Graphics"},{"location":"resources/#gui","text":"Retained Mode Versus Immediate Mode Dear ImGui ImGui in 3D IMGUI for GameDev tools Why Qt and not IMGUI","title":"GUI"},{"location":"resources/#memory","text":"Introduction Game Engine Architecture 2 nd edition - Chapter 3.2.5.1: Alignment and Packing Game Engine Architecture 2 nd edition - Chapter 5.2: Memory Management Gamesutra - Writing a Game Engine from Scratch Part2: Memory : Also covers modern CPU memory access patterns. Are we out of memory? Implementation Randy Gaul's Game Programming Blog - Memory Management : \"Anything that has a very clear and non-variable lifespan should be able to be allocated on a stack.\" IBM - Building your own memory manager for C/C++ projects : A step by step guide on implementing some allocators. ISO C++ FAQ: Is there a way to force new to allocate memory from a specific memory area? : Some discussion on placement new , new , and delete , and what you should know if you are going to manage object's lifetime on your own. GitHub - Memory Allocators Example Use the Memory Windows in the Visual Studio Debugger : The memory windows is very important when debugging memory allocators. You can see the memory layout byte by byte. Memory Patterns in Visual Studio : Another utility for debugging memory allocator. Visual Studio uses certain patterns to mark the state of memory chunks. C++ Casting, or: \"Oh No, They Broke Malloc!\" : On different types of casts ( static_cast , reinterpret_cast , const_cast , dynamic_cast , C-style casts) in C++ and when you should use them. reinterpret_cast is especially important as we will need to cast uintptr_t to actual pointers a lot. When should static_cast, dynamic_cast, const_cast and reinterpret_cast be used? ISO C++ FAQ on Memory Management How does delete[] \u201cknow\u201d the size of the operand array?","title":"Memory"},{"location":"resources/#networking","text":"Gaffer On Games - Game Networking The TRIBES Engine Networking Model or How to Make the Internet Rock for Multi\u00adplayer Games The DOOM III Network Architecture Multiplayer Game Programming Ch. 6, Network Topologies and Sample Games covers how to build out an application layer for your game networking UDP Sockets Rutgers - Programming with UDP sockets Sample BroadcastSender.c and BroadcastReceiver.c","title":"Networking"},{"location":"resources/#serialization-reflection","text":"A C++ 11 Reflection and Serialization library on Github A practical guide to C++ Serialization (use Boost) Serialization and Unserialization (ISO C++) Boost Serialization (Requirements and Other Solutions) C++ Meta-Serialization s11n - Serialization Library Eternity - Serialization Library Cereal - Serialization Library","title":"Serialization / Reflection"},{"location":"resources/#sound","text":"Unreal Audio System Overview Demystifying Audio Middleware FMOD API Overview Game Engine Architecture 2 nd edition - Chapter 13: Audio","title":"Sound"},{"location":"resources/#string-hashing","text":"A StringID Library on Github Practical Hash IDs Minial perfect hashing for game assets Preprocessed strings for asset ids Hash Tables - Introduction","title":"String Hashing"},{"location":"resources/#scene-graph","text":"Transform: Game Engine Architecture 2 nd edition - Chapter 4: 3D Math for Games Mathematics for 3D Game Programming and Computer Graphics 3 rd edition - Chapter 2-4 Godot Engine - Using transforms Unity API - transform Unreal\u2019s FTransform class YouTube - Gimbal Lock Explained","title":"Scene Graph"},{"location":"resources/#visual-studio","text":"Common Macros for Build Commands","title":"Visual Studio"},{"location":"resources/#misc","text":"In-house Engine Development: Process Tips In-house Engine Development: Technical Tips Deriving objects from stings String Interning - Useful Properties & Github Repo Fast C++ Logging Library Stanford CS101 - Bits and Bytes Microsoft Docs - Walkthrough: Create and use your own Dynamic Link Library (C++)","title":"Misc."},{"location":"resources/#misc-c","text":"LearnC++ \u2014 Overloading the comparison operators Cppreference - Array Initialization Cppreference - Priority Queue Cppreference - Parameter pack Cppreference - Constraints and concepts StackOverflow - Why is inline required on static inline variables? StackOverflow - How are C++11 lambdas represented and passed? Variadic Templates Eli Bendersky's Blog - Variadic templates in C++ Bartek's coding blog - Factory With Self-Registering Types : We referenced this article we designing our level auto registration system.","title":"Misc. C++"},{"location":"schedule/","text":"Schedule \u00b6 All of our content is worked on and submitted on a regular basis. Below is our set times for each type of post. Updates will be sent out through our MailChimp list, so we recommend subscribing to that if you want to be the first to hear about all our progress. What? Updated When? Weekly Blogs Posted on Fri. (4PM EST.) Github Tags Posted on Wed. (11:59:59PM EST.) Interviews Done weekly. Posted when edited (2~3 weeks after) Subscribe to our mailing list We publish blogs weekly and interview content once edited. We will only email once a week to let you know they are up!","title":"Schedule"},{"location":"schedule/#schedule","text":"All of our content is worked on and submitted on a regular basis. Below is our set times for each type of post. Updates will be sent out through our MailChimp list, so we recommend subscribing to that if you want to be the first to hear about all our progress. What? Updated When? Weekly Blogs Posted on Fri. (4PM EST.) Github Tags Posted on Wed. (11:59:59PM EST.) Interviews Done weekly. Posted when edited (2~3 weeks after)","title":"Schedule"},{"location":"team/","text":"Team \u00b6 This is the team behind Isetta Engine. They are graduate students studying at Carnegie Mellon's Entertainment Technology Center, all anticipating to graduate in May 2019 with Master of Entertainment Technology degrees. Producer Jared Ettinger \u00b6 Jared is a creative writer and producer from New York. He is excited about the intersection of art and technology, particularly in video games and animation. He hopes to further his production skills by keeping the rest of the team steady on this wild ride. Programmers Caleb Biasco \u00b6 Caleb started programming games in the Video Game Development Club at the University of Minnesota, and hasn't stopped since! Seriously. The madman is making a game engine now, someone stop him. Jacob Wilson \u00b6 Jacob is always taking things apart to understand how they work and sometimes they are able to go back together. His background in physics helps him understand math and mechanics, and he is interested in tool development to help others see the light. Chaojie Zhu \u00b6 Chaojie has a background in software engineering from Shanghai Jiao Tong University, and has specific interests in game AI, self driving vehicles and software engineering. Yidi Zhu \u00b6 Yidi is a gameplay programmer/designer who enjoys making meaningful and playful interactive experiences. He is probably having too much fun in this swamp of game engine development.","title":"Team"},{"location":"team/#team","text":"This is the team behind Isetta Engine. They are graduate students studying at Carnegie Mellon's Entertainment Technology Center, all anticipating to graduate in May 2019 with Master of Entertainment Technology degrees. Producer","title":"Team"},{"location":"team/#jared-ettinger","text":"Jared is a creative writer and producer from New York. He is excited about the intersection of art and technology, particularly in video games and animation. He hopes to further his production skills by keeping the rest of the team steady on this wild ride. Programmers","title":"Jared Ettinger"},{"location":"team/#caleb-biasco","text":"Caleb started programming games in the Video Game Development Club at the University of Minnesota, and hasn't stopped since! Seriously. The madman is making a game engine now, someone stop him.","title":"Caleb Biasco"},{"location":"team/#jacob-wilson","text":"Jacob is always taking things apart to understand how they work and sometimes they are able to go back together. His background in physics helps him understand math and mechanics, and he is interested in tool development to help others see the light.","title":"Jacob Wilson"},{"location":"team/#chaojie-zhu","text":"Chaojie has a background in software engineering from Shanghai Jiao Tong University, and has specific interests in game AI, self driving vehicles and software engineering.","title":"Chaojie Zhu"},{"location":"team/#yidi-zhu","text":"Yidi is a gameplay programmer/designer who enjoys making meaningful and playful interactive experiences. He is probably having too much fun in this swamp of game engine development.","title":"Yidi Zhu"},{"location":"blogs/engine-architecture/","text":"Engine Architecture \u00b6 This past week, we developed the initial design of our engine architecture. The diagram is broken down into layers, where the layers above depend on one or more systems on the layers below. The legend on the right of the diagram describes the meaning behind the colors and shapes; colors are for how the system will be used and shape is whether we will be build. Run-time Engine systems are ones that will be built with the engine and are required at runtime of the game. Offline Engine systems will be packaged with the engine but won't be used at runtime by a user, rather, all actions will be done prior to playing the game. For Engine Development systems are solely for engine developers' benefit and don't need to be packaged with the engine DLL 1 . Going to Make signifies we are building this system from scratch, where Try to Make means we are going to attempt to develop the system. However, if we can't get a functioning system within an allotted time we will substitutes ours for a 3 rd party library. The 3 rd Party systems are all of the systems that we won't be developing ourselves, but rather integrating into the engine and creating a wrapper 2 for the end user. The If Time Permits category are for components which aren't necessary for the engine, however could be useful for the game developer. Architecture Layers \u00b6 Believe it or not, our architecture does have some semblance of organization. On the left side of the diagram, we list our engine's layers. The layers of a game engine range from very vague to extremely specific, and we've made ours somewhat vague to keep us from being architecture astronauts 3 and spending too much time on irrelevant details. But we have made some distinctions here because engines, like most software, have tons of dependencies. Base Layer: This layer contains any library or module that has no dependencies whatsoever, and especially those that are depended upon by most other modules. These are essentially the tools we'll be using to build everything else up. Memory Layer: The memory layer is so sparse because really it's just our memory management getting shoved between everything that needs memory and the actual memory allocators. We want to control all of our allocations through a single manager, so this is kind of a gatekeeper layer. Utility Layer: Utilities span everything that we would still use in multiple modules or systems, but aren't so basic as to be independent of other libraries. Much of our utility layer uses the memory manager but is used by the layer directly above, so we figured this layer would be important to get out of the way before the rest of the engine. Module Layer: A module is a system in our game engine that is initialized at the start of the engine runtime, destroyed at the end of the engine runtime, and is self-contained from any other system at its level. This gives a rather broad definition, but that's also the power of modules: You can add some more in later if you need their features! Most of our modules consist of 3 rd party libraries because modules are generally the brunt of the game engine development work, and we're trying to be mindful of our deadlines! Management Layer: Similarly to the memory layer, the management layer has few systems because it mostly consists of a couple overarching management system. Ours are the networked game management system, which mostly conducts the networking side of the engine, and the scene graph, which is how gameplay-level systems will access most objects and information. Gameplay Layer: The gameplay layer is the final layer of our engine's runtime systems. Gameplay systems are the first systems in our hierarchy that exist mainly for the sake of the gameplay sequence. They either define data or behavior that gets used at a high level in the game, such as scripting, AI, and event messaging. A lot of smaller features can be introduced in this layer without mucking up the rest of the engine, which is also why our only potentially-not-developed system is in this layer, since a lower-level system would probably break much of our systems above it. Build Layer: Lastly, our build layer consists of any engine technology that solely processes assets and code offline, either for the sake of generating the game build or the engine build itself. Our only section in this layer that is not the actual builds of the software is the build resource management section. We would use this to make our build process both effective and fast. The architecture is divided into 10 main sections: core, tools, networking, graphics, input, audio, collisions, gameplay, build resource management, and build. These sections were pulled from our understanding of engine architecture after reading Game Engine Architecture , however other authors categorizations of engine design also overlap with our systems. For example in Game Engine Design and Implementation , Alan Thorn lists the Render, Resource, Scene, Input, and Error Managers as the essential managers which overlap with our Graphics, Build Resource Management, Scene Graph, Input, and Tools respectively cover his categories. We have broken our architecture more granularly to help see which features we need to be developing. For decisions on 3 rd party libraries see our other blog . Core \u00b6 Core systems are the underlying foundations of all other systems/modules like the audio engine and rendering engine. They are like the screws and bearing upon which everything else runs. Due to this nature, a big percentage of the core systems do not depend on other parts and is self-contained (STL, math, memory allocation system, assertions, parsing, string hashing, engine config, and timer are all systems that are dependency-free). Here is a list of our core systems: STL: We will use STL for containers ( vector , queue , list ), algorithms ( sort ), and time ( chrono ). Because it's easy and complete, and we don't want to spend time on rebuilding things we already have plenty of experience with. However, if you are interested and have the time, we encourage you to implement them by yourself, they are good programming exercise anyway Math: Math library is the foundation for graphics and 3D gameplay, and typically includes Vector3, Vector4, Matrix3x3, Matrix4x4, Quaternion, and random number generators. We decided to implement our own math library for the following reasons: we have never implemented it before, it's a good part to practice unit testing, and we can customize it to our own needs. Also, it's probably easier to build it than bring in a 3 rd party one. It finally took us around 25 hours to implement the math library Assertions: Currently, we are using the assertions from Microsoft Foundation Class (MFC) library. Assertions are important to ensure the engine stops when something unexpected happens, that is related to user input error, this forces us as a developer to track down and solve the issue. String hashing: String hashing is the process of converting a string to another type like an int , so it takes less memory, brings faster comparison time, and can be used as keys for hash tables. String hashing is extremely useful for asset look-up. We imported an open-source String ID library for this. Memory allocation: Ideally, we will have no or minimal dynamic memory because the default new and delete operators in C++ are pretty slow and game engines usually need to be super performant and prevent problems like memory fragmentation. We will implement our own memory manager and memory allocators, including stack allocator, pool allocator, heap allocator, etc. The memory manager will be the interface through which our systems interact with memory allocators. Module startup/shutdown: We are implementing a ModuleManager class to manage the startup, update, and shutdown of each submodule, like AudioModule and RenderingModule . It's an easy and safe way to hide access to sub-modules from users and a good way to manage the startup/shutdown sequence of them. Multithreading: To be clear, we are not doing crazy things with multithreading. The only reason we want to bring it in is so we can implement a sequencing technique similar to Unity's Coroutine . We are going to find and bring in a 3 rd party library to do this Serialization and Reflection: It's a very useful system, especially if we are doing networking. But it's too big a system for us to implement, so we decided to find an import a 3 rd party library. Parsers: Parsers are used to read and convert data from data files like XML, JSON, and YAML. We will import a 3 rd party library to do this because it's well established and implemented by other people. Engine Configuration: Our engine will support different configurations, such as window size, logging verbosity, etc. We'll make our own file type for storing configs and implement a library with 3 rd party parsers to read from it. Object handles: Handles are essentially a layer between an actual object and whatever is referencing it. By using a handle to access an object, we can check the validity of the object regardless of how the object is referenced/stored, and we can even know if the object has been moved! Handles provide a better redirection of data than simple pointers do, at little cost of complexity or memory. We plan on making a simple handle system, but depending on our needs we may need to build it up to handle the data more smartly. Async File I/O: All games need to load resource files, like meshes, sound, at runtime. And the process of asset loading should not completely block other processes. To achieve this, we will bring in an async file I/O library to load assets on another thread. Time: A game will have different timelines when running, like real-world time, game time, unscaled time, scaled time, the timeline for a cinematic sequence, etc. We are going to implement our own Time manager and utility classes on top of <chrono> from STL Tools \u00b6 The tools of the engine can be broken out into two subsections: In-engine tools and developer tools. In-engine tools can be used by both engine and game developers. Developer tools, on the other hand, are only for engine developer's benefit. In-Engine Tools: Profiler : The profiler can be a powerful debugging tool for both developing the engine and the game. The Visual Studio profiler isn't packaged with the engine. Therefore, it's useful to have one that can inform the developer of the engine's performance within their game. Implementing a basic profiler isn't too difficult, as the key idea is to measure the time for a frame to process for each system (physics, graphics, etc.). There can be more difficult logic with measuring hierarchical function calls. The difficult part for our team is visualizing this information in an understandable format. Creating a profiler which displays readable information requires a decent UI sense as well as iteration/customization with users which isn't something we have time for. Debug Logging : Debugging is a necessary part of any development, and the debug logging system gives the user access to output to a console (currently set as Visual Studio, however, we may include an in-game console in the future). There are few requirements we wanted from our own logger: Be able to specify the severity of the message (if a message is a critical error or just information). Be able to designate channels which only display messages related to that topics, this stops messages from all systems flooding the scene without removing the message. Write debug information to files to have a record of the events. This system allows us to have a good amount of customizability based on the architecture of our engine as well as being simple enough that we don't foresee significant difficulty with its implementation. The debugging system will also be expanded with the graphics wrapper which for debug drawing and an in-game console. Offline Tools: Unit testing: Unit testing is something that isn't necessarily fun (until your tests pass) but can be necessary for a codebase as large as an engine. These unit tests don't need to be run at runtime nor packed into the engine DLL, but they will ensure the robustness of the engine. The plan is to write base unit testing cases for our public functions in the core library, but won't be full-coverage and may decline in the higher systems based on the development schedule. Networking \u00b6 Networking is straightforward conceptually: You click a button, that click is communicated across a wire all the way to another computer, and it receives the click and responds to it. The model becomes messier as you include details like state synchronization and authority, prediction, and the mode of networking such as peer-to-peer or dedicated server. In fact, it becomes so messy that it's one of the hardest features to retroactively fit into a game! Networking is also a domain that none of us have experience with, which makes us pretty terrified. It doesn't help that every industry developer we've consulted with has remarked on how bad of an idea including networking in our engine is. This means we need to hit it early to determine just whether or not it's doable , let alone in scope. We found some promising libraries online that we want to build from to skip as much of the headache as possible, but we fear that the networking will start simple enough then grow into something unsustainable. In any case, we're going to try networking! Our solutions for state synchronization and game management will be very na\u00efve - e.g. Warcraft 3 style: one person hosts a room, and the others join it. The host is the server and holds all of the world data, and others sync to that. It won't provide the best experience, but our game isn't exactly aiming to be Journey . Graphics \u00b6 Graphics includes rendering objects on the screen, animation of skeletons, skinning, model importing, materials, and calling render update. Really, this is one of the biggest chunks of a game engine, but you can also make it pretty barebones. For our engine, we have decided to use a 3 rd party 3D rendering engine. Although some of us have graphics knowledge and interest, creating a rendering engine which has enough features for creating a game is no small task. The problem is it can include features which could span 3 months on their own. Rather than focusing on something we have an idea about, as well as something that is fairly well documented, we will be integrating a 3 rd party library which has its own unique challenges. With this, we have to choose which 3 rd party library to integrate as well as develop a wrapper around the rendering engine. It is important that the rendering is only exposed as specified by the engine. The graphics will be integrated into other system components, including the module start-up/shut-down, engine update loop, and debug drawing. Since this is such a large component of our game engine, we selected the library early. Input \u00b6 The input is the system which directs all input from external devices into the game engine and up to the game. The input system can have multiple layers and fairly abstracted depending on how much your engine is aiming to support or be cross-platform. For our engine, since we will be focusing on the twin-stick genre, this requires our engine at a minimum to support some type of controller. Although twin-stick shooters can be played on keyboards, Wiimotes, and other peripheral devices, we have chosen to support Xbox 360 controllers. 360 controllers are advantageous because of their compatibility with Windows, plus the fact that we have a few readily available. Ideally, we will abstract our input to be able to support keyboard and mouse. Right now that's a secondary device concern, though. Since most engines rely on the operating system for input and create a wrapper around this, we will also use a 3 rd party library and create a wrapper for the user. Audio \u00b6 Sound plays a big role in almost every immersive experience. Every engine on the market probably includes an audio engine, and it's also good advice that sound in games should get as much attention as other systems and be included in the iteration process to match their huge influence on the final gameplay experience. By talking to industry professionals, we learned that studios usually don't build their own audio system from scratch. Instead, they tend to bring in audio middlewares (to get an overview of frequently used audio tools in the industry, see the post from Somatone Interactive). Therefore, to line up with industry standard and make our workload realistic, we decided to bring in a third party audio library. We started by looking at the requirements of our game - to recap, our engine is going to be used to support a minimal networked twin-stick shooter. For this game, we will need t to: Load different audio clips Play music in loop mode Control volume of each audio clip being played Play multiple instances of the same audio clips at the same time or in a high frequency, like gun firing sounds 3D sound support: Your ally's gun firing sound should be different when he is on your left side than when he is on your right side That's all from the game side. Having an audio engine that supports this feedback will let us develop exciting and juicy gameplay. From an engineering and optimization point of view, though, we have some other requirements: The 3 rd party library should allow us to do memory management for it The 3 rd party library ideally should be widely used in the game industry Collisions \u00b6 A collision system is exactly what it sounds like: The detection of colliders with other colliders including functionality like raycasting. This system is needed to ensure characters don't pass through objects, including other players if online, as well as doing raycasting to detect if you gunshot an enemy or if the player is in the line of sight. A collision system is not equivalent to a physics system, physics includes much more calculation and possibly prediction. The collision detection detects collisions between collidable objects, which can vary in shape depending on what's supported, and have a callback function when triggered. The collision solver stops the colliding objects from passing through one another and can be more advanced as to \"nudge\" enemies running into a wall around corners. The requirements for our collision system are as follows: Primitive colliders: spheres, cubes, and capsules Raycasting (not volume-casting 4 ) A solver to stop objects from passing through others Since we only need a subsection of a collision system's features for our test game as well as in general for twin-stick shooters, we decided to build that functionality ourselves. This is dependent on that the time remaining not to include something like the industry standard PhysX 5 . There is a benefit in developing our own in keeping things simple as well as getting familiar with some of the standard math questions that junior developers are asked when interviewing for a position. Gameplay \u00b6 Game loop: The game loop is the assembly line of a game. A simple version would be checking which key you have pressed on the keyboard, moving an object on the screen depending on the key, rendering it a certain color depending on the key, then repeating. The real hard part of this process is keeping your pieces independent: if your animations can influence your physics, which do you update first? How can you make them not be dependent on one another? We already know we want to make a variable-render fixed-update loop 6 as most modern engines do, but we're anticipating difficulty in organizing our system updates. Scripting: Scripting is a more accessible version of coding. Instead of having to rebuild your engine for your code to change, you can simply pick an updated script file and you're good! This can even be done with games themselves\u2014that's what modding essentially is. We suspect we won't have time for this one (shout out to all the experienced developers who have told us this), but it would be a nice addition as far as game engines are concerned because it greatly reduces iteration time. The plan would almost definitely be importing a 3 rd party scripting engine. Static/Dynamic game objects: \"Static\" objects would have several possible qualities, including fixed transform, fixed lighting, and even memory-allocation static. These options allow the engine to better optimize and structure its data, and so we hope to have a couple of them to better service the game development end of the engine. Event/messaging system: Messaging systems allow objects to communicate with one another dynamically during the game. This is a feature that could be developed by an engine user, but having it within the engine itself can yield more benefits \u00e0 la performance and breadth of effect. Ours will likely be rudimentary, but allowing objects to communicate outside of user code seems almost like a necessity in a tool like a game engine. AI Pathfinding: Most games have computer-driven enemies that the player faces, and for any game involving character movement, you typically need some pathfinding algorithms so that the characters aren't too easy to defeat or frustrating to work with. A baseline of A* will be needed if we have any collisions in the level that could obstruct enemy movement. Build Resource Management \u00b6 As a very necessary step in the game engine usage process, we need some form of build system at the end of our pipeline for the engine. Assets and the level scene graphs 7 will need to be processed for the built executable, and if the build process takes a long time, then caching those processed assets could help speed up the process as well. We will obviously need to have some form of build system in our engine, but we're also interested in using asset caching or other optimizations to make the builds run a bit faster. Build \u00b6 The use of all game engines, in the end, is to be used to make games, so a build of the engine is needed so that game developers can make those games. The engine needs to be packaged up such that it can be given to a game developer, and that developer is able to have the functionality they need without having access to the inner workings of the engine. Our goal with the engine will be to create a CMake, or something of the sort, which will create a DLL library to be included with the game code. The game build will include the engine DLL, and also have a CMake file which creates an executable of the game. Resources \u00b6 The resource page has been updated, too! A DLL is a dynamic-linked library which is Microsoft shared library concept which can be transported around easier than a project and contains information about the compiled project. \u21a9 A wrapper is a class that \"wraps\" around another class to hide/change/add functionality. This is usually done when using other people's libraries to ensure only your features are available. \u21a9 Joel describes this problem as \"When you go too far up, abstraction-wise, you run out of oxygen. Sometimes smart thinkers just don't know when to stop, and they create these absurd, all-encompassing, high-level pictures of the universe that are all good and fine, but don't actually mean anything at all.\" For more, read his blog post here \u21a9 Also known as volume ray casting, volume-casting is when a volume's path is traced along a line/curve to test collisions with other objects. \u21a9 PhysX is NVIDIA's real-time physics engine used by most commercially available game engines such as Unity, Unreal, and Lumberyard. \u21a9 Variable-render refers to the fact that rendering will be updated as fast as the CPU/GPU can allow, not being slowed by a frame rate. All other modules will then be updated with a fixed timestep since some of them dependent on the timestep and can become non-deterministic with a variable timestep. \u21a9 A level scene graph is the scene graph corresponding to a level, similar to a level configuration file. It will contain the information (transformation, behavior, whether it is static, etc.) about the starting game objects. \u21a9","title":"Engine Architecture"},{"location":"blogs/engine-architecture/#engine-architecture","text":"This past week, we developed the initial design of our engine architecture. The diagram is broken down into layers, where the layers above depend on one or more systems on the layers below. The legend on the right of the diagram describes the meaning behind the colors and shapes; colors are for how the system will be used and shape is whether we will be build. Run-time Engine systems are ones that will be built with the engine and are required at runtime of the game. Offline Engine systems will be packaged with the engine but won't be used at runtime by a user, rather, all actions will be done prior to playing the game. For Engine Development systems are solely for engine developers' benefit and don't need to be packaged with the engine DLL 1 . Going to Make signifies we are building this system from scratch, where Try to Make means we are going to attempt to develop the system. However, if we can't get a functioning system within an allotted time we will substitutes ours for a 3 rd party library. The 3 rd Party systems are all of the systems that we won't be developing ourselves, but rather integrating into the engine and creating a wrapper 2 for the end user. The If Time Permits category are for components which aren't necessary for the engine, however could be useful for the game developer.","title":"Engine Architecture"},{"location":"blogs/engine-architecture/#architecture-layers","text":"Believe it or not, our architecture does have some semblance of organization. On the left side of the diagram, we list our engine's layers. The layers of a game engine range from very vague to extremely specific, and we've made ours somewhat vague to keep us from being architecture astronauts 3 and spending too much time on irrelevant details. But we have made some distinctions here because engines, like most software, have tons of dependencies. Base Layer: This layer contains any library or module that has no dependencies whatsoever, and especially those that are depended upon by most other modules. These are essentially the tools we'll be using to build everything else up. Memory Layer: The memory layer is so sparse because really it's just our memory management getting shoved between everything that needs memory and the actual memory allocators. We want to control all of our allocations through a single manager, so this is kind of a gatekeeper layer. Utility Layer: Utilities span everything that we would still use in multiple modules or systems, but aren't so basic as to be independent of other libraries. Much of our utility layer uses the memory manager but is used by the layer directly above, so we figured this layer would be important to get out of the way before the rest of the engine. Module Layer: A module is a system in our game engine that is initialized at the start of the engine runtime, destroyed at the end of the engine runtime, and is self-contained from any other system at its level. This gives a rather broad definition, but that's also the power of modules: You can add some more in later if you need their features! Most of our modules consist of 3 rd party libraries because modules are generally the brunt of the game engine development work, and we're trying to be mindful of our deadlines! Management Layer: Similarly to the memory layer, the management layer has few systems because it mostly consists of a couple overarching management system. Ours are the networked game management system, which mostly conducts the networking side of the engine, and the scene graph, which is how gameplay-level systems will access most objects and information. Gameplay Layer: The gameplay layer is the final layer of our engine's runtime systems. Gameplay systems are the first systems in our hierarchy that exist mainly for the sake of the gameplay sequence. They either define data or behavior that gets used at a high level in the game, such as scripting, AI, and event messaging. A lot of smaller features can be introduced in this layer without mucking up the rest of the engine, which is also why our only potentially-not-developed system is in this layer, since a lower-level system would probably break much of our systems above it. Build Layer: Lastly, our build layer consists of any engine technology that solely processes assets and code offline, either for the sake of generating the game build or the engine build itself. Our only section in this layer that is not the actual builds of the software is the build resource management section. We would use this to make our build process both effective and fast. The architecture is divided into 10 main sections: core, tools, networking, graphics, input, audio, collisions, gameplay, build resource management, and build. These sections were pulled from our understanding of engine architecture after reading Game Engine Architecture , however other authors categorizations of engine design also overlap with our systems. For example in Game Engine Design and Implementation , Alan Thorn lists the Render, Resource, Scene, Input, and Error Managers as the essential managers which overlap with our Graphics, Build Resource Management, Scene Graph, Input, and Tools respectively cover his categories. We have broken our architecture more granularly to help see which features we need to be developing. For decisions on 3 rd party libraries see our other blog .","title":"Architecture Layers"},{"location":"blogs/engine-architecture/#core","text":"Core systems are the underlying foundations of all other systems/modules like the audio engine and rendering engine. They are like the screws and bearing upon which everything else runs. Due to this nature, a big percentage of the core systems do not depend on other parts and is self-contained (STL, math, memory allocation system, assertions, parsing, string hashing, engine config, and timer are all systems that are dependency-free). Here is a list of our core systems: STL: We will use STL for containers ( vector , queue , list ), algorithms ( sort ), and time ( chrono ). Because it's easy and complete, and we don't want to spend time on rebuilding things we already have plenty of experience with. However, if you are interested and have the time, we encourage you to implement them by yourself, they are good programming exercise anyway Math: Math library is the foundation for graphics and 3D gameplay, and typically includes Vector3, Vector4, Matrix3x3, Matrix4x4, Quaternion, and random number generators. We decided to implement our own math library for the following reasons: we have never implemented it before, it's a good part to practice unit testing, and we can customize it to our own needs. Also, it's probably easier to build it than bring in a 3 rd party one. It finally took us around 25 hours to implement the math library Assertions: Currently, we are using the assertions from Microsoft Foundation Class (MFC) library. Assertions are important to ensure the engine stops when something unexpected happens, that is related to user input error, this forces us as a developer to track down and solve the issue. String hashing: String hashing is the process of converting a string to another type like an int , so it takes less memory, brings faster comparison time, and can be used as keys for hash tables. String hashing is extremely useful for asset look-up. We imported an open-source String ID library for this. Memory allocation: Ideally, we will have no or minimal dynamic memory because the default new and delete operators in C++ are pretty slow and game engines usually need to be super performant and prevent problems like memory fragmentation. We will implement our own memory manager and memory allocators, including stack allocator, pool allocator, heap allocator, etc. The memory manager will be the interface through which our systems interact with memory allocators. Module startup/shutdown: We are implementing a ModuleManager class to manage the startup, update, and shutdown of each submodule, like AudioModule and RenderingModule . It's an easy and safe way to hide access to sub-modules from users and a good way to manage the startup/shutdown sequence of them. Multithreading: To be clear, we are not doing crazy things with multithreading. The only reason we want to bring it in is so we can implement a sequencing technique similar to Unity's Coroutine . We are going to find and bring in a 3 rd party library to do this Serialization and Reflection: It's a very useful system, especially if we are doing networking. But it's too big a system for us to implement, so we decided to find an import a 3 rd party library. Parsers: Parsers are used to read and convert data from data files like XML, JSON, and YAML. We will import a 3 rd party library to do this because it's well established and implemented by other people. Engine Configuration: Our engine will support different configurations, such as window size, logging verbosity, etc. We'll make our own file type for storing configs and implement a library with 3 rd party parsers to read from it. Object handles: Handles are essentially a layer between an actual object and whatever is referencing it. By using a handle to access an object, we can check the validity of the object regardless of how the object is referenced/stored, and we can even know if the object has been moved! Handles provide a better redirection of data than simple pointers do, at little cost of complexity or memory. We plan on making a simple handle system, but depending on our needs we may need to build it up to handle the data more smartly. Async File I/O: All games need to load resource files, like meshes, sound, at runtime. And the process of asset loading should not completely block other processes. To achieve this, we will bring in an async file I/O library to load assets on another thread. Time: A game will have different timelines when running, like real-world time, game time, unscaled time, scaled time, the timeline for a cinematic sequence, etc. We are going to implement our own Time manager and utility classes on top of <chrono> from STL","title":"Core"},{"location":"blogs/engine-architecture/#tools","text":"The tools of the engine can be broken out into two subsections: In-engine tools and developer tools. In-engine tools can be used by both engine and game developers. Developer tools, on the other hand, are only for engine developer's benefit. In-Engine Tools: Profiler : The profiler can be a powerful debugging tool for both developing the engine and the game. The Visual Studio profiler isn't packaged with the engine. Therefore, it's useful to have one that can inform the developer of the engine's performance within their game. Implementing a basic profiler isn't too difficult, as the key idea is to measure the time for a frame to process for each system (physics, graphics, etc.). There can be more difficult logic with measuring hierarchical function calls. The difficult part for our team is visualizing this information in an understandable format. Creating a profiler which displays readable information requires a decent UI sense as well as iteration/customization with users which isn't something we have time for. Debug Logging : Debugging is a necessary part of any development, and the debug logging system gives the user access to output to a console (currently set as Visual Studio, however, we may include an in-game console in the future). There are few requirements we wanted from our own logger: Be able to specify the severity of the message (if a message is a critical error or just information). Be able to designate channels which only display messages related to that topics, this stops messages from all systems flooding the scene without removing the message. Write debug information to files to have a record of the events. This system allows us to have a good amount of customizability based on the architecture of our engine as well as being simple enough that we don't foresee significant difficulty with its implementation. The debugging system will also be expanded with the graphics wrapper which for debug drawing and an in-game console. Offline Tools: Unit testing: Unit testing is something that isn't necessarily fun (until your tests pass) but can be necessary for a codebase as large as an engine. These unit tests don't need to be run at runtime nor packed into the engine DLL, but they will ensure the robustness of the engine. The plan is to write base unit testing cases for our public functions in the core library, but won't be full-coverage and may decline in the higher systems based on the development schedule.","title":"Tools"},{"location":"blogs/engine-architecture/#networking","text":"Networking is straightforward conceptually: You click a button, that click is communicated across a wire all the way to another computer, and it receives the click and responds to it. The model becomes messier as you include details like state synchronization and authority, prediction, and the mode of networking such as peer-to-peer or dedicated server. In fact, it becomes so messy that it's one of the hardest features to retroactively fit into a game! Networking is also a domain that none of us have experience with, which makes us pretty terrified. It doesn't help that every industry developer we've consulted with has remarked on how bad of an idea including networking in our engine is. This means we need to hit it early to determine just whether or not it's doable , let alone in scope. We found some promising libraries online that we want to build from to skip as much of the headache as possible, but we fear that the networking will start simple enough then grow into something unsustainable. In any case, we're going to try networking! Our solutions for state synchronization and game management will be very na\u00efve - e.g. Warcraft 3 style: one person hosts a room, and the others join it. The host is the server and holds all of the world data, and others sync to that. It won't provide the best experience, but our game isn't exactly aiming to be Journey .","title":"Networking"},{"location":"blogs/engine-architecture/#graphics","text":"Graphics includes rendering objects on the screen, animation of skeletons, skinning, model importing, materials, and calling render update. Really, this is one of the biggest chunks of a game engine, but you can also make it pretty barebones. For our engine, we have decided to use a 3 rd party 3D rendering engine. Although some of us have graphics knowledge and interest, creating a rendering engine which has enough features for creating a game is no small task. The problem is it can include features which could span 3 months on their own. Rather than focusing on something we have an idea about, as well as something that is fairly well documented, we will be integrating a 3 rd party library which has its own unique challenges. With this, we have to choose which 3 rd party library to integrate as well as develop a wrapper around the rendering engine. It is important that the rendering is only exposed as specified by the engine. The graphics will be integrated into other system components, including the module start-up/shut-down, engine update loop, and debug drawing. Since this is such a large component of our game engine, we selected the library early.","title":"Graphics"},{"location":"blogs/engine-architecture/#input","text":"The input is the system which directs all input from external devices into the game engine and up to the game. The input system can have multiple layers and fairly abstracted depending on how much your engine is aiming to support or be cross-platform. For our engine, since we will be focusing on the twin-stick genre, this requires our engine at a minimum to support some type of controller. Although twin-stick shooters can be played on keyboards, Wiimotes, and other peripheral devices, we have chosen to support Xbox 360 controllers. 360 controllers are advantageous because of their compatibility with Windows, plus the fact that we have a few readily available. Ideally, we will abstract our input to be able to support keyboard and mouse. Right now that's a secondary device concern, though. Since most engines rely on the operating system for input and create a wrapper around this, we will also use a 3 rd party library and create a wrapper for the user.","title":"Input"},{"location":"blogs/engine-architecture/#audio","text":"Sound plays a big role in almost every immersive experience. Every engine on the market probably includes an audio engine, and it's also good advice that sound in games should get as much attention as other systems and be included in the iteration process to match their huge influence on the final gameplay experience. By talking to industry professionals, we learned that studios usually don't build their own audio system from scratch. Instead, they tend to bring in audio middlewares (to get an overview of frequently used audio tools in the industry, see the post from Somatone Interactive). Therefore, to line up with industry standard and make our workload realistic, we decided to bring in a third party audio library. We started by looking at the requirements of our game - to recap, our engine is going to be used to support a minimal networked twin-stick shooter. For this game, we will need t to: Load different audio clips Play music in loop mode Control volume of each audio clip being played Play multiple instances of the same audio clips at the same time or in a high frequency, like gun firing sounds 3D sound support: Your ally's gun firing sound should be different when he is on your left side than when he is on your right side That's all from the game side. Having an audio engine that supports this feedback will let us develop exciting and juicy gameplay. From an engineering and optimization point of view, though, we have some other requirements: The 3 rd party library should allow us to do memory management for it The 3 rd party library ideally should be widely used in the game industry","title":"Audio"},{"location":"blogs/engine-architecture/#collisions","text":"A collision system is exactly what it sounds like: The detection of colliders with other colliders including functionality like raycasting. This system is needed to ensure characters don't pass through objects, including other players if online, as well as doing raycasting to detect if you gunshot an enemy or if the player is in the line of sight. A collision system is not equivalent to a physics system, physics includes much more calculation and possibly prediction. The collision detection detects collisions between collidable objects, which can vary in shape depending on what's supported, and have a callback function when triggered. The collision solver stops the colliding objects from passing through one another and can be more advanced as to \"nudge\" enemies running into a wall around corners. The requirements for our collision system are as follows: Primitive colliders: spheres, cubes, and capsules Raycasting (not volume-casting 4 ) A solver to stop objects from passing through others Since we only need a subsection of a collision system's features for our test game as well as in general for twin-stick shooters, we decided to build that functionality ourselves. This is dependent on that the time remaining not to include something like the industry standard PhysX 5 . There is a benefit in developing our own in keeping things simple as well as getting familiar with some of the standard math questions that junior developers are asked when interviewing for a position.","title":"Collisions"},{"location":"blogs/engine-architecture/#gameplay","text":"Game loop: The game loop is the assembly line of a game. A simple version would be checking which key you have pressed on the keyboard, moving an object on the screen depending on the key, rendering it a certain color depending on the key, then repeating. The real hard part of this process is keeping your pieces independent: if your animations can influence your physics, which do you update first? How can you make them not be dependent on one another? We already know we want to make a variable-render fixed-update loop 6 as most modern engines do, but we're anticipating difficulty in organizing our system updates. Scripting: Scripting is a more accessible version of coding. Instead of having to rebuild your engine for your code to change, you can simply pick an updated script file and you're good! This can even be done with games themselves\u2014that's what modding essentially is. We suspect we won't have time for this one (shout out to all the experienced developers who have told us this), but it would be a nice addition as far as game engines are concerned because it greatly reduces iteration time. The plan would almost definitely be importing a 3 rd party scripting engine. Static/Dynamic game objects: \"Static\" objects would have several possible qualities, including fixed transform, fixed lighting, and even memory-allocation static. These options allow the engine to better optimize and structure its data, and so we hope to have a couple of them to better service the game development end of the engine. Event/messaging system: Messaging systems allow objects to communicate with one another dynamically during the game. This is a feature that could be developed by an engine user, but having it within the engine itself can yield more benefits \u00e0 la performance and breadth of effect. Ours will likely be rudimentary, but allowing objects to communicate outside of user code seems almost like a necessity in a tool like a game engine. AI Pathfinding: Most games have computer-driven enemies that the player faces, and for any game involving character movement, you typically need some pathfinding algorithms so that the characters aren't too easy to defeat or frustrating to work with. A baseline of A* will be needed if we have any collisions in the level that could obstruct enemy movement.","title":"Gameplay"},{"location":"blogs/engine-architecture/#build-resource-management","text":"As a very necessary step in the game engine usage process, we need some form of build system at the end of our pipeline for the engine. Assets and the level scene graphs 7 will need to be processed for the built executable, and if the build process takes a long time, then caching those processed assets could help speed up the process as well. We will obviously need to have some form of build system in our engine, but we're also interested in using asset caching or other optimizations to make the builds run a bit faster.","title":"Build Resource Management"},{"location":"blogs/engine-architecture/#build","text":"The use of all game engines, in the end, is to be used to make games, so a build of the engine is needed so that game developers can make those games. The engine needs to be packaged up such that it can be given to a game developer, and that developer is able to have the functionality they need without having access to the inner workings of the engine. Our goal with the engine will be to create a CMake, or something of the sort, which will create a DLL library to be included with the game code. The game build will include the engine DLL, and also have a CMake file which creates an executable of the game.","title":"Build"},{"location":"blogs/engine-architecture/#resources","text":"The resource page has been updated, too! A DLL is a dynamic-linked library which is Microsoft shared library concept which can be transported around easier than a project and contains information about the compiled project. \u21a9 A wrapper is a class that \"wraps\" around another class to hide/change/add functionality. This is usually done when using other people's libraries to ensure only your features are available. \u21a9 Joel describes this problem as \"When you go too far up, abstraction-wise, you run out of oxygen. Sometimes smart thinkers just don't know when to stop, and they create these absurd, all-encompassing, high-level pictures of the universe that are all good and fine, but don't actually mean anything at all.\" For more, read his blog post here \u21a9 Also known as volume ray casting, volume-casting is when a volume's path is traced along a line/curve to test collisions with other objects. \u21a9 PhysX is NVIDIA's real-time physics engine used by most commercially available game engines such as Unity, Unreal, and Lumberyard. \u21a9 Variable-render refers to the fact that rendering will be updated as fast as the CPU/GPU can allow, not being slowed by a frame rate. All other modules will then be updated with a fixed timestep since some of them dependent on the timestep and can become non-deterministic with a variable timestep. \u21a9 A level scene graph is the scene graph corresponding to a level, similar to a level configuration file. It will contain the information (transformation, behavior, whether it is static, etc.) about the starting game objects. \u21a9","title":"Resources"},{"location":"blogs/week-0/","text":"Preparation \u00b6 Introduction \u00b6 The Isetta Engine is a student-driven project about demystifying game engine development and providing a roadmap and relevant knowledge for novice developers. To do so, our team will make a game engine by ourselves starting from a collection of base frameworks, and document the process, pitfalls, and advice for our audience with periodic blogs. Besides that, we will conduct interviews with experienced professionals to augment our novice perspective. We believe the novice perspective from our blogs and expert perspective from the interviews will nicely come together and form a complete document to help people get started. The reason we think more work needs to be done in this field is that too many game engine developers wait until the completion of the engine, typically years, to talk about their development. For studios, this is because they consider the final product to be the game, not the engine. For others, it may be because the engine is what they see as valuable, not the writing. As a result, these talks typically lose the minutiae of the actual daily struggles that took place in the development process. There are others who document their development which has been going on for years, which makes it a daunting task for newcomers to start following along. Although the project is aimed at helping novice developers, this is not to be used as a sole source of learning engine development. Being new engine developers ourselves, we can't guarantee the way we develop the engine will be correct, which is why interviews will help the project remain grounded. This means others who are learning can use what we've done as a guide and not necessarily the ground truth. The blogs won't be a walkthrough/tutorial/step-by-step instructions on how to develop an engine. We are learning as we go and think our journey is what can be valuable to you. About the Project \u00b6 This project is being done as a student-pitched project at the Entertainment Technology Center (ETC). The ETC is an interdisciplinary Master's degree program at Carnegie Mellon University where students' main focus is working on small teams on a project each semester during a 3-month time period. Throughout the semester, a team's work will be presented to faculty and peers with feedback and critique being presented to help aid in the project development. Our particular project idea has gone through multiple iterations to do the following: Simplify the engine to be feasible within 3 months and Deliver content that would be useful, and hopefully enjoyable, to consume. As of writing this, we've learned that creating content that will satisfy both is difficult and time-consuming, so we will be focusing on writing these milestone-type blogs as well as posting various types of content to test which is the best form of presenting our work. The short project duration also forces us to think clearly about our scope and be lean on the features we include before starting. Schedule \u00b6 During the course of this project (08/26 - 12/16, 2018 ), a blog post will be published every week to share our thoughts and process, and an interview will be published every 1-2 weeks. The interview schedule depends on our progress on the engine itself, as each interview's topic will be themed around our current work. For latest schedule, see our schedule page. Prerequisites \u00b6 Although we will cover some basic features of engine development, it will profoundly help if you have experience in C++ programming and developing software, especially games, as our project won't provide step-by-step instructions on how to do everything. For a list of resources on how to gain related knowledge, please go to the Readings section. Additional resources will be posted on our resource page . Another prerequisite is passion for learning game engine development. As you are still reading this, we assume you are as excited about this as we are. This will be a bumpy ride, but you will have us on your side. Research \u00b6 Being a student-pitched ETC project means that the project needed to pass through a pitch process of consulting and convincing faculty in the program. This allowed us to receive feedback about what could be considered a reasonable/manageable scope and where we might hit challenges for a general project. For this project to be a valid ETC project as well as accomplish our mission statement, there needs to be a fine balance between documentation and development. Before confronting the big monster of engine development and documentation, we thought it would be a good idea to gear up by getting input from people who have actually done this. During our pitch process, we reached out and got the chance to talk to numerous industry professionals and got extremely helpful advice from them. All of these suggestions helped us shape our project into what it is now and provided invaluable knowledge on how to start a game engine. Thus we encourage you, too, to approach professionals and get advice if possible. We've compiled our notes from our conversations with them into a write-up, which will be published soon. Why another engine? \u00b6 Using an existing game engine like Unity , Unreal or Panda3D is always a handy option to make a game. These well-established engines have a strong collection of tools and APIs so that developers can focus on making the game, not the wheels. However, there is the limitation of not having full control over all systems in the engine as well as not knowing how the engine is processing the game logic and assets. These can obstruct the complex systems of an engine, so although you may have an understanding of how a physics or graphics engine works, each engine operates differently and optimizes for different constraints. In terms of learning about game engines and how to develop one, these established engines aren't a good source. Panda3D, originally developed by Disney and expanded by past ETC projects, has an older codebase in 2018 with limited community involvement. It is also not using the current industry standard language (C++). Unity and Unreal are both too massive and too cutting-edge to be suitable engine learning material for novices. In addition, Unity's source code isn't publicly available so you technically can't learn from it. The huge codebase sets a high threshold for any beginner to get started. Roadmap \u00b6 The Isetta Engine will support the most primitive form of networked multiplayer twin-stick shooter game. Networked multiplayer was selected to be a part of the engine because it offers significant design and development challenge on every level of the project, and will help differentiate this engine from others being developed. We decided to create the engine in 3D for two reasons: Most AAA engines are 3D, and 3D requires more math and problem solving for us as developers to learn and grow from. While planning, and before we knew too much about game engines, we had a basic idea of what a game engine would consist of. The image below displays the second/third iteration of what the Isetta engine would look like. We were initially naive thinking we may be able to do both networking as well as physics, however quickly came to grips that would balloon the scope too much. The audio and graphics were and are still planned to be imported from external libraries, and more of the discussion of what is imported and why will be included in a future blog. This diagram of the engine will soon be replaced with more in-depth explanations. Genre \u00b6 As for our choice of the twin-stick shooter genre, we came to the decision after lengthy consideration of the components required to build other game types as well as how that genre would utilize multiplayer. Twin-stick shooters can effectively have little to no physics, which is different from collisions (this will be explained in week 1 blog ). Likewise, the information passed between networked sessions is relatively minimal and not too strict on latency. What's more, a twin-stick shooter specializes in simplistic gameplay that doesn't need a world editor or too much design. In a Skype meeting, Walt Destler explained to us that each game -and more particularly, each genre- requires vastly different netcode solutions. This is also one of the reasons why we prefer netcode over physics, as it can greatly narrow down the genre options. For example, multiplayer shooters, specifically PvP shooters, require small amounts of information to be passed (i.e. bullet and player locations) from server to client with relatively low latency. PvP shooters can also feature client-side prediction 1 as well as the additional requirement of lobby/matchmaking with usually more than 2 players. On the other hand, genres like turn-based strategy require large amounts of information to be passed (all units, decisions, resources, etc.) to all users without too much concern for latency or prediction. Building with an Example Game \u00b6 The other piece of advice we frequently heard from professionals and our faculty alike was the benefit of developing a game in conjunction with the engine. Doing so, they explained, allows you to prove and demonstrate your engine works as expected. The game can also test features to show immediate edge cases of the engine. Another nicety of developing an engine is that feature creep can be prevented when you keep expanding certain features that won't be utilized in the final product. What the game built from this engine won't be is something original or necessarily fun. However, that's not to say a fun, original game couldn't be created from this engine. The idea of our sample game is to intentionally be derivative so features of a basic twin-stick shooter will be already included in the engine, rather than only specific features on our niche experience. The Example Game \u00b6 As stated, the game we are making is a simple twin-stick shooter, reminiscent of a game you would create when first learning game development. It is a simpler derivative of the Call of Duty Black Ops: Dead Ops Arcade mode , though without much of the polished and juicy gameplay Call of Duty is known for. We have already developed a version of our game in Unreal, and we'll be replicating and using it to compare the functionality to our game engine. We will be able to create scenarios in the Unreal version of the game to then replicate in the Isetta Engine version of the game. Having a version of the game already built stops us from having to be concerned with the logic or design of the actual game, so we can focus on the engine doing exactly what the game currently does. It also allows us to pull the assets from the Unreal version of the game directly to the Isetta Engine without having to worry about a bad asset. In other words, if it worked in Unreal, it should work in Isetta because this is our minimal viable product (MVP). The Engine's Components \u00b6 Since this project is already a large undertaking and restricted to such a short time, it simply isn't possible for us to write the code for every part of the engine. If you look at an industry engineer position, you'll see no single person writes code for the entire engine because teams are usually tens to hundreds of people. However, what everyone on the team needs to know how to do is integrate and use libraries created by others. Below is our engine plan broken down into seven oversimplified components: Core Systems : The code underneath everything else which will be more familiar to software developers Graphics, Audio, and Input Engine Tools, which will help debugging and profiling the game Resource Management : How files are processed prior to the game running Gameplay Foundations : How the engine connects to game logic Collisions : The system for collidables but not handling actual physics calculations Networking , which will be layered throughout the engine to enable multiplayer. Of these components, number 1 will have a smattering of imported libraries to assist with problems that we believe wouldn't benefit us or the engine if writing them; 2 will be purely imported libraries because graphics alone could engulf a full 3 months (especially for 3D and animation); and 3 will use an existing profiler because our simplified version of a profiler may end up being unusable due to the lack of UI design. All other systems, though, will be implemented from scratch in the Isetta Engine, and possibly by you if you follow our journey. Readings \u00b6 Creating an engine can be a daunting task, especially considering there is no clear starting point. When we started the project, we looked to others to see how we could be guided, but we were greeted with massive textbooks that were both daunting and dense. As fearful as we were, we read a few of these books and blogs in order to have a better grasp of developing an engine in such a short period. We strongly recommend you read some of these books, or at least the chapters listed, because it will give you a better understanding of the language we use as well as what we are trying to accomplish. Our goal for these blogs is to be able to explain what we are doing and why are doing it. However, we won't be covering the \"how\" because it takes time to develop content that is teachable. This is by no means an end-all list of engine books and resources; it is only what we used/think could be most useful. Additional resources may be found here . Books \u00b6 Game Engine Architecture : This is the definitive game engine book, written by Naughty Dog's Jason Gregory. It shows readers the big picture of what a game engine is and has a complete discussion about each of the major components. Our team read the second edition of this book, because the third edition was just released July 2018. To have a brief idea of a game engine itself, Chapters 1, 4, 5, 7, 14, 15 are especially recommended. Game Programming Patterns : Design patterns are general and reusable solutions to common problems in software engineering. Game developing shares some concepts with that field but has its own problems. This book is easy to read and not only goes over the classic design patterns, but also introduces game-specific design patterns. In our opinion, the most important patterns to know are: Observer pattern: A great way to decouple different systems State pattern: Extensively used for controlling NPCs, animations, etc. Game loop : The heart of almost every game engine Data locality : One of the fundamentals of data-oriented design Object pool : A ubiquitous solution for memory management A Tour of C++ : Modern C++ is always hard to follow, even for experienced programmers. This book is a thin handbook from Bjarne Stroustrup, creator of C++, designed for developers who have programmed before, and gives an overview of modern C++ (C++17). Effective C++ Series : This series of books gives the best practices of C++ and covers most things used in C++ development. It can greatly improve the code quality and hopefully reduce StackOverflow searches. Blogs \u00b6 Handmade Hero : Handmade Hero is an ongoing project to create a complete, professional-quality game from scratch. Casey Muratori, the developer, streams the creation of every line of source code and the videos are also available on Youtube and Handmade Hero website. Gaffer On Games : This is a blog site by Glenn Fiedler, who writes in depth about game topics mainly focusing on physics and networking. It can be a great resource to understand an obscure topic better, or just learn about something you didn't know existed. Red Blob Games : Amit Patel, the author of Red Blob, aims to teach math and computer science topics through the lens of games. His blogs do a very good job of educating on topics that are closely related to games, and offer interactive examples throughout the page to augment just reading. How to Write Your Own C++ Game Engine : This blog is most similar to the blogs we will be trying to write, but on a more regular interval. It talks about Jeff Preshing's journey of writing his own game engine with a smattering of advice and nuances that he encountered. Videos \u00b6 CppCon 2014: Mike Acton \"Data-Oriented Design and C++\" : Mike Acton discusses why data-oriented design is a much more appropriate view of programs than object-oriented design is, and why dealing with the reality of specific hardware and very low-level, platform-oriented optimization is more true than theoretical abstractions. Over the course of the presentation, Acton discusses three big lies of how code has been designed and what action programmers should take to rectify those lies. Coming Soon/Next Week \u00b6 As of now, our team has finished a preliminary architecture design for the engine and the subsystems that will be implemented or imported from third party libraries. Engine development is already underway and in next week's blog we will share our design process for the architecture and the roadmap of development. Besides the Isetta Engine development, the first interview with Adam Serdar, a Senior Game Engineer from Schell Games, will be coming out next week, so stay tuned! Originally published September 5, 2018. John Carmack: \"I am now allowing the client to guess at the results of the users movement until the authoritative response from the server comes through. This is a biiiig architectural change. The client now needs to know about solidity of objects, friction, gravity, etc. I am sad to see the elegant client-as-terminal setup go away, but I am practical above idealistic.\" For more, see: Gaffer on games: What Every Programmer Needs To Know About Game Networking \u21a9","title":"[Week 0] Preparation"},{"location":"blogs/week-0/#preparation","text":"","title":"Preparation"},{"location":"blogs/week-0/#introduction","text":"The Isetta Engine is a student-driven project about demystifying game engine development and providing a roadmap and relevant knowledge for novice developers. To do so, our team will make a game engine by ourselves starting from a collection of base frameworks, and document the process, pitfalls, and advice for our audience with periodic blogs. Besides that, we will conduct interviews with experienced professionals to augment our novice perspective. We believe the novice perspective from our blogs and expert perspective from the interviews will nicely come together and form a complete document to help people get started. The reason we think more work needs to be done in this field is that too many game engine developers wait until the completion of the engine, typically years, to talk about their development. For studios, this is because they consider the final product to be the game, not the engine. For others, it may be because the engine is what they see as valuable, not the writing. As a result, these talks typically lose the minutiae of the actual daily struggles that took place in the development process. There are others who document their development which has been going on for years, which makes it a daunting task for newcomers to start following along. Although the project is aimed at helping novice developers, this is not to be used as a sole source of learning engine development. Being new engine developers ourselves, we can't guarantee the way we develop the engine will be correct, which is why interviews will help the project remain grounded. This means others who are learning can use what we've done as a guide and not necessarily the ground truth. The blogs won't be a walkthrough/tutorial/step-by-step instructions on how to develop an engine. We are learning as we go and think our journey is what can be valuable to you.","title":"Introduction"},{"location":"blogs/week-0/#about-the-project","text":"This project is being done as a student-pitched project at the Entertainment Technology Center (ETC). The ETC is an interdisciplinary Master's degree program at Carnegie Mellon University where students' main focus is working on small teams on a project each semester during a 3-month time period. Throughout the semester, a team's work will be presented to faculty and peers with feedback and critique being presented to help aid in the project development. Our particular project idea has gone through multiple iterations to do the following: Simplify the engine to be feasible within 3 months and Deliver content that would be useful, and hopefully enjoyable, to consume. As of writing this, we've learned that creating content that will satisfy both is difficult and time-consuming, so we will be focusing on writing these milestone-type blogs as well as posting various types of content to test which is the best form of presenting our work. The short project duration also forces us to think clearly about our scope and be lean on the features we include before starting.","title":"About the Project"},{"location":"blogs/week-0/#schedule","text":"During the course of this project (08/26 - 12/16, 2018 ), a blog post will be published every week to share our thoughts and process, and an interview will be published every 1-2 weeks. The interview schedule depends on our progress on the engine itself, as each interview's topic will be themed around our current work. For latest schedule, see our schedule page.","title":"Schedule"},{"location":"blogs/week-0/#prerequisites","text":"Although we will cover some basic features of engine development, it will profoundly help if you have experience in C++ programming and developing software, especially games, as our project won't provide step-by-step instructions on how to do everything. For a list of resources on how to gain related knowledge, please go to the Readings section. Additional resources will be posted on our resource page . Another prerequisite is passion for learning game engine development. As you are still reading this, we assume you are as excited about this as we are. This will be a bumpy ride, but you will have us on your side.","title":"Prerequisites"},{"location":"blogs/week-0/#research","text":"Being a student-pitched ETC project means that the project needed to pass through a pitch process of consulting and convincing faculty in the program. This allowed us to receive feedback about what could be considered a reasonable/manageable scope and where we might hit challenges for a general project. For this project to be a valid ETC project as well as accomplish our mission statement, there needs to be a fine balance between documentation and development. Before confronting the big monster of engine development and documentation, we thought it would be a good idea to gear up by getting input from people who have actually done this. During our pitch process, we reached out and got the chance to talk to numerous industry professionals and got extremely helpful advice from them. All of these suggestions helped us shape our project into what it is now and provided invaluable knowledge on how to start a game engine. Thus we encourage you, too, to approach professionals and get advice if possible. We've compiled our notes from our conversations with them into a write-up, which will be published soon.","title":"Research"},{"location":"blogs/week-0/#why-another-engine","text":"Using an existing game engine like Unity , Unreal or Panda3D is always a handy option to make a game. These well-established engines have a strong collection of tools and APIs so that developers can focus on making the game, not the wheels. However, there is the limitation of not having full control over all systems in the engine as well as not knowing how the engine is processing the game logic and assets. These can obstruct the complex systems of an engine, so although you may have an understanding of how a physics or graphics engine works, each engine operates differently and optimizes for different constraints. In terms of learning about game engines and how to develop one, these established engines aren't a good source. Panda3D, originally developed by Disney and expanded by past ETC projects, has an older codebase in 2018 with limited community involvement. It is also not using the current industry standard language (C++). Unity and Unreal are both too massive and too cutting-edge to be suitable engine learning material for novices. In addition, Unity's source code isn't publicly available so you technically can't learn from it. The huge codebase sets a high threshold for any beginner to get started.","title":"Why another engine?"},{"location":"blogs/week-0/#roadmap","text":"The Isetta Engine will support the most primitive form of networked multiplayer twin-stick shooter game. Networked multiplayer was selected to be a part of the engine because it offers significant design and development challenge on every level of the project, and will help differentiate this engine from others being developed. We decided to create the engine in 3D for two reasons: Most AAA engines are 3D, and 3D requires more math and problem solving for us as developers to learn and grow from. While planning, and before we knew too much about game engines, we had a basic idea of what a game engine would consist of. The image below displays the second/third iteration of what the Isetta engine would look like. We were initially naive thinking we may be able to do both networking as well as physics, however quickly came to grips that would balloon the scope too much. The audio and graphics were and are still planned to be imported from external libraries, and more of the discussion of what is imported and why will be included in a future blog. This diagram of the engine will soon be replaced with more in-depth explanations.","title":"Roadmap"},{"location":"blogs/week-0/#genre","text":"As for our choice of the twin-stick shooter genre, we came to the decision after lengthy consideration of the components required to build other game types as well as how that genre would utilize multiplayer. Twin-stick shooters can effectively have little to no physics, which is different from collisions (this will be explained in week 1 blog ). Likewise, the information passed between networked sessions is relatively minimal and not too strict on latency. What's more, a twin-stick shooter specializes in simplistic gameplay that doesn't need a world editor or too much design. In a Skype meeting, Walt Destler explained to us that each game -and more particularly, each genre- requires vastly different netcode solutions. This is also one of the reasons why we prefer netcode over physics, as it can greatly narrow down the genre options. For example, multiplayer shooters, specifically PvP shooters, require small amounts of information to be passed (i.e. bullet and player locations) from server to client with relatively low latency. PvP shooters can also feature client-side prediction 1 as well as the additional requirement of lobby/matchmaking with usually more than 2 players. On the other hand, genres like turn-based strategy require large amounts of information to be passed (all units, decisions, resources, etc.) to all users without too much concern for latency or prediction.","title":"Genre"},{"location":"blogs/week-0/#building-with-an-example-game","text":"The other piece of advice we frequently heard from professionals and our faculty alike was the benefit of developing a game in conjunction with the engine. Doing so, they explained, allows you to prove and demonstrate your engine works as expected. The game can also test features to show immediate edge cases of the engine. Another nicety of developing an engine is that feature creep can be prevented when you keep expanding certain features that won't be utilized in the final product. What the game built from this engine won't be is something original or necessarily fun. However, that's not to say a fun, original game couldn't be created from this engine. The idea of our sample game is to intentionally be derivative so features of a basic twin-stick shooter will be already included in the engine, rather than only specific features on our niche experience.","title":"Building with an Example Game"},{"location":"blogs/week-0/#the-example-game","text":"As stated, the game we are making is a simple twin-stick shooter, reminiscent of a game you would create when first learning game development. It is a simpler derivative of the Call of Duty Black Ops: Dead Ops Arcade mode , though without much of the polished and juicy gameplay Call of Duty is known for. We have already developed a version of our game in Unreal, and we'll be replicating and using it to compare the functionality to our game engine. We will be able to create scenarios in the Unreal version of the game to then replicate in the Isetta Engine version of the game. Having a version of the game already built stops us from having to be concerned with the logic or design of the actual game, so we can focus on the engine doing exactly what the game currently does. It also allows us to pull the assets from the Unreal version of the game directly to the Isetta Engine without having to worry about a bad asset. In other words, if it worked in Unreal, it should work in Isetta because this is our minimal viable product (MVP).","title":"The Example Game"},{"location":"blogs/week-0/#the-engines-components","text":"Since this project is already a large undertaking and restricted to such a short time, it simply isn't possible for us to write the code for every part of the engine. If you look at an industry engineer position, you'll see no single person writes code for the entire engine because teams are usually tens to hundreds of people. However, what everyone on the team needs to know how to do is integrate and use libraries created by others. Below is our engine plan broken down into seven oversimplified components: Core Systems : The code underneath everything else which will be more familiar to software developers Graphics, Audio, and Input Engine Tools, which will help debugging and profiling the game Resource Management : How files are processed prior to the game running Gameplay Foundations : How the engine connects to game logic Collisions : The system for collidables but not handling actual physics calculations Networking , which will be layered throughout the engine to enable multiplayer. Of these components, number 1 will have a smattering of imported libraries to assist with problems that we believe wouldn't benefit us or the engine if writing them; 2 will be purely imported libraries because graphics alone could engulf a full 3 months (especially for 3D and animation); and 3 will use an existing profiler because our simplified version of a profiler may end up being unusable due to the lack of UI design. All other systems, though, will be implemented from scratch in the Isetta Engine, and possibly by you if you follow our journey.","title":"The Engine's Components"},{"location":"blogs/week-0/#readings","text":"Creating an engine can be a daunting task, especially considering there is no clear starting point. When we started the project, we looked to others to see how we could be guided, but we were greeted with massive textbooks that were both daunting and dense. As fearful as we were, we read a few of these books and blogs in order to have a better grasp of developing an engine in such a short period. We strongly recommend you read some of these books, or at least the chapters listed, because it will give you a better understanding of the language we use as well as what we are trying to accomplish. Our goal for these blogs is to be able to explain what we are doing and why are doing it. However, we won't be covering the \"how\" because it takes time to develop content that is teachable. This is by no means an end-all list of engine books and resources; it is only what we used/think could be most useful. Additional resources may be found here .","title":"Readings"},{"location":"blogs/week-0/#books","text":"Game Engine Architecture : This is the definitive game engine book, written by Naughty Dog's Jason Gregory. It shows readers the big picture of what a game engine is and has a complete discussion about each of the major components. Our team read the second edition of this book, because the third edition was just released July 2018. To have a brief idea of a game engine itself, Chapters 1, 4, 5, 7, 14, 15 are especially recommended. Game Programming Patterns : Design patterns are general and reusable solutions to common problems in software engineering. Game developing shares some concepts with that field but has its own problems. This book is easy to read and not only goes over the classic design patterns, but also introduces game-specific design patterns. In our opinion, the most important patterns to know are: Observer pattern: A great way to decouple different systems State pattern: Extensively used for controlling NPCs, animations, etc. Game loop : The heart of almost every game engine Data locality : One of the fundamentals of data-oriented design Object pool : A ubiquitous solution for memory management A Tour of C++ : Modern C++ is always hard to follow, even for experienced programmers. This book is a thin handbook from Bjarne Stroustrup, creator of C++, designed for developers who have programmed before, and gives an overview of modern C++ (C++17). Effective C++ Series : This series of books gives the best practices of C++ and covers most things used in C++ development. It can greatly improve the code quality and hopefully reduce StackOverflow searches.","title":"Books"},{"location":"blogs/week-0/#blogs","text":"Handmade Hero : Handmade Hero is an ongoing project to create a complete, professional-quality game from scratch. Casey Muratori, the developer, streams the creation of every line of source code and the videos are also available on Youtube and Handmade Hero website. Gaffer On Games : This is a blog site by Glenn Fiedler, who writes in depth about game topics mainly focusing on physics and networking. It can be a great resource to understand an obscure topic better, or just learn about something you didn't know existed. Red Blob Games : Amit Patel, the author of Red Blob, aims to teach math and computer science topics through the lens of games. His blogs do a very good job of educating on topics that are closely related to games, and offer interactive examples throughout the page to augment just reading. How to Write Your Own C++ Game Engine : This blog is most similar to the blogs we will be trying to write, but on a more regular interval. It talks about Jeff Preshing's journey of writing his own game engine with a smattering of advice and nuances that he encountered.","title":"Blogs"},{"location":"blogs/week-0/#videos","text":"CppCon 2014: Mike Acton \"Data-Oriented Design and C++\" : Mike Acton discusses why data-oriented design is a much more appropriate view of programs than object-oriented design is, and why dealing with the reality of specific hardware and very low-level, platform-oriented optimization is more true than theoretical abstractions. Over the course of the presentation, Acton discusses three big lies of how code has been designed and what action programmers should take to rectify those lies.","title":"Videos"},{"location":"blogs/week-0/#coming-soonnext-week","text":"As of now, our team has finished a preliminary architecture design for the engine and the subsystems that will be implemented or imported from third party libraries. Engine development is already underway and in next week's blog we will share our design process for the architecture and the roadmap of development. Besides the Isetta Engine development, the first interview with Adam Serdar, a Senior Game Engineer from Schell Games, will be coming out next week, so stay tuned! Originally published September 5, 2018. John Carmack: \"I am now allowing the client to guess at the results of the users movement until the authoritative response from the server comes through. This is a biiiig architectural change. The client now needs to know about solidity of objects, friction, gravity, etc. I am sad to see the elegant client-as-terminal setup go away, but I am practical above idealistic.\" For more, see: Gaffer on games: What Every Programmer Needs To Know About Game Networking \u21a9","title":"Coming Soon/Next Week"},{"location":"blogs/week-1/","text":"The Building Blocks \u00b6 Byte-Sized Summary \u00b6 General setup: Talk about our development using GitHub, linter, API design, and unit testing -- hint: we are relying on standards established by others. Roadmap : Roadmap of the engine\u2019s development for the next 3 months, broken into thirds, each with systems we will building during it. Version Control : Set up Github repo and decided on workflow. Rendering : Chose Horde3D over Ogre and made a demo of it Module Manager : Made all initialization and deinitialization private and made the module manager a friend class of all modules Audio : Chose FMOD over OpenAL Soft, integrated it into our engine, and made a wrapper on it Networking : Built the GameNetworkingSockets library and successfully ran a sample test Profiler : Selected Brofiler as the profiler and forked the repo in preparation of needed changes. Debug Logging : Programmed a logger to output to VS output window and log files as well as masking messages for channel and verbosity. Error-Handling : Using Microsoft afx headers for assertions temporarily and using standard exceptions. It's hard to believe we technically started less than two weeks ago! During the summer, we pored over planning what the project would include, approximating how much we could accomplish in 3 months and preparing ourselves by reading relative materials. This week we started making decisions on our engine (we have a separate blog on this!) and actual development. Before full-on diving into programming, we set up three systems not directly related to our engine but can ease our development process. First, we set up the version control system with GitHub. We recommend you use version control in engine development just as you would with any kind of programming project. It preserves the project's history (so you can do crazy experiments without worrying about destroying your project!) and allows a team to work simultaneously on the same project together. We have broken our repository into development and feature branches, where each feature branch correspond to one \"block\" in our architecture diagram (as seen below). Second, we set up the Google Linter 1 to ensure the coding style amongst the four programmers remain consistent. A number of professionals we spoke to told us that coding styles can cause arguments between programmers and thus slow down the project. To nip this in the bud, we decided to use an automated system and always default to what was established in the linter, rather than personal opinion. This would give us a consistent code base that someone, such as yourself, may be able to follow. Although we will do our best to follow best practices, we can't guarantee we will. We are more focused on developing our own engine than software engineering as a whole. We have found ourselves using fallbacks when designing since we aren\u2019t experienced with architecture or API design. Our primary fall-back is The Game Engine Architecture book by Jason Gregory; although we don\u2019t want this book to be a required reading for our audience, there are times when we are having discussions where we ask, \u201chow was it done in the book?\u201d When developing the systems themselves we will try to find other sources that we also think could be useful, these will be included on our Resources page which will be linked and updated with each blog. With regards to API design, since we all have Unity experience and are comfortable with their API, some of our decisions will clearly be influenced by that. On this note, we will be trying to develop systems as they would in an engine, but there is no guarantee it will be correct\u2014in fact, it most likely won't be. We will be pulling from our knowledge of the Game Engine Architecture book, since it is a book we have all read; however, we are writing about the topics from our understanding. We believe it is the journey of us talking about the topics, not necessarily the right decisions, which can be beneficial to you. Finally, we set up a unit testing framework. This won't be used by the engine but we see it as a valuable system that can prevent code regression as we add new features. However, as with the coding standards, testing in general can subtract a large chunk of development time. So we are only doing light testing on public functions rather than the more lengthy full-coverage. If you have the time, we suggest you do full-coverage testing for your own engine. Roadmap \u00b6 We have broken our architecture into 3 deadlines, over the next 3 months. The first deadline is September 24, where the base layer, the memory layer, tools, and our imported modules will be completed. The base layer is the groundwork of the engine and thus must be completed because the other systems rely on at least one of these systems in one way or another. The memory layer is the base for all modules, and needs to be done well because of the strong reliance on it. Doing it early allows us the opportunity to struggle through it as well as ample time to optimize. The tools will also be used throughout the rest of the engine which is why it is important to get them ready early. The imported modules are good to bring in early because it ensures that they work (which gives time in the case they don't) and can be a spike in motivation/confidence. Finally, starting the networking as soon as possible is something that we have been advised to do by many because it is integrated deeply with most systems. The next deadline is October 22, where the utility layer, module layer, and networking will be completed. Networking is the biggest chunk of this roadmap phase because it's the largest unknown to us in our engine, and we want to prepare accordingly. The other systems being developed are in line with the future systems depending on them to function. The final deadline is November 26, when we'll enter the feature lock phase. During the weeks following, we will be polishing the code as well as completing the game being built alongside the engine. The gameplay systems are in the highest layer of our engine, which is why it can be held off until the end, and the build portions of our architecture will be focused on at this point. Asset processing and caching can be held off until the end because they are features which convert final assets into playable assets of the final build of the game for better performance. Development \u00b6 This architecture diagram shows the components we have completed, in-progress, and (hopefully never on here) failed. You can assume this to be what 3-4 man-weeks working full-time would look like, which could vary based on your skill. The code can be found on the GitHub repo tagged with week-1. Module Manager \u00b6 As shown in our architecture blog , the module manager takes charge of starting up and shutting down different modules like AudioModule and RenderModule . The design of modules seems to be straightforward. But when we got our hands dirty designing and implementing modules, we found that two factors affected our structure a lot: accessibility and initialization/deinitialization order. Singletons, Globals, or a Manager \u00b6 One of the common design is to make modules as global variables. However, using global variables might be dangerous since not only the ModuleManager , but also anyone else has access to the modules and can accidentally create the same module twice or modify the modules. Also, the initialization order of global variables in different files is unspecified. These two drawbacks can totally break a game engine. To prevent that, Singleton seems to be a better solution: it can restrict the instance to one and with construct on-demand technique 2 , we can also control the initialization order. We found that both of the methods allow engine users to get access to the modules, which should be hidden from them. Also, neither of them can control the deinitialization order. If there is any dependency between two modules, the nondeterministic deinitialization order can crash a game. To fix those two issues, we finally turned to a more complex solution: we make all the constructors, destructors, as well as StartUp() and ShutDown() methods private in the modules and make ModuleManager as a friend class to them to instantiate and destroy them. Since initialization and deinitialization functions are both private and only the ModuleManager is their friend, the creation of a module can be perfectly controlled. Moreover, now we can completely hide the module classes from the engine users. For naming methods, we chose to use StartUp() and ShutDown() functions for initialization and deinitialization instead of constructors and destructors because of readability. We would like to explicitly say that the module now is started or shut down, especially for the deinitialization which usually is done by calling the destructors implicitly. Rendering \u00b6 From our research, we narrowed our candidate libraries down to two: one is Ogre and the other one is Horde3D . Ogre is a powerful and flexible 3D graphics engine. It has a long history and is well known in the game industry for a while. In Game Engine Architecture , Jason Gregory described it as a well-architected, easy-to-learn and easy-to-use 3D rendering engine. Horde3D, relatively, is a younger 3D rendering engine, which just released its first stable release last year (2017). Choosing between these two libraries was hard. Both are well-designed, well-implemented, and can bring good visual capacity to the game engine. Thus, we thought about it more from our project's perspective: Importing the rendering library is supposed to simplify our workload. Building the library should be easy, the documentation should be clear, and the interface should be easy to use. We are not building a game engine around the rendering library, but using the rendering library as a replaceable module. We would like the rendering library to fit in our engine design. The library should be lightweight, flexible and ideally ECS 3 -friendly With the criteria above, we went through the building process and made simple demos with both libraries. For Ogre, we used Ogre 2.1 instead of the popular Ogre 1.x because Ogre 2.x fixed the architecture and performance issues pointed out by Mike Acton 4 . After some experiments, we decided to use Horde3D as our rendering library. We don't think that Ogre is a bad library (its popularity disproves that!), but we believe that Horde3D fits better with the game engine itself for the following reasons: Building Horde3D is much easier than building Ogre 2.1. The build process of Horde3D is so friendly that with one single click in CMake, we can have both GLFW (its dependency library) and Horde3D built. For Ogre, we need to download the dependency and core library from separate repos and build each of them separately. We also had to set the optimization of some components to /Od level (optimization disabled) to make it build successfully! Also, the official documentation for getting started was unclear and provides two sets of non-identical instructions in two different places. Horde3D is more lightweight and designed as components. Ogre 2.1 has already encapsulated the rendering APIs into GameState and RenderSystem. To integrate it into our game engine, we will have to derive our solution from there. Instead, Horde3D only provides a small set of procedure interfaces, which allows us to implement the wrapper module by ourselves. In the first week, we made a simple wrapper around the Horde3D as a proof-of-concept (see the HordeDemo folder). We will finish the wrapper as a module in the upcoming week. Audio \u00b6 With the requirements listed in our architecture blog (a library which allows us to allocate the used memory ourselves and is widely used in the game industry) we started our research and found two strong candidates: OpenAL Soft and FMOD. OpenAL caught our eyes first because it seemed related to OpenGL (it's not, but they do have other things in common), and we found an open source implementation of the standard named OpenAL Soft . On the other hand, we had heard of FMOD because Unity uses it on several platforms, and we think it's a good candidate purely from the huge list of games (with some hit titles) that use it. We chose to give OpenAL Soft a try first because it's open source, while FMOD is owned by a commercial company and is not free (however, it does provide a free license for indie games with a budget under 500k). However, the team member who was assigned the task failed to get anything working in three hours, by the end of which he was still confused and stuck. This is partially due to the lack of Getting Started documentation and tutorials online, and partially because he is not experienced with audio APIs at all. Then we decided to experiment with FMOD. It turns out that FMOD's low-level API is surprisingly easy to integrate and use, and has a comprehensive documentation . We were able to get the library integrated and play our first audio clip in 2 hours, with the help of some Getting Started tutorials 5 . We think that, for novice engine programmers, these kind of tutorials are invaluable. They are a great way to introduce you to a new field and get you up and running quickly. Also, thanks to the FMOD documentation, we easily found the way to control its memory usage with Memory_Initialize() . FMOD also provides a tutorial on managing its memory! After stitching the FMOD API into our project, we continued using it to create a minimal audio engine. It takes care of loading sound files, managing loaded sound assets (with the help of string-id ), playing sound, managing a sound's play mode, managing a sound's lifetime with pause/continue/stop, and prevent memory leaks. You can take a look at the source code: Audio.h , Audio.cpp Who Owns the Sound? \u00b6 In the current design, every AudioSource owns an instance of FMOD::Sound and is responsible for loading their own audio clip. However, this prevents the same loaded FMOD::Sound from playing in multiple instances. For example, if we have a gunfire sound, which can possibly be played 3 times every second, we need to make a new AudioSource and load the same sound by calling FMOD:System::createSound again. This is clearly not what we want and definitely won\u2019t satisfy the game\u2019s requirements. Solution AudioModule now load all audio clips at startup and keep a SoundClipID to loaded FMOD::Sound unordered_map for later access. This guarantees that each audio clip won\u2019t be loaded multiple times. On the AudioSource side, each AudioSource no longer owns an FMOD::Sound . Instead, they only contain a pointer to an FMOD::Sound instance that is managed by the AudioModule , and the instance that the pointer points to can be changed easily. This allows AudioSource s to be reused and allow a single FMOD::Sound to be referenced by multiple AudioSource s and play at the same time. In this design, if the user wants to play a sound, they can create a new instance of AudioSource , and set that AudioSource \u2019s pointer to the already created FMOD::Sound by passing in the name of the sound file, and call AudioSource::Play() . A side note: the SoundClipID mentioned before is the hashed string id (of type int64 ) from the sound clip\u2019s file name, by utilizing the StringID we brought in. Refactoring Broke the Audio \u00b6 AudioModule was written as a singleton class and AudioSource was using it heavily, because AudioModule is the only class to interface with FMOD::System . After refactoring AudioModule to be part of ModuleManager , the AudioSource \u2019s will no longer have direct access to FMOD. In order to save our sound, a static pointer to AudioModule was placed in AudioSource allowing access to the FMOD system. Networking \u00b6 Our original plan for running low-level networking sockets was to use Valve's GameNetworkingSockets because of the feature set it lists, which we hoped is extensive enough for us to avoid pitfalls in network programming. Other options were either too flimsy or too low-level, and since none of us have game networking experience, we want the most straightforward option possible. At the time of writing this, the GameNetworkingSockets library is a pretty big pain to build, involving several installs and build steps. One of us even took a couple of days automating the build process, and it's still not 100% consistent! Beyond this, the resulting API seems to cover our engine's use case, which is as minor as we can possibly make it without removing networking entirely. Unfortunately, the codebase is still heavily in clean-up mode, and that makes it less accessible for integrating our own allocators and APIs with it. Not to mention that very little exists for documentation. Recently, we found another heavily featured networking library named yojimbo , built by Glenn Fiedler of game network programming fame. This library appears to be more ready for integrating into our own engine, albeit with equally as poor documentation (there's a bit of Doxygen to use, I guess). One big plus to this library is that it's not riddled with commented-out code like the GameNetworkingSockets one is at the time of this writing, which makes us lean more toward yojimbo. We've yet to determine the fate of networking in our engine, so you'll likely hear more on that progress from us next time. Debug Logging \u00b6 The logging system can be the quickest form of debugging and getting information about the state of the engine and game. The requirements of our engine are: 1. Be able to pass a formatted string 2. Have different channels and verbosity levels and mask which display 3. Write the logs to a file 4. Write to the Visual Studio output window (not the console) A channel is simply a category the message can be grouped with, such as animation & audio, which helps when multiple programmers are on a project from seeing logs of the other features. The verbosity level is for determining whether the message is important enough to be displayed or whether the programmer wants to see messages which have less priority/consequence. Channel and verbosity are both enums, but the logger implements a mask for both to determine if a log message should be displayed. The logger can also write to a file, but requires the destination to passed to a setter in the case logging to a file is wanted. There was originally only going to be one file where all messages were written to regardless of whether it matched the channel and verbosity masks, because in the case of a crash we would want as much information as possible to debug where the crash might have occurred. This was expanded to having a file where only messages from specific channels are written, in the case you are only interested in that information. There cannot only be a channel file, ie. without an engine file, since the engine file will contain all information including overlapping information with the channel. The channel file is more convenience, but can lack information that we wouldn\u2019t know is necessary in the case of a crash. Why We Need a File System \u00b6 During a later discussion on debug logger, we realized that we were intertwining a file system (including file paths) with the logger which felt wrong. After talking about this, the file system was decided to be extracted and made into its own system, which is still in development. The logger now waits for a pointer to a stream to use rather than keeping track of its own. Error-Handling \u00b6 There are 2 main types of error handling in our engine: assertions and exceptions. Assertions are points in code that expect an expression to evaluate to true, otherwise, crash the program, or break into the debugger in Debug mode. Typically, assertions are removed in release builds to avoid unnecessary crashes, but are a useful tool in debug mode to ensure the code is acting as expected, as it can immediately direct the person\u2019s attention to the place where things might be wrong. Assertions should be primarily used for checking bugs rather than checking things involving user input, where user input for an engine is the developer of the game. We are currently using Microsoft\u2019s afx library as it offers many features, but it requires MFC 6 and we are not sure if it will cause problems in the future For checking errors possibly caused by bad user input (a programmers not understanding a function use), such as trying to access the 4 th element of a Vector3, we use exceptions. We are defaulting to standard library exceptions , and in the case we need a more specific error we will extend the exception class. Profiler \u00b6 For the profiler, we imported Brofiler , a C++ game profiler developed by Vadim Slyusarev. We picked Brofiler because is it is a lightweight and made specifically for games (not software in general) and has an easy-to-read UI. The library is open-source and relatively easy to integrate, although it's a bit outdated and requires some rework for our project. To help keep followers of our work from reworking the profiler with their own engine, we created a fork of the original Brofiler repo with the necessary changes to get it running. It can be found here and will likely make integration in your engine easier! The Brofiler tool has also been used on the game Skyforge and has been integrated into CryEngine, although CryEngine creates a wrapper around Brofiler. With more time, we would do the same, but it doesn't seem necessary for basic usage. Examples and the original repo can be found on the Brofiler github . Math \u00b6 Math isn\u2019t a very complicated system, it\u2019s all the components you would need in a typical math library. Our methodology with the Math library is to create what we need now as well as what we foresee needing but not more. For our math library, we have followed Unity\u2019s API design fairly closely. Version Control \u00b6 We chose Github as our version control platform as we are open sourcing the game engine and hosting a website with all the documents along with the engine. We also established a workflow to help us collaborate effectively. We decided to open a new branch for each feature and only merge it back to the \"trunk\" when finished, because a game engine is really a feature-heavy product. This strategy enables us to work on multiple features at a time without interfering with each other. The common use case is like this: Coming Soon/Next Week \u00b6 We enjoyed this round of blogging! If you have comments about how you think we could improve as well as if we said anything wrong, we would be very grateful to hear about them in the comments section below. On Saturday, September 8, 2018, 3pm Eastern time, we will be live-streaming an interview with Casey Muratori on his channel HandmadeHero on Twitch. We will be posting an edited transcription of the interview at a later time in case you missed it. We'll be checking in again with another progress update soon so stay tuned, and subscribe to our mailing list to get notified! Resources \u00b6 The resource page has been updated to include links we found useful this week, too! Originally published September 7, 2018. A linter is a tool that flags code for errors, bugs, stylistic errors, and suspicious constructs. \u21a9 The construct on-demand technique in the context of the singleton pattern means that the single instance is only constructed when first requested. We can utilize this nature to control initialization order by manually invoking the getter for the singletons in the right order. \u21a9 Entity-component-system (ECS) is an architectural pattern that follows composition over inheritance principle and is mostly used in games. \u21a9 Mike Acton once made some notes on Ogre's source code: bounceapp \u21a9 We referred to two tutorials by Cody Claborn when integrating FMOD API. Setting Up Xcode and Visual Studio for FMOD Development and Making a Basic FMOD Audio Engine in C++ \u21a9 The Microsoft Foundation Class library is an object-oriented C++ library that contains useful macros for exceptions, run-time type identification, serialization, and more. Math Math isn\u2019t a very complicated system, it\u2019s all the components you would need in a typical math library. Our methodology with the Math library is to create what we need now as well as what we foresee needing but not more. For our math library, we have followed Unity\u2019s API design fairly closely. \u21a9","title":"[Week 1] The Building Blocks"},{"location":"blogs/week-1/#the-building-blocks","text":"","title":"The Building Blocks"},{"location":"blogs/week-1/#byte-sized-summary","text":"General setup: Talk about our development using GitHub, linter, API design, and unit testing -- hint: we are relying on standards established by others. Roadmap : Roadmap of the engine\u2019s development for the next 3 months, broken into thirds, each with systems we will building during it. Version Control : Set up Github repo and decided on workflow. Rendering : Chose Horde3D over Ogre and made a demo of it Module Manager : Made all initialization and deinitialization private and made the module manager a friend class of all modules Audio : Chose FMOD over OpenAL Soft, integrated it into our engine, and made a wrapper on it Networking : Built the GameNetworkingSockets library and successfully ran a sample test Profiler : Selected Brofiler as the profiler and forked the repo in preparation of needed changes. Debug Logging : Programmed a logger to output to VS output window and log files as well as masking messages for channel and verbosity. Error-Handling : Using Microsoft afx headers for assertions temporarily and using standard exceptions. It's hard to believe we technically started less than two weeks ago! During the summer, we pored over planning what the project would include, approximating how much we could accomplish in 3 months and preparing ourselves by reading relative materials. This week we started making decisions on our engine (we have a separate blog on this!) and actual development. Before full-on diving into programming, we set up three systems not directly related to our engine but can ease our development process. First, we set up the version control system with GitHub. We recommend you use version control in engine development just as you would with any kind of programming project. It preserves the project's history (so you can do crazy experiments without worrying about destroying your project!) and allows a team to work simultaneously on the same project together. We have broken our repository into development and feature branches, where each feature branch correspond to one \"block\" in our architecture diagram (as seen below). Second, we set up the Google Linter 1 to ensure the coding style amongst the four programmers remain consistent. A number of professionals we spoke to told us that coding styles can cause arguments between programmers and thus slow down the project. To nip this in the bud, we decided to use an automated system and always default to what was established in the linter, rather than personal opinion. This would give us a consistent code base that someone, such as yourself, may be able to follow. Although we will do our best to follow best practices, we can't guarantee we will. We are more focused on developing our own engine than software engineering as a whole. We have found ourselves using fallbacks when designing since we aren\u2019t experienced with architecture or API design. Our primary fall-back is The Game Engine Architecture book by Jason Gregory; although we don\u2019t want this book to be a required reading for our audience, there are times when we are having discussions where we ask, \u201chow was it done in the book?\u201d When developing the systems themselves we will try to find other sources that we also think could be useful, these will be included on our Resources page which will be linked and updated with each blog. With regards to API design, since we all have Unity experience and are comfortable with their API, some of our decisions will clearly be influenced by that. On this note, we will be trying to develop systems as they would in an engine, but there is no guarantee it will be correct\u2014in fact, it most likely won't be. We will be pulling from our knowledge of the Game Engine Architecture book, since it is a book we have all read; however, we are writing about the topics from our understanding. We believe it is the journey of us talking about the topics, not necessarily the right decisions, which can be beneficial to you. Finally, we set up a unit testing framework. This won't be used by the engine but we see it as a valuable system that can prevent code regression as we add new features. However, as with the coding standards, testing in general can subtract a large chunk of development time. So we are only doing light testing on public functions rather than the more lengthy full-coverage. If you have the time, we suggest you do full-coverage testing for your own engine.","title":"Byte-Sized Summary"},{"location":"blogs/week-1/#roadmap","text":"We have broken our architecture into 3 deadlines, over the next 3 months. The first deadline is September 24, where the base layer, the memory layer, tools, and our imported modules will be completed. The base layer is the groundwork of the engine and thus must be completed because the other systems rely on at least one of these systems in one way or another. The memory layer is the base for all modules, and needs to be done well because of the strong reliance on it. Doing it early allows us the opportunity to struggle through it as well as ample time to optimize. The tools will also be used throughout the rest of the engine which is why it is important to get them ready early. The imported modules are good to bring in early because it ensures that they work (which gives time in the case they don't) and can be a spike in motivation/confidence. Finally, starting the networking as soon as possible is something that we have been advised to do by many because it is integrated deeply with most systems. The next deadline is October 22, where the utility layer, module layer, and networking will be completed. Networking is the biggest chunk of this roadmap phase because it's the largest unknown to us in our engine, and we want to prepare accordingly. The other systems being developed are in line with the future systems depending on them to function. The final deadline is November 26, when we'll enter the feature lock phase. During the weeks following, we will be polishing the code as well as completing the game being built alongside the engine. The gameplay systems are in the highest layer of our engine, which is why it can be held off until the end, and the build portions of our architecture will be focused on at this point. Asset processing and caching can be held off until the end because they are features which convert final assets into playable assets of the final build of the game for better performance.","title":"Roadmap"},{"location":"blogs/week-1/#development","text":"This architecture diagram shows the components we have completed, in-progress, and (hopefully never on here) failed. You can assume this to be what 3-4 man-weeks working full-time would look like, which could vary based on your skill. The code can be found on the GitHub repo tagged with week-1.","title":"Development"},{"location":"blogs/week-1/#module-manager","text":"As shown in our architecture blog , the module manager takes charge of starting up and shutting down different modules like AudioModule and RenderModule . The design of modules seems to be straightforward. But when we got our hands dirty designing and implementing modules, we found that two factors affected our structure a lot: accessibility and initialization/deinitialization order.","title":"Module Manager"},{"location":"blogs/week-1/#singletons-globals-or-a-manager","text":"One of the common design is to make modules as global variables. However, using global variables might be dangerous since not only the ModuleManager , but also anyone else has access to the modules and can accidentally create the same module twice or modify the modules. Also, the initialization order of global variables in different files is unspecified. These two drawbacks can totally break a game engine. To prevent that, Singleton seems to be a better solution: it can restrict the instance to one and with construct on-demand technique 2 , we can also control the initialization order. We found that both of the methods allow engine users to get access to the modules, which should be hidden from them. Also, neither of them can control the deinitialization order. If there is any dependency between two modules, the nondeterministic deinitialization order can crash a game. To fix those two issues, we finally turned to a more complex solution: we make all the constructors, destructors, as well as StartUp() and ShutDown() methods private in the modules and make ModuleManager as a friend class to them to instantiate and destroy them. Since initialization and deinitialization functions are both private and only the ModuleManager is their friend, the creation of a module can be perfectly controlled. Moreover, now we can completely hide the module classes from the engine users. For naming methods, we chose to use StartUp() and ShutDown() functions for initialization and deinitialization instead of constructors and destructors because of readability. We would like to explicitly say that the module now is started or shut down, especially for the deinitialization which usually is done by calling the destructors implicitly.","title":"Singletons, Globals, or a Manager"},{"location":"blogs/week-1/#rendering","text":"From our research, we narrowed our candidate libraries down to two: one is Ogre and the other one is Horde3D . Ogre is a powerful and flexible 3D graphics engine. It has a long history and is well known in the game industry for a while. In Game Engine Architecture , Jason Gregory described it as a well-architected, easy-to-learn and easy-to-use 3D rendering engine. Horde3D, relatively, is a younger 3D rendering engine, which just released its first stable release last year (2017). Choosing between these two libraries was hard. Both are well-designed, well-implemented, and can bring good visual capacity to the game engine. Thus, we thought about it more from our project's perspective: Importing the rendering library is supposed to simplify our workload. Building the library should be easy, the documentation should be clear, and the interface should be easy to use. We are not building a game engine around the rendering library, but using the rendering library as a replaceable module. We would like the rendering library to fit in our engine design. The library should be lightweight, flexible and ideally ECS 3 -friendly With the criteria above, we went through the building process and made simple demos with both libraries. For Ogre, we used Ogre 2.1 instead of the popular Ogre 1.x because Ogre 2.x fixed the architecture and performance issues pointed out by Mike Acton 4 . After some experiments, we decided to use Horde3D as our rendering library. We don't think that Ogre is a bad library (its popularity disproves that!), but we believe that Horde3D fits better with the game engine itself for the following reasons: Building Horde3D is much easier than building Ogre 2.1. The build process of Horde3D is so friendly that with one single click in CMake, we can have both GLFW (its dependency library) and Horde3D built. For Ogre, we need to download the dependency and core library from separate repos and build each of them separately. We also had to set the optimization of some components to /Od level (optimization disabled) to make it build successfully! Also, the official documentation for getting started was unclear and provides two sets of non-identical instructions in two different places. Horde3D is more lightweight and designed as components. Ogre 2.1 has already encapsulated the rendering APIs into GameState and RenderSystem. To integrate it into our game engine, we will have to derive our solution from there. Instead, Horde3D only provides a small set of procedure interfaces, which allows us to implement the wrapper module by ourselves. In the first week, we made a simple wrapper around the Horde3D as a proof-of-concept (see the HordeDemo folder). We will finish the wrapper as a module in the upcoming week.","title":"Rendering"},{"location":"blogs/week-1/#audio","text":"With the requirements listed in our architecture blog (a library which allows us to allocate the used memory ourselves and is widely used in the game industry) we started our research and found two strong candidates: OpenAL Soft and FMOD. OpenAL caught our eyes first because it seemed related to OpenGL (it's not, but they do have other things in common), and we found an open source implementation of the standard named OpenAL Soft . On the other hand, we had heard of FMOD because Unity uses it on several platforms, and we think it's a good candidate purely from the huge list of games (with some hit titles) that use it. We chose to give OpenAL Soft a try first because it's open source, while FMOD is owned by a commercial company and is not free (however, it does provide a free license for indie games with a budget under 500k). However, the team member who was assigned the task failed to get anything working in three hours, by the end of which he was still confused and stuck. This is partially due to the lack of Getting Started documentation and tutorials online, and partially because he is not experienced with audio APIs at all. Then we decided to experiment with FMOD. It turns out that FMOD's low-level API is surprisingly easy to integrate and use, and has a comprehensive documentation . We were able to get the library integrated and play our first audio clip in 2 hours, with the help of some Getting Started tutorials 5 . We think that, for novice engine programmers, these kind of tutorials are invaluable. They are a great way to introduce you to a new field and get you up and running quickly. Also, thanks to the FMOD documentation, we easily found the way to control its memory usage with Memory_Initialize() . FMOD also provides a tutorial on managing its memory! After stitching the FMOD API into our project, we continued using it to create a minimal audio engine. It takes care of loading sound files, managing loaded sound assets (with the help of string-id ), playing sound, managing a sound's play mode, managing a sound's lifetime with pause/continue/stop, and prevent memory leaks. You can take a look at the source code: Audio.h , Audio.cpp","title":"Audio"},{"location":"blogs/week-1/#who-owns-the-sound","text":"In the current design, every AudioSource owns an instance of FMOD::Sound and is responsible for loading their own audio clip. However, this prevents the same loaded FMOD::Sound from playing in multiple instances. For example, if we have a gunfire sound, which can possibly be played 3 times every second, we need to make a new AudioSource and load the same sound by calling FMOD:System::createSound again. This is clearly not what we want and definitely won\u2019t satisfy the game\u2019s requirements. Solution AudioModule now load all audio clips at startup and keep a SoundClipID to loaded FMOD::Sound unordered_map for later access. This guarantees that each audio clip won\u2019t be loaded multiple times. On the AudioSource side, each AudioSource no longer owns an FMOD::Sound . Instead, they only contain a pointer to an FMOD::Sound instance that is managed by the AudioModule , and the instance that the pointer points to can be changed easily. This allows AudioSource s to be reused and allow a single FMOD::Sound to be referenced by multiple AudioSource s and play at the same time. In this design, if the user wants to play a sound, they can create a new instance of AudioSource , and set that AudioSource \u2019s pointer to the already created FMOD::Sound by passing in the name of the sound file, and call AudioSource::Play() . A side note: the SoundClipID mentioned before is the hashed string id (of type int64 ) from the sound clip\u2019s file name, by utilizing the StringID we brought in.","title":"Who Owns the Sound?"},{"location":"blogs/week-1/#refactoring-broke-the-audio","text":"AudioModule was written as a singleton class and AudioSource was using it heavily, because AudioModule is the only class to interface with FMOD::System . After refactoring AudioModule to be part of ModuleManager , the AudioSource \u2019s will no longer have direct access to FMOD. In order to save our sound, a static pointer to AudioModule was placed in AudioSource allowing access to the FMOD system.","title":"Refactoring Broke the Audio"},{"location":"blogs/week-1/#networking","text":"Our original plan for running low-level networking sockets was to use Valve's GameNetworkingSockets because of the feature set it lists, which we hoped is extensive enough for us to avoid pitfalls in network programming. Other options were either too flimsy or too low-level, and since none of us have game networking experience, we want the most straightforward option possible. At the time of writing this, the GameNetworkingSockets library is a pretty big pain to build, involving several installs and build steps. One of us even took a couple of days automating the build process, and it's still not 100% consistent! Beyond this, the resulting API seems to cover our engine's use case, which is as minor as we can possibly make it without removing networking entirely. Unfortunately, the codebase is still heavily in clean-up mode, and that makes it less accessible for integrating our own allocators and APIs with it. Not to mention that very little exists for documentation. Recently, we found another heavily featured networking library named yojimbo , built by Glenn Fiedler of game network programming fame. This library appears to be more ready for integrating into our own engine, albeit with equally as poor documentation (there's a bit of Doxygen to use, I guess). One big plus to this library is that it's not riddled with commented-out code like the GameNetworkingSockets one is at the time of this writing, which makes us lean more toward yojimbo. We've yet to determine the fate of networking in our engine, so you'll likely hear more on that progress from us next time.","title":"Networking"},{"location":"blogs/week-1/#debug-logging","text":"The logging system can be the quickest form of debugging and getting information about the state of the engine and game. The requirements of our engine are: 1. Be able to pass a formatted string 2. Have different channels and verbosity levels and mask which display 3. Write the logs to a file 4. Write to the Visual Studio output window (not the console) A channel is simply a category the message can be grouped with, such as animation & audio, which helps when multiple programmers are on a project from seeing logs of the other features. The verbosity level is for determining whether the message is important enough to be displayed or whether the programmer wants to see messages which have less priority/consequence. Channel and verbosity are both enums, but the logger implements a mask for both to determine if a log message should be displayed. The logger can also write to a file, but requires the destination to passed to a setter in the case logging to a file is wanted. There was originally only going to be one file where all messages were written to regardless of whether it matched the channel and verbosity masks, because in the case of a crash we would want as much information as possible to debug where the crash might have occurred. This was expanded to having a file where only messages from specific channels are written, in the case you are only interested in that information. There cannot only be a channel file, ie. without an engine file, since the engine file will contain all information including overlapping information with the channel. The channel file is more convenience, but can lack information that we wouldn\u2019t know is necessary in the case of a crash.","title":"Debug Logging"},{"location":"blogs/week-1/#why-we-need-a-file-system","text":"During a later discussion on debug logger, we realized that we were intertwining a file system (including file paths) with the logger which felt wrong. After talking about this, the file system was decided to be extracted and made into its own system, which is still in development. The logger now waits for a pointer to a stream to use rather than keeping track of its own.","title":"Why We Need a File System"},{"location":"blogs/week-1/#error-handling","text":"There are 2 main types of error handling in our engine: assertions and exceptions. Assertions are points in code that expect an expression to evaluate to true, otherwise, crash the program, or break into the debugger in Debug mode. Typically, assertions are removed in release builds to avoid unnecessary crashes, but are a useful tool in debug mode to ensure the code is acting as expected, as it can immediately direct the person\u2019s attention to the place where things might be wrong. Assertions should be primarily used for checking bugs rather than checking things involving user input, where user input for an engine is the developer of the game. We are currently using Microsoft\u2019s afx library as it offers many features, but it requires MFC 6 and we are not sure if it will cause problems in the future For checking errors possibly caused by bad user input (a programmers not understanding a function use), such as trying to access the 4 th element of a Vector3, we use exceptions. We are defaulting to standard library exceptions , and in the case we need a more specific error we will extend the exception class.","title":"Error-Handling"},{"location":"blogs/week-1/#profiler","text":"For the profiler, we imported Brofiler , a C++ game profiler developed by Vadim Slyusarev. We picked Brofiler because is it is a lightweight and made specifically for games (not software in general) and has an easy-to-read UI. The library is open-source and relatively easy to integrate, although it's a bit outdated and requires some rework for our project. To help keep followers of our work from reworking the profiler with their own engine, we created a fork of the original Brofiler repo with the necessary changes to get it running. It can be found here and will likely make integration in your engine easier! The Brofiler tool has also been used on the game Skyforge and has been integrated into CryEngine, although CryEngine creates a wrapper around Brofiler. With more time, we would do the same, but it doesn't seem necessary for basic usage. Examples and the original repo can be found on the Brofiler github .","title":"Profiler"},{"location":"blogs/week-1/#math","text":"Math isn\u2019t a very complicated system, it\u2019s all the components you would need in a typical math library. Our methodology with the Math library is to create what we need now as well as what we foresee needing but not more. For our math library, we have followed Unity\u2019s API design fairly closely.","title":"Math"},{"location":"blogs/week-1/#version-control","text":"We chose Github as our version control platform as we are open sourcing the game engine and hosting a website with all the documents along with the engine. We also established a workflow to help us collaborate effectively. We decided to open a new branch for each feature and only merge it back to the \"trunk\" when finished, because a game engine is really a feature-heavy product. This strategy enables us to work on multiple features at a time without interfering with each other. The common use case is like this:","title":"Version Control"},{"location":"blogs/week-1/#coming-soonnext-week","text":"We enjoyed this round of blogging! If you have comments about how you think we could improve as well as if we said anything wrong, we would be very grateful to hear about them in the comments section below. On Saturday, September 8, 2018, 3pm Eastern time, we will be live-streaming an interview with Casey Muratori on his channel HandmadeHero on Twitch. We will be posting an edited transcription of the interview at a later time in case you missed it. We'll be checking in again with another progress update soon so stay tuned, and subscribe to our mailing list to get notified!","title":"Coming Soon/Next Week"},{"location":"blogs/week-1/#resources","text":"The resource page has been updated to include links we found useful this week, too! Originally published September 7, 2018. A linter is a tool that flags code for errors, bugs, stylistic errors, and suspicious constructs. \u21a9 The construct on-demand technique in the context of the singleton pattern means that the single instance is only constructed when first requested. We can utilize this nature to control initialization order by manually invoking the getter for the singletons in the right order. \u21a9 Entity-component-system (ECS) is an architectural pattern that follows composition over inheritance principle and is mostly used in games. \u21a9 Mike Acton once made some notes on Ogre's source code: bounceapp \u21a9 We referred to two tutorials by Cody Claborn when integrating FMOD API. Setting Up Xcode and Visual Studio for FMOD Development and Making a Basic FMOD Audio Engine in C++ \u21a9 The Microsoft Foundation Class library is an object-oriented C++ library that contains useful macros for exceptions, run-time type identification, serialization, and more. Math Math isn\u2019t a very complicated system, it\u2019s all the components you would need in a typical math library. Our methodology with the Math library is to create what we need now as well as what we foresee needing but not more. For our math library, we have followed Unity\u2019s API design fairly closely. \u21a9","title":"Resources"},{"location":"blogs/week-10/","text":"Grab A Bucket \u00b6 Byte-Sized Updates \u00b6 Graphics : Added in functionality to the texture class, so it now does more than just loading and can actually be used! Editor Component : Abstracted some previously developed tools into an editor component which is already proving valuable for development! Custom Dynamic Array : We were originally not planning on creating our own data structures and just relying on STL but we gave into the temptation and it revealed some dark secrets. Second Game : We made our second game (which is not a twin stick shooter) and it works! Patch Notes : There's typically not a header for patch notes, but this week we found some really interesting bugs, mainly involving memory, which we think were great learning experiences. Progress may have slowed due to our trip to LA, but we can assure you it wasn't for nothing. Our initial plan to go out there was decided months back; we had a full lineup of industry professionals who agreed to do interviews and look at our engine progress. Of course, in the fast-moving game industry, the best laid plans don't always come together. A number of our planned interviewees had to drop out at the last minute, which left us in a bit of a panic. We reached out to about a dozen other contacts in LA to try and find some other people we could talk with. One of those contacts was at Sony's Santa Monica Studios, and he was able to get us in touch with Jeet Shroff and Florian Strauss, gameplay and technical directors on the latest God of War . What's more, we still had an interview locked in with Insomniac Games' senior engine programmer and ETC alumni Elan Ruskin . We also finally got to meet up and talk shop with Cort Stratton of Unity, who has been supporting us all semester long. Because these were all on-site talks, we were able to get a lot of clear answers to our questions and more direct feedback. While the extra downtime wasn't ideal, it did give us an opportunity to catch up with our classmates who were in LA on internships and new jobs. Ultimately, the trip was more productive for the team than expected. In terms of development, be prepared to see talk about memory throughout the blog because we were leaking memory left, right, and center! Graphics \u00b6 Texture \u00b6 Looking back at Week 5 , we realized we needed to be able to load textures outside just our graphics module, specifically for GUI images. As stated then, ImGui isn't an image loading library, so our images had to be loaded into our engine from another module. Horde3D has texture loading functionality, but how the functionality worked wasn't apparent or well documented. After struggling with Horde3D for too long, we had the texture in BGRA stored in a uint8_t . The ImGui library still needed the image in the renderer's format. The renderer we're using is OpenGL, so the the texture needs to be loaded into OpenGL memory\u2014ImGui provides an example of how to do this, however the sample code alone won't load the texture. Not knowing OpenGL well enough, we ended up spinning up a completely isolated project with a simple OpenGL scene to test texture loading. Following this tutorial , we were able to get the textures loading right away but there was a lot of unneeded code, so we just had to find which parts of the texture loading were necessary. After a quick bit of debugging, we figured out that the texture filtering line was what was stopping the image from being loaded into the engine. In other news, we made the brilliant decision to let our color-blind programmer work on the texture loading, so we almost missed the fact that our red and blue channels were getting flipped! Luckily, he got another developer involved and the bug was found before the code was committed. The texture class originally was set a static loader, but was changed to hold specific file data, height, and width. This texture can then just be passed directly into the GUI system, rather than passing data and size separately. Here is a colored image loaded into the GUI of the engine! Because we are using Horde3D to load the texture, it is storing it as a resource in its resource map and attempts to destroy the texture on Horde's destruction. We originally ran into an issue when trying to destroy the texture ourselves because we were trying to delete something we weren't the owners of. A simple call to Horde3D to release the memory allowed us to implement a Texture::Unload function, and we learned a good lesson on remembering to keep track of who really owns what. Editor Component \u00b6 Last week , we spoke about developing a console, and in the past we had developed smaller tools like an inspector and hierarchy. These tools were developed out of necessity, and we can see a game developer who uses our engine finding them useful, so we thought we should wrap all these tools in an EditorComponent available as an engine component. When we went to do this, we realized the inspector and hierarchy code were embedded in the Transform and Entity , which meant the developer was always stuck with them. They weren't even wrapped in a preprocessor conditional statement! After some refactoring, we were able to pull each of those GUI code blocks into their own component, which also allows for additional flexibility in the future. Honestly, though, the inspector, hierarchy, and console shouldn't be components but rather some type of editor code, because it doesn't make sense to attach them to entities within the scene. However, adding in an editor type to support this would certainly lead down a rabbit-hole of creating an editor, which we can't afford to do. So in the end, we really did decide to package the three components together into an EditorComponent with a menu and key commands to open. The key commands to open the windows prompted us to include modifier keys support for the keyboard. Custom Dynamic Array \u00b6 This past week, we created a custom vector class labeled Array so as not to be confused with Vector3 and our other math classes. The main reason for recreating std::vector was it is one of the most heavily used data structures in our engine yet it doesn't use our memory allocation, and that wasn't sitting well for us. Creating the basic functionality of the class was very straightforward, and using the STL vector as a guide for functions and naming, the implementation was completed relatively quickly. However, to fully mimic the functionality of the std::vector , we needed to implement iterators, which wasn't as straightforward as it seems. We didn't find any amazing resource on the subject, so it's still something we feel shaky on within our Array class. Learning from past mistakes, we tried to do a good unit test coverage of the whole class, and that helped us iron out the easy-to-find/fix bugs (like using prefix operators instead of postfix operators with sizing/capacity changes). This was also before the Array was integrated into the engine, so changes didn't cause massive recompile times\u2014always a plus for rapid development! Unfortunately, as you might expect, once our Array class replaced the STL vectors in the engine, we found way more bugs. Most of the bugs actually lead us to find bugs within our memory management, and for that, they deserve their own sections: A Drawback to Preallocating All Your Memory , Initialization Timing and Memory Management , and Placement New for Arrays is Undefined . These were fairly involved bugs with our memory, so we are glad we ended up implementing the custom vector class. We also still have other memory issues that we weren't able to solve this week (or even understand yet) which we will be writing about in future weeks. One of the issues we found from testing but wasn't resolved until actually running the engine was our move operator for the Array . We were testing the move assignment operator in the test suite which worked as expected; the contents were moved over and everything looked good. However, what wasn't being shown during tests was the move constructor of Array . This function was actually performing a copy rather than an actual move with the memory, so the Array originally holding the contents still had access to the contents. This means that, when one object was being deleted, the other would also have its contents removed\u2014they were linked! We discovered this when we would return an Array from a function. The local copy would perform a move operation to store the contents in the return statement, then it would delete itself, which would in turn delete the returned variable (which shared the same slot of memory) as well. Fortunately it was a simple fix, all we had to do was fix the move constructor, but tracking this down required a little more thought with regards to C++. Second Game \u00b6 As mentioned in a previous blog post , one of our major milestones is that we made a \"real\" game with our engine. This allows us to validate the features we have implemented as well as to find missing features we need. Last time, we made a twin stick shooter that is quite similar with our target game, but this time we wanted to make something different. The major differences between our current engine and the old engine are the networked entities, the event messaging system, and the polished entity system. Based on these new features, we want to make a simple multiplayer competitive game that can utilize those new features best. Finally, we decided to make a 3D fencing game like Nidhogg . Here is the playthrough video of the game: Your browser does not support the video tag. Here are the game features that utilize the new engine features implemented in the last couple weeks: We used NetworkTransform to sync the position of two networked players and their swords We used network callbacks to send different messages across the server and the clients, including the player ready message, the game start message, the player switch sword position message, the player attack message, the attack result message, etc. We used the event messaging system to decouple W10NetworkManager (W10 means week 10) and W10UIManager . The UI automatically changes when a \"UITextChange\" event is raised The process of making the game again proved the fact that making demo games is a good way to test our engine's features. Though we are doing some in-engine tests when implementing different systems, the game breaks a lot more than the in-engine tests do. Here are the bugs we found this time: The order of rotating an object is not correct. In the API, we allow the game developer to rotate the transform in euler angles. We will then convert the euler angles into the quaternion. However, due to the left-to-right multiplication order of matrices, we thought the quaternion should also be left multiplied. It turns out that quaternions are not matrices, so the order should just be yaw * pitch * roll. Otherwise, you will see an artifact like this: Your browser does not support the video tag. Entity::Destroy is still not working properly. This time, the entity is removed from the entity list while its children are not destroyed, and those childrens' parent pointers are still pointing to the deleted parent entity. This is not fixed yet but again, we put it into our to-do list. The prefix and postfix unary operations of our Array were reversed, which broke the priority queue of our event messaging system when we tried to queue multiple messages in the event queue. Also, we found some missing features when making the game: When we tried to change the parent of a NetworkTransform to another network entity on one machine, the changing of the parent didn't get synchronized through the network. Though the gameplay code can add more messages to notify all the clients, it is still a useful feature for the NetworkTransform since parent is a field of our Transform class. There was no way to force the NetworkTransform to be synced. This was problematic at the beginning of the game because if two NetworkTransform s were spawned at different positions, the position wouldn't get synced until it moved (when the first position update message was sent to the server). There was no way to \"force-snap\" the NetworkTransform . Sometimes in a game, we want to set the position, like pick-up behavior where you receive the item instantly or portals that teleport you across the map. However, with the networking module's interpolation mechanism, we would always be stuck with an interpolated position instead of a snapped one. Patch Notes \u00b6 Modifying Input with Modifier Keys \u00b6 Modifier keys are keys on the keyboard which can be pressed alongside other keys to alter the received input. The four modifier keys GLFW supports are CTRL , ALT , SHIFT , and SUPER . The reason we saw the need to add these was for any type of developer feature, you don't want to lose gameplay functionality of your keys when debugging, so with modifier keys you can an easier time debugging. While developing the EditorComponent we thought it would be a good idea to have shortcut keys; other developers might find the same need or will want to use them in a game, and it's no skin off our back to add the functionality since it's already supported by GLFW. This required us to refactor the InputModule to have key-modifier pairs rather than just keys, and then be able to search and remove based on those pairs. To keep things backwards compatible with the old system (the one without modifier keys), the Input class which holds the publicly available commands to access InputModule was changed to have a function for registering and unregistering keys with modifier keys and the old function definition, without modifier keys, was left unchanged with a 0 passed to the InputModule to signify no modifier key. Modifier keys can also be chained to form a mask with the bitwise-OR operator as well, to allow for even more possibilities! A Drawback to Preallocating All Your Memory \u00b6 Way back in the second week of the project, we talked about how our memory management would be all of our own and we would be keeping the dynamic allocations to a minimum. If we also want to avoid a whole bunch of context switches between user and kernel modes, then this means one thing in particular: We should preallocate a massive chunk of memory at startup that we use for the rest of our individual allocations. This is generally a good idea if you're creating your own memory allocation system because you can avoid unnecessary runtime overhead and you can reason about the actual physical layout of your memory to effectively utilize the memory cache. Up until this point in development, we've only seen good results from this too (outside of probably grabbing more memory than we need), but we've just run into the first major problem that this method brings with it: When accessing allocated memory, the system will not complain about your access privileges if you happen to overlap allocations . In our predicament, we were running the game like normal when all of a sudden, our free list memory would run out instantly! After about an hour of debugging, we determined that the last contiguous chunk of memory (or node) in the list was having its size field overwritten at some point. We used some data breakpoints to suss out the actual moment when size was being overwritten, and...it was when we were loading in audio clips. Yeah. You're probably as confused right now as we were at the time. As it turns out, we were working on an Array class which has a capacity field that defines how big the actual array is in memory (the max number of objects it can hold), and a size field that defines how many elements are inside of the array. We had mistakenly not increased the actual capacity when an element was pushed into Array , so even though the object was touching and changing the Nth element in its array, it really only owned the memory for N - M elements and it was changing the memory owned by another object or system without any warning or error . Normally, this would give an instant exception at the exact place where you try to access the memory, but because we technically own all of the memory, the system thinks the access is kosher and lets it go through. This was also not found in our testing because since we owned all the memory and nothing else was using the memory, it was alright to access the next chunk because nothing else was in that space. We won't be changing the foundational strategy to our memory management system anytime soon, but we will be more careful in the future regarding our memory accesses. This occurrence also gives us a pretty good reason to implement a memory tracking system such as the one C++ uses, but for now we're more wary of the performance hit that will cause us rather than the few times we'll run into this kind of bug again in the future. Initialization Timing and Memory Management \u00b6 Having a memory manager is very nice for the reasons stated in the portion above , and we've already seen considerable performance gains from our manager over using the regular new operator. We've even begun to switch over from the STL data structures to our own custom data structures that use our memory allocators! Not everything is peachy though, and even with a very robust memory management system, you won't quite get the flexibility of the new or malloc operations. Why? Because the C++ memory allocation system is always active. Our MemoryManager exists on top of a module within our game engine, meaning the game engine needs to boot up before we can do any memory management. This seems like it would be fine until you get to a pretty common concept in C++: Static variables. Static variable allocation happens when the variable is first found, so for class static variables, that's upon program load, and for function static variables, that's upon running the function for the first time. Static variable deallocation happens at program unload, so after the main function has run and returned. Now, if you think about it, where does the memory manager fit into all of this? Unfortunately it starts after program load and shuts down before program unload (as one might expect!). So if we want to clean things up nicely with our memory management system, then we can do two things: Add a StartUp and/or ShutDown function to every system with a static variable and handle the allocation/deallocation inside of those functions instead of inside of the constructors and destructors. Make our memory management system two phase, which would start up every globally needed system (like Config ) before anything else gets started, then would continue on like normal. Thing 1 deals with our static memory problem pretty nicely, whereas Thing 2 is mostly a nicety for keeping things in the correct order and might allow us to avoid using static for some systems. We've already started doing Thing 1 out of pure necessity, and if we find it convenient enough (i.e. more of our systems need to be started up \"first\"), then we may approach Thing 2 as well. Placement new for Arrays is Undefined \u00b6 When we were implementing the MemoryManager , we provided a NewArr function to allow us to get a continuous memory chunk from our stack memory or our free list memory. The way we do this is by calculating the address of that memory chunk and constructing the array in that specific address by using placement new . It worked well in most cases, but when we used it to allocate an array of strings, we found something very strange. Before we look into the strange things, let's go over the basic properties of a string. A naive implementation would require three fields: The pointer to the allocated memory (the characters), the size of the string (how many characters are we storing), and the capacity of the string (how many characters could we store). On a 64-bit system, the total size of a string object should be 24 bytes. However, MSVC 1 does some optimizations which expands the sizeof(std::string) to 40 bytes. The basic structure is some pointer (maybe pointing to the vtable 2 ) , the pointer to the allocated memory , the size and the capacity . Okay, let's first see how placement new works on a normal std::string object. #include <cstdlib> #include <string> using namespace std ; // just to keep things shorter here int main () { void * allocationPlace = malloc ( 4 * sizeof ( string )); string * str = new ( allocationPlace ) string { \"Testing the normal string\" }; free ( allocationPlace ); } In this code snippet, we first allocate a chunk of \"clean\" memory to test, then we construct a string holding \"Testing the normal string\" at the beginning of that memory chunk. After the placement new, the memory layout is like this: There's no overhead and no overflow. The string takes up exactly the whole 40 bytes. However, when we decide to allocate an array that only contains a single string, unexpected things happen. #include <cstdlib> #include <string> int main () { void * allocationPlace = malloc ( 4 * sizeof ( std :: string )); std :: string * str = new ( allocationPlace ) std :: string [ 1 ]; free ( allocationPlace ); } In theory, an array is the same as a pointer that points to the continuous data. It should have no overhead in memory. However, using the placement new statement, strange bytes are inserted before the string array. After several tests, we figured out that these bytes represent the length of the array. It seems that the placement new statement is adding an additional guard in the front of memory chunk, so if this guard is added every time the placement new constructs an array, it kind of makes sense for us to add an additional 8 bytes to hold the length of the array. Sadly, this doesn't make sense in C++-land. When we tried again to allocate an array of integers instead, there's no guard memory any more! Is there a way to tell whether the guard will be added or not? Not really. We asked Google for help, and from this StackOverflow thread , we learned that the offset may be different for every invocation of the array, so we can hardly use this in our precisely controlled memory manager. Is there a way to construct an array at the specific address without using placement new ? Fortunately, yes, and the solution turns out to be very simple. All we need to do is manually iterate through the array and construct (or allocate) every single element! It's not the most elegant solution, but it's straightforward and doesn't have any undefined behavior hidden inside of it. Debugging this memory issue was hard but very interesting. It also helped us to use Visual Studio's memory inspector to debug, which is a very good practice for us to keep up! Making the Network \"Local\" \u00b6 Originally, our NetworkTransform class checked the world position, rotation, and scale to know whether or not it should be sending any messages over the network. This was due to us wanting consistency for our NetworkTransform behavior; an object whose parent is scaled to 100 units being moved .001 units in local space will move the same distance as an object whose parent is scaled to 1 unit being moved 1 unit in local space, and the straightforward way to keep those distances matching is to just work in world space. Well, as it turns out, world space brings its own slew of messes. For some reason, we didn't test out our networked transforms on parented networked objects, and as you can probably now imagine, it was not just a disaster, but a very bandwidth-wasteful disaster! We decided to shift almost all of our calculations and data to local space afterwards, with the one exception being the case mentioned above; when we want to know whether or not we should be sending an update across the network, we quickly convert to world space and check that. This also got us to realize that we need a more robust set of tools to be doing network features in games. The first is that we should definitely have some form of \"parenting\" functions that utilize network IDs and network messages for keeping parenting synced across the network. The second is that we should be able to force sending a message (i.e. send a transform message across the network regardless of the position, rotation, and scale deltas) in the case that we need to guarantee correctness across the network. We're still iterating quite a bit on our network programming, so you'll no doubt see more things that we patch up in the coming weeks! Coming Soon \u00b6 What's to come in the next few weeks? The engine ideally. We have 2 weeks left before our intended feature lock, and we still have a lot to do... We aren't too, too worried, but just like the game industry, we foresee crunch coming our way in the very near future. We posted our interview with Raymond Graham this past week, and we definitely think you should check it out. We will posting our interview with Jeff Preshing , Elan Ruskin , and Jeet Shroff and Florian Strauss as soon as we can, which will be very soon because we will be releasing a small book of all of our interviews in the coming weeks! Resources \u00b6 Not much was added in the resource section this week, but it still remains a great source. We even find ourselves going there to see what the others on the team are using, especially when debugging systems that aren't our own. If you have any resource that you think we should take a look at, please let us know! Originally Published November 13, 2018. Microsoft Visual C++ ( MSVC ) is an integrated development environment (IDE) for writing and debugging C and C++ programming languages. \u21a9 Virtual table ( vtable ) is a lookup table of functions used to resolve function calls. It's mostly used in C++ inheritance. \u21a9","title":"[Week 10] Grab A Bucket"},{"location":"blogs/week-10/#grab-a-bucket","text":"","title":"Grab A Bucket"},{"location":"blogs/week-10/#byte-sized-updates","text":"Graphics : Added in functionality to the texture class, so it now does more than just loading and can actually be used! Editor Component : Abstracted some previously developed tools into an editor component which is already proving valuable for development! Custom Dynamic Array : We were originally not planning on creating our own data structures and just relying on STL but we gave into the temptation and it revealed some dark secrets. Second Game : We made our second game (which is not a twin stick shooter) and it works! Patch Notes : There's typically not a header for patch notes, but this week we found some really interesting bugs, mainly involving memory, which we think were great learning experiences. Progress may have slowed due to our trip to LA, but we can assure you it wasn't for nothing. Our initial plan to go out there was decided months back; we had a full lineup of industry professionals who agreed to do interviews and look at our engine progress. Of course, in the fast-moving game industry, the best laid plans don't always come together. A number of our planned interviewees had to drop out at the last minute, which left us in a bit of a panic. We reached out to about a dozen other contacts in LA to try and find some other people we could talk with. One of those contacts was at Sony's Santa Monica Studios, and he was able to get us in touch with Jeet Shroff and Florian Strauss, gameplay and technical directors on the latest God of War . What's more, we still had an interview locked in with Insomniac Games' senior engine programmer and ETC alumni Elan Ruskin . We also finally got to meet up and talk shop with Cort Stratton of Unity, who has been supporting us all semester long. Because these were all on-site talks, we were able to get a lot of clear answers to our questions and more direct feedback. While the extra downtime wasn't ideal, it did give us an opportunity to catch up with our classmates who were in LA on internships and new jobs. Ultimately, the trip was more productive for the team than expected. In terms of development, be prepared to see talk about memory throughout the blog because we were leaking memory left, right, and center!","title":"Byte-Sized Updates"},{"location":"blogs/week-10/#graphics","text":"","title":"Graphics"},{"location":"blogs/week-10/#texture","text":"Looking back at Week 5 , we realized we needed to be able to load textures outside just our graphics module, specifically for GUI images. As stated then, ImGui isn't an image loading library, so our images had to be loaded into our engine from another module. Horde3D has texture loading functionality, but how the functionality worked wasn't apparent or well documented. After struggling with Horde3D for too long, we had the texture in BGRA stored in a uint8_t . The ImGui library still needed the image in the renderer's format. The renderer we're using is OpenGL, so the the texture needs to be loaded into OpenGL memory\u2014ImGui provides an example of how to do this, however the sample code alone won't load the texture. Not knowing OpenGL well enough, we ended up spinning up a completely isolated project with a simple OpenGL scene to test texture loading. Following this tutorial , we were able to get the textures loading right away but there was a lot of unneeded code, so we just had to find which parts of the texture loading were necessary. After a quick bit of debugging, we figured out that the texture filtering line was what was stopping the image from being loaded into the engine. In other news, we made the brilliant decision to let our color-blind programmer work on the texture loading, so we almost missed the fact that our red and blue channels were getting flipped! Luckily, he got another developer involved and the bug was found before the code was committed. The texture class originally was set a static loader, but was changed to hold specific file data, height, and width. This texture can then just be passed directly into the GUI system, rather than passing data and size separately. Here is a colored image loaded into the GUI of the engine! Because we are using Horde3D to load the texture, it is storing it as a resource in its resource map and attempts to destroy the texture on Horde's destruction. We originally ran into an issue when trying to destroy the texture ourselves because we were trying to delete something we weren't the owners of. A simple call to Horde3D to release the memory allowed us to implement a Texture::Unload function, and we learned a good lesson on remembering to keep track of who really owns what.","title":"Texture"},{"location":"blogs/week-10/#editor-component","text":"Last week , we spoke about developing a console, and in the past we had developed smaller tools like an inspector and hierarchy. These tools were developed out of necessity, and we can see a game developer who uses our engine finding them useful, so we thought we should wrap all these tools in an EditorComponent available as an engine component. When we went to do this, we realized the inspector and hierarchy code were embedded in the Transform and Entity , which meant the developer was always stuck with them. They weren't even wrapped in a preprocessor conditional statement! After some refactoring, we were able to pull each of those GUI code blocks into their own component, which also allows for additional flexibility in the future. Honestly, though, the inspector, hierarchy, and console shouldn't be components but rather some type of editor code, because it doesn't make sense to attach them to entities within the scene. However, adding in an editor type to support this would certainly lead down a rabbit-hole of creating an editor, which we can't afford to do. So in the end, we really did decide to package the three components together into an EditorComponent with a menu and key commands to open. The key commands to open the windows prompted us to include modifier keys support for the keyboard.","title":"Editor Component"},{"location":"blogs/week-10/#custom-dynamic-array","text":"This past week, we created a custom vector class labeled Array so as not to be confused with Vector3 and our other math classes. The main reason for recreating std::vector was it is one of the most heavily used data structures in our engine yet it doesn't use our memory allocation, and that wasn't sitting well for us. Creating the basic functionality of the class was very straightforward, and using the STL vector as a guide for functions and naming, the implementation was completed relatively quickly. However, to fully mimic the functionality of the std::vector , we needed to implement iterators, which wasn't as straightforward as it seems. We didn't find any amazing resource on the subject, so it's still something we feel shaky on within our Array class. Learning from past mistakes, we tried to do a good unit test coverage of the whole class, and that helped us iron out the easy-to-find/fix bugs (like using prefix operators instead of postfix operators with sizing/capacity changes). This was also before the Array was integrated into the engine, so changes didn't cause massive recompile times\u2014always a plus for rapid development! Unfortunately, as you might expect, once our Array class replaced the STL vectors in the engine, we found way more bugs. Most of the bugs actually lead us to find bugs within our memory management, and for that, they deserve their own sections: A Drawback to Preallocating All Your Memory , Initialization Timing and Memory Management , and Placement New for Arrays is Undefined . These were fairly involved bugs with our memory, so we are glad we ended up implementing the custom vector class. We also still have other memory issues that we weren't able to solve this week (or even understand yet) which we will be writing about in future weeks. One of the issues we found from testing but wasn't resolved until actually running the engine was our move operator for the Array . We were testing the move assignment operator in the test suite which worked as expected; the contents were moved over and everything looked good. However, what wasn't being shown during tests was the move constructor of Array . This function was actually performing a copy rather than an actual move with the memory, so the Array originally holding the contents still had access to the contents. This means that, when one object was being deleted, the other would also have its contents removed\u2014they were linked! We discovered this when we would return an Array from a function. The local copy would perform a move operation to store the contents in the return statement, then it would delete itself, which would in turn delete the returned variable (which shared the same slot of memory) as well. Fortunately it was a simple fix, all we had to do was fix the move constructor, but tracking this down required a little more thought with regards to C++.","title":"Custom Dynamic Array"},{"location":"blogs/week-10/#second-game","text":"As mentioned in a previous blog post , one of our major milestones is that we made a \"real\" game with our engine. This allows us to validate the features we have implemented as well as to find missing features we need. Last time, we made a twin stick shooter that is quite similar with our target game, but this time we wanted to make something different. The major differences between our current engine and the old engine are the networked entities, the event messaging system, and the polished entity system. Based on these new features, we want to make a simple multiplayer competitive game that can utilize those new features best. Finally, we decided to make a 3D fencing game like Nidhogg . Here is the playthrough video of the game: Your browser does not support the video tag. Here are the game features that utilize the new engine features implemented in the last couple weeks: We used NetworkTransform to sync the position of two networked players and their swords We used network callbacks to send different messages across the server and the clients, including the player ready message, the game start message, the player switch sword position message, the player attack message, the attack result message, etc. We used the event messaging system to decouple W10NetworkManager (W10 means week 10) and W10UIManager . The UI automatically changes when a \"UITextChange\" event is raised The process of making the game again proved the fact that making demo games is a good way to test our engine's features. Though we are doing some in-engine tests when implementing different systems, the game breaks a lot more than the in-engine tests do. Here are the bugs we found this time: The order of rotating an object is not correct. In the API, we allow the game developer to rotate the transform in euler angles. We will then convert the euler angles into the quaternion. However, due to the left-to-right multiplication order of matrices, we thought the quaternion should also be left multiplied. It turns out that quaternions are not matrices, so the order should just be yaw * pitch * roll. Otherwise, you will see an artifact like this: Your browser does not support the video tag. Entity::Destroy is still not working properly. This time, the entity is removed from the entity list while its children are not destroyed, and those childrens' parent pointers are still pointing to the deleted parent entity. This is not fixed yet but again, we put it into our to-do list. The prefix and postfix unary operations of our Array were reversed, which broke the priority queue of our event messaging system when we tried to queue multiple messages in the event queue. Also, we found some missing features when making the game: When we tried to change the parent of a NetworkTransform to another network entity on one machine, the changing of the parent didn't get synchronized through the network. Though the gameplay code can add more messages to notify all the clients, it is still a useful feature for the NetworkTransform since parent is a field of our Transform class. There was no way to force the NetworkTransform to be synced. This was problematic at the beginning of the game because if two NetworkTransform s were spawned at different positions, the position wouldn't get synced until it moved (when the first position update message was sent to the server). There was no way to \"force-snap\" the NetworkTransform . Sometimes in a game, we want to set the position, like pick-up behavior where you receive the item instantly or portals that teleport you across the map. However, with the networking module's interpolation mechanism, we would always be stuck with an interpolated position instead of a snapped one.","title":"Second Game"},{"location":"blogs/week-10/#patch-notes","text":"","title":"Patch Notes"},{"location":"blogs/week-10/#modifying-input-with-modifier-keys","text":"Modifier keys are keys on the keyboard which can be pressed alongside other keys to alter the received input. The four modifier keys GLFW supports are CTRL , ALT , SHIFT , and SUPER . The reason we saw the need to add these was for any type of developer feature, you don't want to lose gameplay functionality of your keys when debugging, so with modifier keys you can an easier time debugging. While developing the EditorComponent we thought it would be a good idea to have shortcut keys; other developers might find the same need or will want to use them in a game, and it's no skin off our back to add the functionality since it's already supported by GLFW. This required us to refactor the InputModule to have key-modifier pairs rather than just keys, and then be able to search and remove based on those pairs. To keep things backwards compatible with the old system (the one without modifier keys), the Input class which holds the publicly available commands to access InputModule was changed to have a function for registering and unregistering keys with modifier keys and the old function definition, without modifier keys, was left unchanged with a 0 passed to the InputModule to signify no modifier key. Modifier keys can also be chained to form a mask with the bitwise-OR operator as well, to allow for even more possibilities!","title":"Modifying Input with Modifier Keys"},{"location":"blogs/week-10/#a-drawback-to-preallocating-all-your-memory","text":"Way back in the second week of the project, we talked about how our memory management would be all of our own and we would be keeping the dynamic allocations to a minimum. If we also want to avoid a whole bunch of context switches between user and kernel modes, then this means one thing in particular: We should preallocate a massive chunk of memory at startup that we use for the rest of our individual allocations. This is generally a good idea if you're creating your own memory allocation system because you can avoid unnecessary runtime overhead and you can reason about the actual physical layout of your memory to effectively utilize the memory cache. Up until this point in development, we've only seen good results from this too (outside of probably grabbing more memory than we need), but we've just run into the first major problem that this method brings with it: When accessing allocated memory, the system will not complain about your access privileges if you happen to overlap allocations . In our predicament, we were running the game like normal when all of a sudden, our free list memory would run out instantly! After about an hour of debugging, we determined that the last contiguous chunk of memory (or node) in the list was having its size field overwritten at some point. We used some data breakpoints to suss out the actual moment when size was being overwritten, and...it was when we were loading in audio clips. Yeah. You're probably as confused right now as we were at the time. As it turns out, we were working on an Array class which has a capacity field that defines how big the actual array is in memory (the max number of objects it can hold), and a size field that defines how many elements are inside of the array. We had mistakenly not increased the actual capacity when an element was pushed into Array , so even though the object was touching and changing the Nth element in its array, it really only owned the memory for N - M elements and it was changing the memory owned by another object or system without any warning or error . Normally, this would give an instant exception at the exact place where you try to access the memory, but because we technically own all of the memory, the system thinks the access is kosher and lets it go through. This was also not found in our testing because since we owned all the memory and nothing else was using the memory, it was alright to access the next chunk because nothing else was in that space. We won't be changing the foundational strategy to our memory management system anytime soon, but we will be more careful in the future regarding our memory accesses. This occurrence also gives us a pretty good reason to implement a memory tracking system such as the one C++ uses, but for now we're more wary of the performance hit that will cause us rather than the few times we'll run into this kind of bug again in the future.","title":"A Drawback to Preallocating All Your Memory"},{"location":"blogs/week-10/#initialization-timing-and-memory-management","text":"Having a memory manager is very nice for the reasons stated in the portion above , and we've already seen considerable performance gains from our manager over using the regular new operator. We've even begun to switch over from the STL data structures to our own custom data structures that use our memory allocators! Not everything is peachy though, and even with a very robust memory management system, you won't quite get the flexibility of the new or malloc operations. Why? Because the C++ memory allocation system is always active. Our MemoryManager exists on top of a module within our game engine, meaning the game engine needs to boot up before we can do any memory management. This seems like it would be fine until you get to a pretty common concept in C++: Static variables. Static variable allocation happens when the variable is first found, so for class static variables, that's upon program load, and for function static variables, that's upon running the function for the first time. Static variable deallocation happens at program unload, so after the main function has run and returned. Now, if you think about it, where does the memory manager fit into all of this? Unfortunately it starts after program load and shuts down before program unload (as one might expect!). So if we want to clean things up nicely with our memory management system, then we can do two things: Add a StartUp and/or ShutDown function to every system with a static variable and handle the allocation/deallocation inside of those functions instead of inside of the constructors and destructors. Make our memory management system two phase, which would start up every globally needed system (like Config ) before anything else gets started, then would continue on like normal. Thing 1 deals with our static memory problem pretty nicely, whereas Thing 2 is mostly a nicety for keeping things in the correct order and might allow us to avoid using static for some systems. We've already started doing Thing 1 out of pure necessity, and if we find it convenient enough (i.e. more of our systems need to be started up \"first\"), then we may approach Thing 2 as well.","title":"Initialization Timing and Memory Management"},{"location":"blogs/week-10/#placement-new-for-arrays-is-undefined","text":"When we were implementing the MemoryManager , we provided a NewArr function to allow us to get a continuous memory chunk from our stack memory or our free list memory. The way we do this is by calculating the address of that memory chunk and constructing the array in that specific address by using placement new . It worked well in most cases, but when we used it to allocate an array of strings, we found something very strange. Before we look into the strange things, let's go over the basic properties of a string. A naive implementation would require three fields: The pointer to the allocated memory (the characters), the size of the string (how many characters are we storing), and the capacity of the string (how many characters could we store). On a 64-bit system, the total size of a string object should be 24 bytes. However, MSVC 1 does some optimizations which expands the sizeof(std::string) to 40 bytes. The basic structure is some pointer (maybe pointing to the vtable 2 ) , the pointer to the allocated memory , the size and the capacity . Okay, let's first see how placement new works on a normal std::string object. #include <cstdlib> #include <string> using namespace std ; // just to keep things shorter here int main () { void * allocationPlace = malloc ( 4 * sizeof ( string )); string * str = new ( allocationPlace ) string { \"Testing the normal string\" }; free ( allocationPlace ); } In this code snippet, we first allocate a chunk of \"clean\" memory to test, then we construct a string holding \"Testing the normal string\" at the beginning of that memory chunk. After the placement new, the memory layout is like this: There's no overhead and no overflow. The string takes up exactly the whole 40 bytes. However, when we decide to allocate an array that only contains a single string, unexpected things happen. #include <cstdlib> #include <string> int main () { void * allocationPlace = malloc ( 4 * sizeof ( std :: string )); std :: string * str = new ( allocationPlace ) std :: string [ 1 ]; free ( allocationPlace ); } In theory, an array is the same as a pointer that points to the continuous data. It should have no overhead in memory. However, using the placement new statement, strange bytes are inserted before the string array. After several tests, we figured out that these bytes represent the length of the array. It seems that the placement new statement is adding an additional guard in the front of memory chunk, so if this guard is added every time the placement new constructs an array, it kind of makes sense for us to add an additional 8 bytes to hold the length of the array. Sadly, this doesn't make sense in C++-land. When we tried again to allocate an array of integers instead, there's no guard memory any more! Is there a way to tell whether the guard will be added or not? Not really. We asked Google for help, and from this StackOverflow thread , we learned that the offset may be different for every invocation of the array, so we can hardly use this in our precisely controlled memory manager. Is there a way to construct an array at the specific address without using placement new ? Fortunately, yes, and the solution turns out to be very simple. All we need to do is manually iterate through the array and construct (or allocate) every single element! It's not the most elegant solution, but it's straightforward and doesn't have any undefined behavior hidden inside of it. Debugging this memory issue was hard but very interesting. It also helped us to use Visual Studio's memory inspector to debug, which is a very good practice for us to keep up!","title":"Placement new for Arrays is Undefined"},{"location":"blogs/week-10/#making-the-network-local","text":"Originally, our NetworkTransform class checked the world position, rotation, and scale to know whether or not it should be sending any messages over the network. This was due to us wanting consistency for our NetworkTransform behavior; an object whose parent is scaled to 100 units being moved .001 units in local space will move the same distance as an object whose parent is scaled to 1 unit being moved 1 unit in local space, and the straightforward way to keep those distances matching is to just work in world space. Well, as it turns out, world space brings its own slew of messes. For some reason, we didn't test out our networked transforms on parented networked objects, and as you can probably now imagine, it was not just a disaster, but a very bandwidth-wasteful disaster! We decided to shift almost all of our calculations and data to local space afterwards, with the one exception being the case mentioned above; when we want to know whether or not we should be sending an update across the network, we quickly convert to world space and check that. This also got us to realize that we need a more robust set of tools to be doing network features in games. The first is that we should definitely have some form of \"parenting\" functions that utilize network IDs and network messages for keeping parenting synced across the network. The second is that we should be able to force sending a message (i.e. send a transform message across the network regardless of the position, rotation, and scale deltas) in the case that we need to guarantee correctness across the network. We're still iterating quite a bit on our network programming, so you'll no doubt see more things that we patch up in the coming weeks!","title":"Making the Network \"Local\""},{"location":"blogs/week-10/#coming-soon","text":"What's to come in the next few weeks? The engine ideally. We have 2 weeks left before our intended feature lock, and we still have a lot to do... We aren't too, too worried, but just like the game industry, we foresee crunch coming our way in the very near future. We posted our interview with Raymond Graham this past week, and we definitely think you should check it out. We will posting our interview with Jeff Preshing , Elan Ruskin , and Jeet Shroff and Florian Strauss as soon as we can, which will be very soon because we will be releasing a small book of all of our interviews in the coming weeks!","title":"Coming Soon"},{"location":"blogs/week-10/#resources","text":"Not much was added in the resource section this week, but it still remains a great source. We even find ourselves going there to see what the others on the team are using, especially when debugging systems that aren't our own. If you have any resource that you think we should take a look at, please let us know! Originally Published November 13, 2018. Microsoft Visual C++ ( MSVC ) is an integrated development environment (IDE) for writing and debugging C and C++ programming languages. \u21a9 Virtual table ( vtable ) is a lookup table of functions used to resolve function calls. It's mostly used in C++ inheritance. \u21a9","title":"Resources"},{"location":"blogs/week-11/","text":"The One Where the Team Picks a Font \u00b6 Byte-Sized Updates \u00b6 Collision Solving : Implemented rudimentary collision solving, but still have lots of distance to cover for a decent collision response system. Level Loading : Got loading from startup level to another prebuilt loader in the engine working. There were a few catching points, but overall went smoothly. Asset Processing Tool : Built an asset processing tool to process the Collada files ( .dae ) more efficiently, easier than in the past. GUI : Added more to the GUI system by fully integrating fonts, so font size and type can be changed per GUI function. AI : Implemented an AI pathfinding system by combining vector flow fields and steering behaviors. Full-Feature Game Demo : Started development of the target game that the engine is being built for, which forced some other smaller features. Collision Solving \u00b6 We started collision detection all the way back in week 6 , and it's been growing pretty nicely. At this point, we have fully functional sphere and box collisions and partially functional capsule collisions, which means we can trigger functions in the game in response to when some collider volume enters, exits, or stays inside of another collider volume, which is immensely helpful with a lot of game concepts like trigger points. Unfortunately, figuring out that the collider volumes are intersecting is only half the battle. Now that we've got the collisions getting detected, we need to take it one step further and move colliders away from their counterparts whenever an intersection occurs. This is called collision solving , and it's a crucial concept in video games, where things don't physically inhabit space and so we can't rely on the convenience of physics to help us keep things real. Depending on who you ask, there are three distinct parts of collision solving: The detection. This is where you figure out that two colliders are intersecting with one another, and it's the part that we (mostly) already have figured out. The resolution. This is where you take the two colliders and figure out where exactly they should be moved to so that they are no longer intersecting. This takes the previous problem of collision detection and expands upon it, because we now need not just the fact that the colliders are intersecting but also info on where exactly they're intersecting. The iteration. Now, you technically don't need this one, but you probably don't have a very good generalized solution if you have no need for iteration in your collision solver. Iteration is used in collision solving to determine that your resolutions are acceptable. Let's illustrate with an example: Take three sphere colliders. Two of them start intersecting like this, and the other one is just barely off to the side: After we've resolved the first collision, this is what we end up with: We technically got the first collision resolved, but now we have another one! You could just do one solve every render frame and let this play out over time, but to decrease the number of intersecting colliders at the end of each frame, you can perform multiple iterations of collision resolution every frame. Collision solving is going to be a difficult feature to implement. This is not only because it's a pretty difficult 3D math problem, but also because it's heavily reliant on our previous features, namely collision detection. Bear this in mind as we go on to build up this feature; this may be the first time we see serious compromise in one of our engine systems due to the feature being needed yet out of scope. Getting Started \u00b6 While the 3D math is difficult, the conceptual organization behind collision solving isn't. The important tasks we need to accomplish are (1) figuring out which colliders are actually colliding, (2) figure out where they're colliding on each other, and (3) move them. The first one is very expensive if you perform an O(n^2) check of all colliders against all other colliders, but fortunately as part of our previous collision detection work we implemented a Dynamic Bounding Volume Tree , which is essentially a sorted tree of axis-aligned bounding boxes (AABB) that allows you to cull out most of your collision tests in sparse systems. Unfortunately once we've begun the collision solving process, our colliders will have moved so our Bounding Volume Tree will have been invalidated and we need to regenerate it, but for the time being we're going to ignore that problem. We do plan to address that eventually with a neat little solution using a feature we've already implemented with our Bounding Volume Tree. So with the first task already out of the way, we can focus on tasks number 2 and 3! Task number 2 is unfortunately still pretty daunting though. To get started, we broke up our collision solving into a case-by-case basis; every individual collider will generate its collision point based on its own shape (i.e. Box, Sphere, or Capsule) regardless of the other collider its intersecting with. This isn't terribly correct because it assumes that our other collider is radially symmetric and that's only true for spheres, but we should first focus on getting our collision points calculated in the first place. For spheres, calculating the intersection point in this case is trivial\u2014just find the direction of the other collider and get the point out at the radius. For boxes, we need to check each axis individually to construct the correct intersection point. It seems pretty straightforward at first, but there's actually some tricky parts to it. The naive solution is to check if the point falls outside of the extents of the box on any of the box's local axes, and if so, clamp it to the extents on that axis. That works if our point is outside of the box, but if it's inside of the box, then our collision point doesn't get calculated on the surface of the box\u2014it's calculated on the inside! For the time being we don't have this corrected, but in the next week we'll be implementing a solution that will guarantee at least one of the axes is pushed out to the extents so that we have our intersection point on the surface of the box. Lastly, for capsules, the problem of finding an intersection point may seem daunting since capsules look more complex than spheres or boxes, but in reality it's not that hard. First, we need to calculate the closest point on the \"line segment\" of the capsule (the inner axis of the capsule that caps off at both radial ends) to our point of interest, which we use our CollisionsModule::ClosestPtPointSegment function for. For the next part, we initially checked if that nearest point on either of the caps of the capsule and restricted the intersection point to certain areas if it was. As it turns out, you don't need to do this! With a line segment, the vector between your point and the line will always be perpendicular to the line if the nearest point isn't on one of its ends, like so: That covers our first pass of work for task number 2; now let's take a swing at task number 3. A very naive solution for moving the colliders once their intersections are solved is to just take the difference vector between the two intersection points and split that between the two colliders (also accounting for any static objects that should be immovable). This indeed works, but we'll discuss later why this isn't correct. AABB Disarray \u00b6 With our three tasks done, our collision solving should now work, right? Well, earlier we mentioned that the collision solving system relies on our other systems, and here's where all of that came to light! Our AABB's had a few bugs that made the collisions pretty messy and hard to observe, such as accidentally checking against the x value of the AABB's twice instead of also checking the y values during the intersection test, or regenerating each AABB every frame and making our Bounding Volume Tree inaccurate and much more expensive. While we were doing that, we also optimized out some expensive function calls by just using cached data, which is a very common and typically effective optimization trick! With all of that ironed out and out of the way, let's check our progress on that collision solving! Your browser does not support the video tag. You know what? We'll take it as a win. But we clearly have more work to do. How Does Unity Do It? \u00b6 Following the same pattern as we have before, once we realized that our system doesn't quite meet expectations, we did some field research. And by field research, we mean we opened Unity and tried to reverse engineer its own systems. With the following video, you can actually see some interesting behavior that we didn't consider before. The red objects are movable and the white ones are completely immovable, so we're focusing on how the red ones are moved by the collisions: Your browser does not support the video tag. For our collision solving, the box collisions are notably the poorest as every collider will push away from its face and into one of the corners. In Unity's case, the collisions behave as you'd expect for the most part; when something is being pushed by a box, that thing gets pushed away from the face of the box. What's more interesting is when the box is at an angle to another box\u2014the box whose corner penetrates the other's face is pushed in the direction of the face. This case is annoying to account for because what exactly is the correct solution when a wedge is shoved into something? Do you push it on one side or the other? This got us thinking about our \"generalized\" collision solving algorithm and why it was failing so poorly. Ours takes the difference between the two intersection points and displaces the colliders based on that, but that means that whenever colliders are not perfectly aligned with one another, they'll gradually push one another out of the way\u2014even if they're two box colliders that are oriented exactly the same. So our solution for task number 3, moving the colliders, is incorrect. So what's the correct solution then? Our investigation of Unity's collisions contains the answer. A box collider shoves all other colliders in the direction of its face except for when its wedge is the part that is colliding with the other collider . In the case that the wedge is colliding, then the box collider actually gets moved based on the other collider. This means that the collision resolution is performed on a case-by-case basis, and we need to figure in how the box colliders collide with other colliders to arrive at a proper solution. A Case-by-Case Solution \u00b6 Our second stab at collision resolution is still not very elegant, but it's getting closer to a workable solution. What we know now is that our collisions should be resolved on a case-by-case basis, with the following ranking: (1) box face, (2) spheres and capsules, (3) box wedge. We only need to be concerned with box colliders for now; if another collider has some similar problem, we'll deal with it after getting some correct box collisions. Our basic algorithm for the new solution is to check the types of the colliders involved in the collision first, and if either collider is a box, then we need to check whether or not one of the boxes collided on its edge. If they did, then the collision resolution is determined by the other collider. In the case that the box collider actually collides on its face, then we need to figure out which face it was that collided and push in the direction of its normal. With that implemented, our solution actually isn't looking too terrible! Your browser does not support the video tag. Future Plans with Collision Solving \u00b6 Right now, we're currently building up a better suite of tooling to test our collisions and collision solving. We just recently added some controls to our Bounding Volume Tree testing level so that we can more easily figure out what's going on when we have over 100 colliders flying around in one level. Your browser does not support the video tag. Every second is a beautiful frame of expensively contrived behavior. For the next steps, we've got a few ideas. For one, our box collider intersection detection is incorrect because it can put the intersection point inside of the box, and we have a pretty good idea of how to address that. Our Bounding Volume Tree is also incorrect after the first iteration of our collision solver because we shift around the colliders without regenerating the tree; we could technically regenerate the tree, but that's very expensive, so instead we plan on trying some heuristics for the AABB's to avoid having to ever recalculate the Bounding Volume Tree despite multiple iterations of collision solving. Lastly, our capsule collisions have always looks pretty bad, and the collision resolution for them definitely isn't right, so we'll probably be taking a look into that as well. We would love to do some optimization, but as it stands we can run 50+ colliders in our level at about 60FPS and our target game won't need that many colliders, so optimization isn't exactly a high priority task. Level Loading \u00b6 Level loading as an engine functionality is simply to be able to unload a level and load another level without the engine systems shutting down (or breaking, crashing, exploding...). Our implementation isn't a sophisticated approach and limits the developer to only having one level loaded at any given point, nor can there be persistent entities, so it could definitely be improved upon. The game developer can call LevelManager::LoadLevel at any point during a frame, then the current frame will finish processing all variable updates, and at the end of the frame the level is unloaded and the next level starts to load. The unload level functionality was already established to prevent memory leakage when shutting down the engine, so we relied on that method heavily. The Level::UnloadLevel worked fine for shutting down the engine, but bugs were found in other components that weren't associated with leaking memory but instead with functionality, so we couldn't observe them after the engine/application closed. The main bugs of the level loading system were that GUI entities seemed to be infinitely drawn, our debug drawing system's view matrix was corrupted, and mesh models were not being removed. All of these pointed towards something to do with rendering, and while debugging these problems we found that resizing the viewport relieved the issue for a while, but wasn't the source of the problem. We were able to deduce that it wasn't just the GUI or rendering systems by testing different level loads, so what we're looking for is likely something they share in common: The \"clear screen\" function call and camera. The screen clearing didn't immediately seem to be the problem, so we watched how the camera was being set in the RenderModule . Here, the RenderModule selects the first camera in the list, and in most levels there is only one camera to select from. When we set a breakpoint to watch this region of code, we found that each level's camera was stacking! In CameraComponent::OnEnable , the camera is added to RenderModule 's list of cameras for the level; however, CameraComponent::OnDisable wasn't removing it from the list. This can also be found from deactivating the camera component, which confirms that as the source of our problems. Once this was fixed, all of those problems went away! Another problem we found during our level loading development was from our input system. Input wasn't automatically getting removed for each level. When we took a closer look at our input system, it was clear that we weren't unregistering key bindings; also, deleting the entity was causing undefined functionality, which is another great side-effect enabled by our own memory system. To fix this, we just needed to clear the input in preparation for the next level. As a result of level loading, we found the need to separate the engine input and gameplay input so that we can clear our input callback map on changing levels, discussed in the Patch Notes . The output of loading and unloading the levels from the above video. Your browser does not support the video tag. Asset Processing Tool \u00b6 The current asset pipeline is focused around Horde3D's needs for models, animations, and materials. The models are processed into .geo and .scene.xml files, the animations into .anim , and the materials into .material.xml files. The other file types aren't compressed or packaged in any other file format, although they probably should to be for better loading speeds by the engine and to yield a smaller memory footprint. Alas, we don't have time for that, so let's just deal with Horde3D's formats. Horde3DUtil has a COLLADA conversion application that processes .dae (COLLADA) files to these other file formats, but requires the command line to determine which file to process. In weeks prior, we created a simple batch script that process all .dae files in the folder and subfolder of the script into model and animation files. The problem we have now is that, regardless of if the COLLADA file has mesh or animation data, it will produce geo , scene.xml , and .anim files as well as processing all models, even if they haven't been updated or changed since last processing. This script is great to quickly get things done but isn't great for small tweaks or keeping files/memory to a minimum. We created a tool in WPF to select which file you want processed and what type of output the file should produce. It searches for the base 'Resource' directory, which is required by Horde3D, and everything is based off of that folder. This is what the tool looks like: We realize that other engines like the Arc80 Engine require more tools for cooking assets so that they can have more control of the data. However, with such a small amount of time, we are just making small, quick tools that will provide us with the most benefit. GUI \u00b6 Fonts \u00b6 After we added the awesome gameplay that we have in our second game, something really popped out to us; the text was tiny and boring. The team had been contemplating whether or not we should put the effort into getting fonts into the engine, but after seeing this game really hit the nail in the head, we decided to give it a try! We had already integrated some font functionality into our engine. ...Or so we thought. Soon after we tried to get fonts working, we realized the font functionality wasn't nearly good enough\u2014or even working, really. So we had to rework most of it. Originally the Isetta::Font class was just another name for ImFont , but this also means that the imgui header would need to be included in any file of the game which would like to do anything with font. This felt wrong; it's okay for our GUI class to be included in these source files, but we don't want our external library to be included. To bypass this, we created a \"fake\" class named Font which derived from imgui's ImFont as well as some static helper functions. The static helper functions help the fonts being stored, so we don't need to reload the fonts and can access shared fonts more easily among multiple classes. This task was fairly easy to implement and well worth the effort\u2014we were able to change the score display of our first game to something much prettier! AI Pathfinding \u00b6 AI is not necessarily a part of a game engine. Instead, it's more like a replaceable gameplay system built above the underlying game engine, which is why we have AI almost at the top of our architecture diagram. But also, if a game engine is mostly designed for one game or one type of game, some of the AI code can be baked into the game engine. Since our team is quite interested in implementing an AI system, and our game also needs a simple pathfinding algorithm to guide the enemies to avoid obstacles and move towards the players, we decided to implement the AI pathfinding system inside of our engine. A* Versus Vector Field \u00b6 There are a bunch of pathfinding algorithms in the game AI field, from breadth-first search 1 and Dijkstra's algorithm 2 to all sorts of A* 3 derivations. How can we find one that fits our game best? Let's look back to what our game AI requires: Our game is a top-down twin-stick shooter. Though the game is made in 3D, the players and the enemies can only move in the xz plane. Our game can have hundreds of enemies moving independently, and to reduce the pressure of collision detection and solving, the enemies cannot collide with each other. Our game environment is fixed. The obstacles on the map will not move and there's no level streaming. Based on these requirements, we make these conclusions: Since the environment is fixed, we can process it before the game starts. One of the most common pre-processing methods is to make the map into a grid. Because we can have quite a lot of units calculating their path simultaneously in the same frame, the computation complexity can be significantly higher when it's based on each unit rather than when it's based on each grid cell. Even if enemies don't have collisions between each other, we still don't want them always following the same path and overlap with each other, so outputting a fuzzy path is better than one single best solution. After these considerations, we found that the common A* pathfinding may not be the best one for our game, because it computes separate paths for individual agents even \\ though many paths may have similar sections, and it always returns a fixed, heuristically optimized solution. With further research, we found an alternative method, which is to combine vector flow fields 4 with steering behaviors 5 . It can greatly reduce the computation complexity as well as efficiently simulate the hundreds of units on the map. Building the Vector Field \u00b6 The concept of this pathfinding algorithm is quite straightforward. Since the navigation space is divided into a grid, our calculation is mostly based on the grid cells. The purpose of building a vector field is to calculate what direction a unit should go based on its position and cell relative to its target. To do that, we generally need to build two matrices: The cost matrix and the vector matrix. The cost matrix is a matrix that stores the path distance from each cell to the target cell. It's not the Euclidean distance which calculates the distance regardless the obstacles. Instead, the path distance is how many cells one unit needs to go through to arrive at the target. We are using a wavefront algorithm to generate the cost matrix, which is basically a breadth-first search that updates its neighbors by its path distance plus one. After the cost matrix generation, we now have a grid like this: Your browser does not support the video tag. The vector matrix is also quite simple. It is a matrix that each cell points to the gradient of the grid, which is to point to the neighboring cell with the lowest cost (don't forget about the diagonal case!). After this process, the navigation grid looks like this: Your browser does not support the video tag. One thing that is quite interesting here is when we are adding the sides of the obstacles into the grid, marking the corresponding cells as un-walkable cells, we were actually using the line drawing algorithm we learned from a computer graphics course at our university. This is really an unexpected way we found to relate AI to subjects other than robotics. Integrating Steering Behaviors \u00b6 Once the vector flow field is calculated, we can know the target direction of each cell by looking up the vector matrix. The last thing we need to do is to naturally steer the units to match the target direction. The algorithm here we used is the velocity matching algorithm 6 and the arriving algorithm 7 . It takes the acceleration, the velocity and the Euclidean distance from the unit to the goal into account and drives the unity smoothly. After the integration, we now have the units approaching the target like this: game ai Your browser does not support the video tag. There are still some minor features like multiple target support we haven't implemented yet. They will be patched into the engine next week. Full-Featured Game Demo \u00b6 This past week, we started implementing features of the game we set out to build from the beginning . This game will have slightly more polish than either of the previous games (or other games we end up building) because we'd like this to be a good demonstration of what the engine is capable. As explained in Week 0, the gameplay itself won't be all that fun or exciting, but it will be the capstone of the engine. So far, we've implemented the main menu and most of the UI needed for the game. The main menu needs to be able to transition the player into single player mode, multiplayer mode, and exiting the game. The multiplayer mode needs to allow the player to opt into hosting a game or to connect to another host player. For the hosting option, this is when we will boot up the server (which has yet to be done for the game), but for the connection option the player must be able to input the IP address of the host. The first iteration of our IP address input system aims for looks over functionality. Using the idea that all IPv4 8 addresses are 15 digits long, so the IP address can be formatted as xxx.xxx.xxx.xxx . Then when the user types, the x 's can be replaced with the number and the . won't change. Which turned out like this: Your browser does not support the video tag. Then some of the team commented on that IP addresses aren't always 15 digits, as the numbers are actually 0 to 255 and don't need to fill 3 digits, i.e. 000 for 0. So even though the IP addresses could be represented by 15 characters with four sets of three characters, it is inconvenient to ask players to type extra characters if their IP address doesn't use the hundreds or tens places. So removing the nicety of the x 's, we just went back to the plain implementation. When the single player button is clicked, the game then loads the single player level which has the basic health and score UI. The game could have been done in one level by just turning the UI off once the game starts, but then there would menu entities in the game level\u2014this is what forced us to develop the level loading system. Here is what we have so far! Your browser does not support the video tag. Patch Notes \u00b6 Separating Engine and Gameplay Input \u00b6 The original implementation of the Input and InputModule system was to have specific callback maps for each type of input: mouse, key press/release, etc. However, when the GUIModule system was integrated, the GLFWwindow parameter of the input callback functions was needed for the GUI . We were hiding the GLFWwindow parameter from the input callbacks given to the gameplay developer, so we needed a second set of functions with the GLFWwindow as a parameter. This worked for a while, but it definitely added some confusion about which function the game developer should be subscribing to because they could technically subscribe to the GLFW callback functions. We were also exporting the entire Input class to our DLL. We could have added the \"export to DLL\" attribute to only the needed functions, but that would be annoying for each additional function. When we introduced level loading, we were clearing all memory of the previous level, which included anything encapsulated by entities within the level but did not include our input system. If a developer forgot to unregister their input callbacks, then the callback would most likely cause an exception because it would reference deleted memory. We even noticed internally that with some of our own components/levels, even we\u2014the masterful, all-knowing engine developers\u2014were forgetting to unregister our input callbacks. How could we expect the gameplay developers to remember? The answer was we couldn't, which is why we decided to just clear all input callbacks of the past level when unloading the level. We didn't want to clear the GUIModule connection to the Input however, and if a developer had used the GLFW callbacks, we couldn't clear that callback. So the only way to prevent them from using the GLFW callback functions was to move those into their own class, GLFWInput , which wouldn't be cleared or destroyed until ShutDown . GLFWInput is subsequently not exposed to the game developer and only used for engine internal input; the added benefit here is that there is now less confusion for the game developer of which input callback to subscribe to. Coming Soon \u00b6 As far as the engine goes, our original feature lock was this past Friday! As you might be able to guess, we're not quite ready to lock down the engine yet, but we sure are close! One more week might allow us to put the finishing touches on our engine systems. (But this might be a busy week!) We posted our interview with Martin Middleton this past week, which you should definitely check out. We will be posting our interview with Jeff Preshing this week, and in the next couple of weeks we will be putting up several more. In related news, we're submitting our interviews for publication this week, so hopefully we'll have more news on that next time you hear from us! Resources \u00b6 Not much was added in the resource section this week, but it still remains a great source. We even find ourselves going there to see what the others on the team are using, especially when debugging systems that aren't our own. If you have any resource that you think we should take a look at, please let us know! Originally Published November 19, 2018. Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures; it starts at the tree root and explores all neighbor nodes before looking at children nodes. \u21a9 Dijkstra's algorithm is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks. It's most common variant finds the shortest path by accumulating distances between node pairs in the path. \u21a9 A* (pronounced \"A star\") is an algorithm that is widely used in pathfinding and graph traversal. It is an extension of Dijkstra's algorithm that also uses heuristics to guide its search for better performance. \u21a9 A vector field is an assignment of a vector to each point in a subset of space, such as a plan with a collection of arrows with a given magnitude and direction. Vector fields are often used to model the speed and direction of a moving fluid like a river. \u21a9 A steering behavior is a way of applying various forces to a unit's movement to generate realistic behaviors like chasing, fleeing, following, and more. \u21a9 For more about velocity matching algorithm, please read Artificial Intelligence For Games Page 66 \u21a9 For more about arrive algorithm, please read Artificial Intelligence For Games Page 66 \u21a9 Internet Protocol version 4 ( IPv4 ) is the fourth version of the Internet Protocol (IP). It is one of the core protocols of standards-based internetworking methods in the Internet, and does not guarantee delivery or proper sequencing. \u21a9","title":"[Week 11] The One Where the Team Picks a Font"},{"location":"blogs/week-11/#the-one-where-the-team-picks-a-font","text":"","title":"The One Where the Team Picks a Font"},{"location":"blogs/week-11/#byte-sized-updates","text":"Collision Solving : Implemented rudimentary collision solving, but still have lots of distance to cover for a decent collision response system. Level Loading : Got loading from startup level to another prebuilt loader in the engine working. There were a few catching points, but overall went smoothly. Asset Processing Tool : Built an asset processing tool to process the Collada files ( .dae ) more efficiently, easier than in the past. GUI : Added more to the GUI system by fully integrating fonts, so font size and type can be changed per GUI function. AI : Implemented an AI pathfinding system by combining vector flow fields and steering behaviors. Full-Feature Game Demo : Started development of the target game that the engine is being built for, which forced some other smaller features.","title":"Byte-Sized Updates"},{"location":"blogs/week-11/#collision-solving","text":"We started collision detection all the way back in week 6 , and it's been growing pretty nicely. At this point, we have fully functional sphere and box collisions and partially functional capsule collisions, which means we can trigger functions in the game in response to when some collider volume enters, exits, or stays inside of another collider volume, which is immensely helpful with a lot of game concepts like trigger points. Unfortunately, figuring out that the collider volumes are intersecting is only half the battle. Now that we've got the collisions getting detected, we need to take it one step further and move colliders away from their counterparts whenever an intersection occurs. This is called collision solving , and it's a crucial concept in video games, where things don't physically inhabit space and so we can't rely on the convenience of physics to help us keep things real. Depending on who you ask, there are three distinct parts of collision solving: The detection. This is where you figure out that two colliders are intersecting with one another, and it's the part that we (mostly) already have figured out. The resolution. This is where you take the two colliders and figure out where exactly they should be moved to so that they are no longer intersecting. This takes the previous problem of collision detection and expands upon it, because we now need not just the fact that the colliders are intersecting but also info on where exactly they're intersecting. The iteration. Now, you technically don't need this one, but you probably don't have a very good generalized solution if you have no need for iteration in your collision solver. Iteration is used in collision solving to determine that your resolutions are acceptable. Let's illustrate with an example: Take three sphere colliders. Two of them start intersecting like this, and the other one is just barely off to the side: After we've resolved the first collision, this is what we end up with: We technically got the first collision resolved, but now we have another one! You could just do one solve every render frame and let this play out over time, but to decrease the number of intersecting colliders at the end of each frame, you can perform multiple iterations of collision resolution every frame. Collision solving is going to be a difficult feature to implement. This is not only because it's a pretty difficult 3D math problem, but also because it's heavily reliant on our previous features, namely collision detection. Bear this in mind as we go on to build up this feature; this may be the first time we see serious compromise in one of our engine systems due to the feature being needed yet out of scope.","title":"Collision Solving"},{"location":"blogs/week-11/#getting-started","text":"While the 3D math is difficult, the conceptual organization behind collision solving isn't. The important tasks we need to accomplish are (1) figuring out which colliders are actually colliding, (2) figure out where they're colliding on each other, and (3) move them. The first one is very expensive if you perform an O(n^2) check of all colliders against all other colliders, but fortunately as part of our previous collision detection work we implemented a Dynamic Bounding Volume Tree , which is essentially a sorted tree of axis-aligned bounding boxes (AABB) that allows you to cull out most of your collision tests in sparse systems. Unfortunately once we've begun the collision solving process, our colliders will have moved so our Bounding Volume Tree will have been invalidated and we need to regenerate it, but for the time being we're going to ignore that problem. We do plan to address that eventually with a neat little solution using a feature we've already implemented with our Bounding Volume Tree. So with the first task already out of the way, we can focus on tasks number 2 and 3! Task number 2 is unfortunately still pretty daunting though. To get started, we broke up our collision solving into a case-by-case basis; every individual collider will generate its collision point based on its own shape (i.e. Box, Sphere, or Capsule) regardless of the other collider its intersecting with. This isn't terribly correct because it assumes that our other collider is radially symmetric and that's only true for spheres, but we should first focus on getting our collision points calculated in the first place. For spheres, calculating the intersection point in this case is trivial\u2014just find the direction of the other collider and get the point out at the radius. For boxes, we need to check each axis individually to construct the correct intersection point. It seems pretty straightforward at first, but there's actually some tricky parts to it. The naive solution is to check if the point falls outside of the extents of the box on any of the box's local axes, and if so, clamp it to the extents on that axis. That works if our point is outside of the box, but if it's inside of the box, then our collision point doesn't get calculated on the surface of the box\u2014it's calculated on the inside! For the time being we don't have this corrected, but in the next week we'll be implementing a solution that will guarantee at least one of the axes is pushed out to the extents so that we have our intersection point on the surface of the box. Lastly, for capsules, the problem of finding an intersection point may seem daunting since capsules look more complex than spheres or boxes, but in reality it's not that hard. First, we need to calculate the closest point on the \"line segment\" of the capsule (the inner axis of the capsule that caps off at both radial ends) to our point of interest, which we use our CollisionsModule::ClosestPtPointSegment function for. For the next part, we initially checked if that nearest point on either of the caps of the capsule and restricted the intersection point to certain areas if it was. As it turns out, you don't need to do this! With a line segment, the vector between your point and the line will always be perpendicular to the line if the nearest point isn't on one of its ends, like so: That covers our first pass of work for task number 2; now let's take a swing at task number 3. A very naive solution for moving the colliders once their intersections are solved is to just take the difference vector between the two intersection points and split that between the two colliders (also accounting for any static objects that should be immovable). This indeed works, but we'll discuss later why this isn't correct.","title":"Getting Started"},{"location":"blogs/week-11/#aabb-disarray","text":"With our three tasks done, our collision solving should now work, right? Well, earlier we mentioned that the collision solving system relies on our other systems, and here's where all of that came to light! Our AABB's had a few bugs that made the collisions pretty messy and hard to observe, such as accidentally checking against the x value of the AABB's twice instead of also checking the y values during the intersection test, or regenerating each AABB every frame and making our Bounding Volume Tree inaccurate and much more expensive. While we were doing that, we also optimized out some expensive function calls by just using cached data, which is a very common and typically effective optimization trick! With all of that ironed out and out of the way, let's check our progress on that collision solving! Your browser does not support the video tag. You know what? We'll take it as a win. But we clearly have more work to do.","title":"AABB Disarray"},{"location":"blogs/week-11/#how-does-unity-do-it","text":"Following the same pattern as we have before, once we realized that our system doesn't quite meet expectations, we did some field research. And by field research, we mean we opened Unity and tried to reverse engineer its own systems. With the following video, you can actually see some interesting behavior that we didn't consider before. The red objects are movable and the white ones are completely immovable, so we're focusing on how the red ones are moved by the collisions: Your browser does not support the video tag. For our collision solving, the box collisions are notably the poorest as every collider will push away from its face and into one of the corners. In Unity's case, the collisions behave as you'd expect for the most part; when something is being pushed by a box, that thing gets pushed away from the face of the box. What's more interesting is when the box is at an angle to another box\u2014the box whose corner penetrates the other's face is pushed in the direction of the face. This case is annoying to account for because what exactly is the correct solution when a wedge is shoved into something? Do you push it on one side or the other? This got us thinking about our \"generalized\" collision solving algorithm and why it was failing so poorly. Ours takes the difference between the two intersection points and displaces the colliders based on that, but that means that whenever colliders are not perfectly aligned with one another, they'll gradually push one another out of the way\u2014even if they're two box colliders that are oriented exactly the same. So our solution for task number 3, moving the colliders, is incorrect. So what's the correct solution then? Our investigation of Unity's collisions contains the answer. A box collider shoves all other colliders in the direction of its face except for when its wedge is the part that is colliding with the other collider . In the case that the wedge is colliding, then the box collider actually gets moved based on the other collider. This means that the collision resolution is performed on a case-by-case basis, and we need to figure in how the box colliders collide with other colliders to arrive at a proper solution.","title":"How Does Unity Do It?"},{"location":"blogs/week-11/#a-case-by-case-solution","text":"Our second stab at collision resolution is still not very elegant, but it's getting closer to a workable solution. What we know now is that our collisions should be resolved on a case-by-case basis, with the following ranking: (1) box face, (2) spheres and capsules, (3) box wedge. We only need to be concerned with box colliders for now; if another collider has some similar problem, we'll deal with it after getting some correct box collisions. Our basic algorithm for the new solution is to check the types of the colliders involved in the collision first, and if either collider is a box, then we need to check whether or not one of the boxes collided on its edge. If they did, then the collision resolution is determined by the other collider. In the case that the box collider actually collides on its face, then we need to figure out which face it was that collided and push in the direction of its normal. With that implemented, our solution actually isn't looking too terrible! Your browser does not support the video tag.","title":"A Case-by-Case Solution"},{"location":"blogs/week-11/#future-plans-with-collision-solving","text":"Right now, we're currently building up a better suite of tooling to test our collisions and collision solving. We just recently added some controls to our Bounding Volume Tree testing level so that we can more easily figure out what's going on when we have over 100 colliders flying around in one level. Your browser does not support the video tag. Every second is a beautiful frame of expensively contrived behavior. For the next steps, we've got a few ideas. For one, our box collider intersection detection is incorrect because it can put the intersection point inside of the box, and we have a pretty good idea of how to address that. Our Bounding Volume Tree is also incorrect after the first iteration of our collision solver because we shift around the colliders without regenerating the tree; we could technically regenerate the tree, but that's very expensive, so instead we plan on trying some heuristics for the AABB's to avoid having to ever recalculate the Bounding Volume Tree despite multiple iterations of collision solving. Lastly, our capsule collisions have always looks pretty bad, and the collision resolution for them definitely isn't right, so we'll probably be taking a look into that as well. We would love to do some optimization, but as it stands we can run 50+ colliders in our level at about 60FPS and our target game won't need that many colliders, so optimization isn't exactly a high priority task.","title":"Future Plans with Collision Solving"},{"location":"blogs/week-11/#level-loading","text":"Level loading as an engine functionality is simply to be able to unload a level and load another level without the engine systems shutting down (or breaking, crashing, exploding...). Our implementation isn't a sophisticated approach and limits the developer to only having one level loaded at any given point, nor can there be persistent entities, so it could definitely be improved upon. The game developer can call LevelManager::LoadLevel at any point during a frame, then the current frame will finish processing all variable updates, and at the end of the frame the level is unloaded and the next level starts to load. The unload level functionality was already established to prevent memory leakage when shutting down the engine, so we relied on that method heavily. The Level::UnloadLevel worked fine for shutting down the engine, but bugs were found in other components that weren't associated with leaking memory but instead with functionality, so we couldn't observe them after the engine/application closed. The main bugs of the level loading system were that GUI entities seemed to be infinitely drawn, our debug drawing system's view matrix was corrupted, and mesh models were not being removed. All of these pointed towards something to do with rendering, and while debugging these problems we found that resizing the viewport relieved the issue for a while, but wasn't the source of the problem. We were able to deduce that it wasn't just the GUI or rendering systems by testing different level loads, so what we're looking for is likely something they share in common: The \"clear screen\" function call and camera. The screen clearing didn't immediately seem to be the problem, so we watched how the camera was being set in the RenderModule . Here, the RenderModule selects the first camera in the list, and in most levels there is only one camera to select from. When we set a breakpoint to watch this region of code, we found that each level's camera was stacking! In CameraComponent::OnEnable , the camera is added to RenderModule 's list of cameras for the level; however, CameraComponent::OnDisable wasn't removing it from the list. This can also be found from deactivating the camera component, which confirms that as the source of our problems. Once this was fixed, all of those problems went away! Another problem we found during our level loading development was from our input system. Input wasn't automatically getting removed for each level. When we took a closer look at our input system, it was clear that we weren't unregistering key bindings; also, deleting the entity was causing undefined functionality, which is another great side-effect enabled by our own memory system. To fix this, we just needed to clear the input in preparation for the next level. As a result of level loading, we found the need to separate the engine input and gameplay input so that we can clear our input callback map on changing levels, discussed in the Patch Notes . The output of loading and unloading the levels from the above video. Your browser does not support the video tag.","title":"Level Loading"},{"location":"blogs/week-11/#asset-processing-tool","text":"The current asset pipeline is focused around Horde3D's needs for models, animations, and materials. The models are processed into .geo and .scene.xml files, the animations into .anim , and the materials into .material.xml files. The other file types aren't compressed or packaged in any other file format, although they probably should to be for better loading speeds by the engine and to yield a smaller memory footprint. Alas, we don't have time for that, so let's just deal with Horde3D's formats. Horde3DUtil has a COLLADA conversion application that processes .dae (COLLADA) files to these other file formats, but requires the command line to determine which file to process. In weeks prior, we created a simple batch script that process all .dae files in the folder and subfolder of the script into model and animation files. The problem we have now is that, regardless of if the COLLADA file has mesh or animation data, it will produce geo , scene.xml , and .anim files as well as processing all models, even if they haven't been updated or changed since last processing. This script is great to quickly get things done but isn't great for small tweaks or keeping files/memory to a minimum. We created a tool in WPF to select which file you want processed and what type of output the file should produce. It searches for the base 'Resource' directory, which is required by Horde3D, and everything is based off of that folder. This is what the tool looks like: We realize that other engines like the Arc80 Engine require more tools for cooking assets so that they can have more control of the data. However, with such a small amount of time, we are just making small, quick tools that will provide us with the most benefit.","title":"Asset Processing Tool"},{"location":"blogs/week-11/#gui","text":"","title":"GUI"},{"location":"blogs/week-11/#fonts","text":"After we added the awesome gameplay that we have in our second game, something really popped out to us; the text was tiny and boring. The team had been contemplating whether or not we should put the effort into getting fonts into the engine, but after seeing this game really hit the nail in the head, we decided to give it a try! We had already integrated some font functionality into our engine. ...Or so we thought. Soon after we tried to get fonts working, we realized the font functionality wasn't nearly good enough\u2014or even working, really. So we had to rework most of it. Originally the Isetta::Font class was just another name for ImFont , but this also means that the imgui header would need to be included in any file of the game which would like to do anything with font. This felt wrong; it's okay for our GUI class to be included in these source files, but we don't want our external library to be included. To bypass this, we created a \"fake\" class named Font which derived from imgui's ImFont as well as some static helper functions. The static helper functions help the fonts being stored, so we don't need to reload the fonts and can access shared fonts more easily among multiple classes. This task was fairly easy to implement and well worth the effort\u2014we were able to change the score display of our first game to something much prettier!","title":"Fonts"},{"location":"blogs/week-11/#ai-pathfinding","text":"AI is not necessarily a part of a game engine. Instead, it's more like a replaceable gameplay system built above the underlying game engine, which is why we have AI almost at the top of our architecture diagram. But also, if a game engine is mostly designed for one game or one type of game, some of the AI code can be baked into the game engine. Since our team is quite interested in implementing an AI system, and our game also needs a simple pathfinding algorithm to guide the enemies to avoid obstacles and move towards the players, we decided to implement the AI pathfinding system inside of our engine.","title":"AI Pathfinding"},{"location":"blogs/week-11/#a-versus-vector-field","text":"There are a bunch of pathfinding algorithms in the game AI field, from breadth-first search 1 and Dijkstra's algorithm 2 to all sorts of A* 3 derivations. How can we find one that fits our game best? Let's look back to what our game AI requires: Our game is a top-down twin-stick shooter. Though the game is made in 3D, the players and the enemies can only move in the xz plane. Our game can have hundreds of enemies moving independently, and to reduce the pressure of collision detection and solving, the enemies cannot collide with each other. Our game environment is fixed. The obstacles on the map will not move and there's no level streaming. Based on these requirements, we make these conclusions: Since the environment is fixed, we can process it before the game starts. One of the most common pre-processing methods is to make the map into a grid. Because we can have quite a lot of units calculating their path simultaneously in the same frame, the computation complexity can be significantly higher when it's based on each unit rather than when it's based on each grid cell. Even if enemies don't have collisions between each other, we still don't want them always following the same path and overlap with each other, so outputting a fuzzy path is better than one single best solution. After these considerations, we found that the common A* pathfinding may not be the best one for our game, because it computes separate paths for individual agents even \\ though many paths may have similar sections, and it always returns a fixed, heuristically optimized solution. With further research, we found an alternative method, which is to combine vector flow fields 4 with steering behaviors 5 . It can greatly reduce the computation complexity as well as efficiently simulate the hundreds of units on the map.","title":"A* Versus Vector Field"},{"location":"blogs/week-11/#building-the-vector-field","text":"The concept of this pathfinding algorithm is quite straightforward. Since the navigation space is divided into a grid, our calculation is mostly based on the grid cells. The purpose of building a vector field is to calculate what direction a unit should go based on its position and cell relative to its target. To do that, we generally need to build two matrices: The cost matrix and the vector matrix. The cost matrix is a matrix that stores the path distance from each cell to the target cell. It's not the Euclidean distance which calculates the distance regardless the obstacles. Instead, the path distance is how many cells one unit needs to go through to arrive at the target. We are using a wavefront algorithm to generate the cost matrix, which is basically a breadth-first search that updates its neighbors by its path distance plus one. After the cost matrix generation, we now have a grid like this: Your browser does not support the video tag. The vector matrix is also quite simple. It is a matrix that each cell points to the gradient of the grid, which is to point to the neighboring cell with the lowest cost (don't forget about the diagonal case!). After this process, the navigation grid looks like this: Your browser does not support the video tag. One thing that is quite interesting here is when we are adding the sides of the obstacles into the grid, marking the corresponding cells as un-walkable cells, we were actually using the line drawing algorithm we learned from a computer graphics course at our university. This is really an unexpected way we found to relate AI to subjects other than robotics.","title":"Building the Vector Field"},{"location":"blogs/week-11/#integrating-steering-behaviors","text":"Once the vector flow field is calculated, we can know the target direction of each cell by looking up the vector matrix. The last thing we need to do is to naturally steer the units to match the target direction. The algorithm here we used is the velocity matching algorithm 6 and the arriving algorithm 7 . It takes the acceleration, the velocity and the Euclidean distance from the unit to the goal into account and drives the unity smoothly. After the integration, we now have the units approaching the target like this: game ai Your browser does not support the video tag. There are still some minor features like multiple target support we haven't implemented yet. They will be patched into the engine next week.","title":"Integrating Steering Behaviors"},{"location":"blogs/week-11/#full-featured-game-demo","text":"This past week, we started implementing features of the game we set out to build from the beginning . This game will have slightly more polish than either of the previous games (or other games we end up building) because we'd like this to be a good demonstration of what the engine is capable. As explained in Week 0, the gameplay itself won't be all that fun or exciting, but it will be the capstone of the engine. So far, we've implemented the main menu and most of the UI needed for the game. The main menu needs to be able to transition the player into single player mode, multiplayer mode, and exiting the game. The multiplayer mode needs to allow the player to opt into hosting a game or to connect to another host player. For the hosting option, this is when we will boot up the server (which has yet to be done for the game), but for the connection option the player must be able to input the IP address of the host. The first iteration of our IP address input system aims for looks over functionality. Using the idea that all IPv4 8 addresses are 15 digits long, so the IP address can be formatted as xxx.xxx.xxx.xxx . Then when the user types, the x 's can be replaced with the number and the . won't change. Which turned out like this: Your browser does not support the video tag. Then some of the team commented on that IP addresses aren't always 15 digits, as the numbers are actually 0 to 255 and don't need to fill 3 digits, i.e. 000 for 0. So even though the IP addresses could be represented by 15 characters with four sets of three characters, it is inconvenient to ask players to type extra characters if their IP address doesn't use the hundreds or tens places. So removing the nicety of the x 's, we just went back to the plain implementation. When the single player button is clicked, the game then loads the single player level which has the basic health and score UI. The game could have been done in one level by just turning the UI off once the game starts, but then there would menu entities in the game level\u2014this is what forced us to develop the level loading system. Here is what we have so far! Your browser does not support the video tag.","title":"Full-Featured Game Demo"},{"location":"blogs/week-11/#patch-notes","text":"","title":"Patch Notes"},{"location":"blogs/week-11/#separating-engine-and-gameplay-input","text":"The original implementation of the Input and InputModule system was to have specific callback maps for each type of input: mouse, key press/release, etc. However, when the GUIModule system was integrated, the GLFWwindow parameter of the input callback functions was needed for the GUI . We were hiding the GLFWwindow parameter from the input callbacks given to the gameplay developer, so we needed a second set of functions with the GLFWwindow as a parameter. This worked for a while, but it definitely added some confusion about which function the game developer should be subscribing to because they could technically subscribe to the GLFW callback functions. We were also exporting the entire Input class to our DLL. We could have added the \"export to DLL\" attribute to only the needed functions, but that would be annoying for each additional function. When we introduced level loading, we were clearing all memory of the previous level, which included anything encapsulated by entities within the level but did not include our input system. If a developer forgot to unregister their input callbacks, then the callback would most likely cause an exception because it would reference deleted memory. We even noticed internally that with some of our own components/levels, even we\u2014the masterful, all-knowing engine developers\u2014were forgetting to unregister our input callbacks. How could we expect the gameplay developers to remember? The answer was we couldn't, which is why we decided to just clear all input callbacks of the past level when unloading the level. We didn't want to clear the GUIModule connection to the Input however, and if a developer had used the GLFW callbacks, we couldn't clear that callback. So the only way to prevent them from using the GLFW callback functions was to move those into their own class, GLFWInput , which wouldn't be cleared or destroyed until ShutDown . GLFWInput is subsequently not exposed to the game developer and only used for engine internal input; the added benefit here is that there is now less confusion for the game developer of which input callback to subscribe to.","title":"Separating Engine and Gameplay Input"},{"location":"blogs/week-11/#coming-soon","text":"As far as the engine goes, our original feature lock was this past Friday! As you might be able to guess, we're not quite ready to lock down the engine yet, but we sure are close! One more week might allow us to put the finishing touches on our engine systems. (But this might be a busy week!) We posted our interview with Martin Middleton this past week, which you should definitely check out. We will be posting our interview with Jeff Preshing this week, and in the next couple of weeks we will be putting up several more. In related news, we're submitting our interviews for publication this week, so hopefully we'll have more news on that next time you hear from us!","title":"Coming Soon"},{"location":"blogs/week-11/#resources","text":"Not much was added in the resource section this week, but it still remains a great source. We even find ourselves going there to see what the others on the team are using, especially when debugging systems that aren't our own. If you have any resource that you think we should take a look at, please let us know! Originally Published November 19, 2018. Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures; it starts at the tree root and explores all neighbor nodes before looking at children nodes. \u21a9 Dijkstra's algorithm is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks. It's most common variant finds the shortest path by accumulating distances between node pairs in the path. \u21a9 A* (pronounced \"A star\") is an algorithm that is widely used in pathfinding and graph traversal. It is an extension of Dijkstra's algorithm that also uses heuristics to guide its search for better performance. \u21a9 A vector field is an assignment of a vector to each point in a subset of space, such as a plan with a collection of arrows with a given magnitude and direction. Vector fields are often used to model the speed and direction of a moving fluid like a river. \u21a9 A steering behavior is a way of applying various forces to a unit's movement to generate realistic behaviors like chasing, fleeing, following, and more. \u21a9 For more about velocity matching algorithm, please read Artificial Intelligence For Games Page 66 \u21a9 For more about arrive algorithm, please read Artificial Intelligence For Games Page 66 \u21a9 Internet Protocol version 4 ( IPv4 ) is the fourth version of the Internet Protocol (IP). It is one of the core protocols of standards-based internetworking methods in the Internet, and does not guarantee delivery or proper sequencing. \u21a9","title":"Resources"},{"location":"blogs/week-12/","text":"Getting Ready for Game Jam! \u00b6 Byte-Sized Updates \u00b6 Audio : Refactored the audio system to be more versatile and usable, as well as added in 3D audio. Primitives : Game entity primitives were created for the developer to be able to quickly instantiate a cube and other shapes, without having to make the models themselves. Config Editor Tool : A tool was developed to help developers modify the configuration file, without knowing apriori what property they want to modify. Build : We created a tool for exporting the header files of the engine, as well as other needed files and folders to be used in a standalone project. Patch Notes : We have a lot. Turns out as you begin to use your engine features you will find many more bugs than your original tests. For the past week or so, and particularly this week, we have shifted some of our focus from adding big systems/features to making some smaller style of life changes, better called fixes to API. In this week especially, you will read about how we changed some of our API not for performance but to help game developers better be able to use the engine. One of the main reasons for this shift in focus is because we are holding a game jam on December 1 st using exclusively our engine! It is going to be a short, short game jam only running about 8 hours because we are going to using our fellow students to see what breaks. We know things will probably, almost definitely, break but we are excited to see what some other developers can do with our engine, that's why you make an engine to begin with right? Audio \u00b6 There was a much needed update to our audio module this week\u2014the addition of positional (spatial) audio, which then triggered some other refactors. There are two components to spatial audio: The position in space where the audio is being played and the position where the \"ears\", the listening object, are. Starting with the listener, we added a new component named AudioListener which is almost an empty component that adds/removes itself from a listener list in AudioModule during its OnEnable / OnDisable . In AudioModule 's update then, prior to updating the FMODSystem , the transform of the first listener in the list is used to position FMOD's listener. If no listener exists, the 3D sound will be heard as 2D. If a tree falls in the wood and there's no listener to hear it, in Isetta, it will be like you're standing right next to it. Having multiple listeners is supported by FMOD in some sense; however, this could cause additional confusion for the game developer, so we decided not to support it. That was the easy part, now for the 3D positioned sources. Since the audio was one of the first modules and features implemented in the engine it had some legacy code that needed to be refactored to keep the engine more consistent. Implementing the actual 3D audio was as simple as telling FMOD that the audio was 3D and updating FMODs understanding of the position of each 3D AudioSource . However, there were also other FMOD features like muting the audio and setting the hearing minimum and maximum distance to implement so we can make the audio more versatile, plus how hard could it be to support that... It turns out implementing them isn't so hard but making sure that different configurations don't break the engine or FMOD isn't as easy. Talking more about the legacy changes of AudioSource , all the audio was loaded by specifying the file paths in the config file from the resources folder which was then loaded on engine startup. To use the audio, the AudioSource then required the same filepath as specified in the config file. This was very limiting and tedious because every file had to be specified in two places: the configuration file and the place you wanted to use it, thus it's prone to issues. Our first change was to create a new struct, AudioClip which would house the loaded FMODSound which then could have additional audio files loaded from outside the config file. Each AudioSource would then hold a reference to an AudioClip rather than an individual FMODSound . However, the config files loaded on start up seemed unnecessary and added confusion for even just us so we just removed that completely. AudioSource was also allowed to have a string on construction which would find the corresponding AudioClip , but this again wasn't intuitive and caused errors even internally, so we replaced that constructor with one that would only take AudioClip s. AudioClip s would be the only thing in our engine to know of the audio's file path, everything else would have an AudioClip reference. These are small usability changes, however we are finding as we are building more small tech demos and our feature demo, that usability is a larger concern on the user-facing components than it was for our low-level systems and needs more iteration not for bug fixing, although there is still a lot of that, for easier use. Scene Primitives \u00b6 Rapid iteration is a hot-topic when developing a game, and while we are under no fallacy using our engine would be good for rapid development, we do want to make things easier for us and any possible users. One way for us to do this is making the instantiation of primitives such as cubes, cylinders, capsules, grids, quads, and spheres as easy as possible. All of these shapes make slapping together a prototype or testing a feature much easier. Prior to this endeavor we only had cube and sphere meshes available to use, and a user still needed to know where exactly those \".scene.xml/.geo\" files were in the directory structure. Since the MeshComponent s require a file path, typing a path out for each object would be annoying. Plus, these mesh files are in the engine resources not the game resources, and there isn't a good way for the developer to specify that. We spent some time getting reacquainted with Blender to make these simple shapes, and the only thing we focused on was shape size and vertex/poly count. We wanted the sizes to be similar, if not identical to Unity's, so that people who have used Unity would have an understanding of how to use the objects, these sizes are also versatile in use. In Unity when you create one of these objects there is also the assumption you want a collider attached. While that makes sense for Unity, our collision system isn't as robust so we didn't want a bunch of unused colliders to be attached, plus then a developer might delete it off the entity right after instantiation\u2014what a waste of performance! So for our primitive system, you can optionally specify if you want a collider attached. The system also caused us to change our entity creation system, see in our patch notes . Config Editor Tool \u00b6 With everyone on the team starting to use the engine to make mini games, more and more there's been this need to know what values are configurable. One of our developers has been itching to do some tool development for the engine, so when another team member asked for something that was all he needed. Knowing not much time could be spent towards creating tools versus engine development, the tool is fairly simple. The goal of the tool was to display all of the CVar variables in the engine, with their default values, and be able to load a config file as well as save a user-generated one. The main advantage to this tool isn't necessarily speed, because editing a text file will always be faster, but being able to see all the different options the developer has. First, it has the user specify the directory of the engine, and if none is specified it uses the current application directory. It then searches through all the files of the directory, using the command prompt, doing a findstr 1 , similar to a Linux grep 2 , for any CVar variable. It then groups the variables based on the file the variable was found, i.e. if found in AudioModule.h it will group it to AudioModule . Each group and its CVar property is then displayed with a value field for the user to modify. The value field defaults to the default value of the CVar so if the user decides to not change the value, they can rely on that the engine developer put a reasonable default value... The user can also load a configuration file of their choosing if they wish to only change a few of the values but keep the most of them the same. The properties and values can then be saved out to a filename and path of their choosing. Here is the application: Build \u00b6 Exporting Headers \u00b6 For people to be able to use the engine, without developing on the engine or building it through Visual Studio, the engine needs to have a downloadable folder/zip file containing the headers, dll files, and static library files of the engine. These files are scattered throughout the engine directory. Even worse, there are other files, .cpp files, in the same folders so we can't just copy the folders. The directory structure needs to be preserved as well, because the header files reference one another by path. With each change of a header file or build, the process needs to be repeated which could just be copying over the changed files, but we do a naive approach of exporting everything again. This isn't too bad for us since the files being copied are relatively small in size and number. Rather than having someone do this manually, we created a batch script (originally a shell script) to copy the files into a separate, identical folder structure. The batch file also copies over engine resources that are required for the engine such as the graphics pipelines, a default font, shaders, primitives, etc. as well as the external headers that are needed by our header files. It's a fairly simple script of copying files, but very necessary to ensure a game external to the main engine solution maintains an up-to-date version of the engine. Patch Notes \u00b6 Levels Menu \u00b6 Last week we wrote about level loading and we created a simple component that would transition to a level specified in the constructor. Since we are students, we need to show our work off to our faculty soon, which we thought would be nice to have a level select menu. We currently have no system for persistent objects, which is something we would like to add and don't think it would be too difficult but just don't have the bandwidth, so currently all input and entities are deleted on level unload. To browse all of our levels, we could add a level menu to each level but then we would need to remove the level menu component when done. Rather than altering all of our levels to have a level menu component we wanted to do a quick hack... Since we clear game developer input on level unload, we couldn't use that, but we can use our uncleared input, GLFWInput . This input is only available to use within the engine and isn't built out to the dll for game developers to use. When the GLFWInput callback happens on the level menu, it checks to see if there is an instance of level menu in the level and if not, creates one. We had to make the input registration handle of the component static as well so the input could be unsubscribed, otherwise the callback would happen multiple times on a single click. This is definitely a hack, but we're the engine developers what we say goes right? Debug Drawing Optimization \u00b6 We revisited our debug drawing code to see if we should bother adding in a shading model to our solid objects, the jury isn't out on that yet, but we noticed that a) we weren't taking full advantage of our transform system and b) the code had an easy optimization. First the optimization. As talked about with the original debug drawing post , the vertex shader uses the model view projection matrix to transform vertex points in world space to camera space. The original implementation had the model, view, and projection matrices passed in as separate uniform variables to the vertex shader which would then perform the matrix multiplication for each vertex. This was clearly wasteful because the matrix doesn't change for each vertex and had 2 matrix multiplications for each vertex that resulted in the same final matrix. Rather than passing 3 matrices to the shader we calculated the matrix then passed the matrix. This is an example of when incremental/developing code just gets left in and forgotten about. For using the transform system, we were calculating the inverse of the camera transform matrix each call, however our transform class caches that value so we can just use that. There are definitely other optimizations, not only for debug drawing, throughout the engine that we would love to do, but probably won't get the chance to. Capsule Height Bug \u00b6 When developing the primitives we found that the debug drawing capsule for the capsule collider was too large for the created Blender model. At first, we thought it was our bad Blender skills, but we then compared our model to a model in Unity and our scale was fine. We started by looking at CapsuleCollider::Update because the debug capsule on the primitive was coming from this component, but things looked fine there. But when looking into the debug drawing capsule draw, we realized the problem was in there -- we had never thought of checking the capsule size before! What was happening was the length of the capsule's centerline was being calculated with height and scale, then added and subtracted from the center point to get the two endpoints of the capsule. However, this would generate a capsule with double the height specified; the line length should have been halved then added/subtracted to get the endpoints with the actual height. This then scared us because how the heck was our collision code working with the busted capsule height... well that's because in the collision tests we had copied the concept of height from the debug drawing so that's why collision was working and we quickly replaced the code there too. We'd like to propose other solutions which would have stopped us from having to change the code twice. One would be to define a Capsule class which has no understanding of the debug drawing nor collisions then things could be passed along as needed. The other is from a GDC talk by Ludovic Chabant, where the schema used to define the capsules in the first place are defining the endpoints not a center and height; then expose it to the user as a height and center, this would avoid the problem from the beginning. Collider Debug Shearing \u00b6 Another problem we found with our Collider s has nothing to do with the actual collision model or calculation but rather how they are drawn again. We understand that ordering of matrix multiplication matters, and the typical convention of with transformation matrices is: translation x rotation x scale . However in Collider we had done translation x scale x rotation , we think we did this to avoid shearing in the case of a weird rotation but actually this causes shearing! We are going to attribute this to a lapse of judgment and just made the fix, no one to be the wiser. API Design Improvements \u00b6 As the game jam approaches, we are thinking more and more about the usability and API design of our engine. The first thing we looked at is the GetTransform method in both Component and Entity classes, the typical getter method that only allows the developer to read from the property. This seemed clunky, especially from our experience with C# properties. To change this, one option would be making a public Transform* const variable and let the users access it. It seems like a good option, but C++ requires that we initialize all const member variables in the constructor\u2014meaning we need to pass a reference or pointer to Transform as a parameter into the constructor\u2014which is impossible in our engine because the users will inherit our base Component class and define their own constructors. Which is why, at the time, we chose the straightforward implementation of having a getter with the GetTransform method. However, as we were making more and more games, we realized that calling that method each time we want the transform is pretty annoying. So, we got rid of GetTransform in the end. We looked at our code structure and came up with a solution that's hacky but works great for now\u2014we created a static Transform* variable inside the Component class, and every time before constructing a new Component , we set that static variable to the Transform that the new Component should point to. Then, in Component 's default constructor, it can just use the static transform variable and initialize it's const Transform* member (see the code snippet below). It will be propagated to all derived constructors due to the inheritance. With this solution, we can just write transform->GetWorldPos instead of GetTransform().GetWorldPos , so much cleaner! After this fix, we did the same thing for Component 's Entity* . class Entity { template < typename T , typename ... Args > T * Entity :: AddComponent ( Args && ... args ) { // ... // Set the data so the next Component can pick them up in constructor Component :: curEntity = this ; Component :: curTransform = transform ; T * component = MemoryManager :: NewOnFreeList < T > ( std :: forward < Args > ( args )...); // ... } } class Component { Component :: Component () : entity { curEntity }, transform ( curTransform ) {} static class Entity * curEntity ; static class Transform * curTransform ; } However, this solution has a big potential downside\u2014it changes the shared states of the Component class which makes it not thread-safe. If we have multiple threads creating new components at the same time, they might write to the static variable together and override each other's value and cause some problems. Luckily, we are not doing any multithreading stuff in our engine now. But if we do in the future, we need to keep things like this in mind. Another API design we fixed along the way is the consistency across different functions in Component and Transform . Component::GetTransform used to return a const reference which requires us to use the dot operator to access its members and methods, while GetEntity returns a pointer to the entity which requires the arrow operator for member access. It's a bad API design because it's hard to keep track which method is returning what and a lot of the times we need intellisense 3 to tell us what to do. Making them all just plain member pointer variables made our life a little easier. The last change is that we changed the API of creating entities. We used to have both Level::AddEntity and an ADD_ENTITY macro that's both available for the user to create new entities. We first renamed ADD_ENTITY to CREATE_ENTITY because the word \"add\" means that the entity is being added to something that owns it but the user doesn't need to understand that. Then, we realized CREATE_ENTITY expands to LeveManager::Instance().currentLevel->AddEntity , which is essentially same as the function call, just available in different scopes. But it wasn't immediately clear when to use macro for a naive developer, and we might have a lot of them in our game jam! So, we killed CREATE_ENTITY , hid Level::AddEntity from the user by making it private, and added a static method to Entity as Entity::CreateEntity . It works perfectly conceptually, but as picky as us, we notice it has two \"Entity\"s in its name, so we renamed it to Entity::Instantiate (+1 score for team readability!). A thing to notice when making API changes like this is that it will touch a huge amount of files in the codebase and would thus cause merge disasters as we are using Git for source control. We had to be very careful about that as excessive merge conflicts will \"disarm\" a team member for at least one hour and thus slow everything down. Internally, we call this kind of changes \"4am changes\" as that's the time when no one is making changes to their repo and thus less possibility for merge conflicts. As we were making these API design changes (and updating our audio module), we ended up with some naive guidelines for good API design\u2014they might be helpful to you too! Expose only one way to do simple tasks like creating entities Keep API consistent across classes and methods Ideally, let function and variable names explain itself without comments Memory Manager Upgrades \u00b6 We implemented our memory manager pretty early on in the progress, but we really didn't touch it much since then. Now as most features have achieved minimum viable product (MVP) state, we got some time to go back and polish our memory manager. The first long wanted feature we added is expanding the freelist allocator, which will give us a lot more flexibility when developing games, and running out of memory no longer means a crash\u2014it can just grab another chunk from the operating system! Actually implementing this is pretty easy because the freelist allocator is set up for that, we just needed to create a list to keep track of the memory head pointers and free them all together in the destructor. What we found to be a bit tricky is that we don't know how much the freelist should expand each time. Luckily we have the config file, so we decided to add an entry to the config file and let the user decide. As we were working on freelist allocator expansion, we noticed that our poor pool allocator is sitting in the corner without expanding capability but has the potential. So we also implemented expansion feature on pool allocators too! Now we are no longer afraid of running out of memory (at the same time, we may still run out of system memory!). Another upgrade in the memory manager is we refactored all the allocators to follow RAII . The reason why we were not following RAII is that memory manager, who is holding all the allocators, need information like allocator sizes from the config file to properly initialize allocators. However, the configs are not available when the memory manager is constructed, so we had to run another method, MemoryManager::StartUp to properly initialize it after construction. This causes some performance waste as we are calling the copy assignment operator for each allocator in StartUp . We also had to call Erase methods in MemoryManager::ShutDown on allocators to free their memory and effectively end their lifetime, which we later realized should really be the job of destructors. Finally, we realized StartUp and ShutDown are the conventions of modules and memory manager is not really a module. So we refactored memory manager and allocators to all follow RAII. As for the config information, we made the compromise to have it read the engine configuration file prior to the modules being instantiated, so we just put it before the memory manager's construction. The drawback to this is that any CVar not static or part of a struct that is declared in config (or strictly declared in the config header file) will not be able to be read from the initial config.cfg. We are okay with this because we currently hold all of the CVar s in config, however if we want game developers to be able to define their own without declaring them as static, we will need to move the reading of user.cfg until after level load or have a different config file that is read at that time. This is something that we won't concern ourselves too much with for now. This change has no big effects on other systems but it made memory manager nicer and easier to manage. Performance Optimization \u00b6 During the process of upgrading the pool allocator we realized that nothing is really using it, and that feels like a waste of work! So we looked at our systems and found two perfect candidates that can be refactored to be used pool allocators\u2014entities and nodes in the BV Tree \u2014both are instantiated and deleted very frequently and are of fixed sizes. It's a pretty straightforward refactor. After changing it, we ran a simple benchmark that creates 250 entities and deletes them every frame and here are the results: It may not be as huge as we expected, but it's still a solid 20% improvement! Exposing More Functionalities from Horde3D \u00b6 We are having a better and better understanding of Horde3D as we have worked on it for almost one semester long. At this point when the engine is getting closer to be finished, we would like to expose more functionalities from Horde3D to help the game developers and ourselves as well. The two additional features we added (or exposed) to our game engine are particles and skeleton joints. Adding particles wasn't a hard task. It was just like introducing the animation component, using the RenderModule to hold all emitters and update them every update. Exposing the joints in the skeleton was harder. There are two main reasons. One is that our engine and Horde3D have two different transform hierarchy trees. If we want to construct the same joint hierarchy tree, we need to recalculate the whole tree every update since it is Horde3D that updates the transformation information of the joints. The second reason is that Horde3D provides h3dGetTransform function (paired with h3dSetTransform ), but only in local space. If we want to take the world position of the joint, we need to use h3dGetNodeTransMats function, which returns a 4x4 matrix that is hard to calculate its world scale. To deal with those problems, we made two decisions: one is to only provide joint transformation query without constructing the joint hierarchy tree, the other one is to assume that there's no scaling joints and calculate its position and rotation. After all these decisions and implementations, we now have these two videos! Your browser does not support the video tag. Your browser does not support the video tag. Coming Soon \u00b6 We are rapidly coming to a close on our engine development, which is exciting and sad. With only a few systems left (and who knows how many bugs) we are going to start putting more effort in replicating the feature game demo that we set out to have from the start. In addition to that we are going to start to wrap-up on our blogs by creating postmortems for our big systems and other aspects of our project. This last week we published our interview with Jeff Preshing , and he gave us some great insight in the skills that an engine developer needs to know (and how most of that comes from being forced into doing it at some point). We also published all of our interviews, 10 in total, in book format through our school's press. There is a free pdf available here , or if you are interested in having a physical copy of the interviews with a badass front cover you can purchase it here (the cost is basically to cover printing). Resources \u00b6 Since most of our systems are now more central to our personal implementation, our code may be a good place for you to check out and learn more about some of the features we are talking about this week. We are bound to have bugs and TODO s littered throughout the engine and welcome any help! We will try to make sure we publish all of our sources, including the small ones too, in the next few weeks! Originally Published November 27, 2018. findstr is a command for the Windows command prompt that searches files for a specified string, it has additional options for printing file names and line numbers among other options. \u21a9 grep is a command for a Linux command line that will search specific files for a regular expression or string. When compared with Window's findstr it is much more flexible in its capabilities. \u21a9 Intellisense is an intelligent code completion feature in Microsoft Visual Studio that is capable of detailing information about the code and objects that the programmer is working with while coding. \u21a9","title":"[Week 12] Getting Ready for Game Jam!"},{"location":"blogs/week-12/#getting-ready-for-game-jam","text":"","title":"Getting Ready for Game Jam!"},{"location":"blogs/week-12/#byte-sized-updates","text":"Audio : Refactored the audio system to be more versatile and usable, as well as added in 3D audio. Primitives : Game entity primitives were created for the developer to be able to quickly instantiate a cube and other shapes, without having to make the models themselves. Config Editor Tool : A tool was developed to help developers modify the configuration file, without knowing apriori what property they want to modify. Build : We created a tool for exporting the header files of the engine, as well as other needed files and folders to be used in a standalone project. Patch Notes : We have a lot. Turns out as you begin to use your engine features you will find many more bugs than your original tests. For the past week or so, and particularly this week, we have shifted some of our focus from adding big systems/features to making some smaller style of life changes, better called fixes to API. In this week especially, you will read about how we changed some of our API not for performance but to help game developers better be able to use the engine. One of the main reasons for this shift in focus is because we are holding a game jam on December 1 st using exclusively our engine! It is going to be a short, short game jam only running about 8 hours because we are going to using our fellow students to see what breaks. We know things will probably, almost definitely, break but we are excited to see what some other developers can do with our engine, that's why you make an engine to begin with right?","title":"Byte-Sized Updates"},{"location":"blogs/week-12/#audio","text":"There was a much needed update to our audio module this week\u2014the addition of positional (spatial) audio, which then triggered some other refactors. There are two components to spatial audio: The position in space where the audio is being played and the position where the \"ears\", the listening object, are. Starting with the listener, we added a new component named AudioListener which is almost an empty component that adds/removes itself from a listener list in AudioModule during its OnEnable / OnDisable . In AudioModule 's update then, prior to updating the FMODSystem , the transform of the first listener in the list is used to position FMOD's listener. If no listener exists, the 3D sound will be heard as 2D. If a tree falls in the wood and there's no listener to hear it, in Isetta, it will be like you're standing right next to it. Having multiple listeners is supported by FMOD in some sense; however, this could cause additional confusion for the game developer, so we decided not to support it. That was the easy part, now for the 3D positioned sources. Since the audio was one of the first modules and features implemented in the engine it had some legacy code that needed to be refactored to keep the engine more consistent. Implementing the actual 3D audio was as simple as telling FMOD that the audio was 3D and updating FMODs understanding of the position of each 3D AudioSource . However, there were also other FMOD features like muting the audio and setting the hearing minimum and maximum distance to implement so we can make the audio more versatile, plus how hard could it be to support that... It turns out implementing them isn't so hard but making sure that different configurations don't break the engine or FMOD isn't as easy. Talking more about the legacy changes of AudioSource , all the audio was loaded by specifying the file paths in the config file from the resources folder which was then loaded on engine startup. To use the audio, the AudioSource then required the same filepath as specified in the config file. This was very limiting and tedious because every file had to be specified in two places: the configuration file and the place you wanted to use it, thus it's prone to issues. Our first change was to create a new struct, AudioClip which would house the loaded FMODSound which then could have additional audio files loaded from outside the config file. Each AudioSource would then hold a reference to an AudioClip rather than an individual FMODSound . However, the config files loaded on start up seemed unnecessary and added confusion for even just us so we just removed that completely. AudioSource was also allowed to have a string on construction which would find the corresponding AudioClip , but this again wasn't intuitive and caused errors even internally, so we replaced that constructor with one that would only take AudioClip s. AudioClip s would be the only thing in our engine to know of the audio's file path, everything else would have an AudioClip reference. These are small usability changes, however we are finding as we are building more small tech demos and our feature demo, that usability is a larger concern on the user-facing components than it was for our low-level systems and needs more iteration not for bug fixing, although there is still a lot of that, for easier use.","title":"Audio"},{"location":"blogs/week-12/#scene-primitives","text":"Rapid iteration is a hot-topic when developing a game, and while we are under no fallacy using our engine would be good for rapid development, we do want to make things easier for us and any possible users. One way for us to do this is making the instantiation of primitives such as cubes, cylinders, capsules, grids, quads, and spheres as easy as possible. All of these shapes make slapping together a prototype or testing a feature much easier. Prior to this endeavor we only had cube and sphere meshes available to use, and a user still needed to know where exactly those \".scene.xml/.geo\" files were in the directory structure. Since the MeshComponent s require a file path, typing a path out for each object would be annoying. Plus, these mesh files are in the engine resources not the game resources, and there isn't a good way for the developer to specify that. We spent some time getting reacquainted with Blender to make these simple shapes, and the only thing we focused on was shape size and vertex/poly count. We wanted the sizes to be similar, if not identical to Unity's, so that people who have used Unity would have an understanding of how to use the objects, these sizes are also versatile in use. In Unity when you create one of these objects there is also the assumption you want a collider attached. While that makes sense for Unity, our collision system isn't as robust so we didn't want a bunch of unused colliders to be attached, plus then a developer might delete it off the entity right after instantiation\u2014what a waste of performance! So for our primitive system, you can optionally specify if you want a collider attached. The system also caused us to change our entity creation system, see in our patch notes .","title":"Scene Primitives"},{"location":"blogs/week-12/#config-editor-tool","text":"With everyone on the team starting to use the engine to make mini games, more and more there's been this need to know what values are configurable. One of our developers has been itching to do some tool development for the engine, so when another team member asked for something that was all he needed. Knowing not much time could be spent towards creating tools versus engine development, the tool is fairly simple. The goal of the tool was to display all of the CVar variables in the engine, with their default values, and be able to load a config file as well as save a user-generated one. The main advantage to this tool isn't necessarily speed, because editing a text file will always be faster, but being able to see all the different options the developer has. First, it has the user specify the directory of the engine, and if none is specified it uses the current application directory. It then searches through all the files of the directory, using the command prompt, doing a findstr 1 , similar to a Linux grep 2 , for any CVar variable. It then groups the variables based on the file the variable was found, i.e. if found in AudioModule.h it will group it to AudioModule . Each group and its CVar property is then displayed with a value field for the user to modify. The value field defaults to the default value of the CVar so if the user decides to not change the value, they can rely on that the engine developer put a reasonable default value... The user can also load a configuration file of their choosing if they wish to only change a few of the values but keep the most of them the same. The properties and values can then be saved out to a filename and path of their choosing. Here is the application:","title":"Config Editor Tool"},{"location":"blogs/week-12/#build","text":"","title":"Build"},{"location":"blogs/week-12/#exporting-headers","text":"For people to be able to use the engine, without developing on the engine or building it through Visual Studio, the engine needs to have a downloadable folder/zip file containing the headers, dll files, and static library files of the engine. These files are scattered throughout the engine directory. Even worse, there are other files, .cpp files, in the same folders so we can't just copy the folders. The directory structure needs to be preserved as well, because the header files reference one another by path. With each change of a header file or build, the process needs to be repeated which could just be copying over the changed files, but we do a naive approach of exporting everything again. This isn't too bad for us since the files being copied are relatively small in size and number. Rather than having someone do this manually, we created a batch script (originally a shell script) to copy the files into a separate, identical folder structure. The batch file also copies over engine resources that are required for the engine such as the graphics pipelines, a default font, shaders, primitives, etc. as well as the external headers that are needed by our header files. It's a fairly simple script of copying files, but very necessary to ensure a game external to the main engine solution maintains an up-to-date version of the engine.","title":"Exporting Headers"},{"location":"blogs/week-12/#patch-notes","text":"","title":"Patch Notes"},{"location":"blogs/week-12/#levels-menu","text":"Last week we wrote about level loading and we created a simple component that would transition to a level specified in the constructor. Since we are students, we need to show our work off to our faculty soon, which we thought would be nice to have a level select menu. We currently have no system for persistent objects, which is something we would like to add and don't think it would be too difficult but just don't have the bandwidth, so currently all input and entities are deleted on level unload. To browse all of our levels, we could add a level menu to each level but then we would need to remove the level menu component when done. Rather than altering all of our levels to have a level menu component we wanted to do a quick hack... Since we clear game developer input on level unload, we couldn't use that, but we can use our uncleared input, GLFWInput . This input is only available to use within the engine and isn't built out to the dll for game developers to use. When the GLFWInput callback happens on the level menu, it checks to see if there is an instance of level menu in the level and if not, creates one. We had to make the input registration handle of the component static as well so the input could be unsubscribed, otherwise the callback would happen multiple times on a single click. This is definitely a hack, but we're the engine developers what we say goes right?","title":"Levels Menu"},{"location":"blogs/week-12/#debug-drawing-optimization","text":"We revisited our debug drawing code to see if we should bother adding in a shading model to our solid objects, the jury isn't out on that yet, but we noticed that a) we weren't taking full advantage of our transform system and b) the code had an easy optimization. First the optimization. As talked about with the original debug drawing post , the vertex shader uses the model view projection matrix to transform vertex points in world space to camera space. The original implementation had the model, view, and projection matrices passed in as separate uniform variables to the vertex shader which would then perform the matrix multiplication for each vertex. This was clearly wasteful because the matrix doesn't change for each vertex and had 2 matrix multiplications for each vertex that resulted in the same final matrix. Rather than passing 3 matrices to the shader we calculated the matrix then passed the matrix. This is an example of when incremental/developing code just gets left in and forgotten about. For using the transform system, we were calculating the inverse of the camera transform matrix each call, however our transform class caches that value so we can just use that. There are definitely other optimizations, not only for debug drawing, throughout the engine that we would love to do, but probably won't get the chance to.","title":"Debug Drawing Optimization"},{"location":"blogs/week-12/#capsule-height-bug","text":"When developing the primitives we found that the debug drawing capsule for the capsule collider was too large for the created Blender model. At first, we thought it was our bad Blender skills, but we then compared our model to a model in Unity and our scale was fine. We started by looking at CapsuleCollider::Update because the debug capsule on the primitive was coming from this component, but things looked fine there. But when looking into the debug drawing capsule draw, we realized the problem was in there -- we had never thought of checking the capsule size before! What was happening was the length of the capsule's centerline was being calculated with height and scale, then added and subtracted from the center point to get the two endpoints of the capsule. However, this would generate a capsule with double the height specified; the line length should have been halved then added/subtracted to get the endpoints with the actual height. This then scared us because how the heck was our collision code working with the busted capsule height... well that's because in the collision tests we had copied the concept of height from the debug drawing so that's why collision was working and we quickly replaced the code there too. We'd like to propose other solutions which would have stopped us from having to change the code twice. One would be to define a Capsule class which has no understanding of the debug drawing nor collisions then things could be passed along as needed. The other is from a GDC talk by Ludovic Chabant, where the schema used to define the capsules in the first place are defining the endpoints not a center and height; then expose it to the user as a height and center, this would avoid the problem from the beginning.","title":"Capsule Height Bug"},{"location":"blogs/week-12/#collider-debug-shearing","text":"Another problem we found with our Collider s has nothing to do with the actual collision model or calculation but rather how they are drawn again. We understand that ordering of matrix multiplication matters, and the typical convention of with transformation matrices is: translation x rotation x scale . However in Collider we had done translation x scale x rotation , we think we did this to avoid shearing in the case of a weird rotation but actually this causes shearing! We are going to attribute this to a lapse of judgment and just made the fix, no one to be the wiser.","title":"Collider Debug Shearing"},{"location":"blogs/week-12/#api-design-improvements","text":"As the game jam approaches, we are thinking more and more about the usability and API design of our engine. The first thing we looked at is the GetTransform method in both Component and Entity classes, the typical getter method that only allows the developer to read from the property. This seemed clunky, especially from our experience with C# properties. To change this, one option would be making a public Transform* const variable and let the users access it. It seems like a good option, but C++ requires that we initialize all const member variables in the constructor\u2014meaning we need to pass a reference or pointer to Transform as a parameter into the constructor\u2014which is impossible in our engine because the users will inherit our base Component class and define their own constructors. Which is why, at the time, we chose the straightforward implementation of having a getter with the GetTransform method. However, as we were making more and more games, we realized that calling that method each time we want the transform is pretty annoying. So, we got rid of GetTransform in the end. We looked at our code structure and came up with a solution that's hacky but works great for now\u2014we created a static Transform* variable inside the Component class, and every time before constructing a new Component , we set that static variable to the Transform that the new Component should point to. Then, in Component 's default constructor, it can just use the static transform variable and initialize it's const Transform* member (see the code snippet below). It will be propagated to all derived constructors due to the inheritance. With this solution, we can just write transform->GetWorldPos instead of GetTransform().GetWorldPos , so much cleaner! After this fix, we did the same thing for Component 's Entity* . class Entity { template < typename T , typename ... Args > T * Entity :: AddComponent ( Args && ... args ) { // ... // Set the data so the next Component can pick them up in constructor Component :: curEntity = this ; Component :: curTransform = transform ; T * component = MemoryManager :: NewOnFreeList < T > ( std :: forward < Args > ( args )...); // ... } } class Component { Component :: Component () : entity { curEntity }, transform ( curTransform ) {} static class Entity * curEntity ; static class Transform * curTransform ; } However, this solution has a big potential downside\u2014it changes the shared states of the Component class which makes it not thread-safe. If we have multiple threads creating new components at the same time, they might write to the static variable together and override each other's value and cause some problems. Luckily, we are not doing any multithreading stuff in our engine now. But if we do in the future, we need to keep things like this in mind. Another API design we fixed along the way is the consistency across different functions in Component and Transform . Component::GetTransform used to return a const reference which requires us to use the dot operator to access its members and methods, while GetEntity returns a pointer to the entity which requires the arrow operator for member access. It's a bad API design because it's hard to keep track which method is returning what and a lot of the times we need intellisense 3 to tell us what to do. Making them all just plain member pointer variables made our life a little easier. The last change is that we changed the API of creating entities. We used to have both Level::AddEntity and an ADD_ENTITY macro that's both available for the user to create new entities. We first renamed ADD_ENTITY to CREATE_ENTITY because the word \"add\" means that the entity is being added to something that owns it but the user doesn't need to understand that. Then, we realized CREATE_ENTITY expands to LeveManager::Instance().currentLevel->AddEntity , which is essentially same as the function call, just available in different scopes. But it wasn't immediately clear when to use macro for a naive developer, and we might have a lot of them in our game jam! So, we killed CREATE_ENTITY , hid Level::AddEntity from the user by making it private, and added a static method to Entity as Entity::CreateEntity . It works perfectly conceptually, but as picky as us, we notice it has two \"Entity\"s in its name, so we renamed it to Entity::Instantiate (+1 score for team readability!). A thing to notice when making API changes like this is that it will touch a huge amount of files in the codebase and would thus cause merge disasters as we are using Git for source control. We had to be very careful about that as excessive merge conflicts will \"disarm\" a team member for at least one hour and thus slow everything down. Internally, we call this kind of changes \"4am changes\" as that's the time when no one is making changes to their repo and thus less possibility for merge conflicts. As we were making these API design changes (and updating our audio module), we ended up with some naive guidelines for good API design\u2014they might be helpful to you too! Expose only one way to do simple tasks like creating entities Keep API consistent across classes and methods Ideally, let function and variable names explain itself without comments","title":"API Design Improvements"},{"location":"blogs/week-12/#memory-manager-upgrades","text":"We implemented our memory manager pretty early on in the progress, but we really didn't touch it much since then. Now as most features have achieved minimum viable product (MVP) state, we got some time to go back and polish our memory manager. The first long wanted feature we added is expanding the freelist allocator, which will give us a lot more flexibility when developing games, and running out of memory no longer means a crash\u2014it can just grab another chunk from the operating system! Actually implementing this is pretty easy because the freelist allocator is set up for that, we just needed to create a list to keep track of the memory head pointers and free them all together in the destructor. What we found to be a bit tricky is that we don't know how much the freelist should expand each time. Luckily we have the config file, so we decided to add an entry to the config file and let the user decide. As we were working on freelist allocator expansion, we noticed that our poor pool allocator is sitting in the corner without expanding capability but has the potential. So we also implemented expansion feature on pool allocators too! Now we are no longer afraid of running out of memory (at the same time, we may still run out of system memory!). Another upgrade in the memory manager is we refactored all the allocators to follow RAII . The reason why we were not following RAII is that memory manager, who is holding all the allocators, need information like allocator sizes from the config file to properly initialize allocators. However, the configs are not available when the memory manager is constructed, so we had to run another method, MemoryManager::StartUp to properly initialize it after construction. This causes some performance waste as we are calling the copy assignment operator for each allocator in StartUp . We also had to call Erase methods in MemoryManager::ShutDown on allocators to free their memory and effectively end their lifetime, which we later realized should really be the job of destructors. Finally, we realized StartUp and ShutDown are the conventions of modules and memory manager is not really a module. So we refactored memory manager and allocators to all follow RAII. As for the config information, we made the compromise to have it read the engine configuration file prior to the modules being instantiated, so we just put it before the memory manager's construction. The drawback to this is that any CVar not static or part of a struct that is declared in config (or strictly declared in the config header file) will not be able to be read from the initial config.cfg. We are okay with this because we currently hold all of the CVar s in config, however if we want game developers to be able to define their own without declaring them as static, we will need to move the reading of user.cfg until after level load or have a different config file that is read at that time. This is something that we won't concern ourselves too much with for now. This change has no big effects on other systems but it made memory manager nicer and easier to manage.","title":"Memory Manager Upgrades"},{"location":"blogs/week-12/#performance-optimization","text":"During the process of upgrading the pool allocator we realized that nothing is really using it, and that feels like a waste of work! So we looked at our systems and found two perfect candidates that can be refactored to be used pool allocators\u2014entities and nodes in the BV Tree \u2014both are instantiated and deleted very frequently and are of fixed sizes. It's a pretty straightforward refactor. After changing it, we ran a simple benchmark that creates 250 entities and deletes them every frame and here are the results: It may not be as huge as we expected, but it's still a solid 20% improvement!","title":"Performance Optimization"},{"location":"blogs/week-12/#exposing-more-functionalities-from-horde3d","text":"We are having a better and better understanding of Horde3D as we have worked on it for almost one semester long. At this point when the engine is getting closer to be finished, we would like to expose more functionalities from Horde3D to help the game developers and ourselves as well. The two additional features we added (or exposed) to our game engine are particles and skeleton joints. Adding particles wasn't a hard task. It was just like introducing the animation component, using the RenderModule to hold all emitters and update them every update. Exposing the joints in the skeleton was harder. There are two main reasons. One is that our engine and Horde3D have two different transform hierarchy trees. If we want to construct the same joint hierarchy tree, we need to recalculate the whole tree every update since it is Horde3D that updates the transformation information of the joints. The second reason is that Horde3D provides h3dGetTransform function (paired with h3dSetTransform ), but only in local space. If we want to take the world position of the joint, we need to use h3dGetNodeTransMats function, which returns a 4x4 matrix that is hard to calculate its world scale. To deal with those problems, we made two decisions: one is to only provide joint transformation query without constructing the joint hierarchy tree, the other one is to assume that there's no scaling joints and calculate its position and rotation. After all these decisions and implementations, we now have these two videos! Your browser does not support the video tag. Your browser does not support the video tag.","title":"Exposing More Functionalities from Horde3D"},{"location":"blogs/week-12/#coming-soon","text":"We are rapidly coming to a close on our engine development, which is exciting and sad. With only a few systems left (and who knows how many bugs) we are going to start putting more effort in replicating the feature game demo that we set out to have from the start. In addition to that we are going to start to wrap-up on our blogs by creating postmortems for our big systems and other aspects of our project. This last week we published our interview with Jeff Preshing , and he gave us some great insight in the skills that an engine developer needs to know (and how most of that comes from being forced into doing it at some point). We also published all of our interviews, 10 in total, in book format through our school's press. There is a free pdf available here , or if you are interested in having a physical copy of the interviews with a badass front cover you can purchase it here (the cost is basically to cover printing).","title":"Coming Soon"},{"location":"blogs/week-12/#resources","text":"Since most of our systems are now more central to our personal implementation, our code may be a good place for you to check out and learn more about some of the features we are talking about this week. We are bound to have bugs and TODO s littered throughout the engine and welcome any help! We will try to make sure we publish all of our sources, including the small ones too, in the next few weeks! Originally Published November 27, 2018. findstr is a command for the Windows command prompt that searches files for a specified string, it has additional options for printing file names and line numbers among other options. \u21a9 grep is a command for a Linux command line that will search specific files for a regular expression or string. When compared with Window's findstr it is much more flexible in its capabilities. \u21a9 Intellisense is an intelligent code completion feature in Microsoft Visual Studio that is capable of detailing information about the code and objects that the programmer is working with while coding. \u21a9","title":"Resources"},{"location":"blogs/week-13/","text":"Is This a Game Engine? \u00b6 Byte-Sized Updates \u00b6 Full-Feature Demo Game : Implemented hitscan to be used in our feature game. The Knight Game : Developed another game that flexes some of our new components and our collision system that hasn't been shown in a game just yet. Collision Solving : Wrapped up our collision solver by making it moderately more correct, and added a \"mass\" feature. Build System : Set up a build pipeline for our engine after one too many rebuilds aggravated our programmers to no end. Networked Game Management : Added functions like start/stop server and client, added callbacks related to client connection status, and added networked level loading. Network Discovery : Added LAN (local area network) broadcasting functionality so we no longer need to type in IP by hand! Monitoring Memory Leaks : We added memory allocation/free tracking and let the user know what's leaking on our free list by object name and amount. Patch Notes : As we get to the end we seem to be patching more holes than making an actual boat. As you can see from the architectural diagram, the engine looks complete! We are still hunting bugs as we develop, but we are at feature lock (hopefully). This is actually harder for us than being still in development because there is so much more we would like to add and iterate on. We have accepted that we have to stop development to ensure the final release isn't broken. We are really focused on three things this week: Preparing for our game jam Starting our engine postmortems Finishing our full-feature demo game, including finishing necessary features for the game Now the real question we are asking is: Full-Feature Demo Game \u00b6 We're continuing to build our feature game . We still needed a few engine features discussed in this blog to finish the game, so we used that as an excuse to do some more engine development before putting down serious work into the game's foundation. (That being said, we've already made a basic version of it, so it shouldn't be too hard to just expand upon that!) Hitscan \u00b6 One of the features that we've begun work on is the shooting system. In our first game (and even in our Unreal example!), we effectively did collision checks on all of the enemies for each bullet. That's a really simple method for checking if bullets hit an object and it's easy to wrap one's mind around, but it's not terribly performant. How could you do this check faster, though? The answer is hitscan . Hitscan is simply a method of checking whether or not a bullet hits an object. It's pretty simple conceptually; when a gun is fired, you create a line from the gun in the direction of the gunshot and you check if that intersects any objects. You take the nearest intersected object and apply the gunshot to that object. Simple, right? When you go deeper into the feature, you might realize that there are more details that spill out of a hitscan system than you would think at first glance. For instance, what about bullet travel? Some guns shoot bullet more slowly than others. You can't just instantly check for an intersection if you want the bullet to have travel time, you need to simulate the travel time by only checking intersections with certain parts of the line segment depending on how long it's been since we fired the bullet. Instant hitscan travel Hitscan traveling over time Why Hitscan? \u00b6 Above, we said that checking every object against every other object wouldn't be performant, but with a good collision system with spatial partitioning 1 , it would be pretty good, right? We can actually just reason about this to get a good answer, so long as we know what goes behind \"entities\" in an engine like ours. Each entity has a transform, and each transform needs to recalculate its own and all of its children's transformation matrices if it's moved, rotated, or scaled during the current frame. After doing all of that math, we still need to perform collision tests for every bullet object. But we're still not done! If a collision occurs and we remove the bullet from the scene, then we possibly need to regenerate our spatial partition in our collision system for the next frame. All of these operations aren't even including the overhead of data and runtime an entity might carry itself, being a generic object type that's used in our scene hierarchy. If that's not a good enough reason for investigating a faster method of bullet detection for you, then you will probably want to skip this section! However, we like to squeeze what we can out of our engine in terms of performance, and hitscan also poses a pretty interesting problem, so we moved forward with implementation. Implementing Hitscan \u00b6 Like we said earlier, hitscan is something that gets larger and larger as you add more features into it. Fortunately, it's also naturally upgradable if you're mindful of how you implement it initially. We won't delve into too many details here because it's not quite engine development, but it is still a pretty interesting system to develop. We started our Hitscan component with a few decisions: Each bullet should not be its own entity (obviously) Each bullet should have its own properties, so if our gun starts shooting different bullets, our existing bullets won't change Bullets get blocked by any collider that is not a trigger Since we're building up from these decisions, they're the least likely to change in our system now, so it's important that they're chosen correctly. The one that may still change is that our bullets could get selectively blocked by different layers of colliders, but after a certain in point in development, we'll likely have to cement that decision in or face consequences later. Hitscan is Easy (with an Existing Collision System) \u00b6 For our first decision, we need a different method of simulating each bullet than giving each one its own Entity and (expensively) moving it using our scene hierarchy. Luckily, we've already implemented raycasting in our collisions , so we can leverage that functionality with the Ray class for performing hitscans. We also need to keep track of the travel distance for each bullet as well. In all, it should look a little something like this: // Check if the bullet collides with anything if ( Collisions :: Raycast ( bullet . ray , & hit , bulletRange ) && hit . GetDistance () - bullet . travel < bullet . speed * deltaTime ) { bullets . remove ( bullet ); // \u2026 Get the hit collider and run functions on it continue ; } // Propogate the bullet through the space bullet . travel += bullet . speed * deltaTime ; if ( bullet . travel > bullet . range ) { bullets . remove ( bullet ); } It's pretty straightforward. We check a raycast in our scene with our bullet, and if it hits anything then we kill it and run the appropriate code, and if it doesn't hit anything then we keep propagating it through the scene. That's it! Granted, this relies heavily on our collisions system with its intersection testing code, but once that's in place, the gameplay code is pretty easy. Being Mindful of the Data \u00b6 Our second decision is a bit more interesting of a problem. For every bullet to have its own properties, we could store the bullet's properties in itself, and that would be easy. But think about that for a moment\u2014how often will we be changing the properties of our gun? Maybe a few times in a game with power-ups? It's not zero, so we can't just forgo each bullet having their own properties, but since the number is so low, we need to think of something that will be less wasteful than repeating data way more than it needs to be. This is where pointers are nice. We could use an unordered_map from bullet to property or something, but that's much more complex than we need. With pointers to property objects, we only need to store eight bytes worth of data in each bullet to have arbitrarily complex properties on them! Bullets that contain their own data Bullets that contain pointers to their shared data It's so simple and straightforward...what's the catch? For those of you who were suspicious of this solution, you were right to be wary. One thing that we need to be mindful of is when we should \"get rid of\" the property objects. When we've destroyed all of the bullets that are using a particular property, do we just leave it for eternity? And if we destroy the property objects, how do we know that no other bullets are referencing it? This problem is solved with reference counting, where we keep track of the number of bullets using the property object and we decrement our count whenever one of those bullets is destroyed. We also need to keep track of when properties on our gun change so that we can create a new property object for the bullets after that point. That's mostly just writing mutator functions for any of the properties on the gun, which is simply boilerplate. With all of that set up, the code boils down to this: // If any properties changed or we don't have any property objects if ( propertiesChanged || bulletProps . size () == 0 ) { // Create our property object and assign it to the bullet bullet . props = & bulletProps . emplace_back ( properties ); propertiesChanged = false ; } else { // We can just use our most recent property object bullet . props = & bulletProps . back (); } // Don't forget to increase the reference count! ++ bullet . props -> refCount ; The Results \u00b6 After these things, most of the work was iterative. The gun has more properties than the bullets do since we need things like fire rate, and we can implement more properties on the bullet in the monolith Update function like a piercing property. The work on the hitscan system was quick and iterative, and we made a lot of improvements over time. This is one of the perks of having a built engine underneath our game code because we didn't have to deal with any engine rebuilding to iterate. That being said, we did have to revisit our engine code a few times to provide functionality that we were missing! Check out some progress videos of our hitscan work! Your browser does not support the video tag. Your browser does not support the video tag. Your browser does not support the video tag. And here's a quick snapshot of the performance of our hitscan implementation. The top left contains frame rate info, and to its immediate right is our hitscan data. Over 4,000 shots at ~20ms is not bad, especially when you consider that 95% of the frame time is spent on the `DebugDraw` calls! ![Sick Hitscan Performance](../images/blogs/week-13/sick_hitscan_perf.png) ## The Knight Game We've spent the last couple of weeks creating another game, or something that sort of resembles one... The gist of the game is an infinite runner where you, as the knight, run from a barrier (a collider) and slash training dummies (capsules) to rack up a high score. The focused-on engine features were colliders and animation as we had yet to really have a game focus on either (although we included animation in our first game). The development was relatively intermittent since we were just adding in features that we thought might be interesting and we definitely didn't have a design plan going into it. The development started by gathering additional assets and reworking them in Blender, which as you would expect for a programmer, took longer than estimated. Our struggle with this was that the downloaded models were scaled at 100 times the unit scale of our engine and Horde3D's engine. Scaling a model in-engine isn't that difficult, but it is nice to have a model already at scale, so we attempted to scale in modeling software which broke the connection between the mesh/skeletal rig and animation. Initially, we had a model file which was a COLLADA (.dae) model with mesh and rig and we also had other .dae files which were purely animation with the same rig, no mesh data. Without scaling we were able to successfully import the mesh and animation into Unity, but once the mesh and animation were scaled, something about the .dae file format broke the connection, and so not even Unity could process the files this way. After a night of struggling with that, we decided to use animation files which contained the mesh (although only .anim files were being exported with the asset processor) which worked even after scaling. The biggest struggle of this was that we were at the whim of two things out of our control: 1) the .dae file format and 2) the Horde3D rendering engine. Although we could read the .dae file, once they were turned into Horde3D's .geo and .anim files, all bets were off for whether the file was being processed properly or if the actual rendering/animation code was breaking. Here is the knight moving at correct scale: Your browser does not support the video tag. Once the actual animation was working in the engine, we wanted to \"attach\" a sword to the knight character which would follow a certain joint in its rig. Prior [`MeshComponent::GetJointWorldTransform`](week-12.md#expose-more-functionalities-from-horde3d) there was no way to do this. The functionality before wasn't sufficient because although the sword could have been parented to the knight, there is no way to get the hand's transform information. With this function, the sword can be attached to the hands and will move through an animation. We were then able to get the knight chopping with his sword! Your browser does not support the video tag. With the assets set up, we wanted to focus on the collision of the sword with an \"enemy\". Keeping it simple, we just wanted to ensure we could align a collider with a mesh as desired. What we found out was, first, the pivot of the sword was in a terrible location, but more importantly, the collider drawn was displaying shearing as spoken about [last week](week-12.md#collider-debug-shearing). After fixing that, we attached a `CollisionHandler` to a box to cause a splitting effect. This was a great exercise in using our own API. It had been a few weeks since some of us had touched collisions, so remembering how they worked took a minute. Because there weren't any examples of how to use the `CollisionHandler` before, it hadn't occurred to even use some functionality, such as the `IgnoreCollisionLayer`. Here is how it was looking: Your browser does not support the video tag. Well that obviously looks (and is!) broken. There were a few things with the collision system that needed small bug fixes. One of those fixes was with regards to a box and capsule, we had noticed that a simple case of box-capsule collision wasn't being detected. Going into the box-capsule code was a nightmare for us because this particular collision detection was just not performed the way we needed this particular collision detection to be performed. Luckily, the error was found quickly by using our debug drawing system to find where the contact and closest point were and stepping through the code. In the end, we got the collision working (hopefully the full-feature demo game won't need more fixes)! With all of the necessary capsule collisions working, the knight who had a capsule collider surrounding him could be killed, and the alive enemies who were represented as capsules could be cycled (basically a pool that would translate to a position on the other side of the screen once they went off-screen). When we began cycling the enemies, the game started to feel like an actual game with a lose state and a goal! However there was something weird happening with when the capsules collided with the wall to reset their position. The collision would happen and be detected\u2014everything working so far. The `CollisionHandler` callback would then set the position of the entity to the other side of the screen\u2014that works. Then the object would teleport back to point of collision... wait what? The solution deals with the ordering of `CollisionSolver::Update` and when the collision callbacks occur; [read more below](#collision-solving-and-collision-callbacks). With all these bugs ironed out, the game took shape and although simple is slightly enjoyable to play. Here's what a slice of gameplay looks like: Your browser does not support the video tag. ## Collision Solving Collisions and physics, in general, is a deep field in video games, and beyond features and correctness, optimizations can always be done. As such, we decided to wrap up our collision solving system by getting it to be close to correct and leave it at that. One very convenient aspect of the collisions part of the field is that getting everything solved and good _eventually_ is usually good enough\u2014it will just mean a couple rendered frames of jitter or so. ### The Math of a Box [Last time](week-11.md#future-plans-with-collision-solving), we mentioned that we had a pretty good idea for how to make our box collision responses more...correct. Previously, our boxes would correctly detect the collision, but on most of the edges the box would shove itself and the other collider completely out of the way of each other if the collision point was far enough from the center of one of the box's faces: Your browser does not support the video tag. The reason this happened is because our collision detection for box colliders would pass the collision point to the collision solver, which would then try to extrapolate the push direction of the collision from that point. This might not seem like a problem for things like spheres, but for boxes, it's a different story. When the collision point on that box is on a face instead of an edge, it's really easy to figure out which face of the box determines which direction the colliders are pushed in: ![Easy Box Collision Solve](../images/blogs/week-13/box_collision_solve_easy.png) But what about when the collision point is on an edge? That is, when the point lies at the max extent of two faces as opposed to just one? ![Not As Easy Box Collision Solve](../images/blogs/week-13/box_collision_solve_not_as_easy.png) In our case, the box would get pushed to the side because code is run sequentially. When determining the furthest distance on an axis, our code always checks in the order of the x-axis, then the y-axis, then the z-axis. If the point is equally distanced on the x-axis and the z-axis, then the x-axis wins. This can be solved by doing some of the collision solving work while we're detecting the collision point. The other collider is probably not perfectly distanced at the corner of our box collider; it's probably a little closer to one face or the other. While we're determining the collision point, we can also check for the furthest axis distance _without being constrained to the extents of the box_. We store that information for later, where the collision solver can then respond appropriately: ![Solved the Not Easy Box Collision](../images/blogs/week-13/solved_not_easy_box_collision.png) Your browser does not support the video tag. ### That Epsilon Spice We may have gotten some silky smooth box colliding now, but we're not devoid of troubles yet: Your browser does not support the video tag. As neat of a gameplay mechanic as stickiness can be, it's not exactly great as a game developer to have to make all of your games gum-themed. Fortunately, this problem is easy to solve. When collision solving, you shouldn't just put the colliders in the correct position depending on the collision\u2014you should put them a little further, an epsilon amount, from the solved position. That way, you aren't colliding again after you solve the collision. ### \"Mass\" in a Collision System One last small feature one of our developers convinced us to develop for the collisions system is something that's usually thought of more as a physics parameter: mass. Obviously, we're not developing an entire physics system for this one last feature, so what do we mean by \"mass\"? Well, the way our collision solver currently works, everything is considered to be equally movable barring any static colliders. This means that a box that is 100x the size of another box can get shoved around by the tiny box as easily as by any other collider. Your browser does not support the video tag. This happens because each collider receives an equal part of the collision response when wrapping up the collision solver calculations. Thankfully for us, having a system that calculates out a shared collision response that gets split between colliders is really convenient when we want to upgrade the system to unevenly distribute that response. By adding a `mass` field to the `Collider` class, we can now just check against the weighting of the colliders to determine how \"heavy\" certain objects should be! Your browser does not support the video tag. > An interesting feature that this also adds to our engine is \"negative mass\". That effectively makes objects counteract any objects that push on them, which is actually something that could be useful for gameplay! ## Build System [Last week](week-12.md#exporting-headers), we talked about how we needed to export all of our headers in order for people to use our built engine. This got us most of the way there, but we were quickly finding that we need to rebuild our engine DLL's often during gameplay development (so many bugs!) then re-run our export tool and do a bunch of file shifting. The cherry on top? We would need to update our Git repository with the engine and everything, which would usually involve annoying merge problems (our opinion of Git has steadily dropped throughout the semester). Being programmers, we decided that this repetition was not worth our time and we started drafting up a build system for the engine. We've already been using [Jenkins](https://jenkins.io) on a shared machine in the room for building our website, so we've already got the pipeline there, we just need to automate the steps for building the engine. Thanks to our batch file that handles the exporting of the build and the header files, the main technical part that we needed to add was building each configuration of the engine, which just involved getting `devenv.exe` from the Visual Studio program files added to the PATH environment variable and running it with the correct parameters in order to rebuild. Once that was working, we still needed the computer to push the built engine to a shared location. We decided on that should be our [feature game repository](https://github.com/Isetta-Team/Isetta-Game), and set off in the land of Git commands. Adding the newly built files to a commit and pushing that to the repo was easy, but getting the commit message to be our most recent engine commit message was a bit more annoying in batch script because we had to be mindful of escape characters (which is ^ in batch). After we got a build pipeline working, we quickly realized that pushing up a new build regardless of whether we have built the engine at that commit is not a good idea. If we want to debug a problematic build, we'd have to look through dozens or hundreds of commits! So the last feature we built into our build system was an up-to-date verification system, which we already had a good reference for from a past Jenkins function, but it still caused quite a few headaches. The basic idea was to write our pushed commits to a build history file, which we then read on later builds to confirm that that commit hasn't already been built and pushed. Unfortunately, Jenkins does not remember system variables across build steps, so we had to come up with a very janky solution of creating and conditionally deleting a file to determine whether or not we should be trying to build the engine again. All in all, this whole process took about 12 hours worth of development time. Was it worth it? In the long run, it very likely would be; after all, we were already feeling pretty aggravated because of our build times. However, in the short term, we may not see too many dividends. ## Networking ### Networked Game Management This week we finally got the chance to extend our networking module and add game management features. The features our game needs are pretty simple - clients and server just need to know when each other is connected and disconnected, so we added four callbacks: * `OnConnectedToServer`: a client callback called when a client successfully connects to the server, mainly for initialization right after the connection is established * `OnDisconnectedFromServer`: a client callback called when the client is disconnected from the server - either caused by losing connection or manually calling `StopClient`. Developers can use this to stop their networking features and keep track of networking state * `OnClientConnected`: a server callback called when any client is connected. To provide more valuable information like client's IP address, we also created a `ClientInfo` struct that's sent to the server when the client is connected. * `OnClientDisconnected`: a server callback called when any client is disconnected. Because the client may drop out any time without notifying the server about that, this event is monitored on the server every frame and is called when any client's state changed The new addition of our [`Delegate`](#delegates) data structure made implementing these callbacks super easy! All we needed to do is create an interface for subscribing and unsubscribing from the events, finding the right places to invoke the callbacks and send network messages, and making several maps to track clients' state. Now, we can get nice messages like this by using the new callbacks (we were running host so we get all callbacks): ![Using network callbacks](../images/blogs/week-13/network_callbacks.png) We then discovered that our API for creating server and client were not unified with one being called `CreateServer` while another called `ConnectToServer`, and we didn't have a function for \"starting as host\". We refactored those functions into `StartClient`, `StartServer`, `CloseClient`, `CloseServer`, etc., which we think will be more intuitive for the developers. #### Network Role Monitoring Another feature we need for the game is to easily get the network role of their instance, \"am I running as host? Or server? Or client?\" Luckily, yojimbo already exposes simple state querying functions like `Client::IsRunning` so we were able to easily make a wrapper on top of those for the developers to use. When debugging them, we also made a component that draws network information through our GUI as shown in the picture below. In the end, we thought this can also be useful for the game developers, so we refactored it and put it in our engine. ![Network Monitor](../images/blogs/week-13/network_monitor.png) #### Updates to Network Messaging API One of the things we left in our sea of TODOs is unifying `GenerateNetworkMessage` and `SendNetworkMessage` but we never found a good way to do it, neither did we have a better reason than, \"so developers can write less code\". Well, the need finally comes when we want to send a message from the server to all of the clients. If you remember our long journey of evolving network messages in the [week 7 blog post,](week-7.md#networking) each message generated on the server is only meant to be sent to a single client and `GenerateMessageFromServer` always take one parameter as client index. So when you want to send the same message to all clients, who do you generate it for? Here is the need in our API! Therefore, we decided to change our `SendMessage` functions from taking a message pointer to taking an `Action ` that initialize the message. Inside the function body, it will generate a new message and initialize that message with the given initializer. This also saves the developer from the tedious work of `reinterpret_cast` messages to their message type every time. By doing this, we effectively eliminated the need for calling both `Generate` and `Send` - they are nicely unified in one now! \u0002wzxhzdk:2\u0003 However, we also realized we shouldn't remove the old method because there are still scenarios where we need to pass in already built messages. For example, a lot of times the client would send a message to the server and the server needs to then broadcast that message to all clients. So we decided to leave it in\u2014however, this might be a potentially confusing API design for our users. #### Network Load Level The last feature we need in terms of game management is networked level loading. We thought about making it in the game and keeping it in the game, but this really seems like something many games need and would be good to have in the engine. However, to do that, we had to bring in `LevelManager` to our `NetworkingModule`. The `LevelManager` is used within a `NetworkModule` callback to signal all clients to also load the same level as the server, thus cannot be separated from the module. This can be seen as code pollution in some sense, but we decided to tolerate it at this stage. As mentioned before, we already have some other built-in network messages like `ClientConnectedMessage`, so it's easy to create another `LoadLevelMessage`. The rest of the task is again finding the right place to inject the code (like registering the callback for the message and sending the message) and making nice interfaces for game developers. A tricky part along the process is actually figuring out how to let messages hold and serialize strings. We initially created a `const char*` member variable in the messages without thinking too much about it but it didn't work! Because the string data we pass into the message is generated by another module and is a temporary object, and that temporary object deletes the data as soon as it's destroyed. We fixed it by changing the type to `char[]` so we can make a copy of that string to the message and let the message own that copy. Then the problem is how long do we want the `char[]` to be. For IPs it's easy - just make it 16 bytes long. But for things like level names we can never be sure how long they are, in the end, we gave it 64 cause we really don't think users will have level names more than 64 characters long\u2026 If they do, we give them an error for now. ### Network Discovery This is probably the last new feature we are adding to the game so it's a little sad to write about. Our demo game is supposed to be played with controllers and thus players should be able to create and join rooms using their controllers. However, with our current setup the user always need to _know_ and _type in_ the IP address of the host they want to join with a keyboard. Based on our previous experience of playing LAN (Local Area Network) games and using Unity's networking system, we had the idea of making a `NetworkDiscovery` class, which allows the hosts to broadcast messages to all devices within the same LAN and allows those devices to respond to the messages. The API design for this class is super easy: `Start/StopBroadcasting`, `Start/StopListening`, and `Add/RemoveMessageCallback` are all we needed. `StartBroadcasting` also accepts the message, broadcast duration, and interval as parameters. We again used our `Delegate` data structure to manage callbacks. The actual implementation is, however, not as simple as the API. We looked through yojimbo and the two networking libraries it's using ([netcode.io](https://github.com/networkprotocol/netcode.io) and [reliable.io](https://github.com/networkprotocol/reliable.io)) for message broadcasting functionalities but surprisingly didn't find anything useful! So we had to get our hands dirty with some basic socket programming. As we don't care about reliability, sequence, latency, etc \u2014 because we are just broadcasting messages where losing messages is really not a big deal \u2014 socket programming is shockingly easy! We followed this [tutorial](https://www.cs.rutgers.edu/~pxk/417/notes/sockets/udp.html) from Rutgers University for basic sockets set up and communication, and referred to the two source files ([BroadcastSender.c](http://cs.ecs.baylor.edu/~donahoo/practical/CSockets/code/BroadcastSender.c) and [BroadcastReceiver.c](http://cs.ecs.baylor.edu/~donahoo/practical/CSockets/code/BroadcastReceiver.c)) by Baylor University on how to configure sockets for broadcasting \u2014 and we are done! Your browser does not support the video tag. With `NetworkDiscovery`, we no longer rely on specifying server IP address in the config because it's included in the broadcast message. This will definitely create a better experience for our demo game's players, and be useful for developers who want to implement similar functionality \u2014 even though it can be used for a lot more! ## Monitoring Memory Leaks One of the side effects of having our own memory manager is that we never realized how much memory can be leaking on the freelist because it's just always silent. Good news is that we can easily track every single allocation and free! By doing that we can not only dump all of the memory leaks and blame the developers, but also provide valuable information for them to find and fix them! Even a small feature like this took us some iterations to get right. In the beginning, we were only tracking `Freelist::Alloc` and `Freelist::Free`. It does provide us with _some_ information, but I would say it helps much when all it presents is something like this: ![Memory Leaks Alloc](../images/blogs/week-13/memory_leak_1.png) > See the nice arrows?! Yeah, it took us a good 10 minutes to find their Unicode and figure out how to print them. We wanted more information! So we also added tracking to our `Freelist::New`, `Freelist::Delete` utilities and their array counterparts. So we get something like this when we forget to free these things: \u0002wzxhzdk:3\u0003 ![Memory Leaks New Delete](../images/blogs/week-13/memory_leak_2.png) They are nicer! But it still took a while to figure out what's leaking, so we added some string manipulation to \"translate\" the leaked resources to something that we can recognize, and got this: ![Memory Leaks String Manipulation](../images/blogs/week-13/memory_leak_3.png) It's a lot of unexpected work, and the performance may not be pretty\u2026 But the work definitely pays off as it's much easier to identify who is leaking, and we wrapped memory tracking in `_DEBUG` preprocessors so they won't get run in release mode. By adding this feature we not only fixed a bunch of memory leaks but also found some fishy allocations. For example, we once had several array allocations of size 0 - turned out they are from `Array `s of capacity 0, which is a corner case we should have taken care of. Last but not least, if you don't have memory leaks, our engine actually praises you! ![No memory leak](../images/blogs/week-13/memory_leak_4.png) > We should be doing more of this! Developers always love appreciation to their hard work, right? ## Engine vs. Game Resources In the past week, we have been working on getting more and more ready for our game jam. Part of that has been moving our levels from inside the engine to the `IsettaTestbed`. The reason for this shift is because these levels aren't really part of the engine, but rather levels that we are using to test out systems and demo different features. We are also going to leverage these levels for our game jammers as examples of how to use different features of the engine. This shift also caused us to move resource files, like model and audio files, from the engine to the testbed. However, there are certain resources like the primitive meshes and lighting shaders that should just be packaged with the engine. Part of this change affected the [`ExportHeaders` batch script](week-12.md#exporting-headers) which needed to copy over the resources from the engine that are needed to use the engine in general. This presented the problem of how should the engine know what path files should be loaded from, the engine or game? If we are packaging some of them with the engine while obviously the game developer will have their own files, how will we know? One solution, which we didn't think was a good solution, was to have the user \"import\" (copy) the engine resource files to their local game resources directory. There are a few problems we considered with this, one being it relies on the game developer to remember to copy files, which we had forgotten to do multiple times. The other is it muddles the game developer's folder with engine resources that they don't necessarily need knowledge of. So our alternate solution was to add an additional `engine_path` `CVar` to the config file, and similar to the `resource_path` which specifies where the game developer's resources are, the `engine_path` is the directory of the engine resources. Ideally, we would be able to know where the engine resources are relative to the game, however since our engine isn't something that is installed in the program files we can't guarantee its location and would rather allow the user the flexibility of moving it to where they would like. Our first idea with implementing the switch in the engine was to use `engine_path` as the path during `EngineLoop::StartUp` then switch back to `resource_path` before`EngineLoop::Update`, however, there were multiple problems with that. One of the problems was that `RenderModule` was caching the `resource_path` on its own startup (which would be `engine_path` at the time) causing the resources loaded after startup to be using the `engine_path` directory, even after reverting back to `resource_path` after startup so that wouldn't work. Also, another problem comes with components that are used by the game developer, such as during a level load, components like `LightComponent` should load its default light shader from the engine so the game developer doesn't _need_ to create their own, but can if they want. There are also the `Primitive` entities whose resources should also be loaded from the engine, every game developer doesn't need to create their own cube model. These systems won't work by setting the `resource_path` to the `engine_path` at startup. Ok, the next solution is to modify how the resources are loaded. Rather than caching `resource_path` and always prepending `resource_path` to the file path, it needs to be specified if it is \"in engine\" or \"in game\". For most components, this doesn't need to be exposed to the developer, and ideally it wouldn't be at all because anything the developer is specifying is \"in game\". However, for the `MeshComponent` which primitives rely on, we need to be able to specify within the constructor if it should load from the engine directory. For other components, like `LightComponent`, we don't have the option exposed to the user, however this is actually limiting features of the lighting in the case the game developer wants to specify their own default lighting (using the default constructor) they won't be able to because it will point to the engine path. This solution doesn't seem like the best and we think this is one of the systems we might put more time into when creating our next engine so not only is it easier to do something like this but easier to use in general. > Woah! Hit the brakes, we can't wait until our next engine to put more time into this. After writing this we found there was a **huge** problem with this method and had to completely refactor, the refactored changes. So there are two problems with this method: one occurs when running the executable and the other is associated with Horde3D's asset processing. For a while, we had been thinking about the consequences of our decisions when running the executable (.exe), however, until this week we hadn't really tried to. What happened was exactly as expected, a crash, because the resources weren't located in the same relative path as they were when debugging. So just get the resources in the build folder, right? Our first change was to perform a post-build copy of the resource files into the build folder. However, the engine path from the build folder wasn't guaranteed to be the same as for the project, thus would be wrong in the configuration file as well. This means the solution suggested above just wouldn't work, unless the engine path from the project was the same as from the build folder. That's a big if and big assumption. The other problem associated with Horde3D's asset processing is some shader and texture properties are just assumed to be in a certain folder directory, it is baked into the model description files during processing. We could dig into Horde3D to attempt to change this, but this is something we have told ourselves we don't want to modify Horde3D and would be better to work around. There is no way during load for us to know if this file is located in the engine or game resources. We could keep a list of resources in the engine directory, but that just doesn't feel right either. So _completely throw out the solution we had suggested above_. After basically reverting the idea of an `engine_path`, which wasn't nice in its own right because specifying paths in a config file isn't very clean, we came up with two solutions. The first solution was to copy the resources of the engine into the game resources folder (which is what we were trying to avoid to begin with). The problem with this solution is that we are putting the engine resources in their own folder to abstract those details from a developer. A game developer shouldn't need to see the render pipeline, for example, if they aren't interested in it. Even copying the files during a build event is still messy because although the developer isn't physically copying them, they would still see the files while browsing their resource folder. Our other solution was to copy the engine resources and game resources to a resources folder in the build folder, which was already needed to run the executable, and change the working directory of the solution to be the same as the build folder. The solution solves most of the problems that we have and most importantly lets us run both the solution and executable. There are two big drawbacks to this solution: 1) engine resources can and will be overwritten if a game resource has the same path and name 2) copying the resources to the build folder is a little unreliable. We worked for awhile trying to come up with a better way of copying the resource files, and the best solution we were able to do was on an actual build all the files of the resource folder will be copied then when individual resource files change, so long as they were included in the last build, those will be changed and copied over. Files that are added won't be copied before an actual build is triggered. Filesystem management hasn't been one of our biggest priorities with this engine, especially with being stuck with Horde3D's choices and assumptions, so we aren't going to advocate for this approach. One problem was that we didn't know what the requirements of a good filesystem were almost until it was too late; we all have vocalized that next time developing an engine this is something that should have more thought with early on. ## Patch Notes ### Collision Solving and Collision Callbacks As mentioned within [The Knight Game](#the-knight-game) section, the order of when the collision callbacks are called and the collision solving matters. Prior to any change, we were performing our collision checks which would then trigger any corresponding collision callback (OnEnter, OnStay, OnExit), then the collision solver would resolve the collisions so no two colliders were interpenetrating. The problem came from altering the position of a transform in the collision callback. The movement that would occur in the callback would basically be reverted/ignored because after the callback the solver would try to solve an outdated BVTree, it is outdated because the entity was moved but the tree wasn't updated. The solver would resolve the collision by shifting the collider out of the other with a closest point heuristic, this movement essentially overwrites any movement that happens in the callbacks. So why doesn't the callback update the tree? Why not just always have an up-to-date tree? The main reason is performance. The act of updating the tree would then require getting all AABB collision pairs and then intersections being recalculated, this is basically performing the `CollisionModule::Update` a second time. It already is a performance hog and now we have to do it twice! Well not quite. The just as easy, but not necessarily apparent solution, was to delay the callbacks for collisions until after the solver has solved, so the callbacks are separated from intersection tests. This delay wouldn't affect which callbacks were being called because the callbacks could be cached until after the solver had finished, thus aptly labeled `CollisionModule::LateUpdate`. The new order is now update the `BVTree`, grab the AABB collision pairs, test for intersections, solve for collisions, _then_ perform the callbacks. Just another example of how much ordering matters in a game engine. ### Static Entities When we were implementing our [collision solver](week-11.md#collision-solving), we added static attribute to the entities to mark whether the entity is moveable by the collision or anything else. However, we were not utilizing that attribute to manage the movement of the entity elsewhere. This week, we went back to fix this missing feature and to make the transform of a static entity unchangeable. It sounds simple, right? The only thing we needed to do seems to be checking the static-ness before changing the transform, with any `SetXXX` in the `Transform` class. This is not necessarily true. The main reason of it is that we cannot assign the transformation information, including the position, the rotation and the scale when creating the entity. And with the static attribute, we can neither set the transform later. This causes all static entities to be instantiated at the origin of the world with no rotation or scale, they all must be there! Poor statics! We changed our definition of static attribute later to solve this problem. The transform of a static entity is no longer unchangeable throughout the lifetime of the entity. Instead, it can be moved wherever and however you want until the level finish loading. As we are using `Level::Load` to replace the level scene file, this makes so much sense when compared with Unity and Unreal Editor where static entities/GameObjects can be moved before we hit the play button! ### Revisiting `transform` in the `Component` Class Last week, when we were replacing all `GetTransform` function to `transform` variable, we introduced two static member variables in `Component` class to deal with the const initialization issue: since we can only assign the value of a `Transform* const` in the constructor and we cannot pass in the transform for it to point to, we can just make a shared state in the `Component` class for the component to read in its constructor. It seems working well but, as mentioned last week, this method changes and exposes (not directly available for the game developer though) the shared state. This code smelled so bad that it kept us awake at night. This week, we came up with another method that can remove the static variable in the class but is still relatively unsafe, internally. Since C++ doesn't have a strict management of constant variables, we can cast the variable to `Transform&` and assign our transform to it. Before we removed the `GetTransform` function, the `AddComponent` was like this: \u0002wzxhzdk:4\u0003 After last week change, the `AddComponent` is like this: \u0002wzxhzdk:5\u0003 And now, it's like this: \u0002wzxhzdk:6\u0003 We still cannot sleep like a baby. It was us who set the transform pointer to be a pointer that points to a constant value; it was also us who broke the law and casted it to a normal pointer. This behavior is so dangerous that if any game developer tries to use this trick and change the transform of a component, the game will break, most likely. Luckily, we know what we are doing here, so we might just leave it here, for now. > This is where we would really love a C++ guru to save us. If you happen to be one, let us know how to fix it in the comments! ### Delegates One thing we forgot to mention weeks ago is that we abstracted our [callback-handle structure](week-3.md#input-module) out. Originally, we only had callbacks in input module, so we used member functions like `RegisterCallback` and `UnregisterCallback` in `InputModule` to handle them. Later we found that the game engine is much more event-driven than we thought. In addition to the event messaging system, which is designed for global events, we still need a unified structure to deal with point-to-point event subscription. Thus, just like what [Casey Muratori](../interviews/CaseyMuratori-interview.md#problem-2-the-complexity-explosion) said before, we wrote it out first, and then we pulled it out. This structure is called `Delegate`. Implementing the `Delegate` is not hard. The interface is simple, with only four functions: `Subscribe`, `Unsubscribe`, `Invoke` and `Clear`. In the first two functions, we integrated the `HandleBin` we implemented before so that it can revoke the handles after it's unsubscribed. It was a little bit tricky when we were implementing the `Invoke` function. It iterates through all the handle-function pairs and invokes the corresponding functions. However, simply using a range-based for loop is not enough. When the callback function is unsubscribing itself, it will invalidate the iterator we are visiting and thus break the `for` loop in the `Invoke` function. How do we deal with that? We decided to use a two phase strategy to invoke all the callbacks. In the first pass, we filtered all the callbacks out to a new array. We then called all callbacks in the new array in the second pass. Since unsubscribing only changes the original pair array, the array we are iterating through now is unaffected and safe. The related code is like this: \u0002wzxhzdk:7\u0003 There was another problem we needed to solve in `Delegate`. Due to the handle revocation, it's very hard to tell whether a handle originally owned by some object is still valid or not. Since the handle can be revoked during unsubscribing and be re-assigned when another object, the old owner can accidentally unsubscribe others' callback with the handle. The solution to that is we changed the starting number of the handle bin from 0 to 1 and defined that if a handle is 0, it is invalid. So now in every `Unsubscribe` function, we reset the passed-in handle to 0 to invalidate it. ### Move Assignment Operator for Arrays When we were working on [hitscan](#hitscan) for the feature game, we heavily used our `Collisions::Raycast` function, and eventually realized that we could probably use a `Collisions::RaycastAll` function as well. The addition of this function was straightforward because we basically only had to take the `Raycast` code and make it utilize an `Array` for any hits. Our solution for this uncovered both an amazing performance boost and a massive memory leak! Doubtlessly, these things were related. As it turns out, our move assignment operator for `Array` was _mostly_ correct, except for the fact that it didn't free all of our previous array data. We set up the operator to compare the data of the current and moved array, and delete if they were different. No more memory leak! (Also this brought our performance back to where it was previously, so we had to pull some other tricks out of our hat to bring up the performance). ### Window We've had a `WindowModule` since [week 2](week-2.md#the-window-module), but only the `RenderModule` and `GUIModule` had direct access to it. We've had some need for some window-like features but have been able to just shove them in other classes up until now. However, for our [knight game](#the-knight-game) we needed to use the actual window size but there was no good access to it. So we decided why not, like all our other modules, have an accessing class, appropriately named `Window`. The class is relatively simple acting as an access point to get width and height of the window as well as change some other GLFW properties. There are a few properties like the cursor sprite and visibility that we would like to change but ImGui is also managing those properties so it isn't as simple as adding in a function but actually getting it to function with multiple modules. ### Coming Soon Not much is left to be coming soon. We have one last blog post we will make about our last bit of development as well as posting about our GameJam. We will be trying to add some additional maneuverability to our documentation by adding a compendium tab, which will chronologize each section of the engine week by week as well as roping in the relevant interviews to that topic. We will also have postmortems coming to wrap up our project as a whole, so be on the look out for that. ## [Resources](/resources.md) Probably no one has added much to the page this week, since it has been more focus on refining and bug fixing over adding new features. Hopefully it will get one last boost in the next few weeks as we dumb any resources we've been hoarding on there! _Originally Published December 3, 2018._ Spatial partitioning is defined by Bob Nystrom as a way to \"efficiently locate objects by storing them in a data structure organized by their positions\". If you are interested in learning more, refer to the Spatial Partitioning chapter in his Game Programming Patterns book. \u21a9","title":"[Week 13] Is This a Game Engine?"},{"location":"blogs/week-13/#is-this-a-game-engine","text":"","title":"Is This a Game Engine?"},{"location":"blogs/week-13/#byte-sized-updates","text":"Full-Feature Demo Game : Implemented hitscan to be used in our feature game. The Knight Game : Developed another game that flexes some of our new components and our collision system that hasn't been shown in a game just yet. Collision Solving : Wrapped up our collision solver by making it moderately more correct, and added a \"mass\" feature. Build System : Set up a build pipeline for our engine after one too many rebuilds aggravated our programmers to no end. Networked Game Management : Added functions like start/stop server and client, added callbacks related to client connection status, and added networked level loading. Network Discovery : Added LAN (local area network) broadcasting functionality so we no longer need to type in IP by hand! Monitoring Memory Leaks : We added memory allocation/free tracking and let the user know what's leaking on our free list by object name and amount. Patch Notes : As we get to the end we seem to be patching more holes than making an actual boat. As you can see from the architectural diagram, the engine looks complete! We are still hunting bugs as we develop, but we are at feature lock (hopefully). This is actually harder for us than being still in development because there is so much more we would like to add and iterate on. We have accepted that we have to stop development to ensure the final release isn't broken. We are really focused on three things this week: Preparing for our game jam Starting our engine postmortems Finishing our full-feature demo game, including finishing necessary features for the game Now the real question we are asking is:","title":"Byte-Sized Updates"},{"location":"blogs/week-13/#full-feature-demo-game","text":"We're continuing to build our feature game . We still needed a few engine features discussed in this blog to finish the game, so we used that as an excuse to do some more engine development before putting down serious work into the game's foundation. (That being said, we've already made a basic version of it, so it shouldn't be too hard to just expand upon that!)","title":"Full-Feature Demo Game"},{"location":"blogs/week-13/#hitscan","text":"One of the features that we've begun work on is the shooting system. In our first game (and even in our Unreal example!), we effectively did collision checks on all of the enemies for each bullet. That's a really simple method for checking if bullets hit an object and it's easy to wrap one's mind around, but it's not terribly performant. How could you do this check faster, though? The answer is hitscan . Hitscan is simply a method of checking whether or not a bullet hits an object. It's pretty simple conceptually; when a gun is fired, you create a line from the gun in the direction of the gunshot and you check if that intersects any objects. You take the nearest intersected object and apply the gunshot to that object. Simple, right? When you go deeper into the feature, you might realize that there are more details that spill out of a hitscan system than you would think at first glance. For instance, what about bullet travel? Some guns shoot bullet more slowly than others. You can't just instantly check for an intersection if you want the bullet to have travel time, you need to simulate the travel time by only checking intersections with certain parts of the line segment depending on how long it's been since we fired the bullet. Instant hitscan travel Hitscan traveling over time","title":"Hitscan"},{"location":"blogs/week-13/#why-hitscan","text":"Above, we said that checking every object against every other object wouldn't be performant, but with a good collision system with spatial partitioning 1 , it would be pretty good, right? We can actually just reason about this to get a good answer, so long as we know what goes behind \"entities\" in an engine like ours. Each entity has a transform, and each transform needs to recalculate its own and all of its children's transformation matrices if it's moved, rotated, or scaled during the current frame. After doing all of that math, we still need to perform collision tests for every bullet object. But we're still not done! If a collision occurs and we remove the bullet from the scene, then we possibly need to regenerate our spatial partition in our collision system for the next frame. All of these operations aren't even including the overhead of data and runtime an entity might carry itself, being a generic object type that's used in our scene hierarchy. If that's not a good enough reason for investigating a faster method of bullet detection for you, then you will probably want to skip this section! However, we like to squeeze what we can out of our engine in terms of performance, and hitscan also poses a pretty interesting problem, so we moved forward with implementation.","title":"Why Hitscan?"},{"location":"blogs/week-13/#implementing-hitscan","text":"Like we said earlier, hitscan is something that gets larger and larger as you add more features into it. Fortunately, it's also naturally upgradable if you're mindful of how you implement it initially. We won't delve into too many details here because it's not quite engine development, but it is still a pretty interesting system to develop. We started our Hitscan component with a few decisions: Each bullet should not be its own entity (obviously) Each bullet should have its own properties, so if our gun starts shooting different bullets, our existing bullets won't change Bullets get blocked by any collider that is not a trigger Since we're building up from these decisions, they're the least likely to change in our system now, so it's important that they're chosen correctly. The one that may still change is that our bullets could get selectively blocked by different layers of colliders, but after a certain in point in development, we'll likely have to cement that decision in or face consequences later.","title":"Implementing Hitscan"},{"location":"blogs/week-13/#hitscan-is-easy-with-an-existing-collision-system","text":"For our first decision, we need a different method of simulating each bullet than giving each one its own Entity and (expensively) moving it using our scene hierarchy. Luckily, we've already implemented raycasting in our collisions , so we can leverage that functionality with the Ray class for performing hitscans. We also need to keep track of the travel distance for each bullet as well. In all, it should look a little something like this: // Check if the bullet collides with anything if ( Collisions :: Raycast ( bullet . ray , & hit , bulletRange ) && hit . GetDistance () - bullet . travel < bullet . speed * deltaTime ) { bullets . remove ( bullet ); // \u2026 Get the hit collider and run functions on it continue ; } // Propogate the bullet through the space bullet . travel += bullet . speed * deltaTime ; if ( bullet . travel > bullet . range ) { bullets . remove ( bullet ); } It's pretty straightforward. We check a raycast in our scene with our bullet, and if it hits anything then we kill it and run the appropriate code, and if it doesn't hit anything then we keep propagating it through the scene. That's it! Granted, this relies heavily on our collisions system with its intersection testing code, but once that's in place, the gameplay code is pretty easy.","title":"Hitscan is Easy (with an Existing Collision System)"},{"location":"blogs/week-13/#being-mindful-of-the-data","text":"Our second decision is a bit more interesting of a problem. For every bullet to have its own properties, we could store the bullet's properties in itself, and that would be easy. But think about that for a moment\u2014how often will we be changing the properties of our gun? Maybe a few times in a game with power-ups? It's not zero, so we can't just forgo each bullet having their own properties, but since the number is so low, we need to think of something that will be less wasteful than repeating data way more than it needs to be. This is where pointers are nice. We could use an unordered_map from bullet to property or something, but that's much more complex than we need. With pointers to property objects, we only need to store eight bytes worth of data in each bullet to have arbitrarily complex properties on them! Bullets that contain their own data Bullets that contain pointers to their shared data It's so simple and straightforward...what's the catch? For those of you who were suspicious of this solution, you were right to be wary. One thing that we need to be mindful of is when we should \"get rid of\" the property objects. When we've destroyed all of the bullets that are using a particular property, do we just leave it for eternity? And if we destroy the property objects, how do we know that no other bullets are referencing it? This problem is solved with reference counting, where we keep track of the number of bullets using the property object and we decrement our count whenever one of those bullets is destroyed. We also need to keep track of when properties on our gun change so that we can create a new property object for the bullets after that point. That's mostly just writing mutator functions for any of the properties on the gun, which is simply boilerplate. With all of that set up, the code boils down to this: // If any properties changed or we don't have any property objects if ( propertiesChanged || bulletProps . size () == 0 ) { // Create our property object and assign it to the bullet bullet . props = & bulletProps . emplace_back ( properties ); propertiesChanged = false ; } else { // We can just use our most recent property object bullet . props = & bulletProps . back (); } // Don't forget to increase the reference count! ++ bullet . props -> refCount ;","title":"Being Mindful of the Data"},{"location":"blogs/week-13/#the-results","text":"After these things, most of the work was iterative. The gun has more properties than the bullets do since we need things like fire rate, and we can implement more properties on the bullet in the monolith Update function like a piercing property. The work on the hitscan system was quick and iterative, and we made a lot of improvements over time. This is one of the perks of having a built engine underneath our game code because we didn't have to deal with any engine rebuilding to iterate. That being said, we did have to revisit our engine code a few times to provide functionality that we were missing! Check out some progress videos of our hitscan work! Your browser does not support the video tag. Your browser does not support the video tag. Your browser does not support the video tag. And here's a quick snapshot of the performance of our hitscan implementation. The top left contains frame rate info, and to its immediate right is our hitscan data. Over 4,000 shots at ~20ms is not bad, especially when you consider that 95% of the frame time is spent on the `DebugDraw` calls! ![Sick Hitscan Performance](../images/blogs/week-13/sick_hitscan_perf.png) ## The Knight Game We've spent the last couple of weeks creating another game, or something that sort of resembles one... The gist of the game is an infinite runner where you, as the knight, run from a barrier (a collider) and slash training dummies (capsules) to rack up a high score. The focused-on engine features were colliders and animation as we had yet to really have a game focus on either (although we included animation in our first game). The development was relatively intermittent since we were just adding in features that we thought might be interesting and we definitely didn't have a design plan going into it. The development started by gathering additional assets and reworking them in Blender, which as you would expect for a programmer, took longer than estimated. Our struggle with this was that the downloaded models were scaled at 100 times the unit scale of our engine and Horde3D's engine. Scaling a model in-engine isn't that difficult, but it is nice to have a model already at scale, so we attempted to scale in modeling software which broke the connection between the mesh/skeletal rig and animation. Initially, we had a model file which was a COLLADA (.dae) model with mesh and rig and we also had other .dae files which were purely animation with the same rig, no mesh data. Without scaling we were able to successfully import the mesh and animation into Unity, but once the mesh and animation were scaled, something about the .dae file format broke the connection, and so not even Unity could process the files this way. After a night of struggling with that, we decided to use animation files which contained the mesh (although only .anim files were being exported with the asset processor) which worked even after scaling. The biggest struggle of this was that we were at the whim of two things out of our control: 1) the .dae file format and 2) the Horde3D rendering engine. Although we could read the .dae file, once they were turned into Horde3D's .geo and .anim files, all bets were off for whether the file was being processed properly or if the actual rendering/animation code was breaking. Here is the knight moving at correct scale: Your browser does not support the video tag. Once the actual animation was working in the engine, we wanted to \"attach\" a sword to the knight character which would follow a certain joint in its rig. Prior [`MeshComponent::GetJointWorldTransform`](week-12.md#expose-more-functionalities-from-horde3d) there was no way to do this. The functionality before wasn't sufficient because although the sword could have been parented to the knight, there is no way to get the hand's transform information. With this function, the sword can be attached to the hands and will move through an animation. We were then able to get the knight chopping with his sword! Your browser does not support the video tag. With the assets set up, we wanted to focus on the collision of the sword with an \"enemy\". Keeping it simple, we just wanted to ensure we could align a collider with a mesh as desired. What we found out was, first, the pivot of the sword was in a terrible location, but more importantly, the collider drawn was displaying shearing as spoken about [last week](week-12.md#collider-debug-shearing). After fixing that, we attached a `CollisionHandler` to a box to cause a splitting effect. This was a great exercise in using our own API. It had been a few weeks since some of us had touched collisions, so remembering how they worked took a minute. Because there weren't any examples of how to use the `CollisionHandler` before, it hadn't occurred to even use some functionality, such as the `IgnoreCollisionLayer`. Here is how it was looking: Your browser does not support the video tag. Well that obviously looks (and is!) broken. There were a few things with the collision system that needed small bug fixes. One of those fixes was with regards to a box and capsule, we had noticed that a simple case of box-capsule collision wasn't being detected. Going into the box-capsule code was a nightmare for us because this particular collision detection was just not performed the way we needed this particular collision detection to be performed. Luckily, the error was found quickly by using our debug drawing system to find where the contact and closest point were and stepping through the code. In the end, we got the collision working (hopefully the full-feature demo game won't need more fixes)! With all of the necessary capsule collisions working, the knight who had a capsule collider surrounding him could be killed, and the alive enemies who were represented as capsules could be cycled (basically a pool that would translate to a position on the other side of the screen once they went off-screen). When we began cycling the enemies, the game started to feel like an actual game with a lose state and a goal! However there was something weird happening with when the capsules collided with the wall to reset their position. The collision would happen and be detected\u2014everything working so far. The `CollisionHandler` callback would then set the position of the entity to the other side of the screen\u2014that works. Then the object would teleport back to point of collision... wait what? The solution deals with the ordering of `CollisionSolver::Update` and when the collision callbacks occur; [read more below](#collision-solving-and-collision-callbacks). With all these bugs ironed out, the game took shape and although simple is slightly enjoyable to play. Here's what a slice of gameplay looks like: Your browser does not support the video tag. ## Collision Solving Collisions and physics, in general, is a deep field in video games, and beyond features and correctness, optimizations can always be done. As such, we decided to wrap up our collision solving system by getting it to be close to correct and leave it at that. One very convenient aspect of the collisions part of the field is that getting everything solved and good _eventually_ is usually good enough\u2014it will just mean a couple rendered frames of jitter or so. ### The Math of a Box [Last time](week-11.md#future-plans-with-collision-solving), we mentioned that we had a pretty good idea for how to make our box collision responses more...correct. Previously, our boxes would correctly detect the collision, but on most of the edges the box would shove itself and the other collider completely out of the way of each other if the collision point was far enough from the center of one of the box's faces: Your browser does not support the video tag. The reason this happened is because our collision detection for box colliders would pass the collision point to the collision solver, which would then try to extrapolate the push direction of the collision from that point. This might not seem like a problem for things like spheres, but for boxes, it's a different story. When the collision point on that box is on a face instead of an edge, it's really easy to figure out which face of the box determines which direction the colliders are pushed in: ![Easy Box Collision Solve](../images/blogs/week-13/box_collision_solve_easy.png) But what about when the collision point is on an edge? That is, when the point lies at the max extent of two faces as opposed to just one? ![Not As Easy Box Collision Solve](../images/blogs/week-13/box_collision_solve_not_as_easy.png) In our case, the box would get pushed to the side because code is run sequentially. When determining the furthest distance on an axis, our code always checks in the order of the x-axis, then the y-axis, then the z-axis. If the point is equally distanced on the x-axis and the z-axis, then the x-axis wins. This can be solved by doing some of the collision solving work while we're detecting the collision point. The other collider is probably not perfectly distanced at the corner of our box collider; it's probably a little closer to one face or the other. While we're determining the collision point, we can also check for the furthest axis distance _without being constrained to the extents of the box_. We store that information for later, where the collision solver can then respond appropriately: ![Solved the Not Easy Box Collision](../images/blogs/week-13/solved_not_easy_box_collision.png) Your browser does not support the video tag. ### That Epsilon Spice We may have gotten some silky smooth box colliding now, but we're not devoid of troubles yet: Your browser does not support the video tag. As neat of a gameplay mechanic as stickiness can be, it's not exactly great as a game developer to have to make all of your games gum-themed. Fortunately, this problem is easy to solve. When collision solving, you shouldn't just put the colliders in the correct position depending on the collision\u2014you should put them a little further, an epsilon amount, from the solved position. That way, you aren't colliding again after you solve the collision. ### \"Mass\" in a Collision System One last small feature one of our developers convinced us to develop for the collisions system is something that's usually thought of more as a physics parameter: mass. Obviously, we're not developing an entire physics system for this one last feature, so what do we mean by \"mass\"? Well, the way our collision solver currently works, everything is considered to be equally movable barring any static colliders. This means that a box that is 100x the size of another box can get shoved around by the tiny box as easily as by any other collider. Your browser does not support the video tag. This happens because each collider receives an equal part of the collision response when wrapping up the collision solver calculations. Thankfully for us, having a system that calculates out a shared collision response that gets split between colliders is really convenient when we want to upgrade the system to unevenly distribute that response. By adding a `mass` field to the `Collider` class, we can now just check against the weighting of the colliders to determine how \"heavy\" certain objects should be! Your browser does not support the video tag. > An interesting feature that this also adds to our engine is \"negative mass\". That effectively makes objects counteract any objects that push on them, which is actually something that could be useful for gameplay! ## Build System [Last week](week-12.md#exporting-headers), we talked about how we needed to export all of our headers in order for people to use our built engine. This got us most of the way there, but we were quickly finding that we need to rebuild our engine DLL's often during gameplay development (so many bugs!) then re-run our export tool and do a bunch of file shifting. The cherry on top? We would need to update our Git repository with the engine and everything, which would usually involve annoying merge problems (our opinion of Git has steadily dropped throughout the semester). Being programmers, we decided that this repetition was not worth our time and we started drafting up a build system for the engine. We've already been using [Jenkins](https://jenkins.io) on a shared machine in the room for building our website, so we've already got the pipeline there, we just need to automate the steps for building the engine. Thanks to our batch file that handles the exporting of the build and the header files, the main technical part that we needed to add was building each configuration of the engine, which just involved getting `devenv.exe` from the Visual Studio program files added to the PATH environment variable and running it with the correct parameters in order to rebuild. Once that was working, we still needed the computer to push the built engine to a shared location. We decided on that should be our [feature game repository](https://github.com/Isetta-Team/Isetta-Game), and set off in the land of Git commands. Adding the newly built files to a commit and pushing that to the repo was easy, but getting the commit message to be our most recent engine commit message was a bit more annoying in batch script because we had to be mindful of escape characters (which is ^ in batch). After we got a build pipeline working, we quickly realized that pushing up a new build regardless of whether we have built the engine at that commit is not a good idea. If we want to debug a problematic build, we'd have to look through dozens or hundreds of commits! So the last feature we built into our build system was an up-to-date verification system, which we already had a good reference for from a past Jenkins function, but it still caused quite a few headaches. The basic idea was to write our pushed commits to a build history file, which we then read on later builds to confirm that that commit hasn't already been built and pushed. Unfortunately, Jenkins does not remember system variables across build steps, so we had to come up with a very janky solution of creating and conditionally deleting a file to determine whether or not we should be trying to build the engine again. All in all, this whole process took about 12 hours worth of development time. Was it worth it? In the long run, it very likely would be; after all, we were already feeling pretty aggravated because of our build times. However, in the short term, we may not see too many dividends. ## Networking ### Networked Game Management This week we finally got the chance to extend our networking module and add game management features. The features our game needs are pretty simple - clients and server just need to know when each other is connected and disconnected, so we added four callbacks: * `OnConnectedToServer`: a client callback called when a client successfully connects to the server, mainly for initialization right after the connection is established * `OnDisconnectedFromServer`: a client callback called when the client is disconnected from the server - either caused by losing connection or manually calling `StopClient`. Developers can use this to stop their networking features and keep track of networking state * `OnClientConnected`: a server callback called when any client is connected. To provide more valuable information like client's IP address, we also created a `ClientInfo` struct that's sent to the server when the client is connected. * `OnClientDisconnected`: a server callback called when any client is disconnected. Because the client may drop out any time without notifying the server about that, this event is monitored on the server every frame and is called when any client's state changed The new addition of our [`Delegate`](#delegates) data structure made implementing these callbacks super easy! All we needed to do is create an interface for subscribing and unsubscribing from the events, finding the right places to invoke the callbacks and send network messages, and making several maps to track clients' state. Now, we can get nice messages like this by using the new callbacks (we were running host so we get all callbacks): ![Using network callbacks](../images/blogs/week-13/network_callbacks.png) We then discovered that our API for creating server and client were not unified with one being called `CreateServer` while another called `ConnectToServer`, and we didn't have a function for \"starting as host\". We refactored those functions into `StartClient`, `StartServer`, `CloseClient`, `CloseServer`, etc., which we think will be more intuitive for the developers. #### Network Role Monitoring Another feature we need for the game is to easily get the network role of their instance, \"am I running as host? Or server? Or client?\" Luckily, yojimbo already exposes simple state querying functions like `Client::IsRunning` so we were able to easily make a wrapper on top of those for the developers to use. When debugging them, we also made a component that draws network information through our GUI as shown in the picture below. In the end, we thought this can also be useful for the game developers, so we refactored it and put it in our engine. ![Network Monitor](../images/blogs/week-13/network_monitor.png) #### Updates to Network Messaging API One of the things we left in our sea of TODOs is unifying `GenerateNetworkMessage` and `SendNetworkMessage` but we never found a good way to do it, neither did we have a better reason than, \"so developers can write less code\". Well, the need finally comes when we want to send a message from the server to all of the clients. If you remember our long journey of evolving network messages in the [week 7 blog post,](week-7.md#networking) each message generated on the server is only meant to be sent to a single client and `GenerateMessageFromServer` always take one parameter as client index. So when you want to send the same message to all clients, who do you generate it for? Here is the need in our API! Therefore, we decided to change our `SendMessage` functions from taking a message pointer to taking an `Action ` that initialize the message. Inside the function body, it will generate a new message and initialize that message with the given initializer. This also saves the developer from the tedious work of `reinterpret_cast` messages to their message type every time. By doing this, we effectively eliminated the need for calling both `Generate` and `Send` - they are nicely unified in one now! \u0002wzxhzdk:2\u0003 However, we also realized we shouldn't remove the old method because there are still scenarios where we need to pass in already built messages. For example, a lot of times the client would send a message to the server and the server needs to then broadcast that message to all clients. So we decided to leave it in\u2014however, this might be a potentially confusing API design for our users. #### Network Load Level The last feature we need in terms of game management is networked level loading. We thought about making it in the game and keeping it in the game, but this really seems like something many games need and would be good to have in the engine. However, to do that, we had to bring in `LevelManager` to our `NetworkingModule`. The `LevelManager` is used within a `NetworkModule` callback to signal all clients to also load the same level as the server, thus cannot be separated from the module. This can be seen as code pollution in some sense, but we decided to tolerate it at this stage. As mentioned before, we already have some other built-in network messages like `ClientConnectedMessage`, so it's easy to create another `LoadLevelMessage`. The rest of the task is again finding the right place to inject the code (like registering the callback for the message and sending the message) and making nice interfaces for game developers. A tricky part along the process is actually figuring out how to let messages hold and serialize strings. We initially created a `const char*` member variable in the messages without thinking too much about it but it didn't work! Because the string data we pass into the message is generated by another module and is a temporary object, and that temporary object deletes the data as soon as it's destroyed. We fixed it by changing the type to `char[]` so we can make a copy of that string to the message and let the message own that copy. Then the problem is how long do we want the `char[]` to be. For IPs it's easy - just make it 16 bytes long. But for things like level names we can never be sure how long they are, in the end, we gave it 64 cause we really don't think users will have level names more than 64 characters long\u2026 If they do, we give them an error for now. ### Network Discovery This is probably the last new feature we are adding to the game so it's a little sad to write about. Our demo game is supposed to be played with controllers and thus players should be able to create and join rooms using their controllers. However, with our current setup the user always need to _know_ and _type in_ the IP address of the host they want to join with a keyboard. Based on our previous experience of playing LAN (Local Area Network) games and using Unity's networking system, we had the idea of making a `NetworkDiscovery` class, which allows the hosts to broadcast messages to all devices within the same LAN and allows those devices to respond to the messages. The API design for this class is super easy: `Start/StopBroadcasting`, `Start/StopListening`, and `Add/RemoveMessageCallback` are all we needed. `StartBroadcasting` also accepts the message, broadcast duration, and interval as parameters. We again used our `Delegate` data structure to manage callbacks. The actual implementation is, however, not as simple as the API. We looked through yojimbo and the two networking libraries it's using ([netcode.io](https://github.com/networkprotocol/netcode.io) and [reliable.io](https://github.com/networkprotocol/reliable.io)) for message broadcasting functionalities but surprisingly didn't find anything useful! So we had to get our hands dirty with some basic socket programming. As we don't care about reliability, sequence, latency, etc \u2014 because we are just broadcasting messages where losing messages is really not a big deal \u2014 socket programming is shockingly easy! We followed this [tutorial](https://www.cs.rutgers.edu/~pxk/417/notes/sockets/udp.html) from Rutgers University for basic sockets set up and communication, and referred to the two source files ([BroadcastSender.c](http://cs.ecs.baylor.edu/~donahoo/practical/CSockets/code/BroadcastSender.c) and [BroadcastReceiver.c](http://cs.ecs.baylor.edu/~donahoo/practical/CSockets/code/BroadcastReceiver.c)) by Baylor University on how to configure sockets for broadcasting \u2014 and we are done! Your browser does not support the video tag. With `NetworkDiscovery`, we no longer rely on specifying server IP address in the config because it's included in the broadcast message. This will definitely create a better experience for our demo game's players, and be useful for developers who want to implement similar functionality \u2014 even though it can be used for a lot more! ## Monitoring Memory Leaks One of the side effects of having our own memory manager is that we never realized how much memory can be leaking on the freelist because it's just always silent. Good news is that we can easily track every single allocation and free! By doing that we can not only dump all of the memory leaks and blame the developers, but also provide valuable information for them to find and fix them! Even a small feature like this took us some iterations to get right. In the beginning, we were only tracking `Freelist::Alloc` and `Freelist::Free`. It does provide us with _some_ information, but I would say it helps much when all it presents is something like this: ![Memory Leaks Alloc](../images/blogs/week-13/memory_leak_1.png) > See the nice arrows?! Yeah, it took us a good 10 minutes to find their Unicode and figure out how to print them. We wanted more information! So we also added tracking to our `Freelist::New`, `Freelist::Delete` utilities and their array counterparts. So we get something like this when we forget to free these things: \u0002wzxhzdk:3\u0003 ![Memory Leaks New Delete](../images/blogs/week-13/memory_leak_2.png) They are nicer! But it still took a while to figure out what's leaking, so we added some string manipulation to \"translate\" the leaked resources to something that we can recognize, and got this: ![Memory Leaks String Manipulation](../images/blogs/week-13/memory_leak_3.png) It's a lot of unexpected work, and the performance may not be pretty\u2026 But the work definitely pays off as it's much easier to identify who is leaking, and we wrapped memory tracking in `_DEBUG` preprocessors so they won't get run in release mode. By adding this feature we not only fixed a bunch of memory leaks but also found some fishy allocations. For example, we once had several array allocations of size 0 - turned out they are from `Array `s of capacity 0, which is a corner case we should have taken care of. Last but not least, if you don't have memory leaks, our engine actually praises you! ![No memory leak](../images/blogs/week-13/memory_leak_4.png) > We should be doing more of this! Developers always love appreciation to their hard work, right? ## Engine vs. Game Resources In the past week, we have been working on getting more and more ready for our game jam. Part of that has been moving our levels from inside the engine to the `IsettaTestbed`. The reason for this shift is because these levels aren't really part of the engine, but rather levels that we are using to test out systems and demo different features. We are also going to leverage these levels for our game jammers as examples of how to use different features of the engine. This shift also caused us to move resource files, like model and audio files, from the engine to the testbed. However, there are certain resources like the primitive meshes and lighting shaders that should just be packaged with the engine. Part of this change affected the [`ExportHeaders` batch script](week-12.md#exporting-headers) which needed to copy over the resources from the engine that are needed to use the engine in general. This presented the problem of how should the engine know what path files should be loaded from, the engine or game? If we are packaging some of them with the engine while obviously the game developer will have their own files, how will we know? One solution, which we didn't think was a good solution, was to have the user \"import\" (copy) the engine resource files to their local game resources directory. There are a few problems we considered with this, one being it relies on the game developer to remember to copy files, which we had forgotten to do multiple times. The other is it muddles the game developer's folder with engine resources that they don't necessarily need knowledge of. So our alternate solution was to add an additional `engine_path` `CVar` to the config file, and similar to the `resource_path` which specifies where the game developer's resources are, the `engine_path` is the directory of the engine resources. Ideally, we would be able to know where the engine resources are relative to the game, however since our engine isn't something that is installed in the program files we can't guarantee its location and would rather allow the user the flexibility of moving it to where they would like. Our first idea with implementing the switch in the engine was to use `engine_path` as the path during `EngineLoop::StartUp` then switch back to `resource_path` before`EngineLoop::Update`, however, there were multiple problems with that. One of the problems was that `RenderModule` was caching the `resource_path` on its own startup (which would be `engine_path` at the time) causing the resources loaded after startup to be using the `engine_path` directory, even after reverting back to `resource_path` after startup so that wouldn't work. Also, another problem comes with components that are used by the game developer, such as during a level load, components like `LightComponent` should load its default light shader from the engine so the game developer doesn't _need_ to create their own, but can if they want. There are also the `Primitive` entities whose resources should also be loaded from the engine, every game developer doesn't need to create their own cube model. These systems won't work by setting the `resource_path` to the `engine_path` at startup. Ok, the next solution is to modify how the resources are loaded. Rather than caching `resource_path` and always prepending `resource_path` to the file path, it needs to be specified if it is \"in engine\" or \"in game\". For most components, this doesn't need to be exposed to the developer, and ideally it wouldn't be at all because anything the developer is specifying is \"in game\". However, for the `MeshComponent` which primitives rely on, we need to be able to specify within the constructor if it should load from the engine directory. For other components, like `LightComponent`, we don't have the option exposed to the user, however this is actually limiting features of the lighting in the case the game developer wants to specify their own default lighting (using the default constructor) they won't be able to because it will point to the engine path. This solution doesn't seem like the best and we think this is one of the systems we might put more time into when creating our next engine so not only is it easier to do something like this but easier to use in general. > Woah! Hit the brakes, we can't wait until our next engine to put more time into this. After writing this we found there was a **huge** problem with this method and had to completely refactor, the refactored changes. So there are two problems with this method: one occurs when running the executable and the other is associated with Horde3D's asset processing. For a while, we had been thinking about the consequences of our decisions when running the executable (.exe), however, until this week we hadn't really tried to. What happened was exactly as expected, a crash, because the resources weren't located in the same relative path as they were when debugging. So just get the resources in the build folder, right? Our first change was to perform a post-build copy of the resource files into the build folder. However, the engine path from the build folder wasn't guaranteed to be the same as for the project, thus would be wrong in the configuration file as well. This means the solution suggested above just wouldn't work, unless the engine path from the project was the same as from the build folder. That's a big if and big assumption. The other problem associated with Horde3D's asset processing is some shader and texture properties are just assumed to be in a certain folder directory, it is baked into the model description files during processing. We could dig into Horde3D to attempt to change this, but this is something we have told ourselves we don't want to modify Horde3D and would be better to work around. There is no way during load for us to know if this file is located in the engine or game resources. We could keep a list of resources in the engine directory, but that just doesn't feel right either. So _completely throw out the solution we had suggested above_. After basically reverting the idea of an `engine_path`, which wasn't nice in its own right because specifying paths in a config file isn't very clean, we came up with two solutions. The first solution was to copy the resources of the engine into the game resources folder (which is what we were trying to avoid to begin with). The problem with this solution is that we are putting the engine resources in their own folder to abstract those details from a developer. A game developer shouldn't need to see the render pipeline, for example, if they aren't interested in it. Even copying the files during a build event is still messy because although the developer isn't physically copying them, they would still see the files while browsing their resource folder. Our other solution was to copy the engine resources and game resources to a resources folder in the build folder, which was already needed to run the executable, and change the working directory of the solution to be the same as the build folder. The solution solves most of the problems that we have and most importantly lets us run both the solution and executable. There are two big drawbacks to this solution: 1) engine resources can and will be overwritten if a game resource has the same path and name 2) copying the resources to the build folder is a little unreliable. We worked for awhile trying to come up with a better way of copying the resource files, and the best solution we were able to do was on an actual build all the files of the resource folder will be copied then when individual resource files change, so long as they were included in the last build, those will be changed and copied over. Files that are added won't be copied before an actual build is triggered. Filesystem management hasn't been one of our biggest priorities with this engine, especially with being stuck with Horde3D's choices and assumptions, so we aren't going to advocate for this approach. One problem was that we didn't know what the requirements of a good filesystem were almost until it was too late; we all have vocalized that next time developing an engine this is something that should have more thought with early on. ## Patch Notes ### Collision Solving and Collision Callbacks As mentioned within [The Knight Game](#the-knight-game) section, the order of when the collision callbacks are called and the collision solving matters. Prior to any change, we were performing our collision checks which would then trigger any corresponding collision callback (OnEnter, OnStay, OnExit), then the collision solver would resolve the collisions so no two colliders were interpenetrating. The problem came from altering the position of a transform in the collision callback. The movement that would occur in the callback would basically be reverted/ignored because after the callback the solver would try to solve an outdated BVTree, it is outdated because the entity was moved but the tree wasn't updated. The solver would resolve the collision by shifting the collider out of the other with a closest point heuristic, this movement essentially overwrites any movement that happens in the callbacks. So why doesn't the callback update the tree? Why not just always have an up-to-date tree? The main reason is performance. The act of updating the tree would then require getting all AABB collision pairs and then intersections being recalculated, this is basically performing the `CollisionModule::Update` a second time. It already is a performance hog and now we have to do it twice! Well not quite. The just as easy, but not necessarily apparent solution, was to delay the callbacks for collisions until after the solver has solved, so the callbacks are separated from intersection tests. This delay wouldn't affect which callbacks were being called because the callbacks could be cached until after the solver had finished, thus aptly labeled `CollisionModule::LateUpdate`. The new order is now update the `BVTree`, grab the AABB collision pairs, test for intersections, solve for collisions, _then_ perform the callbacks. Just another example of how much ordering matters in a game engine. ### Static Entities When we were implementing our [collision solver](week-11.md#collision-solving), we added static attribute to the entities to mark whether the entity is moveable by the collision or anything else. However, we were not utilizing that attribute to manage the movement of the entity elsewhere. This week, we went back to fix this missing feature and to make the transform of a static entity unchangeable. It sounds simple, right? The only thing we needed to do seems to be checking the static-ness before changing the transform, with any `SetXXX` in the `Transform` class. This is not necessarily true. The main reason of it is that we cannot assign the transformation information, including the position, the rotation and the scale when creating the entity. And with the static attribute, we can neither set the transform later. This causes all static entities to be instantiated at the origin of the world with no rotation or scale, they all must be there! Poor statics! We changed our definition of static attribute later to solve this problem. The transform of a static entity is no longer unchangeable throughout the lifetime of the entity. Instead, it can be moved wherever and however you want until the level finish loading. As we are using `Level::Load` to replace the level scene file, this makes so much sense when compared with Unity and Unreal Editor where static entities/GameObjects can be moved before we hit the play button! ### Revisiting `transform` in the `Component` Class Last week, when we were replacing all `GetTransform` function to `transform` variable, we introduced two static member variables in `Component` class to deal with the const initialization issue: since we can only assign the value of a `Transform* const` in the constructor and we cannot pass in the transform for it to point to, we can just make a shared state in the `Component` class for the component to read in its constructor. It seems working well but, as mentioned last week, this method changes and exposes (not directly available for the game developer though) the shared state. This code smelled so bad that it kept us awake at night. This week, we came up with another method that can remove the static variable in the class but is still relatively unsafe, internally. Since C++ doesn't have a strict management of constant variables, we can cast the variable to `Transform&` and assign our transform to it. Before we removed the `GetTransform` function, the `AddComponent` was like this: \u0002wzxhzdk:4\u0003 After last week change, the `AddComponent` is like this: \u0002wzxhzdk:5\u0003 And now, it's like this: \u0002wzxhzdk:6\u0003 We still cannot sleep like a baby. It was us who set the transform pointer to be a pointer that points to a constant value; it was also us who broke the law and casted it to a normal pointer. This behavior is so dangerous that if any game developer tries to use this trick and change the transform of a component, the game will break, most likely. Luckily, we know what we are doing here, so we might just leave it here, for now. > This is where we would really love a C++ guru to save us. If you happen to be one, let us know how to fix it in the comments! ### Delegates One thing we forgot to mention weeks ago is that we abstracted our [callback-handle structure](week-3.md#input-module) out. Originally, we only had callbacks in input module, so we used member functions like `RegisterCallback` and `UnregisterCallback` in `InputModule` to handle them. Later we found that the game engine is much more event-driven than we thought. In addition to the event messaging system, which is designed for global events, we still need a unified structure to deal with point-to-point event subscription. Thus, just like what [Casey Muratori](../interviews/CaseyMuratori-interview.md#problem-2-the-complexity-explosion) said before, we wrote it out first, and then we pulled it out. This structure is called `Delegate`. Implementing the `Delegate` is not hard. The interface is simple, with only four functions: `Subscribe`, `Unsubscribe`, `Invoke` and `Clear`. In the first two functions, we integrated the `HandleBin` we implemented before so that it can revoke the handles after it's unsubscribed. It was a little bit tricky when we were implementing the `Invoke` function. It iterates through all the handle-function pairs and invokes the corresponding functions. However, simply using a range-based for loop is not enough. When the callback function is unsubscribing itself, it will invalidate the iterator we are visiting and thus break the `for` loop in the `Invoke` function. How do we deal with that? We decided to use a two phase strategy to invoke all the callbacks. In the first pass, we filtered all the callbacks out to a new array. We then called all callbacks in the new array in the second pass. Since unsubscribing only changes the original pair array, the array we are iterating through now is unaffected and safe. The related code is like this: \u0002wzxhzdk:7\u0003 There was another problem we needed to solve in `Delegate`. Due to the handle revocation, it's very hard to tell whether a handle originally owned by some object is still valid or not. Since the handle can be revoked during unsubscribing and be re-assigned when another object, the old owner can accidentally unsubscribe others' callback with the handle. The solution to that is we changed the starting number of the handle bin from 0 to 1 and defined that if a handle is 0, it is invalid. So now in every `Unsubscribe` function, we reset the passed-in handle to 0 to invalidate it. ### Move Assignment Operator for Arrays When we were working on [hitscan](#hitscan) for the feature game, we heavily used our `Collisions::Raycast` function, and eventually realized that we could probably use a `Collisions::RaycastAll` function as well. The addition of this function was straightforward because we basically only had to take the `Raycast` code and make it utilize an `Array` for any hits. Our solution for this uncovered both an amazing performance boost and a massive memory leak! Doubtlessly, these things were related. As it turns out, our move assignment operator for `Array` was _mostly_ correct, except for the fact that it didn't free all of our previous array data. We set up the operator to compare the data of the current and moved array, and delete if they were different. No more memory leak! (Also this brought our performance back to where it was previously, so we had to pull some other tricks out of our hat to bring up the performance). ### Window We've had a `WindowModule` since [week 2](week-2.md#the-window-module), but only the `RenderModule` and `GUIModule` had direct access to it. We've had some need for some window-like features but have been able to just shove them in other classes up until now. However, for our [knight game](#the-knight-game) we needed to use the actual window size but there was no good access to it. So we decided why not, like all our other modules, have an accessing class, appropriately named `Window`. The class is relatively simple acting as an access point to get width and height of the window as well as change some other GLFW properties. There are a few properties like the cursor sprite and visibility that we would like to change but ImGui is also managing those properties so it isn't as simple as adding in a function but actually getting it to function with multiple modules. ### Coming Soon Not much is left to be coming soon. We have one last blog post we will make about our last bit of development as well as posting about our GameJam. We will be trying to add some additional maneuverability to our documentation by adding a compendium tab, which will chronologize each section of the engine week by week as well as roping in the relevant interviews to that topic. We will also have postmortems coming to wrap up our project as a whole, so be on the look out for that. ## [Resources](/resources.md) Probably no one has added much to the page this week, since it has been more focus on refining and bug fixing over adding new features. Hopefully it will get one last boost in the next few weeks as we dumb any resources we've been hoarding on there! _Originally Published December 3, 2018._ Spatial partitioning is defined by Bob Nystrom as a way to \"efficiently locate objects by storing them in a data structure organized by their positions\". If you are interested in learning more, refer to the Spatial Partitioning chapter in his Game Programming Patterns book. \u21a9","title":"The Results"},{"location":"blogs/week-14/","text":"The Last Blog. \u00b6 Byte-Sized Updates \u00b6 Documentary : Our team producer filmed our journey and process of developing a game engine. We have a short trailer, and the full documentary is 10 minutes long. Game Jam : Held a short game jam with nine successful (non-crashing) games! Videos and downloads of those games below!! Full-Feature Demo Game : Completed our target game, with all the features of the original one showed all the way back in Week 0 (and a few more)! Engine Reflection : Reflected on our progress as engine developers and the development of the engine. As the title states, this is the last blog post for the Isetta Engine project. We have spent the last 15 weeks building a game engine and documenting our decisions through these blogs, and now we have to say goodbye. This has been an arduous journey for us and we are glad to say, we've built a game engine. There are certainly bugs littered throughout the engine, but for us, having a bug free engine was never the intention\u2014it was to learn the engine development process. We feel we have been able to do this, and we hope that those of you who have had a chance to read our previous blogs now think that engine development isn't as scary or mysterious as you may have thought before. But before we truly sign off, we have some last bits of updates you may be interested in reading. Documentary \u00b6 As you might have seen on our homepage's timeline, we have been working on a documentary on top of all of the other things. While the developers on the team were trying to demystify the technical aspect of game engine development, our producer and resident creative was really pushing to help sell our work to the non-engine developers of the world. What we found was that most laypeople don't have an understanding of what a game engine is, and why would they? Most of them just care about the games made with the engine. However, there is a human side to game engine development that we think is valuable for people to see and understand, which the documentary attempts to portray. The documentary is 10 minutes summarizing our journey from developers to engine programmers. For a trailer: And if you are still interested, the full documentary is right here! Game Jam \u00b6 As we've been mentioning in our past few weeks' blogs, we were planning, and now have, held a game jam. The game jam was never part of the plan for the 15 weeks of development on the engine, particularly because we never bought into the illusion that people would want to use our engine over any other option. However, as the weeks progressed, we often found our discussions gravitating towards the question of what a developer would be left with as a result of our decisions. We figured if we were talking about the hypothetical developer of our engine so much, why not put our engine to the test? We were also encouraged by our advisers and faculty to figure out ways to measure our project's success. (Disclaimer: We aren't saying that this is necessarily the right way to do that.) Preparing for the Jam \u00b6 We spent a few weeks in preparation, which mainly focused on the API design of the engine and trying to test the features we thought the game jammers would use. In the days prior to the game jam, we converted our Git repo's README into a webpage on our website, putting more emphasis into the accuracy, explanation, and readability of the content. And the night prior to the game jam, we thought it would be a good idea to comment the codebase that would most likely be peeked at by the jammers, like the scene header files and anything we made that inherited from our Component class. We also made the bold (and dumb) choice of doing some refactors of the codebase that same night while we were commenting. How could we have thought that was a good idea? Well, it was about 2 or 3 in the morning, and we saw that a method name was misleading. So we fixed that. Oh, and that function should be private because it's just used internally. Oh, and this function doesn't seem to be used, we should remove it! And as you might expect, especially when doing this while tired and stressed, it ended terribly; our engine build began failing and many merge conflicts popped up that needed to be solved. We don't recommend changing your engine the night before any type of release, and this extends to \"fixing\" a bug. Take it from people who unwisely did the opposite: You cannot guarantee that your bug fixes are truly fixes, and they may even be worse than the original bug was! In hindsight, we probably should have created a stable version of the engine a few days prior, then just performed bug fixing on that version so we could quickly revert in the case of a problem. But this also heavily relies on having a reliable testing framework and smoke tests to prevent regression, which are things that we did not have. How Did It Go? \u00b6 To give context to our game jam, it was a six-hour event on Saturday, December 1 st , 2018, where we invited all of our classmates at our Master's program, although we encouraged a more technical audience. We had over 25 people sign up during the weeks leading up to the event, with a total attendance of 12 developers. Of those 12 people, 10 were programmers and 2 were designers. At the beginning of the game jam, we had people filtering in every so often, so we didn't have a true start time or demonstrative walkthrough of setting up the engine. We just pointed to our GitHub repo and our starter documentation on the website, then set people loose! By doing this, we were able to see where people were getting stuck during installation and real-time update our setup documentation to expand on those specific steps. Overall, everyone's setup went relatively smooth because once the jammers had loaded the EmptyLevel , we knew that the engine and template project were setup properly. After this, questions exponentially decayed for the rest of the game jam. Some of the early questions we were asked were: Does your engine have physics? No. Does your engine have an editor? No, that's out of scope. How much of the graphics system does the developer need to be aware of? Hardly any of it. The question was asked about specific implementations of the graphics, which ended up not being terribly relevant once they started developing. How do I create a [level/component/entity]? We've got some nice documentation that covers that! Each of the jammers had a different approach to learning about the engine since they had no prior experience with it. We had provided usage documentation through our website, an executable of our tech demo levels, and the engine source code through GitHub. We didn't give any direction for design or restrictions in what the jammers could use\u2014the engine was simply their toybox. Our only \"constraint\" was the theme of the game jam: A game you could play drunk. Our selection of this theme was thinking that this would help keep implementations simple without worrying about too complex of mechanics, which would hopefully help the developers get things done in the short time we had. In the end, we had nine successful games with only two games crashing on startup. Screenshots of those games can be seen below, and video capture can be seen in the games video below . We were pretty stunned with what our jammers were able to accomplish with our engine in such a short period of time! The jammers also helped us find some pretty serious bugs, which you can read more about in our Patch Notes . After the game jam ended, we showcased all the games and held an awards ceremony. The awards were as follows: * Jammer's Choice: The game the jammers like the most * Isetta's Choice: The game the team liked the most * Best Themed: The game that matched the theme of \"a game you could play drunk\" the best * Was That Made in Unreal?: The game that looked the prettiest * And 0st Penguin: The game that tried something crazy but didn't fully succeed (named after a similar award given in our Master's program). In the days following the game jam, we created a survey to get some results about how the jammers felt about the jam and to answer some of our questions. We'll share the responses with you! We'd highly recommend if you are making an engine to hold a game jam at least once during development, although the more jams you hold, the more usable your engine will likely become. It will test the usability, find bugs, and ensure you aren't making assumptions that only you can follow. Our two cautions associated with this are: 1) have solid setup and usage documentation, especially if you have sparse comments in your code, and 2) be at a semi-stable time of development\u2014you don't want your engine breaking during the game jam\u2014and establish what features your engine specifically supports so your jammers don't think they have the capabilities of some other engine that they're used to. If your engine is only for your game, we'd still recommend hosting a game jam (even internally if you have to!) because at worst, you find some bugs, and at best, you will have some cool examples of how people have used your engine. Our main reason to hold a game jam, though, is fun. It was a huge motivation boost for us as engine developers to see the cool things that we didn't imagine were possible to make with our engine! You can download and play some of the games made by our jammers here ! Full-Feature Demo Game \u00b6 After implementing necessary networking features for our final game last week, we finally dove into gameplay programming for the game. As it turns out, our engine was still pretty easy to use, and we implemented the gameplay features in two days. We spent some more time adding sound and designing the level to make the game a bit more interesting. As this is the last part of our project and we wanted to get the game running as fast as possible, our gameplay code got really dirty! The GameManager ended up being like an almighty god who knows everything. (We also used singletons in the game excessively!) However, as we went into the polishing phase, we started noticing more bugs in our engine. For example, box colliders would push other colliders in weird ways during collision solving. We talk more about how those were fixed in the Patch Notes below. After a bit of polish and a lot of sweat, here is our final game: The Last of Isetta ! As a side note, the reason why we pushed the game to the final stage rather than making it earlier was because, at this stage, we are not really interested in making the game as it's basically writing code that we're all fairly used to already. But we made a enjoyable game anyway! Engine Reflection \u00b6 So what does it mean to be done with the engine? It certainly doesn't mean it's bug free, stable, or even feature complete. For us, being done just means we don't have any more time to dedicate to developing it. Then how can we say we are done? Well, because we were able to complete our feature demo game and we've accomplished what we set out to do: Learn engine development. What we would love to see happen is for the engine to become more stable by external developers issuing fixes. However, the engine being stable isn't an important factor for our project; in fact, the Isetta Engine being this novice, messy result is somewhat more important. Regardless of the bugs, the engine is still usable to create games. Those games might not be as complicated as a Unity or Unreal game, but they are games unrestricted by the toolboxes of Unity and Unreal. Finally, since our goal was to help demystify game engine development or at least make it more approachable, we have created some additional documentation. You may have already seen our Compendium tab , which threads together the blog posts from each week with the relevant interviews and corresponding postmortem for a specific subsystem of the engine. In these postmortems, we even list what went right and what went wrong (although we ran out of time to fully elaborate on each of them). We want the compendium to be a source that can be easily approachable for others because we recognize that our weekly blogs are not that. There is also an overall engine postmortem within the compendium pages, which is more akin to a regular game postmortem. It covers three of the key lessons learned and three of the most important things that went right and wrong in-depth. If you plan on developing any type of game engine, not just a twin-stick engine or a game engine to learn engine development, we think you may be able to get something out of these postmortems . Patch Notes \u00b6 Collision Solving was Unsolved \u00b6 We had a lot of problems left in our collision solving system, but most of them we were willing to ignore (for instance, our capsule-box collisions are horrendous and mostly unusable!). However, there were a few that were total blockers: Boxes were really bad; we made the assumption of uniform scaling for the box colliders, which allowed us to use the largest distance from the box's center to determine which face of the box should be pushing the other colliders. Obviously if one dimension of the box is scaled a lot larger than the other, then that dimension is way more likely to push the other collider! So we instead determined the correct face to push with by the smallest distance from the box's faces . Another problem with box colliders was that, when another collider would collide with the box's edge, the box would give that collider its center as the reference point to solve the collision with. This would cause spheres to shift up or down along the edge whenever it touched an edge because its force vector would point towards the center of the box! This was solved by constricting the box's reference point to the orthogonal plane of the colliding edge that also passes through the sphere's center point\u2014this effectively restrains our collision solve to only push on that plane, which makes it correct. Debug Drawing Can Be Non-Helpful Sometimes \u00b6 Simple problems thankfully have simple solutions, but when you don't see the simple problems, they can become very nasty, too. One instance of this was our debug drawing system. Our colliders were drawing their shapes in the incorrect position depending on how the center was offset from the actual center of the entity. This wasn't typically an issue, until we began to set up the level for our final game! Thankfully we found this problem thanks to our robust transform class and our nice in-game inspector. Coming Soon \u00b6 Well...nothing, as far as engine development is concerned. We are still working on postmortems for the project and the engine subsystems that we'd like to share with you all. Where our project postmortem is similar to a postmortem from Gamasutra or GDC, the subsystem postmortems are more about us reflecting on a few of the big lessons we learned as well as listing all of the things we wish we knew before starting that particular subsystem. There are few more pieces to the project we will be uploading just to wrap things up, but the engine will cease to be developed and we're done writing blogs for this project. Bear in mind, this project isn't dead or being left behind; we aren't continuing with it because we don't want it to become how to make your 5 th engine. We want our learnings to still be inviting for the prospective engine developers who have yet to build their first. If we continued to iterate on the website, it would be disingenuous to continue calling ourselves novice engine developers. Resources \u00b6 This page may be updated as we move on because it purely a source of external links, helping direct you to where you may want to look or read about a topic. We may find additional links that could be helpful in our other endeavors, that we want people to know about here. The page also has comments, so if you know of good resource for a topic we have listed please feel free to write about it in the comments. Originally Published December 11, 2018.","title":"[Week 14] The Last Blog."},{"location":"blogs/week-14/#the-last-blog","text":"","title":"The Last Blog."},{"location":"blogs/week-14/#byte-sized-updates","text":"Documentary : Our team producer filmed our journey and process of developing a game engine. We have a short trailer, and the full documentary is 10 minutes long. Game Jam : Held a short game jam with nine successful (non-crashing) games! Videos and downloads of those games below!! Full-Feature Demo Game : Completed our target game, with all the features of the original one showed all the way back in Week 0 (and a few more)! Engine Reflection : Reflected on our progress as engine developers and the development of the engine. As the title states, this is the last blog post for the Isetta Engine project. We have spent the last 15 weeks building a game engine and documenting our decisions through these blogs, and now we have to say goodbye. This has been an arduous journey for us and we are glad to say, we've built a game engine. There are certainly bugs littered throughout the engine, but for us, having a bug free engine was never the intention\u2014it was to learn the engine development process. We feel we have been able to do this, and we hope that those of you who have had a chance to read our previous blogs now think that engine development isn't as scary or mysterious as you may have thought before. But before we truly sign off, we have some last bits of updates you may be interested in reading.","title":"Byte-Sized Updates"},{"location":"blogs/week-14/#documentary","text":"As you might have seen on our homepage's timeline, we have been working on a documentary on top of all of the other things. While the developers on the team were trying to demystify the technical aspect of game engine development, our producer and resident creative was really pushing to help sell our work to the non-engine developers of the world. What we found was that most laypeople don't have an understanding of what a game engine is, and why would they? Most of them just care about the games made with the engine. However, there is a human side to game engine development that we think is valuable for people to see and understand, which the documentary attempts to portray. The documentary is 10 minutes summarizing our journey from developers to engine programmers. For a trailer: And if you are still interested, the full documentary is right here!","title":"Documentary"},{"location":"blogs/week-14/#game-jam","text":"As we've been mentioning in our past few weeks' blogs, we were planning, and now have, held a game jam. The game jam was never part of the plan for the 15 weeks of development on the engine, particularly because we never bought into the illusion that people would want to use our engine over any other option. However, as the weeks progressed, we often found our discussions gravitating towards the question of what a developer would be left with as a result of our decisions. We figured if we were talking about the hypothetical developer of our engine so much, why not put our engine to the test? We were also encouraged by our advisers and faculty to figure out ways to measure our project's success. (Disclaimer: We aren't saying that this is necessarily the right way to do that.)","title":"Game Jam"},{"location":"blogs/week-14/#preparing-for-the-jam","text":"We spent a few weeks in preparation, which mainly focused on the API design of the engine and trying to test the features we thought the game jammers would use. In the days prior to the game jam, we converted our Git repo's README into a webpage on our website, putting more emphasis into the accuracy, explanation, and readability of the content. And the night prior to the game jam, we thought it would be a good idea to comment the codebase that would most likely be peeked at by the jammers, like the scene header files and anything we made that inherited from our Component class. We also made the bold (and dumb) choice of doing some refactors of the codebase that same night while we were commenting. How could we have thought that was a good idea? Well, it was about 2 or 3 in the morning, and we saw that a method name was misleading. So we fixed that. Oh, and that function should be private because it's just used internally. Oh, and this function doesn't seem to be used, we should remove it! And as you might expect, especially when doing this while tired and stressed, it ended terribly; our engine build began failing and many merge conflicts popped up that needed to be solved. We don't recommend changing your engine the night before any type of release, and this extends to \"fixing\" a bug. Take it from people who unwisely did the opposite: You cannot guarantee that your bug fixes are truly fixes, and they may even be worse than the original bug was! In hindsight, we probably should have created a stable version of the engine a few days prior, then just performed bug fixing on that version so we could quickly revert in the case of a problem. But this also heavily relies on having a reliable testing framework and smoke tests to prevent regression, which are things that we did not have.","title":"Preparing for the Jam"},{"location":"blogs/week-14/#how-did-it-go","text":"To give context to our game jam, it was a six-hour event on Saturday, December 1 st , 2018, where we invited all of our classmates at our Master's program, although we encouraged a more technical audience. We had over 25 people sign up during the weeks leading up to the event, with a total attendance of 12 developers. Of those 12 people, 10 were programmers and 2 were designers. At the beginning of the game jam, we had people filtering in every so often, so we didn't have a true start time or demonstrative walkthrough of setting up the engine. We just pointed to our GitHub repo and our starter documentation on the website, then set people loose! By doing this, we were able to see where people were getting stuck during installation and real-time update our setup documentation to expand on those specific steps. Overall, everyone's setup went relatively smooth because once the jammers had loaded the EmptyLevel , we knew that the engine and template project were setup properly. After this, questions exponentially decayed for the rest of the game jam. Some of the early questions we were asked were: Does your engine have physics? No. Does your engine have an editor? No, that's out of scope. How much of the graphics system does the developer need to be aware of? Hardly any of it. The question was asked about specific implementations of the graphics, which ended up not being terribly relevant once they started developing. How do I create a [level/component/entity]? We've got some nice documentation that covers that! Each of the jammers had a different approach to learning about the engine since they had no prior experience with it. We had provided usage documentation through our website, an executable of our tech demo levels, and the engine source code through GitHub. We didn't give any direction for design or restrictions in what the jammers could use\u2014the engine was simply their toybox. Our only \"constraint\" was the theme of the game jam: A game you could play drunk. Our selection of this theme was thinking that this would help keep implementations simple without worrying about too complex of mechanics, which would hopefully help the developers get things done in the short time we had. In the end, we had nine successful games with only two games crashing on startup. Screenshots of those games can be seen below, and video capture can be seen in the games video below . We were pretty stunned with what our jammers were able to accomplish with our engine in such a short period of time! The jammers also helped us find some pretty serious bugs, which you can read more about in our Patch Notes . After the game jam ended, we showcased all the games and held an awards ceremony. The awards were as follows: * Jammer's Choice: The game the jammers like the most * Isetta's Choice: The game the team liked the most * Best Themed: The game that matched the theme of \"a game you could play drunk\" the best * Was That Made in Unreal?: The game that looked the prettiest * And 0st Penguin: The game that tried something crazy but didn't fully succeed (named after a similar award given in our Master's program). In the days following the game jam, we created a survey to get some results about how the jammers felt about the jam and to answer some of our questions. We'll share the responses with you! We'd highly recommend if you are making an engine to hold a game jam at least once during development, although the more jams you hold, the more usable your engine will likely become. It will test the usability, find bugs, and ensure you aren't making assumptions that only you can follow. Our two cautions associated with this are: 1) have solid setup and usage documentation, especially if you have sparse comments in your code, and 2) be at a semi-stable time of development\u2014you don't want your engine breaking during the game jam\u2014and establish what features your engine specifically supports so your jammers don't think they have the capabilities of some other engine that they're used to. If your engine is only for your game, we'd still recommend hosting a game jam (even internally if you have to!) because at worst, you find some bugs, and at best, you will have some cool examples of how people have used your engine. Our main reason to hold a game jam, though, is fun. It was a huge motivation boost for us as engine developers to see the cool things that we didn't imagine were possible to make with our engine! You can download and play some of the games made by our jammers here !","title":"How Did It Go?"},{"location":"blogs/week-14/#full-feature-demo-game","text":"After implementing necessary networking features for our final game last week, we finally dove into gameplay programming for the game. As it turns out, our engine was still pretty easy to use, and we implemented the gameplay features in two days. We spent some more time adding sound and designing the level to make the game a bit more interesting. As this is the last part of our project and we wanted to get the game running as fast as possible, our gameplay code got really dirty! The GameManager ended up being like an almighty god who knows everything. (We also used singletons in the game excessively!) However, as we went into the polishing phase, we started noticing more bugs in our engine. For example, box colliders would push other colliders in weird ways during collision solving. We talk more about how those were fixed in the Patch Notes below. After a bit of polish and a lot of sweat, here is our final game: The Last of Isetta ! As a side note, the reason why we pushed the game to the final stage rather than making it earlier was because, at this stage, we are not really interested in making the game as it's basically writing code that we're all fairly used to already. But we made a enjoyable game anyway!","title":"Full-Feature Demo Game"},{"location":"blogs/week-14/#engine-reflection","text":"So what does it mean to be done with the engine? It certainly doesn't mean it's bug free, stable, or even feature complete. For us, being done just means we don't have any more time to dedicate to developing it. Then how can we say we are done? Well, because we were able to complete our feature demo game and we've accomplished what we set out to do: Learn engine development. What we would love to see happen is for the engine to become more stable by external developers issuing fixes. However, the engine being stable isn't an important factor for our project; in fact, the Isetta Engine being this novice, messy result is somewhat more important. Regardless of the bugs, the engine is still usable to create games. Those games might not be as complicated as a Unity or Unreal game, but they are games unrestricted by the toolboxes of Unity and Unreal. Finally, since our goal was to help demystify game engine development or at least make it more approachable, we have created some additional documentation. You may have already seen our Compendium tab , which threads together the blog posts from each week with the relevant interviews and corresponding postmortem for a specific subsystem of the engine. In these postmortems, we even list what went right and what went wrong (although we ran out of time to fully elaborate on each of them). We want the compendium to be a source that can be easily approachable for others because we recognize that our weekly blogs are not that. There is also an overall engine postmortem within the compendium pages, which is more akin to a regular game postmortem. It covers three of the key lessons learned and three of the most important things that went right and wrong in-depth. If you plan on developing any type of game engine, not just a twin-stick engine or a game engine to learn engine development, we think you may be able to get something out of these postmortems .","title":"Engine Reflection"},{"location":"blogs/week-14/#patch-notes","text":"","title":"Patch Notes"},{"location":"blogs/week-14/#collision-solving-was-unsolved","text":"We had a lot of problems left in our collision solving system, but most of them we were willing to ignore (for instance, our capsule-box collisions are horrendous and mostly unusable!). However, there were a few that were total blockers: Boxes were really bad; we made the assumption of uniform scaling for the box colliders, which allowed us to use the largest distance from the box's center to determine which face of the box should be pushing the other colliders. Obviously if one dimension of the box is scaled a lot larger than the other, then that dimension is way more likely to push the other collider! So we instead determined the correct face to push with by the smallest distance from the box's faces . Another problem with box colliders was that, when another collider would collide with the box's edge, the box would give that collider its center as the reference point to solve the collision with. This would cause spheres to shift up or down along the edge whenever it touched an edge because its force vector would point towards the center of the box! This was solved by constricting the box's reference point to the orthogonal plane of the colliding edge that also passes through the sphere's center point\u2014this effectively restrains our collision solve to only push on that plane, which makes it correct.","title":"Collision Solving was Unsolved"},{"location":"blogs/week-14/#debug-drawing-can-be-non-helpful-sometimes","text":"Simple problems thankfully have simple solutions, but when you don't see the simple problems, they can become very nasty, too. One instance of this was our debug drawing system. Our colliders were drawing their shapes in the incorrect position depending on how the center was offset from the actual center of the entity. This wasn't typically an issue, until we began to set up the level for our final game! Thankfully we found this problem thanks to our robust transform class and our nice in-game inspector.","title":"Debug Drawing Can Be Non-Helpful Sometimes"},{"location":"blogs/week-14/#coming-soon","text":"Well...nothing, as far as engine development is concerned. We are still working on postmortems for the project and the engine subsystems that we'd like to share with you all. Where our project postmortem is similar to a postmortem from Gamasutra or GDC, the subsystem postmortems are more about us reflecting on a few of the big lessons we learned as well as listing all of the things we wish we knew before starting that particular subsystem. There are few more pieces to the project we will be uploading just to wrap things up, but the engine will cease to be developed and we're done writing blogs for this project. Bear in mind, this project isn't dead or being left behind; we aren't continuing with it because we don't want it to become how to make your 5 th engine. We want our learnings to still be inviting for the prospective engine developers who have yet to build their first. If we continued to iterate on the website, it would be disingenuous to continue calling ourselves novice engine developers.","title":"Coming Soon"},{"location":"blogs/week-14/#resources","text":"This page may be updated as we move on because it purely a source of external links, helping direct you to where you may want to look or read about a topic. We may find additional links that could be helpful in our other endeavors, that we want people to know about here. The page also has comments, so if you know of good resource for a topic we have listed please feel free to write about it in the comments. Originally Published December 11, 2018.","title":"Resources"},{"location":"blogs/week-2/","text":"Ramping Up \u00b6 Notice : Since we were still finding our way with the post for week 1, we published without documenting our full process. We will do better in future weeks about this by more strictly pairing our blogs with our repo tagged code. This is the changelist for week 1: Added section about Byte Size Summary Added paragraph about our API design Added section about Module Management Added paragraph to end of Audio Added section about Debug Logging Added section about Error-Handling Added section about Math Byte-Sized Updates \u00b6 Memory : Implemented memory allocation utilities, stack allocator, and pool allocator Graphics : Wrapped Horde3D as RenderModule and GLFW as WindowModule and improved inter-module communication model Engine Config : Wrote a config file parser, storing values in console variables to be accessed by other systems and eventually an in-game console. Networking : Determined to move forward with yojimbo and will be expanding our control of the network in the coming week. Here is the updated architecture diagram with our progress. The code can be found on the GitHub repo tagged with week-2. Memory \u00b6 As mentioned in many sources (like Mike Acton's talk , this table of latency numbers , and this blog ), good memory management and memory access patterns are the reason why modern games can run so fast with such stunning visuals and complex systems. Thus, we decided to tame this beast early. Let's look at the problems we are trying to solve: Speed: malloc , free , new , and delete are actually pretty slow operations. Because they need to call into the Operating System to get the memory they requested, they involve the switch between user mode and kernel mode. Also, the memory returned by standard memory allocation functions may not satisfy our alignment requirement. Improperly aligned memory can severely affect the speed when using objects stored there, because some modern CPUs can only read properly aligned data. For example, if our processor can only read 4-byte aligned memory, and we have a uint32 (of size 4 byte) stored at memory address 0x2 , the processor will have to read both 0x0 and 0x4 . Then it will have to combine them to return the uint32 value you want. While if the value is stored at a properly aligned address, the processor only needs to read one line. By preventing frequent malloc and free calls and aligning memory, we can increase our program's speed significantly. Memory fragmentation: When we are doing dynamic memory allocations / de-allocations, we may leave small free memory gaps between used memory. In situations where the sum of those small gaps are big enough to satisfy a new memory allocation request, we cannot find a contiguous memory chunk for the new request. This situation is depicted in the following image. These two problems can be solved effectively by managing memory by ourselves rather than leaving it to the operating system. We learned the concept of several different types of memory allocators by referring to Jason's book and a bunch of other resources . The most useful ones are: Stack Allocator and Pool Allocator. You can refer to the resources and our Git repo for implementation details. We are going to focus on how they can be used in our engine here. Stack Allocator \u00b6 A Stack Allocator works just like a normal stack data structure. When the allocator is constructed, it grabs a big chunk of memory from the OS, and returns small chunks of them when someone requests memory. It keeps track of how much memory is used by keeping track of the top of the stack. Stack Allocators can solve both speed and fragmentation issues very well, but aren't all that easy to use. As the objects' memory is organized like a stack, deleting an object at the bottom of the stack without deleting the ones at the top is impossible (meaning you always have to delete newly created object first). Also, when you delete an object, you need to pass in a \"marker\" to the stack allocator. This way it knows where to free its memory. However, passing a marker also requires you to keep track of each object's marker during their lifetime, which can quickly get tedious and ugly. Fortunately, Stack Allocators can be extremely useful without causing annoying side effects. For instance, by using a Stack Allocator as a basis, we can make a Single Frame Allocator. The Single Frame Allocator is a special usage of Stack Allocator which clears itself at the end of each frame. You can allocate memory from it like crazy and not have to worry about freeing them; they are all safely cleared at the end of the frame! However, the fact that it clears every frame means that it tends to be used for passing information between different sub-systems during a single frame. Be careful not to point a pointer to such allocated memory from the last frame! A more useful type of stack allocator is the Double Buffered Allocator, which allows you to pass information into the next frame. We will talk about it in our next blog once we get it implemented. Pool Allocator \u00b6 The Pool Allocator solves the problems in a different way and is a great companion for the Object Pool Pattern . A Pool Allocator grabs a big chunk of memory from the OS. Then, it and separate the memory into a list of small chunks with identical sizes. Each small chunk is just big enough to hold a single element. The Pool Allocator then returns an element-- or the raw memory that can hold an element to you-- when requested, and adds it back to the list when the element gets deleted. So both allocation and deallocation take constant time because it's just deletion and insertion operations on a linked list. Also, as each element is of the same size, defragmentation won't happen for a Pool Allocator and new ing and delete ing can happen in any order. The constraint, however, is that every allocation from a Pool Allocator is of the same size. You can, of course, request more than one chunk of memory from a Pool Allocator, but the returned memory chunks are not guaranteed to be contiguous. Benchmarking \u00b6 To make sure we are actually making our engine faster by using custom allocators, we run a benchmark to compare their speeds. Here is the test case we used: Allocate the memory for 10,000 AudioSource s and deallocate them. In Debug malloc and free: 0.002306s new and delete: 0.002499s Stack Allocator: 0.000647s (3.5 times faster than malloc and free ) Pool Allocator: 0.001315s (1.7 times faster than malloc and free ) In Release malloc and free: 0.000536s new and delete: 0.000495s Stack Allocator: 0.000012s (44.57 times faster than malloc and free ) Pool Allocator: 0.000048s (11.17 times faster than malloc and free ) As the data shows, custom allocators are indeed a lot faster, especially in release mode. Proposed Memory Management Patterns in Our Engine \u00b6 After discussing as a team, we also decided on part of our memory management strategies. We will allocate a big chunk of memory that's enough for the game from operating system when the game starts with a Stack Allocator, and all later memory requests will be satisfied within that big chunk. This is a relatively old-school approach, but it guarantees that once we can start up the game correctly, we are ready to go for the rest of the game. It won't crash midway because of insufficient memory. For the systems that are \"persistent\"\u2014those who are always there when the game is running 1 , we will put them at the bottom of the big stack. Then, we will have a part of memory for level specific assets, such as meshes, materials, etc. Next, we will have several chunks dedicated for individual Pool Allocators to hold game objects that need to be pooled (like bullets and mobs). Then, we will have a Single Frame Allocator and a Double Buffered Allocator. On top of everything, we will have the memory left for occasional dynamic allocation. The whole structure can be visualized like this: We are still discussing if we should introduce defragmentation 2 into our engine. The results will be posted in later weeks. And once we have the memory manager done, we will start refactoring our modules and other systems to use them for their memory management. Graphics \u00b6 Last week, we proved that Horde3D can satisfy our needs by making a simple demo. This week, we moved on to making a wrapper that would integrate it into our engine architecture and make it more accessible for our users. At the same time, we found that GLFW 3 (the dependency brought in by Horde3D) can also handle input events, including those from game controllers, so we also started wrapping that into our game engine. In the original design of our modules, we assumed that each module should be self-contained and have no dependency on other modules. Thus, the constructor should be completely empty and the StartUp() function should take no external parameters. However, GLFW disproves this assumption. The Window Module \u00b6 Inside RenderModule , GLFW opens a window and returns a handle of that window. The handle is supposed to be used by the RenderModule for operating the viewport, as well as, by the InputModule for receiving all inputs. In this case, instead of using StartUp() function (which is inherited from IModule interface), we need to find another mechanic to pass the shared variable. The first solution that came into our mind was to extract the shared parts into a WindowModule (which implements IModule interface). From there, we'd put the original RenderModule and InputModule into the WindowModule as submodules. This would no longer implement the IModule interface. The WindowModule will manage the initialization, update, and deinitialization of its submodules and pass them the shared handle they need. This solution seemed to be a clean way to solve the dependency issue. What's more, it worked in the current version of our game engine. However, this design has some potential downsides. It not only hides the two submodules from the module manager, but also binds the initialization and update processes of the two submodules together. Hiding submodules also negatively affects the readability\u2014 RenderModule and InputModule technically won't exist since they are not managed by the module manager anymore. But we also need InputModule to update before everything and RenderModule to update after everything during the engine loop... This design won't allow for that, though, since the modules are bundled together by WindowModule ! The Module Solution \u00b6 To fix these issues, we finally decided to keep everything flat and managed by the module manager. To solve the dependency issue, we are allowing StartUp() implementations to take parameters on a per-module basis. This method also means we are better off saying \"au revoir\" to the IModule interface, which we aren't terribly sad about. This solution has the chance of breaking coherency, but for now it works fine and really improves our readability! Engine Config \u00b6 The engine configuration (or config, as the cool kids say) is a file which contains variables with assigned values by the developer. It is initialized once the project is loaded. The information contained within these files is usually laid out like this: variable=value , with some type of comment system (# for our implementation). Having a configuration file prevents developers from hard-coding values into the modules themselves. In this way, they can be dynamically changed for different game requirements. # A sample engine config file # comments bullet_damage = 2 # declaring a variable and value player_health= 100 # spacing doesn't matter between '=' spawner_origin = (1,1, 10) # vector3 format unused_string = string # undefined keys will not be read Requirements \u00b6 The requirement of our engine configuration file/parser is to be able to: Read lines as key=value, ignoring whitespace and comments Store the value as the typed value (not a string) Usable for types of int , float , string , and Vector3 Extendable for including other types Store the key/values within a specified variable to be used by other systems (Additional) Quickly read/write the key=value pairs at runtime Doing this allows for an easy integration of in-game console Design Decisions \u00b6 The reasoning behind storing the key/value as a typed entity is to avoid users from converting the value from a string. This becomes important if the developer doesn't cache the configuration value, because a string conversion for each frame can become expensive and unnecessary. This also leaves the system open for use with an in-game console, so that values can be checked frequently without losing CPU time (another way to avoid this could be to have a callback on value set). Storing the key/values within a variable was another design decision to stop developers from doing a lookup on a map for the value, which could cause 2 problems: There is no way of stopping the user from doing this each frame, which can be expensive. Throughout the code, config parameters are referenced by strings. Strings can be easily mistyped and changed without a compiler error. This leaves them prone to super-errors, especially when the number of occurrences is high. By defining a variable within the Config class, and only allowing access through that variable, we can prevent the errors that come with using strings. The compile will complain for bad references. These types of variables are called console variables, also known as CVar s. Problems with Configuration \u00b6 The engine configuration development was broken into 2 problems: Create a file parser which can read the key/values as strings and remove whitespace, empty lines, and comments; Store the values as the typed value specified by the developer which are accessed by variable (not a dictionary look-up). Problem 1 is trivial, but the naive thought process of problem 2 can require runtime polymorphism 4 . The naive thought process we had with problem 2 was, within the configuration parser, you have the key and value as strings. You also have string and type in the dictionary of key/value, so we thought you could just pair them. But you can't. Casting a variable to a type of another variable requires runtime polymorphism/reflection. Understanding that even though you can match key from the file with key in the dictionary, but can't store the value was tough hurdle for us to get over. Neither our engine or C++ supports reflection currently, so there must be another way. Comparison with Other Engines \u00b6 To confirm that's how CVar systems work (or more accurately don't work), we looked into the 3 largest open source engines: Unreal, CryEngine, and Lumberyard (although only a fork of CryEngine) for how they do configuration variables/parsing and for inspiration. First we looked into the developer-facing documentation to try to better understand the functionality and features of the open source engines. Then, we looked at the source code. The documentation confirmed that config variables were defined as console variables and defined with types (sometimes). Tracking down where the ICVar struct was created in each of the engines was a big task. However, it confirmed we had the right idea of what to do, albeit on a much smaller scale. The specific files we found useful have been linked in the Resources page. What We Ended On \u00b6 To store the map of keys and varying-typed values requires a base class for the types to derive from. This base class also can't be templated for the map. Each derived class needs to implement a method GetType() which returns the type of the derived value. Each CVar read from the configuration is defined in the engine config class as a typed CVar , which is registered within a dictionary of string-ids and ICVar pointers. When the config file is read, the dictionary is searched for the key and the GetType() method determines what type value is cast to through a switch statement. Additional CVar s can be registered in other modules, but since the configuration file is read prior to startup, these CVar s won't be stored from the file. However, these CVar s will be registered as keys in the dictionary and can be used exclusively as console variables. Networking \u00b6 Last week for networking, we discussed how we were looking at two different 3 rd party libraries to do our low-level packet management, GameNetworkingSockets and yojimbo . The two libraries are actually pretty similar in functionality (as many networking layers are\u2014there's only so much you can do with packets!), so our decision on which to move forward with fell to usability and support. The GameNetworkingSockets library is functionally sound, as far as we know. Since it comes from Valve, we know it's been put to the test. However, Valve is still pretty new to open-sourcing, and it shows: The build process to create the library file has around five dependencies you have to manually set up! This also points to the fact they may have developed the library from Linux initially, because the setup involved for installing the dependencies is much easier on Linux than it is on Windows. Another four points that weigh heavily against Valve are the scattered codebase (being pulled from Valve's own development pipeline will do that), the considerably frequent commenting-out of code, the lack of any recommended sample files, and lack of documentation. What's more, yojimbo seems to be clear on all of those points! The documentation isn't superb, but there is indeed some documentation that we can build with Doxygen . The library also gives a good number of examples for using the API; they're mostly within a single, uncommented C++ source file, but hey! We'll take what we can get. Beyond that, yojimbo was written with open-source in mind, so it's a clean and organized job, and better yet, the build process is minimal. In fact, the author even encourages users to pull pieces out and customize sections of the code to their needs . The Decision \u00b6 On that note, the Isetta team is moving forward with yojimbo for now. The jury is still out on whether or not we're nixing networking from our feature list, but we'll be determining that within the next two weeks. The first steps we've taken on networking have been simple ones, but we have results! Going beyond just using one of the testing functions packaged with yojimbo, we integrated said function into our Frankenstein-esque testbed to ensure that it would work correctly alongside the rest of our libraries and implementations. And it does! For now. We only have a simple back and forth messaging system running on a solo machine so that we can wrap our minds around the data flow of the library. Our next steps will be to: Get that connection cross-computer, and Make it do something a little more involved and...exciting. At that point, we suspect we'll know enough about the terrors of networking to make the call on keeping it in our engine. Patch Notes \u00b6 Logging \u00b6 The Logger system's log functions were originally called through static functions, but we decided we wanted to know the file name and line number of the call site of the log. One method of achieving this would be forcing the __FILE__ and __LINE__ macros passed into each log call, which is repetitive and makes the logger hard to use. The macros couldn't just be placed within the logger file as they would always expand to be the same file and line number. The other option was to create a set of macros for the log which expanded to passing in the macros by default. The macro route removes the scope operator from the log. It could be argued this increases readability. We selected the macro route, so the log functions were removed and replaced with a LogObject which can be found in the Logger.h . Assertions \u00b6 The original decision was to use Microsoft's afx headers as the library because it offered so many features. These headers required MFC 5 , which worked in isolated tests but caused issues pretty quickly with the audio system and logger. Rather than fighting the issues, we decided it would be quicker to write our own assert and the other features of afx were cool but ultimately extra and unnecessary. MFC was removed from the engine. ToString/FromString \u00b6 One of the discussions we had this past week was about how to handle the methods ToString , which converts from object to string, and FromString , which converts from string to object. There were 3 thoughts we had on how to handle these: Have a pure abstract class IStringEnumerable which has ToString / FromString as abstract methods that classes inherit from have to implement Just put ToString / FromString as methods as needed without an interface Have a static converter class which holds the ToString / FromString methods for all classes needed. We liked the idea of having these methods attached to the classes themselves, which ruled out 3. In terms of the abstract class, we decided not to use one because the abstraction wasn't necessary. Having this abstract class at this point in time was for the sole reason to abstract. We had no immediate need or use for it, and may end up on the route of multiple inheritance if we choose to use it. So we decided to follow the advice of some professionals, like Casey Muratori 6 , and keep it simple. If we do end up needing it, we can refactor. Git Flow \u00b6 In week 1, we described our Git flow as creating feature branches which were eventually merged into code-review. After code review, we created local hotfix branches and merged fixes into code-review branch. When everything was done, we merged code-review branch into the master branch and tagged it. However, we found that this can stop new development prior to pushing a weekly tag because they can't be pushed to the code-review branch. The code-review branch is the only branch where our individual work can converge. Therefore, we've adopted the GitFlow system. We renamed code-review to \"staging\" and added a new develop branch. We will still have our individual feature branches, and can always merge our work into the develop branch. Right before we start the code review session, we merge all commits into the staging branch. After the code review session, we will push hotfixes to the staging branch, and new development can be pushed to the develop branch. When everything is fixed, we will merge the staging branch into the master branch and tag it. Coming Soon \u00b6 This past Saturday, September 8, 2018, we interviewed Casey Muratori of Handmade Hero. There was a lot of great information about engine development, and we will be posting a concise, edited version soon. Right now, though, you can check out the full video . We would appreciate any feedback or questions you may have about our content or what we are doing in the comments. Resources \u00b6 The resource page has been updated to include links we found useful this week, too! Originally published September 14, 2018. Jason Gregory refers to persistent game data as \"LSR\" data, Load-and-Stay-Resident, as seen in Game Engine Architecture section 15.4.2 \u21a9 Fragmentation is when a lot of memory allocations or files take up noncontinuous chunks of space, leaving awkward bubbles that can't be used by anyone. So defragmentation is the process of reordering those objects so that we can clear up a cleaner, bigger stretch of free space or memory. \u21a9 GLFW is a library utility for creating windows and receiving input from the window. \u21a9 Runtime or dynamic polymorphism is when an overridden class or method is determined at runtime as opposed to compile time. This allows for us to change some behavior of our program depending on the data type it's operating with. \u21a9 The Microsoft Foundation Class library is an object-oriented C++ library that contains useful macros for exceptions, run-time type identification, serialization, and more. \u21a9 Casey Muratori is a game engine developer and creator of Handmade Hero , a web series documenting his efforts in building a game engine from scratch. We interviewed him as part of this project, and his interview can currently be found here . \u21a9","title":"[Week 2] Ramping Up"},{"location":"blogs/week-2/#ramping-up","text":"Notice : Since we were still finding our way with the post for week 1, we published without documenting our full process. We will do better in future weeks about this by more strictly pairing our blogs with our repo tagged code. This is the changelist for week 1: Added section about Byte Size Summary Added paragraph about our API design Added section about Module Management Added paragraph to end of Audio Added section about Debug Logging Added section about Error-Handling Added section about Math","title":"Ramping Up"},{"location":"blogs/week-2/#byte-sized-updates","text":"Memory : Implemented memory allocation utilities, stack allocator, and pool allocator Graphics : Wrapped Horde3D as RenderModule and GLFW as WindowModule and improved inter-module communication model Engine Config : Wrote a config file parser, storing values in console variables to be accessed by other systems and eventually an in-game console. Networking : Determined to move forward with yojimbo and will be expanding our control of the network in the coming week. Here is the updated architecture diagram with our progress. The code can be found on the GitHub repo tagged with week-2.","title":"Byte-Sized Updates"},{"location":"blogs/week-2/#memory","text":"As mentioned in many sources (like Mike Acton's talk , this table of latency numbers , and this blog ), good memory management and memory access patterns are the reason why modern games can run so fast with such stunning visuals and complex systems. Thus, we decided to tame this beast early. Let's look at the problems we are trying to solve: Speed: malloc , free , new , and delete are actually pretty slow operations. Because they need to call into the Operating System to get the memory they requested, they involve the switch between user mode and kernel mode. Also, the memory returned by standard memory allocation functions may not satisfy our alignment requirement. Improperly aligned memory can severely affect the speed when using objects stored there, because some modern CPUs can only read properly aligned data. For example, if our processor can only read 4-byte aligned memory, and we have a uint32 (of size 4 byte) stored at memory address 0x2 , the processor will have to read both 0x0 and 0x4 . Then it will have to combine them to return the uint32 value you want. While if the value is stored at a properly aligned address, the processor only needs to read one line. By preventing frequent malloc and free calls and aligning memory, we can increase our program's speed significantly. Memory fragmentation: When we are doing dynamic memory allocations / de-allocations, we may leave small free memory gaps between used memory. In situations where the sum of those small gaps are big enough to satisfy a new memory allocation request, we cannot find a contiguous memory chunk for the new request. This situation is depicted in the following image. These two problems can be solved effectively by managing memory by ourselves rather than leaving it to the operating system. We learned the concept of several different types of memory allocators by referring to Jason's book and a bunch of other resources . The most useful ones are: Stack Allocator and Pool Allocator. You can refer to the resources and our Git repo for implementation details. We are going to focus on how they can be used in our engine here.","title":"Memory"},{"location":"blogs/week-2/#stack-allocator","text":"A Stack Allocator works just like a normal stack data structure. When the allocator is constructed, it grabs a big chunk of memory from the OS, and returns small chunks of them when someone requests memory. It keeps track of how much memory is used by keeping track of the top of the stack. Stack Allocators can solve both speed and fragmentation issues very well, but aren't all that easy to use. As the objects' memory is organized like a stack, deleting an object at the bottom of the stack without deleting the ones at the top is impossible (meaning you always have to delete newly created object first). Also, when you delete an object, you need to pass in a \"marker\" to the stack allocator. This way it knows where to free its memory. However, passing a marker also requires you to keep track of each object's marker during their lifetime, which can quickly get tedious and ugly. Fortunately, Stack Allocators can be extremely useful without causing annoying side effects. For instance, by using a Stack Allocator as a basis, we can make a Single Frame Allocator. The Single Frame Allocator is a special usage of Stack Allocator which clears itself at the end of each frame. You can allocate memory from it like crazy and not have to worry about freeing them; they are all safely cleared at the end of the frame! However, the fact that it clears every frame means that it tends to be used for passing information between different sub-systems during a single frame. Be careful not to point a pointer to such allocated memory from the last frame! A more useful type of stack allocator is the Double Buffered Allocator, which allows you to pass information into the next frame. We will talk about it in our next blog once we get it implemented.","title":"Stack Allocator"},{"location":"blogs/week-2/#pool-allocator","text":"The Pool Allocator solves the problems in a different way and is a great companion for the Object Pool Pattern . A Pool Allocator grabs a big chunk of memory from the OS. Then, it and separate the memory into a list of small chunks with identical sizes. Each small chunk is just big enough to hold a single element. The Pool Allocator then returns an element-- or the raw memory that can hold an element to you-- when requested, and adds it back to the list when the element gets deleted. So both allocation and deallocation take constant time because it's just deletion and insertion operations on a linked list. Also, as each element is of the same size, defragmentation won't happen for a Pool Allocator and new ing and delete ing can happen in any order. The constraint, however, is that every allocation from a Pool Allocator is of the same size. You can, of course, request more than one chunk of memory from a Pool Allocator, but the returned memory chunks are not guaranteed to be contiguous.","title":"Pool Allocator"},{"location":"blogs/week-2/#benchmarking","text":"To make sure we are actually making our engine faster by using custom allocators, we run a benchmark to compare their speeds. Here is the test case we used: Allocate the memory for 10,000 AudioSource s and deallocate them. In Debug malloc and free: 0.002306s new and delete: 0.002499s Stack Allocator: 0.000647s (3.5 times faster than malloc and free ) Pool Allocator: 0.001315s (1.7 times faster than malloc and free ) In Release malloc and free: 0.000536s new and delete: 0.000495s Stack Allocator: 0.000012s (44.57 times faster than malloc and free ) Pool Allocator: 0.000048s (11.17 times faster than malloc and free ) As the data shows, custom allocators are indeed a lot faster, especially in release mode.","title":"Benchmarking"},{"location":"blogs/week-2/#proposed-memory-management-patterns-in-our-engine","text":"After discussing as a team, we also decided on part of our memory management strategies. We will allocate a big chunk of memory that's enough for the game from operating system when the game starts with a Stack Allocator, and all later memory requests will be satisfied within that big chunk. This is a relatively old-school approach, but it guarantees that once we can start up the game correctly, we are ready to go for the rest of the game. It won't crash midway because of insufficient memory. For the systems that are \"persistent\"\u2014those who are always there when the game is running 1 , we will put them at the bottom of the big stack. Then, we will have a part of memory for level specific assets, such as meshes, materials, etc. Next, we will have several chunks dedicated for individual Pool Allocators to hold game objects that need to be pooled (like bullets and mobs). Then, we will have a Single Frame Allocator and a Double Buffered Allocator. On top of everything, we will have the memory left for occasional dynamic allocation. The whole structure can be visualized like this: We are still discussing if we should introduce defragmentation 2 into our engine. The results will be posted in later weeks. And once we have the memory manager done, we will start refactoring our modules and other systems to use them for their memory management.","title":"Proposed Memory Management Patterns in Our Engine"},{"location":"blogs/week-2/#graphics","text":"Last week, we proved that Horde3D can satisfy our needs by making a simple demo. This week, we moved on to making a wrapper that would integrate it into our engine architecture and make it more accessible for our users. At the same time, we found that GLFW 3 (the dependency brought in by Horde3D) can also handle input events, including those from game controllers, so we also started wrapping that into our game engine. In the original design of our modules, we assumed that each module should be self-contained and have no dependency on other modules. Thus, the constructor should be completely empty and the StartUp() function should take no external parameters. However, GLFW disproves this assumption.","title":"Graphics"},{"location":"blogs/week-2/#the-window-module","text":"Inside RenderModule , GLFW opens a window and returns a handle of that window. The handle is supposed to be used by the RenderModule for operating the viewport, as well as, by the InputModule for receiving all inputs. In this case, instead of using StartUp() function (which is inherited from IModule interface), we need to find another mechanic to pass the shared variable. The first solution that came into our mind was to extract the shared parts into a WindowModule (which implements IModule interface). From there, we'd put the original RenderModule and InputModule into the WindowModule as submodules. This would no longer implement the IModule interface. The WindowModule will manage the initialization, update, and deinitialization of its submodules and pass them the shared handle they need. This solution seemed to be a clean way to solve the dependency issue. What's more, it worked in the current version of our game engine. However, this design has some potential downsides. It not only hides the two submodules from the module manager, but also binds the initialization and update processes of the two submodules together. Hiding submodules also negatively affects the readability\u2014 RenderModule and InputModule technically won't exist since they are not managed by the module manager anymore. But we also need InputModule to update before everything and RenderModule to update after everything during the engine loop... This design won't allow for that, though, since the modules are bundled together by WindowModule !","title":"The Window Module"},{"location":"blogs/week-2/#the-module-solution","text":"To fix these issues, we finally decided to keep everything flat and managed by the module manager. To solve the dependency issue, we are allowing StartUp() implementations to take parameters on a per-module basis. This method also means we are better off saying \"au revoir\" to the IModule interface, which we aren't terribly sad about. This solution has the chance of breaking coherency, but for now it works fine and really improves our readability!","title":"The Module Solution"},{"location":"blogs/week-2/#engine-config","text":"The engine configuration (or config, as the cool kids say) is a file which contains variables with assigned values by the developer. It is initialized once the project is loaded. The information contained within these files is usually laid out like this: variable=value , with some type of comment system (# for our implementation). Having a configuration file prevents developers from hard-coding values into the modules themselves. In this way, they can be dynamically changed for different game requirements. # A sample engine config file # comments bullet_damage = 2 # declaring a variable and value player_health= 100 # spacing doesn't matter between '=' spawner_origin = (1,1, 10) # vector3 format unused_string = string # undefined keys will not be read","title":"Engine Config"},{"location":"blogs/week-2/#requirements","text":"The requirement of our engine configuration file/parser is to be able to: Read lines as key=value, ignoring whitespace and comments Store the value as the typed value (not a string) Usable for types of int , float , string , and Vector3 Extendable for including other types Store the key/values within a specified variable to be used by other systems (Additional) Quickly read/write the key=value pairs at runtime Doing this allows for an easy integration of in-game console","title":"Requirements"},{"location":"blogs/week-2/#design-decisions","text":"The reasoning behind storing the key/value as a typed entity is to avoid users from converting the value from a string. This becomes important if the developer doesn't cache the configuration value, because a string conversion for each frame can become expensive and unnecessary. This also leaves the system open for use with an in-game console, so that values can be checked frequently without losing CPU time (another way to avoid this could be to have a callback on value set). Storing the key/values within a variable was another design decision to stop developers from doing a lookup on a map for the value, which could cause 2 problems: There is no way of stopping the user from doing this each frame, which can be expensive. Throughout the code, config parameters are referenced by strings. Strings can be easily mistyped and changed without a compiler error. This leaves them prone to super-errors, especially when the number of occurrences is high. By defining a variable within the Config class, and only allowing access through that variable, we can prevent the errors that come with using strings. The compile will complain for bad references. These types of variables are called console variables, also known as CVar s.","title":"Design Decisions"},{"location":"blogs/week-2/#problems-with-configuration","text":"The engine configuration development was broken into 2 problems: Create a file parser which can read the key/values as strings and remove whitespace, empty lines, and comments; Store the values as the typed value specified by the developer which are accessed by variable (not a dictionary look-up). Problem 1 is trivial, but the naive thought process of problem 2 can require runtime polymorphism 4 . The naive thought process we had with problem 2 was, within the configuration parser, you have the key and value as strings. You also have string and type in the dictionary of key/value, so we thought you could just pair them. But you can't. Casting a variable to a type of another variable requires runtime polymorphism/reflection. Understanding that even though you can match key from the file with key in the dictionary, but can't store the value was tough hurdle for us to get over. Neither our engine or C++ supports reflection currently, so there must be another way.","title":"Problems with Configuration"},{"location":"blogs/week-2/#comparison-with-other-engines","text":"To confirm that's how CVar systems work (or more accurately don't work), we looked into the 3 largest open source engines: Unreal, CryEngine, and Lumberyard (although only a fork of CryEngine) for how they do configuration variables/parsing and for inspiration. First we looked into the developer-facing documentation to try to better understand the functionality and features of the open source engines. Then, we looked at the source code. The documentation confirmed that config variables were defined as console variables and defined with types (sometimes). Tracking down where the ICVar struct was created in each of the engines was a big task. However, it confirmed we had the right idea of what to do, albeit on a much smaller scale. The specific files we found useful have been linked in the Resources page.","title":"Comparison with Other Engines"},{"location":"blogs/week-2/#what-we-ended-on","text":"To store the map of keys and varying-typed values requires a base class for the types to derive from. This base class also can't be templated for the map. Each derived class needs to implement a method GetType() which returns the type of the derived value. Each CVar read from the configuration is defined in the engine config class as a typed CVar , which is registered within a dictionary of string-ids and ICVar pointers. When the config file is read, the dictionary is searched for the key and the GetType() method determines what type value is cast to through a switch statement. Additional CVar s can be registered in other modules, but since the configuration file is read prior to startup, these CVar s won't be stored from the file. However, these CVar s will be registered as keys in the dictionary and can be used exclusively as console variables.","title":"What We Ended On"},{"location":"blogs/week-2/#networking","text":"Last week for networking, we discussed how we were looking at two different 3 rd party libraries to do our low-level packet management, GameNetworkingSockets and yojimbo . The two libraries are actually pretty similar in functionality (as many networking layers are\u2014there's only so much you can do with packets!), so our decision on which to move forward with fell to usability and support. The GameNetworkingSockets library is functionally sound, as far as we know. Since it comes from Valve, we know it's been put to the test. However, Valve is still pretty new to open-sourcing, and it shows: The build process to create the library file has around five dependencies you have to manually set up! This also points to the fact they may have developed the library from Linux initially, because the setup involved for installing the dependencies is much easier on Linux than it is on Windows. Another four points that weigh heavily against Valve are the scattered codebase (being pulled from Valve's own development pipeline will do that), the considerably frequent commenting-out of code, the lack of any recommended sample files, and lack of documentation. What's more, yojimbo seems to be clear on all of those points! The documentation isn't superb, but there is indeed some documentation that we can build with Doxygen . The library also gives a good number of examples for using the API; they're mostly within a single, uncommented C++ source file, but hey! We'll take what we can get. Beyond that, yojimbo was written with open-source in mind, so it's a clean and organized job, and better yet, the build process is minimal. In fact, the author even encourages users to pull pieces out and customize sections of the code to their needs .","title":"Networking"},{"location":"blogs/week-2/#the-decision","text":"On that note, the Isetta team is moving forward with yojimbo for now. The jury is still out on whether or not we're nixing networking from our feature list, but we'll be determining that within the next two weeks. The first steps we've taken on networking have been simple ones, but we have results! Going beyond just using one of the testing functions packaged with yojimbo, we integrated said function into our Frankenstein-esque testbed to ensure that it would work correctly alongside the rest of our libraries and implementations. And it does! For now. We only have a simple back and forth messaging system running on a solo machine so that we can wrap our minds around the data flow of the library. Our next steps will be to: Get that connection cross-computer, and Make it do something a little more involved and...exciting. At that point, we suspect we'll know enough about the terrors of networking to make the call on keeping it in our engine.","title":"The Decision"},{"location":"blogs/week-2/#patch-notes","text":"","title":"Patch Notes"},{"location":"blogs/week-2/#logging","text":"The Logger system's log functions were originally called through static functions, but we decided we wanted to know the file name and line number of the call site of the log. One method of achieving this would be forcing the __FILE__ and __LINE__ macros passed into each log call, which is repetitive and makes the logger hard to use. The macros couldn't just be placed within the logger file as they would always expand to be the same file and line number. The other option was to create a set of macros for the log which expanded to passing in the macros by default. The macro route removes the scope operator from the log. It could be argued this increases readability. We selected the macro route, so the log functions were removed and replaced with a LogObject which can be found in the Logger.h .","title":"Logging"},{"location":"blogs/week-2/#assertions","text":"The original decision was to use Microsoft's afx headers as the library because it offered so many features. These headers required MFC 5 , which worked in isolated tests but caused issues pretty quickly with the audio system and logger. Rather than fighting the issues, we decided it would be quicker to write our own assert and the other features of afx were cool but ultimately extra and unnecessary. MFC was removed from the engine.","title":"Assertions"},{"location":"blogs/week-2/#tostringfromstring","text":"One of the discussions we had this past week was about how to handle the methods ToString , which converts from object to string, and FromString , which converts from string to object. There were 3 thoughts we had on how to handle these: Have a pure abstract class IStringEnumerable which has ToString / FromString as abstract methods that classes inherit from have to implement Just put ToString / FromString as methods as needed without an interface Have a static converter class which holds the ToString / FromString methods for all classes needed. We liked the idea of having these methods attached to the classes themselves, which ruled out 3. In terms of the abstract class, we decided not to use one because the abstraction wasn't necessary. Having this abstract class at this point in time was for the sole reason to abstract. We had no immediate need or use for it, and may end up on the route of multiple inheritance if we choose to use it. So we decided to follow the advice of some professionals, like Casey Muratori 6 , and keep it simple. If we do end up needing it, we can refactor.","title":"ToString/FromString"},{"location":"blogs/week-2/#git-flow","text":"In week 1, we described our Git flow as creating feature branches which were eventually merged into code-review. After code review, we created local hotfix branches and merged fixes into code-review branch. When everything was done, we merged code-review branch into the master branch and tagged it. However, we found that this can stop new development prior to pushing a weekly tag because they can't be pushed to the code-review branch. The code-review branch is the only branch where our individual work can converge. Therefore, we've adopted the GitFlow system. We renamed code-review to \"staging\" and added a new develop branch. We will still have our individual feature branches, and can always merge our work into the develop branch. Right before we start the code review session, we merge all commits into the staging branch. After the code review session, we will push hotfixes to the staging branch, and new development can be pushed to the develop branch. When everything is fixed, we will merge the staging branch into the master branch and tag it.","title":"Git Flow"},{"location":"blogs/week-2/#coming-soon","text":"This past Saturday, September 8, 2018, we interviewed Casey Muratori of Handmade Hero. There was a lot of great information about engine development, and we will be posting a concise, edited version soon. Right now, though, you can check out the full video . We would appreciate any feedback or questions you may have about our content or what we are doing in the comments.","title":"Coming Soon"},{"location":"blogs/week-2/#resources","text":"The resource page has been updated to include links we found useful this week, too! Originally published September 14, 2018. Jason Gregory refers to persistent game data as \"LSR\" data, Load-and-Stay-Resident, as seen in Game Engine Architecture section 15.4.2 \u21a9 Fragmentation is when a lot of memory allocations or files take up noncontinuous chunks of space, leaving awkward bubbles that can't be used by anyone. So defragmentation is the process of reordering those objects so that we can clear up a cleaner, bigger stretch of free space or memory. \u21a9 GLFW is a library utility for creating windows and receiving input from the window. \u21a9 Runtime or dynamic polymorphism is when an overridden class or method is determined at runtime as opposed to compile time. This allows for us to change some behavior of our program depending on the data type it's operating with. \u21a9 The Microsoft Foundation Class library is an object-oriented C++ library that contains useful macros for exceptions, run-time type identification, serialization, and more. \u21a9 Casey Muratori is a game engine developer and creator of Handmade Hero , a web series documenting his efforts in building a game engine from scratch. We interviewed him as part of this project, and his interview can currently be found here . \u21a9","title":"Resources"},{"location":"blogs/week-3/","text":"\"Completing\" the Core \u00b6 Byte-Sized Updates \u00b6 Architecture Update : We've made changes to our design as expected, systems were added and removed based on the criteria of the test game. Input Module : Implemented the input module with polling and callback registration interface. Engine Loop : Moved from a while loop in main to an engine loop with fixed update time step, variable rendering. Timer : Created a clock class to support multiple clocks in-engine. Memory Management : Implemented double buffered allocator and object handle, and currently halfway through defragmentation. Filesystem : System to create/read/write files synchronously and asynchronously using Microsoft's API. Networking : Created the networking module and wrapper, made some rudimentary message objects, and connected our team from across the void (i.e., the space between our computers) Architecture Update \u00b6 The engine systems are progressing smoothly (almost too smoothly...), but at this point, we are close to completing our Core systems. However, as expected, the architecture we planned at the beginning wasn't perfect and is changing: GUI \u00b6 We added a GUI module to the module layer after finding out that Horde3D doesn't come with GUI support. Looking at the Horde3D community's extensions, we found a HordeGUI solution. However, it hasn't been updated for a few years, so we instead decided to use the industry standard Dear ImGui . We will be integrating it and creating a wrapper in the coming weeks. Profiler \u00b6 For the past few weeks we have had the Profiler marked as complete, and although it is integrated, we decided it wouldn't be fair to mark as complete until the engine is actually using the profiler. We are waiting until we have a system complete to insert profiler code, so we don't have to refactor the code and the profiler macros. Expect profiling to happen in the next few weeks. Serialization \u00b6 Serialization was originally marked as \"Try to Make\", and as we were about to start development we discovered our networking connections library, yojimbo, already had a sophisticated serialization system. Rather than attempt to decouple yojimbo's serialization and add our own, we are going to utilize what we already have imported and integrated. For the time being, we are marking serialization as complete; however, we may revisit this later down the line if we need a more advanced solution or just want to make our own. STL Wrapper \u00b6 After the last few weeks of looking at the box labeled STL Wrapper , we aren't sure what we were thinking. We are just going to continue using the STL 1 library without obstructing it anymore. We have created our own alias file to shorten some names of the types, such as uint8_t to U8 , though we wouldn't go as far as labeling this as a wrapper. Data Structures \u00b6 As part of the network programming, we realized we would be encountering several similar data problems that can be solved by some basic data structures. That said, it's important for us to retain control over those data structures, have them using our own systems (e.g., memory management), and be correct for our needs. The first instance of this was our RingBuffer class, which sacrificed a tiny bit of memory for a boost in CPU performance by using one extra slot in the buffer that solely exists as the end slot. Input Module \u00b6 As we mentioned in last week's blog post , we are using GLFW 2 to receive and notify input events. Unlike other modules which are completely hidden from the game developer by scene entity data, the input module is one that not only processes low-level operations, like receiving inputs, but also directly interacts with them. The developers should be able to know when an input is triggered as well as run custom code to react to that event. This requires us to design a simple, friendly but versatile interfaces for game developers to use. We divided the whole input system into two parts: an InputModule class to do the low-level operations, and an Input class to provide an interface for the game developers. For each input signal (like mouse buttons and keyboard keys), we found that providing multiple ways for the game developer to use is necessary and approachable. Unity provides polling methods like GetKeyDown so that the game developer can check if a key is pressed down in Update . In addition to that, Unreal provides callback binding, where game developers can bind custom callbacks to various input events. According to those API designs, we decided to provide both the polling Get function and the callback registering function. GLFW natively supports these two methods, so adding the functions for each simplifies our backend in wrapping the GLFW library. Since we have the register functions, we also need to provide corresponding functions to enable the game developers to unregister their callbacks. However, that task is not as simple as it seems. We are using std::function 3 for callback functions, and there is no way to check if two std::function objects are equal or not. Thus, without any external identification, we can't remove the callback from a callback list (the list is used to maintain multiple callbacks bound on one single key). To solve that problem, we came up with three solutions: When registering a callback from users, ask for a unique name to index it Return a handle during registration and use that to index the callbacks Don't allow unregistering, only provide ClearAll function After some discussion, we all agreed to apply the second solution. The first solution can be costly and the uniqueness of the function name isn't guaranteed because it is controlled by the game developers. In the third solution, the game developer totally loses control of their callbacks. Even though the second is adding an extra layer, using handles to store and find callbacks is still the preferred way. Engine Loop \u00b6 As mentioned by Casey during our interview , a game engine is not like common software that always wait for user input, process it and then give the output. It's more like a self-contained loop that simulates the game every frame and reacts simultaneously to the user inputs. The engine loop is the core loop used in the engine to take charge of any submodules and update them in the rate they prefer. When working with the engine loop, we needed to decide both how fast the loop should run and in what rate all the submodules should update. After reading several articles (especially the one from Game Programming Patterns ), we found that there are three major ways to update the game: Run as fast as the processor can and simulate the game with delta time Run the game in fixed delta time like 60fps Run the simulation in fixed delta time and run the rendering in variable delta time Running as fast as the processor can is the most straightforward way to simulate the game. It's simple to implement and you don't have to care about the interval time. However, it's important to calculate the delta time between the last update and this update then run the simulation with that delta time, otherwise, the game will behave differently depending on the computer's specs. Even with the delta time mechanics, the game still remains nondeterministic. On a good computer (one with lots of CPU), the update runs more frequently and the object's movement look smoother; while on a poor computer, the objects run less smooth, almost like with lag, and can even clip into other objects due to the low update frequency and high deltas in time. Then, what about running the game at a specific rate? This can solve the issue that even on different computers, the update rate remains the same and the simulation loop will run as expected. However, in larger games with heavier rendering loops, there is still the potential for bugs. Let's say we keep the game running in 60fps so that each has a fixed timestep of about 0.016s. If we have too many objects to render, the render loop itself could cost less than 0.020s for every update. This will the break the assumption that the update loop can run every 0.016s. In this case, every four updates will cost 0.080s while five updates should be simulated during that time. This slows down the game and breaks the whole timeline. The reason why it will get slower is that the simulation update runs alongside the render update and never has the chance to catch up with the timeline when the game is behind where it should be. The solution to this issue is to give the simulation a chance to catch up and let the rendering update go as fast as it can. This is called Fixed update time step, variable rendering . In every update, the loop will add the delta time onto an accumulateTime variable which indicates the amount of time the game hasn't simulated yet. It will then run the simulation update several times until the accumulateTime is lower than the fixed time step we set before. Timer \u00b6 For Unity game developers and some other game programmers, getting a time is as simple as a function call like Time.deltaTime . However, the inner system of a game engine clock is more complex. The clock which Time.deltaTime function communicates with is only a game time clock, but a game engine has way more clocks than expected. For example, every animation may have its own clock and the networking module should also have an individual clock so that it won't get timed out if the game pauses. With that in mind, implementing a clock class is quite simple. C++ has a great <chrono> library with a high resolution timer to help us build our clock. Memory Management \u00b6 Double Buffered Allocator \u00b6 As mentioned last week , the first thing we did this week was to implement a double buffered allocator. Different from a single frame allocator, a double buffered allocator only clears its memory at the end of the next frame. For example, memory allocated during frame n is cleared at the end of frame n + 1 , and those allocated during frame n + 1 will be cleared at the end of frame n + 2 . This allows us to nicely pass information to the next frame without worrying about leaking memory long-term. It will come in handy for things like velocity calculations, which are things that require us to know the position of the object in the last frame. For implementation, a double buffered allocator is actually a wrapper on top of the stack allocator. It holds two stack allocators (A, B) and marks one of them as an active stack allocator each frame. At the end of each frame, it swaps A and B and clears the newly activated one. For example, if A is the active allocator of frame n , all memory allocation requests will be handled by A during frame n . At the end of frame n , B becomes the active allocator and is cleared. So during frame n + 1 , memory allocation will be done by B, and those pointers to memory in A are still valid. At the end of n + 1 , A is again activated and cleared. Non-templated Pool Allocator \u00b6 The pool allocator we implemented last week was templated, which means it only returns newly constructed objects when you call Get instead of raw memory pointers. One of our team members suggested it would be useful to have a more generic pool allocator that only keeps a list of raw memory chunks instead of objects, which is more accurately labeled the pool allocator, which has already proven useful. Object Handles \u00b6 This week, we decided to implement a naive memory defragmentation system and see how it works for us. So we implemented object handles first, which is a prerequisite of defragmentation (for a review on what object handles are, see our architecture post ). Defragmentation works by removing \"holes\" of free memory between objects. This is achieved by moving objects in memory so they are packed nicely into a contiguous area, leaving enough contiguous space for later allocations. Maybe you've noticed that the address of the objects will change during this process, and will invalidate the pointers pointing to the old addresses. To fix this issue, we made a middleman between the game developers and the actual allocated memory of objects. This middleman is called the object handle. In our system, the HandleEntry is what holds the pointer to actual objects. It is structured like so: class HandleEntry { U32 uniqueID ; void * ptr ; bool isEmpty { true }; }; We are keeping a static fixed-size array of HandleEntry s. Every time we need to create a new dynamic object, we search through the array and find the first available HandleEntry , construct the object, and set the three variables of that entry, where 'ptr' is the pointer to the actual object. This routine returns the ObjectHandle (finally!) to our newly created object, which keeps track of the index to our assigned HandleEntry and the uniqueID . ObjectHandle is defined like this: template < typename T > class ObjectHandle { T * operator -> (); T & operator * (); U32 uniqueID ; U32 index ; }; The usage of an object handle resembles that of a raw pointer - you access members through the operator -> and dereference it through the operator * . When you call those pointers, the object handle is responsible for finding the corresponding HandleEntry in the static array and cast its void* ptr to T* . The best part is, when we move objects around in memory, all we need to do is to change the value of ptr in the corresponding HandleEntry to point to the destination address. The end user doesn't need to do anything - they can just use the object handle as if nothing happened! Our implementation of object handle requires minimal changes from the game developers. The typical dynamic allocation routine will change from this: auto ptr = new MyObj (); ptr -> SomeMember (); ptr -> someVariable = newValue ; delete ptr ; to this: auto handle = OurNewUtility < MyObj > (); handle -> SomeMember (); // same as the good ol' days // maybe defragmentation happens here, but we don't need to care! handle -> someVariable = newValue ; // also the same! OurDeleteUtility ( handle ); The reason we keep track of uniqueID s is that they can help us prevent the stale pointer problem and double deletion problem. The stale pointer problem happens when you try to access a pointer when it's already deleted. This operation would usually crash your application. But what's worse than that is if you access a pointer when the old object was deleted and a new object was constructed in its address in memory, you may corrupt your game silently. In our system, whenever someone accesses an object through ObjectHandle , we would first check if the corresponding HandleEntry is deleted (through isEmpty ), if it's not, we check if the HandleEntry 's uniqueID is the same with the ObjectHandle 's. This way, we can make sure we never access a deleted object or an unwanted object. The double deletion problems happen when you try to delete an object twice - the 2 nd deletion will cause undefined behavior or a crash. To prevent this, we do the same checking as for stale pointer before deletion to make sure the HandleEntry is not empty and has the same uniqueID . We also added interfaces in the MemoryManager class for accessing the various allocators, like single frame and double buffered allocators, to make them easier to use and hide implementation details. And this is all we have for memory management this week! We are also stumbling our way through defragmentation, and will share our progress next week. In addition memory management will need to be integrated into the rest of the engine. If you would like to learn more about object handles and defragmentation, we would recommend section 5.2 Memory Management and section 15.5 Object References and World Queries of Game Engine Architecture 2 nd edition . Filesystem \u00b6 The Filesystem, labeled Async File I/O in our architecture diagram, is the system that handles all creation, opening, reading, and writing files. The reason to make the filesystem its own system rather than having each module/file/system handle its own file I/O is to be able to have some platform independence. In our interview with Tommy Refenes , he mentioned how he was able to port his games in just a few days because he segregated his core and modules so that the game only communicates with the wrapper. In this way, when porting to a new platform, only the core and graphics need to be replaced/re-written. The requirements of the system is to: create/open/read/write files close files when finished read/write asynchronously (async) and have a callback to alert on completion Initially, the engine was using the STL library's fstream and the streambuf was extended to be async for writing, not reading. Since reading files asynchronously was more important than async writing (textures, models, configurations, etc. all depend on loading; the logger is the primary writer), we went back to exploring our options. One option was to continue to extend the STL library for async read, but instead, we reconsidered and went with Microsoft's file API ( fileapi.h ). The reason to use Microsoft's API was that our engine is only being developed for Windows platforms so it eliminates the need for cross-platform compatibility, and we had also been told that the majority of the core system gets replaced when changing platform anyway. In addition, the Microsoft API is almost guaranteed to be faster than something we develop since it has a deeper knowledge of the OS than we do. The first part of using the Microsoft API was to understand how it works, which meant reading the documentation. In addition to the Microsoft documentation, we needed to read information about how async file I/O works and the typical practices involved\u2014 all of these links can be found on the resource page . The first attempt at file I/O was just using Read/Write calls which allowed for async file read/write, however when testing using multiple calls (even to the same file) the API returned an error of not being able to perform multiple I/O operations. Back to research, which led us to listen to a Handmade Hero Q/A when Casey Muratori mentioned I/O completion ports which then took us back to Microsoft documentation. Basically, I/O completion ports allow for processing multiple async I/O requests which is exactly what was needed. After fighting against the Microsoft file API in trying to implement what was necessary, we were able to get something resembling a filesystem meeting our requirements. What is currently implemented with the filesystem is async file read/write, synchronous file read, creating a file (as well as directories to the file), and completion system which deletes pointers. The synchronous file read was not originally in our plans, however, we found quickly that when reading a file such as the configuration file, you don't want the engine to proceed until it has the information to start-up the modules. The creation of directories was also a late addition because there was an assumption that the folders would be created if needed and not have to be done manually. The original projection for the filesystem was 1-2 days, but it ultimately took a full week of development. This was for two reasons: We didn't know the terminology associated with asynchronous file I/O, which made learning what we didn't know difficult. There was a learning curve to the Microsoft file API before anything could be implemented. The filesystem is complete for now, though more could still be done. A cancel operation could be implemented because the naive one commented out causes memory leaks (not implemented because we can't foresee an immediate use). An easier task would be a synchronous write call, however, we don't see a use case where you would want to stall your update for a write. Another more advanced extension would be extending the filesystem for a different platform, like OSX or Linux, because this is a small step in porting the engine. Networking \u00b6 Now that we've gotten into some actual network programming, we should probably introduce some of the concepts that we're building on top of. Much of what we've learned about network programming comes from Multiplayer Game Programming by Joshua Glazer and Sanjay Madhav particularly, and a good reference/example that you could use to learn networking independently from a game engine is bshokati's C++ networking project . Just like game engines, computer networking is built on layers, and the typical order (from bottom up) of those layers in game networking works like this: physical layer, link layer, network layer, transport layer, and application layer . Each layer has a duty, but they can follow those duties in different ways, and the different implementations of a layer are called a protocol . The game networking layers is a modified version of the OSI Model 4 . The physical layer is quite literally physical: The wires and radio waves used to physically transfer information across a network. The link layer is the lowest level of the software involved in networking. Very fundamental units of transmission are handled here (they're called frames, and are smaller than packets!) and protocols dictate the data transfer rates and frameworks. The network layer builds on the link layer by making connections and communications across the network more generally usable. The link layer technically can do everything already, but it's inconvenient and rigid. The network layer provides a packet 5 framework and Internet protocol (IP) addresses to simplify the end-use of the network. The transport layer takes the network layer and extends it to individual processes on the computer. It implements ports and port binding to ensure the correct information is being sent and received. It's also the layer where the ubiquitous UDP and TCP live ( see below for details on these). Lastly, the application layer takes the generalized set of layers beneath it and uses them for your actual multiplayer game. Typically you will have some client-server 6 or peer-to-peer 7 handshake[ 8 code here to finish out the network connection, as well as whatever network behavior your game actually needs. At the transport layer, the two most used protocols are the transmission control protocol (TCP) and the user datagram protocol (UDP) . TCP is what most of the internet uses, as it's reliable and guarantees correctly-ordered information (imagine if the email your boss sent you became scrambled on its way to your computer!). UDP, on the other hand, is what fast-paced, constantly changing applications use, including video games and video live streaming. Some of the packets may come into your computer out of order or they could be lost entirely! But this is okay, because we only care about the most recent information, and it happens to come in quickly so losing things here and there won't hurt. If we had to use an analogy, TCP would be like requesting and confirming a drink of water from someone every time you need a drink, whereas UDP would be like someone spraying a hose of water at your face while you try to catch it in your mouth 9 . By the way, much of our initial design of the networking module is informed by Ch. 6, Network Topologies and Sample Games, from Multiplayer Game Programming , if you want to learn more. The Actual Coding Part \u00b6 The 3 rd party networking library yojimbo contains logic for everything through the transport layer essentially\u2014even options for using UDP or TCP with the handshake. This makes the burden on us of making a networked game engine in less than three months far less significant, although it's certainly still not insignificant. The furthest yojimbo goes is providing an abstract Message class that can be implemented to be serialized, and broken into packets to be sent across the server. We still have all of the logic on the application layer, which not only includes connection management but also any gameplay needs like object/world-state replication and serialized commands. The first thing we did with yojimbo was taking its client-server test and put it inside our engine. It didn't use any of the engine components or modules, but is technically in our project (we're done, right?). We used it as an opportunity to learn the yojimbo library at a high level before tackling problems with it. We then extended it to use user control from our Input module. After we successfully brought the yojimbo test into our engine, we began to abstract the yojimbo functions into our own Networking module, which meant we could hide details about processing, sending, and receiving of messages. The basic loop for an action game network is: Read in new packets, reconstruct them into messages, step the simulation forward one step, queue up new outgoing messages, then break them into packets and send those 10 . The ordering of that cycle could depend on what your game does, but that's the basic form of it. We only want to concern ourselves with the high level of the process, though. We want to put our letter in the mailbox and expect it to make its way to its recipient eventually. \"Messages\" can include information for anything\u2014which we anticipate will be a big design problem for us in the future\u2014but right now, they only include rudimentary types, like integers or strings. At a high level, we know that we may be sending lots and lots of messages every frame, especially from the server. For optimization's sake, we implemented a ring buffer 11 class to store our messages in so that we don't have to manually cycle elements through our arrays. We also modified small parts of the yojimbo codebase so that it better fits our code architecture, since specializing it to our own code would make it both easier to use and read and more performant. The Networking module that we created has two sides: client and server. The client is every user who connects to the server, which is the machine with the authoritative definition of important game information. yojimbo doesn't support peer-to-peer networking, but we also knew that a client-server model would be much easier for us to do because peer-to-peer requires complex conflict resolution and significantly more bandwidth from every client. In fact, many factors of peer-to-peer networking essentially use the client-server model. Making the Networked Push \u00b6 Once we established our basic wrapper overtop of the yojimbo library, we set off to homebrew our own network test. This should have been fairly straightforward like a mirror of the yojimbo client-server test, but for some reason, it just wasn't working! Over the course of several days, one of our developers dug deeper and deeper into the yojimbo codebase to find the solution. This was a blessing and a curse. yojimbo is over 10,000 lines of unfamiliar code, and learning it could make the engine's networking component much more effective. But networked code can be intimidating to debug\u2014even on a single, localhost 12 machine because of the physical and link layers. The journey came to an end when he mangled our wrapping code beyond recognition. He managed to disable a default parameter in the construction for the Address class, which contains both an IPv4 value and a port. As it turns out, yojimbo's Address objects can be instantiated with some default port value so long as an IP address is given! And if the other networked machine happens to be using a custom port, then the packets get lost at the transport layer. Once that was corrected, everything worked! And our developer went on his way to cleaning up the mess he made\u2026 Game-like Networking \u00b6 The simple tests were cool, but if it's just on the same machine, then you could cut out all of the middleman and have a more efficient and far less confusing system. No, we needed to take the next step before we were satisfied with our progress. Amazingly, this was much easier progress to make than the simple single-machine test, and this is largely because of yojimbo. By inputting our local network IP addresses instead of localhost addresses, we connected two of our computers (over the same connection) almost immediately after finishing the single-machine test. It was amazing! We could say \"Hi!\" to each other with just the press of a single key. We didn't stop there. We wanted it to at least look more like a game, so our other developers pitched in and utilized our basic network module to send rudimentary signals across two machines to play an audio file along with playing and stopping an animation. To top it all off, we quickly refactored the backend of the networking module to run more than just one client per server, and we all got into one session spamming these audio and animation messages over the network! At this point, it's looking like we're going down the networked game engine route, and we're mentally prepared for that. yojimbo has been a great shortcut regarding the link through transport layers, and now we have our real big obstacles to tackle now: state synchronization and a more comprehensive system of messages that could be used somewhat generically. Coming Soon \u00b6 Released today was Casey Muratori's, developer of Handmade Hero, interview . We are working on editing the interview Tommy Refenes of Team Meat for next week. We would appreciate any feedback or questions you may have about our content or what we are doing in the comments. Resources \u00b6 The resource page has been updated to include links we found useful this week, too! Originally published September 21, 2018. STL stands for standard library which is the C++ library containing most of the needed data structures. It is known to not be best for performance, however, will save us time not implementing them. \u21a9 GLFW is a library utility for creating windows and receiving input from the window. \u21a9 std::function is a container for lambda functions, see cppreference . We have renamed std::function to Action in our aliases for simpler calling. \u21a9 OSI stands for Open System Interconnection and is a standard for networking layers. \u21a9 A packet is formatted data that is sent over a network. Virtual objects are serialized and broken into these small chunks before being sent, and different protocols expect different formats for packets. \u21a9 The client-server model in networking has one central server that all other machines (the \"clients\") connect to. This server is typically the authority on all important and possibly conflicting information. \u21a9 The peer-to-peer model in networking connects every machine to one another, which requires more bandwidth per client and more complex data authority handling but avoids needing a dedicated server. Peer-to-peer is generally harder to implement than client-server. \u21a9 A handshake in networking is an automated negotiation process for creating a connection between two machines, typically a client and a server. The process requires the machines to exchange special packets before the connection can be established. \u21a9 Giving credit where credit is due: https://www.reddit.com/r/ProgrammerHumor/comments/9gcwgw/tcp_vs_udp/ \u21a9 yojimbo's creator Glenn Fiedler goes into more depth about this loop in the context of yojimbo: https://github.com/networkprotocol/yojimbo/issues/25#issuecomment-265082392 \u21a9 A ring buffer (or circular buffer) is a FIFO data structure which is broadly used for transmitting data between asynchronous processes. See more: https://en.wikipedia.org/wiki/Circular_buffer \u21a9 A localhost address in networking is specifically addressed 127.0.0.1. Packets that are sent here are not technically sent anywhere, they're just sent up to the next layer for processing. \u21a9","title":"[Week 3] \"Completing\" the Core"},{"location":"blogs/week-3/#completing-the-core","text":"","title":"\"Completing\" the Core"},{"location":"blogs/week-3/#byte-sized-updates","text":"Architecture Update : We've made changes to our design as expected, systems were added and removed based on the criteria of the test game. Input Module : Implemented the input module with polling and callback registration interface. Engine Loop : Moved from a while loop in main to an engine loop with fixed update time step, variable rendering. Timer : Created a clock class to support multiple clocks in-engine. Memory Management : Implemented double buffered allocator and object handle, and currently halfway through defragmentation. Filesystem : System to create/read/write files synchronously and asynchronously using Microsoft's API. Networking : Created the networking module and wrapper, made some rudimentary message objects, and connected our team from across the void (i.e., the space between our computers)","title":"Byte-Sized Updates"},{"location":"blogs/week-3/#architecture-update","text":"The engine systems are progressing smoothly (almost too smoothly...), but at this point, we are close to completing our Core systems. However, as expected, the architecture we planned at the beginning wasn't perfect and is changing:","title":"Architecture Update"},{"location":"blogs/week-3/#gui","text":"We added a GUI module to the module layer after finding out that Horde3D doesn't come with GUI support. Looking at the Horde3D community's extensions, we found a HordeGUI solution. However, it hasn't been updated for a few years, so we instead decided to use the industry standard Dear ImGui . We will be integrating it and creating a wrapper in the coming weeks.","title":"GUI"},{"location":"blogs/week-3/#profiler","text":"For the past few weeks we have had the Profiler marked as complete, and although it is integrated, we decided it wouldn't be fair to mark as complete until the engine is actually using the profiler. We are waiting until we have a system complete to insert profiler code, so we don't have to refactor the code and the profiler macros. Expect profiling to happen in the next few weeks.","title":"Profiler"},{"location":"blogs/week-3/#serialization","text":"Serialization was originally marked as \"Try to Make\", and as we were about to start development we discovered our networking connections library, yojimbo, already had a sophisticated serialization system. Rather than attempt to decouple yojimbo's serialization and add our own, we are going to utilize what we already have imported and integrated. For the time being, we are marking serialization as complete; however, we may revisit this later down the line if we need a more advanced solution or just want to make our own.","title":"Serialization"},{"location":"blogs/week-3/#stl-wrapper","text":"After the last few weeks of looking at the box labeled STL Wrapper , we aren't sure what we were thinking. We are just going to continue using the STL 1 library without obstructing it anymore. We have created our own alias file to shorten some names of the types, such as uint8_t to U8 , though we wouldn't go as far as labeling this as a wrapper.","title":"STL Wrapper"},{"location":"blogs/week-3/#data-structures","text":"As part of the network programming, we realized we would be encountering several similar data problems that can be solved by some basic data structures. That said, it's important for us to retain control over those data structures, have them using our own systems (e.g., memory management), and be correct for our needs. The first instance of this was our RingBuffer class, which sacrificed a tiny bit of memory for a boost in CPU performance by using one extra slot in the buffer that solely exists as the end slot.","title":"Data Structures"},{"location":"blogs/week-3/#input-module","text":"As we mentioned in last week's blog post , we are using GLFW 2 to receive and notify input events. Unlike other modules which are completely hidden from the game developer by scene entity data, the input module is one that not only processes low-level operations, like receiving inputs, but also directly interacts with them. The developers should be able to know when an input is triggered as well as run custom code to react to that event. This requires us to design a simple, friendly but versatile interfaces for game developers to use. We divided the whole input system into two parts: an InputModule class to do the low-level operations, and an Input class to provide an interface for the game developers. For each input signal (like mouse buttons and keyboard keys), we found that providing multiple ways for the game developer to use is necessary and approachable. Unity provides polling methods like GetKeyDown so that the game developer can check if a key is pressed down in Update . In addition to that, Unreal provides callback binding, where game developers can bind custom callbacks to various input events. According to those API designs, we decided to provide both the polling Get function and the callback registering function. GLFW natively supports these two methods, so adding the functions for each simplifies our backend in wrapping the GLFW library. Since we have the register functions, we also need to provide corresponding functions to enable the game developers to unregister their callbacks. However, that task is not as simple as it seems. We are using std::function 3 for callback functions, and there is no way to check if two std::function objects are equal or not. Thus, without any external identification, we can't remove the callback from a callback list (the list is used to maintain multiple callbacks bound on one single key). To solve that problem, we came up with three solutions: When registering a callback from users, ask for a unique name to index it Return a handle during registration and use that to index the callbacks Don't allow unregistering, only provide ClearAll function After some discussion, we all agreed to apply the second solution. The first solution can be costly and the uniqueness of the function name isn't guaranteed because it is controlled by the game developers. In the third solution, the game developer totally loses control of their callbacks. Even though the second is adding an extra layer, using handles to store and find callbacks is still the preferred way.","title":"Input Module"},{"location":"blogs/week-3/#engine-loop","text":"As mentioned by Casey during our interview , a game engine is not like common software that always wait for user input, process it and then give the output. It's more like a self-contained loop that simulates the game every frame and reacts simultaneously to the user inputs. The engine loop is the core loop used in the engine to take charge of any submodules and update them in the rate they prefer. When working with the engine loop, we needed to decide both how fast the loop should run and in what rate all the submodules should update. After reading several articles (especially the one from Game Programming Patterns ), we found that there are three major ways to update the game: Run as fast as the processor can and simulate the game with delta time Run the game in fixed delta time like 60fps Run the simulation in fixed delta time and run the rendering in variable delta time Running as fast as the processor can is the most straightforward way to simulate the game. It's simple to implement and you don't have to care about the interval time. However, it's important to calculate the delta time between the last update and this update then run the simulation with that delta time, otherwise, the game will behave differently depending on the computer's specs. Even with the delta time mechanics, the game still remains nondeterministic. On a good computer (one with lots of CPU), the update runs more frequently and the object's movement look smoother; while on a poor computer, the objects run less smooth, almost like with lag, and can even clip into other objects due to the low update frequency and high deltas in time. Then, what about running the game at a specific rate? This can solve the issue that even on different computers, the update rate remains the same and the simulation loop will run as expected. However, in larger games with heavier rendering loops, there is still the potential for bugs. Let's say we keep the game running in 60fps so that each has a fixed timestep of about 0.016s. If we have too many objects to render, the render loop itself could cost less than 0.020s for every update. This will the break the assumption that the update loop can run every 0.016s. In this case, every four updates will cost 0.080s while five updates should be simulated during that time. This slows down the game and breaks the whole timeline. The reason why it will get slower is that the simulation update runs alongside the render update and never has the chance to catch up with the timeline when the game is behind where it should be. The solution to this issue is to give the simulation a chance to catch up and let the rendering update go as fast as it can. This is called Fixed update time step, variable rendering . In every update, the loop will add the delta time onto an accumulateTime variable which indicates the amount of time the game hasn't simulated yet. It will then run the simulation update several times until the accumulateTime is lower than the fixed time step we set before.","title":"Engine Loop"},{"location":"blogs/week-3/#timer","text":"For Unity game developers and some other game programmers, getting a time is as simple as a function call like Time.deltaTime . However, the inner system of a game engine clock is more complex. The clock which Time.deltaTime function communicates with is only a game time clock, but a game engine has way more clocks than expected. For example, every animation may have its own clock and the networking module should also have an individual clock so that it won't get timed out if the game pauses. With that in mind, implementing a clock class is quite simple. C++ has a great <chrono> library with a high resolution timer to help us build our clock.","title":"Timer"},{"location":"blogs/week-3/#memory-management","text":"","title":"Memory Management"},{"location":"blogs/week-3/#double-buffered-allocator","text":"As mentioned last week , the first thing we did this week was to implement a double buffered allocator. Different from a single frame allocator, a double buffered allocator only clears its memory at the end of the next frame. For example, memory allocated during frame n is cleared at the end of frame n + 1 , and those allocated during frame n + 1 will be cleared at the end of frame n + 2 . This allows us to nicely pass information to the next frame without worrying about leaking memory long-term. It will come in handy for things like velocity calculations, which are things that require us to know the position of the object in the last frame. For implementation, a double buffered allocator is actually a wrapper on top of the stack allocator. It holds two stack allocators (A, B) and marks one of them as an active stack allocator each frame. At the end of each frame, it swaps A and B and clears the newly activated one. For example, if A is the active allocator of frame n , all memory allocation requests will be handled by A during frame n . At the end of frame n , B becomes the active allocator and is cleared. So during frame n + 1 , memory allocation will be done by B, and those pointers to memory in A are still valid. At the end of n + 1 , A is again activated and cleared.","title":"Double Buffered Allocator"},{"location":"blogs/week-3/#non-templated-pool-allocator","text":"The pool allocator we implemented last week was templated, which means it only returns newly constructed objects when you call Get instead of raw memory pointers. One of our team members suggested it would be useful to have a more generic pool allocator that only keeps a list of raw memory chunks instead of objects, which is more accurately labeled the pool allocator, which has already proven useful.","title":"Non-templated Pool Allocator"},{"location":"blogs/week-3/#object-handles","text":"This week, we decided to implement a naive memory defragmentation system and see how it works for us. So we implemented object handles first, which is a prerequisite of defragmentation (for a review on what object handles are, see our architecture post ). Defragmentation works by removing \"holes\" of free memory between objects. This is achieved by moving objects in memory so they are packed nicely into a contiguous area, leaving enough contiguous space for later allocations. Maybe you've noticed that the address of the objects will change during this process, and will invalidate the pointers pointing to the old addresses. To fix this issue, we made a middleman between the game developers and the actual allocated memory of objects. This middleman is called the object handle. In our system, the HandleEntry is what holds the pointer to actual objects. It is structured like so: class HandleEntry { U32 uniqueID ; void * ptr ; bool isEmpty { true }; }; We are keeping a static fixed-size array of HandleEntry s. Every time we need to create a new dynamic object, we search through the array and find the first available HandleEntry , construct the object, and set the three variables of that entry, where 'ptr' is the pointer to the actual object. This routine returns the ObjectHandle (finally!) to our newly created object, which keeps track of the index to our assigned HandleEntry and the uniqueID . ObjectHandle is defined like this: template < typename T > class ObjectHandle { T * operator -> (); T & operator * (); U32 uniqueID ; U32 index ; }; The usage of an object handle resembles that of a raw pointer - you access members through the operator -> and dereference it through the operator * . When you call those pointers, the object handle is responsible for finding the corresponding HandleEntry in the static array and cast its void* ptr to T* . The best part is, when we move objects around in memory, all we need to do is to change the value of ptr in the corresponding HandleEntry to point to the destination address. The end user doesn't need to do anything - they can just use the object handle as if nothing happened! Our implementation of object handle requires minimal changes from the game developers. The typical dynamic allocation routine will change from this: auto ptr = new MyObj (); ptr -> SomeMember (); ptr -> someVariable = newValue ; delete ptr ; to this: auto handle = OurNewUtility < MyObj > (); handle -> SomeMember (); // same as the good ol' days // maybe defragmentation happens here, but we don't need to care! handle -> someVariable = newValue ; // also the same! OurDeleteUtility ( handle ); The reason we keep track of uniqueID s is that they can help us prevent the stale pointer problem and double deletion problem. The stale pointer problem happens when you try to access a pointer when it's already deleted. This operation would usually crash your application. But what's worse than that is if you access a pointer when the old object was deleted and a new object was constructed in its address in memory, you may corrupt your game silently. In our system, whenever someone accesses an object through ObjectHandle , we would first check if the corresponding HandleEntry is deleted (through isEmpty ), if it's not, we check if the HandleEntry 's uniqueID is the same with the ObjectHandle 's. This way, we can make sure we never access a deleted object or an unwanted object. The double deletion problems happen when you try to delete an object twice - the 2 nd deletion will cause undefined behavior or a crash. To prevent this, we do the same checking as for stale pointer before deletion to make sure the HandleEntry is not empty and has the same uniqueID . We also added interfaces in the MemoryManager class for accessing the various allocators, like single frame and double buffered allocators, to make them easier to use and hide implementation details. And this is all we have for memory management this week! We are also stumbling our way through defragmentation, and will share our progress next week. In addition memory management will need to be integrated into the rest of the engine. If you would like to learn more about object handles and defragmentation, we would recommend section 5.2 Memory Management and section 15.5 Object References and World Queries of Game Engine Architecture 2 nd edition .","title":"Object Handles"},{"location":"blogs/week-3/#filesystem","text":"The Filesystem, labeled Async File I/O in our architecture diagram, is the system that handles all creation, opening, reading, and writing files. The reason to make the filesystem its own system rather than having each module/file/system handle its own file I/O is to be able to have some platform independence. In our interview with Tommy Refenes , he mentioned how he was able to port his games in just a few days because he segregated his core and modules so that the game only communicates with the wrapper. In this way, when porting to a new platform, only the core and graphics need to be replaced/re-written. The requirements of the system is to: create/open/read/write files close files when finished read/write asynchronously (async) and have a callback to alert on completion Initially, the engine was using the STL library's fstream and the streambuf was extended to be async for writing, not reading. Since reading files asynchronously was more important than async writing (textures, models, configurations, etc. all depend on loading; the logger is the primary writer), we went back to exploring our options. One option was to continue to extend the STL library for async read, but instead, we reconsidered and went with Microsoft's file API ( fileapi.h ). The reason to use Microsoft's API was that our engine is only being developed for Windows platforms so it eliminates the need for cross-platform compatibility, and we had also been told that the majority of the core system gets replaced when changing platform anyway. In addition, the Microsoft API is almost guaranteed to be faster than something we develop since it has a deeper knowledge of the OS than we do. The first part of using the Microsoft API was to understand how it works, which meant reading the documentation. In addition to the Microsoft documentation, we needed to read information about how async file I/O works and the typical practices involved\u2014 all of these links can be found on the resource page . The first attempt at file I/O was just using Read/Write calls which allowed for async file read/write, however when testing using multiple calls (even to the same file) the API returned an error of not being able to perform multiple I/O operations. Back to research, which led us to listen to a Handmade Hero Q/A when Casey Muratori mentioned I/O completion ports which then took us back to Microsoft documentation. Basically, I/O completion ports allow for processing multiple async I/O requests which is exactly what was needed. After fighting against the Microsoft file API in trying to implement what was necessary, we were able to get something resembling a filesystem meeting our requirements. What is currently implemented with the filesystem is async file read/write, synchronous file read, creating a file (as well as directories to the file), and completion system which deletes pointers. The synchronous file read was not originally in our plans, however, we found quickly that when reading a file such as the configuration file, you don't want the engine to proceed until it has the information to start-up the modules. The creation of directories was also a late addition because there was an assumption that the folders would be created if needed and not have to be done manually. The original projection for the filesystem was 1-2 days, but it ultimately took a full week of development. This was for two reasons: We didn't know the terminology associated with asynchronous file I/O, which made learning what we didn't know difficult. There was a learning curve to the Microsoft file API before anything could be implemented. The filesystem is complete for now, though more could still be done. A cancel operation could be implemented because the naive one commented out causes memory leaks (not implemented because we can't foresee an immediate use). An easier task would be a synchronous write call, however, we don't see a use case where you would want to stall your update for a write. Another more advanced extension would be extending the filesystem for a different platform, like OSX or Linux, because this is a small step in porting the engine.","title":"Filesystem"},{"location":"blogs/week-3/#networking","text":"Now that we've gotten into some actual network programming, we should probably introduce some of the concepts that we're building on top of. Much of what we've learned about network programming comes from Multiplayer Game Programming by Joshua Glazer and Sanjay Madhav particularly, and a good reference/example that you could use to learn networking independently from a game engine is bshokati's C++ networking project . Just like game engines, computer networking is built on layers, and the typical order (from bottom up) of those layers in game networking works like this: physical layer, link layer, network layer, transport layer, and application layer . Each layer has a duty, but they can follow those duties in different ways, and the different implementations of a layer are called a protocol . The game networking layers is a modified version of the OSI Model 4 . The physical layer is quite literally physical: The wires and radio waves used to physically transfer information across a network. The link layer is the lowest level of the software involved in networking. Very fundamental units of transmission are handled here (they're called frames, and are smaller than packets!) and protocols dictate the data transfer rates and frameworks. The network layer builds on the link layer by making connections and communications across the network more generally usable. The link layer technically can do everything already, but it's inconvenient and rigid. The network layer provides a packet 5 framework and Internet protocol (IP) addresses to simplify the end-use of the network. The transport layer takes the network layer and extends it to individual processes on the computer. It implements ports and port binding to ensure the correct information is being sent and received. It's also the layer where the ubiquitous UDP and TCP live ( see below for details on these). Lastly, the application layer takes the generalized set of layers beneath it and uses them for your actual multiplayer game. Typically you will have some client-server 6 or peer-to-peer 7 handshake[ 8 code here to finish out the network connection, as well as whatever network behavior your game actually needs. At the transport layer, the two most used protocols are the transmission control protocol (TCP) and the user datagram protocol (UDP) . TCP is what most of the internet uses, as it's reliable and guarantees correctly-ordered information (imagine if the email your boss sent you became scrambled on its way to your computer!). UDP, on the other hand, is what fast-paced, constantly changing applications use, including video games and video live streaming. Some of the packets may come into your computer out of order or they could be lost entirely! But this is okay, because we only care about the most recent information, and it happens to come in quickly so losing things here and there won't hurt. If we had to use an analogy, TCP would be like requesting and confirming a drink of water from someone every time you need a drink, whereas UDP would be like someone spraying a hose of water at your face while you try to catch it in your mouth 9 . By the way, much of our initial design of the networking module is informed by Ch. 6, Network Topologies and Sample Games, from Multiplayer Game Programming , if you want to learn more.","title":"Networking"},{"location":"blogs/week-3/#the-actual-coding-part","text":"The 3 rd party networking library yojimbo contains logic for everything through the transport layer essentially\u2014even options for using UDP or TCP with the handshake. This makes the burden on us of making a networked game engine in less than three months far less significant, although it's certainly still not insignificant. The furthest yojimbo goes is providing an abstract Message class that can be implemented to be serialized, and broken into packets to be sent across the server. We still have all of the logic on the application layer, which not only includes connection management but also any gameplay needs like object/world-state replication and serialized commands. The first thing we did with yojimbo was taking its client-server test and put it inside our engine. It didn't use any of the engine components or modules, but is technically in our project (we're done, right?). We used it as an opportunity to learn the yojimbo library at a high level before tackling problems with it. We then extended it to use user control from our Input module. After we successfully brought the yojimbo test into our engine, we began to abstract the yojimbo functions into our own Networking module, which meant we could hide details about processing, sending, and receiving of messages. The basic loop for an action game network is: Read in new packets, reconstruct them into messages, step the simulation forward one step, queue up new outgoing messages, then break them into packets and send those 10 . The ordering of that cycle could depend on what your game does, but that's the basic form of it. We only want to concern ourselves with the high level of the process, though. We want to put our letter in the mailbox and expect it to make its way to its recipient eventually. \"Messages\" can include information for anything\u2014which we anticipate will be a big design problem for us in the future\u2014but right now, they only include rudimentary types, like integers or strings. At a high level, we know that we may be sending lots and lots of messages every frame, especially from the server. For optimization's sake, we implemented a ring buffer 11 class to store our messages in so that we don't have to manually cycle elements through our arrays. We also modified small parts of the yojimbo codebase so that it better fits our code architecture, since specializing it to our own code would make it both easier to use and read and more performant. The Networking module that we created has two sides: client and server. The client is every user who connects to the server, which is the machine with the authoritative definition of important game information. yojimbo doesn't support peer-to-peer networking, but we also knew that a client-server model would be much easier for us to do because peer-to-peer requires complex conflict resolution and significantly more bandwidth from every client. In fact, many factors of peer-to-peer networking essentially use the client-server model.","title":"The Actual Coding Part"},{"location":"blogs/week-3/#making-the-networked-push","text":"Once we established our basic wrapper overtop of the yojimbo library, we set off to homebrew our own network test. This should have been fairly straightforward like a mirror of the yojimbo client-server test, but for some reason, it just wasn't working! Over the course of several days, one of our developers dug deeper and deeper into the yojimbo codebase to find the solution. This was a blessing and a curse. yojimbo is over 10,000 lines of unfamiliar code, and learning it could make the engine's networking component much more effective. But networked code can be intimidating to debug\u2014even on a single, localhost 12 machine because of the physical and link layers. The journey came to an end when he mangled our wrapping code beyond recognition. He managed to disable a default parameter in the construction for the Address class, which contains both an IPv4 value and a port. As it turns out, yojimbo's Address objects can be instantiated with some default port value so long as an IP address is given! And if the other networked machine happens to be using a custom port, then the packets get lost at the transport layer. Once that was corrected, everything worked! And our developer went on his way to cleaning up the mess he made\u2026","title":"Making the Networked Push"},{"location":"blogs/week-3/#game-like-networking","text":"The simple tests were cool, but if it's just on the same machine, then you could cut out all of the middleman and have a more efficient and far less confusing system. No, we needed to take the next step before we were satisfied with our progress. Amazingly, this was much easier progress to make than the simple single-machine test, and this is largely because of yojimbo. By inputting our local network IP addresses instead of localhost addresses, we connected two of our computers (over the same connection) almost immediately after finishing the single-machine test. It was amazing! We could say \"Hi!\" to each other with just the press of a single key. We didn't stop there. We wanted it to at least look more like a game, so our other developers pitched in and utilized our basic network module to send rudimentary signals across two machines to play an audio file along with playing and stopping an animation. To top it all off, we quickly refactored the backend of the networking module to run more than just one client per server, and we all got into one session spamming these audio and animation messages over the network! At this point, it's looking like we're going down the networked game engine route, and we're mentally prepared for that. yojimbo has been a great shortcut regarding the link through transport layers, and now we have our real big obstacles to tackle now: state synchronization and a more comprehensive system of messages that could be used somewhat generically.","title":"Game-like Networking"},{"location":"blogs/week-3/#coming-soon","text":"Released today was Casey Muratori's, developer of Handmade Hero, interview . We are working on editing the interview Tommy Refenes of Team Meat for next week. We would appreciate any feedback or questions you may have about our content or what we are doing in the comments.","title":"Coming Soon"},{"location":"blogs/week-3/#resources","text":"The resource page has been updated to include links we found useful this week, too! Originally published September 21, 2018. STL stands for standard library which is the C++ library containing most of the needed data structures. It is known to not be best for performance, however, will save us time not implementing them. \u21a9 GLFW is a library utility for creating windows and receiving input from the window. \u21a9 std::function is a container for lambda functions, see cppreference . We have renamed std::function to Action in our aliases for simpler calling. \u21a9 OSI stands for Open System Interconnection and is a standard for networking layers. \u21a9 A packet is formatted data that is sent over a network. Virtual objects are serialized and broken into these small chunks before being sent, and different protocols expect different formats for packets. \u21a9 The client-server model in networking has one central server that all other machines (the \"clients\") connect to. This server is typically the authority on all important and possibly conflicting information. \u21a9 The peer-to-peer model in networking connects every machine to one another, which requires more bandwidth per client and more complex data authority handling but avoids needing a dedicated server. Peer-to-peer is generally harder to implement than client-server. \u21a9 A handshake in networking is an automated negotiation process for creating a connection between two machines, typically a client and a server. The process requires the machines to exchange special packets before the connection can be established. \u21a9 Giving credit where credit is due: https://www.reddit.com/r/ProgrammerHumor/comments/9gcwgw/tcp_vs_udp/ \u21a9 yojimbo's creator Glenn Fiedler goes into more depth about this loop in the context of yojimbo: https://github.com/networkprotocol/yojimbo/issues/25#issuecomment-265082392 \u21a9 A ring buffer (or circular buffer) is a FIFO data structure which is broadly used for transmitting data between asynchronous processes. See more: https://en.wikipedia.org/wiki/Circular_buffer \u21a9 A localhost address in networking is specifically addressed 127.0.0.1. Packets that are sent here are not technically sent anywhere, they're just sent up to the next layer for processing. \u21a9","title":"Resources"},{"location":"blogs/week-4/","text":"Buried in Comments \u00b6 Byte-Sized Updates \u00b6 This past week, progress has slowed down significantly for a few reasons. We are at a point now where we are starting to integrate the engine systems together more heavily, which has stopped the development of new features. For example, we are beginning to integrate the memory manager with the other subsystems, but don't have enough to talk in-depth about this week. Also, our GUI system is heavily reliant on the window system and has its own built-in input, which needed to be abstracted out so we could feed in our own. More to come on that next week. The biggest road bump this past week was going back through the code and adding Doxygen comments for all of our public facing functions and member variables. This took each of us much longer than expected and we can definitely understand why many developers suggest waiting until a feature is settled to comment on it. We also did a large-scale code review with our faculty and spent a considerable amount of time fixing things. Finally, we have created a demo of the engine's functionality at this point, which includes: Networking, Audio, Rendering (with animation), Configuration file loading, etc. The demo is located on the demo branch and, at the time of writing, is this commit . Check out these sick shots. Memory \u00b6 Our Naive, Naive Defragmentation \u00b6 Last week we laid down the foundation for defragmentation, and this week we implemented our first version, naive defragmentation, with simple algorithms. This is how we did it: Everything we discuss here is managed by our MemoryArena class. When creating a new object with OurNewUtility<T>() , add the new object's address and its handle index into an ordered map, addressIndexMap , which is always sorted by objects' memory address We are also using this map when finding empty memory for the new object to see if there's enough space after the last existing object When deleting an object, remove the corresponding pair from the map Have a utility function that looks at the i th object and i-1 th object, and see if there's empty space (referred to as \"holes\" in our last blog) between them. If so, move the i th object left to fill the hole and update its address in the addressIndexMap . The object's size information is stored in its HandleEntry and can be looked up through the index stored in addressIndexMap Starting from i = 0 , every frame, we execute the above utility on the i th object to the i + n th object, where n can be set to any number. This means we are defragmenting n objects every frame. Before entering next frame, increment i by n . If we keep doing it, the MemoryArena will stay defragmented. As you can see, this linear technique kinda works, but is inefficient and definitely not the industry standard. If we look at the time complexity: Step 2 is O(logn) Step 2.a is O(logn) Step 2.b is O(logn) Step 3 is O(logn) plus time for moving memory There are many O(logn)s going on, when what we want is O(1). And space-wise, the addressIndexMap is also incurring 16 bytes of memory overhead for each object. However, this naive implementation works for now, and we're not even close to dipping below 60 frames per second! (Ignore all of the systems we don't have running yet) The public interface is also defined well, which allows us to swap our defragmentation algorithm without touching anything outside of the memory manager. So we decided to keep it for now. In the future, if we have time, our plan is to update our defragmentation to use a customized free list 1 data structure. Again, we are not 100% sure about it for now and will update in the future. Patch Notes \u00b6 Math Library/Unit Testing \u00b6 We recently received a pull request for our repo suggesting a fix on our math library. For our Matrix4, we had copied most of Matrix3 (since they are mostly the same), though we forgot to change the 9 to 16 in the for loops. Also, since they were similar in functionality, we hadn't written unit tests for Matrix4 which caused us to have a bug. This is a great demonstration of why unit testing is important, but won't change too much on how we are testing because of time. Coming Soon \u00b6 Sometime this next week we will publish our interview with Team Meat's Tommy Refenes . He was able to give us great advice on creating portable architecture as well as how to best work with others as an engine developer. We would appreciate any feedback or questions you may have about our content or what we are doing in the comments. Resources \u00b6 The resource page has been updated to include links we found useful this week, too! Originally published September 28, 2018. A free list is a memory management data structure that uses a linked list which points to successive free regions of memory that can be utilized for allocation individually. \u21a9","title":"[Week 4] Buried in Comments"},{"location":"blogs/week-4/#buried-in-comments","text":"","title":"Buried in Comments"},{"location":"blogs/week-4/#byte-sized-updates","text":"This past week, progress has slowed down significantly for a few reasons. We are at a point now where we are starting to integrate the engine systems together more heavily, which has stopped the development of new features. For example, we are beginning to integrate the memory manager with the other subsystems, but don't have enough to talk in-depth about this week. Also, our GUI system is heavily reliant on the window system and has its own built-in input, which needed to be abstracted out so we could feed in our own. More to come on that next week. The biggest road bump this past week was going back through the code and adding Doxygen comments for all of our public facing functions and member variables. This took each of us much longer than expected and we can definitely understand why many developers suggest waiting until a feature is settled to comment on it. We also did a large-scale code review with our faculty and spent a considerable amount of time fixing things. Finally, we have created a demo of the engine's functionality at this point, which includes: Networking, Audio, Rendering (with animation), Configuration file loading, etc. The demo is located on the demo branch and, at the time of writing, is this commit . Check out these sick shots.","title":"Byte-Sized Updates"},{"location":"blogs/week-4/#memory","text":"","title":"Memory"},{"location":"blogs/week-4/#our-naive-naive-defragmentation","text":"Last week we laid down the foundation for defragmentation, and this week we implemented our first version, naive defragmentation, with simple algorithms. This is how we did it: Everything we discuss here is managed by our MemoryArena class. When creating a new object with OurNewUtility<T>() , add the new object's address and its handle index into an ordered map, addressIndexMap , which is always sorted by objects' memory address We are also using this map when finding empty memory for the new object to see if there's enough space after the last existing object When deleting an object, remove the corresponding pair from the map Have a utility function that looks at the i th object and i-1 th object, and see if there's empty space (referred to as \"holes\" in our last blog) between them. If so, move the i th object left to fill the hole and update its address in the addressIndexMap . The object's size information is stored in its HandleEntry and can be looked up through the index stored in addressIndexMap Starting from i = 0 , every frame, we execute the above utility on the i th object to the i + n th object, where n can be set to any number. This means we are defragmenting n objects every frame. Before entering next frame, increment i by n . If we keep doing it, the MemoryArena will stay defragmented. As you can see, this linear technique kinda works, but is inefficient and definitely not the industry standard. If we look at the time complexity: Step 2 is O(logn) Step 2.a is O(logn) Step 2.b is O(logn) Step 3 is O(logn) plus time for moving memory There are many O(logn)s going on, when what we want is O(1). And space-wise, the addressIndexMap is also incurring 16 bytes of memory overhead for each object. However, this naive implementation works for now, and we're not even close to dipping below 60 frames per second! (Ignore all of the systems we don't have running yet) The public interface is also defined well, which allows us to swap our defragmentation algorithm without touching anything outside of the memory manager. So we decided to keep it for now. In the future, if we have time, our plan is to update our defragmentation to use a customized free list 1 data structure. Again, we are not 100% sure about it for now and will update in the future.","title":"Our Naive, Naive Defragmentation"},{"location":"blogs/week-4/#patch-notes","text":"","title":"Patch Notes"},{"location":"blogs/week-4/#math-libraryunit-testing","text":"We recently received a pull request for our repo suggesting a fix on our math library. For our Matrix4, we had copied most of Matrix3 (since they are mostly the same), though we forgot to change the 9 to 16 in the for loops. Also, since they were similar in functionality, we hadn't written unit tests for Matrix4 which caused us to have a bug. This is a great demonstration of why unit testing is important, but won't change too much on how we are testing because of time.","title":"Math Library/Unit Testing"},{"location":"blogs/week-4/#coming-soon","text":"Sometime this next week we will publish our interview with Team Meat's Tommy Refenes . He was able to give us great advice on creating portable architecture as well as how to best work with others as an engine developer. We would appreciate any feedback or questions you may have about our content or what we are doing in the comments.","title":"Coming Soon"},{"location":"blogs/week-4/#resources","text":"The resource page has been updated to include links we found useful this week, too! Originally published September 28, 2018. A free list is a memory management data structure that uses a linked list which points to successive free regions of memory that can be utilized for allocation individually. \u21a9","title":"Resources"},{"location":"blogs/week-5/","text":"Back in Business! \u00b6 Byte-Sized Updates \u00b6 Goodbye Module Manager : Removed the manager that handled the lifecycle of all of our modules because redundancy. Scene Graph Object Model : Determined to use a blend of data-oriented and object-oriented design for our scene graph object model for the rest of development (or until we change our minds). GUI : Created a module for the GUI system, and learned why having a sample game is important for engine development. More on Horde3D : Fixed the issues that Horde3D's lightweight-ness brought back to us. Memory : As the team is integrating memory manager to their modules, we got a better sense of how to do it and made some updates. We added a free list allocator, abandoned our \"all static\" fantasy and is updating our alloc API design. Networking : Replaced the memory management with our own system and discussed some big design decisions for the networking backend coming down the line. Although things look like they are not changing much, most of the current work is stemming from the need to rework systems after integration. The dependencies of a system are driving additional development on that system, almost like feature requests. We are pushing forward with an increased focus on the game's needs, and we're planning to start the game as soon as possible to find more cracks in our system. Goodbye Module Manager \u00b6 You will not be missed. What ModuleManager became was a container that held all of the modules and dictated their lifecycle. It was originally meant to act as a protective barrier for the game developer, as described in Week 1 , to stop them from \"accidentally\" starting or stopping a module. So why are we removing something that is at the center of our engine, holding all of the module's lifecycle? Well, that's exactly the reason we are removing the manager. We could easily see the module manager becoming the central hub of the engine, i.e. everything needs to go through the module manager to work, which doesn't make sense. This decision stemmed from our conversation about the needs of the SceneGraph . The InputModule needs to be updated prior to the SceneGraph update, and the AudioModule needs to be updated after (see code below); the game developer should be able to use the input in their gameplay code and trigger audio that is played on the same frame as its triggered. ModuleManager :: Update () { inputModule -> Update (); // Need SceneGraph.Update() here audioModule -> Update (); } Since SceneGraph isn't a module, it shouldn't be part of ModuleManager and ModuleManager only has the Update function, so then how do you get SceneGraph update between them? There are numerous hacky ways: Pass functions as parameters to ModuleManager 's Update and RenderUpdate Have PreUpdate , Update , LateUpdate functions in ModuleManager , and the same three for the RenderUpdate Neither of these felt right and it kept begging the question: What was ModuleManager providing us that we couldn't just get by having the modules as member variables of the EngineLoop ? The answer was nothing. It was protecting them no more than the EngineLoop could, so we cut out the middleman. The decision also made us reconsider our update loops. Firstly, we were calling the 2 update functions SimulationUpdate and RenderUpdate , but realized those are inaccurate names. We've since renamed them and will be referencing them in the future as FixedUpdate and VariableUpdate . These names better portray the functionality and remove the misconception that RenderUpdate should only have rendering functions inside. With this out of the way, we started considering why we have our input, audio, gameplay, and all other update functions except network and physics inside FixedUpdate . By having these updates within FixedUpdate , the gameplay will appear to be stuttery and input won't be captured as frequently as rendering occurs. Networking and physics are the only 2 systems that are dependent on deterministic behavior 1 , everything else can use a variable delta time. So now our new update loop looks like: EngineLoop :: Update () { while (...) { for (...) { FixedUpdate ( intervalTime ); } VariableUpdate ( deltaTime ); } } EngineLoop :: VariableUpdate ( deltaTime ) { inputModule -> Update (); // should happen prior to gameplay updates sceneGraph -> Update (); sceneGraph -> LateUpdate (); // happens after the regular gameplay, but not after anything else so gameplay can still affect the current frame audioModule -> Update (); // happen after gameplay to play triggered audio and positioned based on gameplay renderModule -> Update (); // happen after gameplay for positioning and visibility guiModule -> Update (); // happen after rendering so it is displayed ontop of render windowModule -> Update (); // happen after render, gui, input to swap buffers because each depend on window handle memoryManager -> Update (); // happen at the end of the frame to clear single frame allocator and swap double frame allocators } EngineLoop :: FixedUpdate ( intervalTime ) { networking -> Update (); // happens first to receive, process, and send packets sceneGraph -> FixedUpdate (); // happens before physics so collisions can still be solved if one occurs physicsModule -> Update (); // after all gameplay to solve any collisions } Laying out our updates this way also let us reconcile some other conversations we were having. One of those conversations was how MemoryManager 's single and double frame allocators \"frames\" were peculiar. They were updated based on the simulation tick, but typically you associate a frame with rendering; this felt wrong. Since we weren't using the frame allocators at the time, we ignored the peculiarity. However, now the MemoryManager updates in the same frame as rendering, thus the frame allocators correspond to actual frames. Scene Graph Object Model \u00b6 Now that we're getting to our high-level systems like the scene graph, we decided to sit down and determine what our object model would be. At its most basic, object models for game programming can be split into roughly two schools of design: data-oriented and object-oriented. It's All in the Data \u00b6 Data-oriented design is a very strongly - touted and thoroughly discussed philosophy that is essentially built around one concept: everything is data, and turning that data into something else is the end goal. More specifically, this manifests itself into a lot of thinking about data locality 2 and minimal branching 3 , and how to process similar data all together quickly. It came into the public eye around the seventh game console generation because CPUs began to be more powerful than necessary and memory was still slow, so game programmers did what they do best and yelled at the artists figured out how to best operate around the machine's memory accesses and data caches. If you don't come from video games, you very well may have never heard of data-oriented design. That's fair\u2014it's not really necessary unless you have a running-time limited system like video games. But in the video game world, data-oriented design has only gotten more and more popular, especially now that GPUs are becoming more generally programmable and CPUs are getting enough cores that they're effectively GPU-lites. And as game developers, we do want to use the best options available for our systems, but we face a conundrum: data-oriented programming tends to be very difficult and brittle in big, messy systems like a game engine . To OOP or not to OOP \u00b6 This is where object-oriented programming (or OOP) comes in. Everyone knows it, and programmers who have had as little as one programming class are already \"indoctrinated\" with OOP practices. It's very simple to think about conceptually, and its abstractions tend to make code cleaner when you having communicative or shared systems. However, OOP is almost on the opposite side of the spectrum from data-oriented programming, and it tends to prevent optimization at a high level. So you can imagine why game developers don't particularly like OOP. So which one do we go with? Data-oriented or object-oriented? Turns out, it's not a binary choice! Data-oriented concepts such as keeping similar data packed together or minding the data cache can still be put into practice with an object-oriented architecture, and the most significant blemish with OOP is inheritance ( this covers some of the misuses ), which we can specifically avoid while we're using other OOP concepts like composition and interfaces. We acknowledge that we don't have the time or experience to handle the difficulty of fully utilizing data-oriented design, and as a short yet ambitious project, we want to keep moving at top speed! So we settled on an object-oriented composition approach, where we can try to utilize data-oriented practices in cut-and-dry cases that won't cause more complication. Essentially, this means we're going to have objects very similar to Unity's GameObjects with their Components. Is it derivative? Slightly. But you know what? Convenient object models are used for a reason! GUI \u00b6 Why do we need GUI? \u00b6 GUI, or Graphical User Interface, is one way a player is able to interact with the game and also allows the developer to display important information to the player. As mentioned in the week 3 blog , Horde3D isn't packaged directly with a user interface. However, our sample game needs some basic UI features. We had assumed any rendering library wouldn't be complete without a programmable GUI, which was our naive assumption from looking at Ogre3D as the tentpole. So with that, let's look at the demo to see what we need: The gameplay needs are fairly basic: static text for \"Health\" and \"Score\", dynamic text for the score value, and a filled rectangle to display the health bar. However, we realized that the gameplay demo isn't a complete picture of what the game needs (especially in the case of multiplayer). The actual game flow is depicted below, with needs of a menu to select single player or multiplayer and, at the very least, to be able to select whether the game client will be hosting or which host to connect to. These type of decisions require something like a button for selection. While thinking of the UI flow of the game, we did a quick mock-up of what the menu may look like. The menu would probably require a title and (as mentioned) buttons, as well as some way of choosing single player or multiplayer. For multiplayer, if you are a host, there would need to be feedback to alert you of the number of clients in your lobby as well as an additional button to start the game (the host holds all the authority in our game, because they will be acting as the server the clients communicate with). If connecting to a host, the most basic solution would be to have an input field for the host's IP address (a more complicated one would be to have a list of lobbies to connect to, which we may try doing with local addresses). \"But why stop here?\", we thought. What we have listed is only ~5 GUI features and most engines have way more than that. If we are already taking cues from Unity, why not have the full functionality of their GUI system , especially because we have found some of their features very handy in making games? Plus, a \"real\" game engine would have much more than this to accommodate their developers, we don't want to just do the bare minimum. And although doing slightly more than the bare minimum is a good idea, we quickly realized this was a very poor mindset to have. So Many GUI Options \u00b6 So even before laying out the needs of the game, we went shopping for a library. Although Horde3D doesn't have GUI by default (we were realizing how Horde3D was able to remain so lightweight), we found a Horde extension that added 3D and 2D GUI to the rendering engine. When doing the initial research into the extension, we found a few Horde3D forums that pointed to an extension. To our dismay, there were replies stating that it wasn't the best solution. In addition, the extension was written in 2010 (8 years ago at the time of writing) which we thought didn't bode well since Horde3D has changed a lot in that time. The lack of maintenance of the extension could cause a serious headache. We then directed our attention to 2 other libraries: Dear ImGui (ImGui) and Qt . We swiftly made a decision to use ImGui because Qt has an open source/commercial policy that we didn't want to accidentally break. ImGui also had the added benefit of looking simple to integrate with only a few files, and there were ample resources for the library. Both the FAQ on the Github page and the repo owner are currently very active. Another benefit is it is in C++, which meant we wouldn't need an interface with another language. We also looked at Unity's GUI system to see what features a \"typical\" engine has which acted as a baseline of what we thought we needed. ImGui has overlapping functionality with Unity, so it passed that check. GUIModule \u00b6 Like our other modules, the GUI module was broken into the module class, GUIModule and an interface class, GUI , for the game developer. Using ImGui's GLFW OpenGL example, we were able to have ImGui and our demo running simultaneous, but not really fully connected. When deciding our StartUp hierarchy, we decided to keep it as a flat structure. This meant the window module needed to be started prior to the rendering/input so the window handle 4 could be passed to those modules. This decision proved to be the right one when implementing the GUI system, because the GUI also needs the window handle which was easy to provide. The other benefit of doing the explicit ordering of the module's StartUp / ShutDown / Update is the GUI should be started after the window but also needs to be rendered (updated) after the render update otherwise the render module will clear what the GUI just displayed. The only hurdle which required redesign when integrating ImGui was that of the input system. ImGui handles its own input by setting the GLFW callbacks, however, our own input system also sets the GLFW callbacks. So depending on the startup order, one of the modules would clobber the other's callbacks in GLFW. ImGui was designed such that the callbacks could be optionally set on initialization, which allowed us to be able to add ImGui's callbacks to our own input system rather than on startup. Immediate vs Retained Modes \u00b6 With ImGui minimally integrated the main discussion became on how we would like to use the GUI system (and in turn how the game developers would use it). We evaluated 2 options: IMGUI (not to be confused with ImGui) and retained-mode (although not knowing that's what it was called). IMGUI stands for immediate mode GUI which is a code-driven GUI system where on each rendering frame the application needs to issue the draw commands of the GUI (the GUI is not stored for multiple frames); where retained , also known as canvas/scene graph/object-based UI (this is how we originally labeled it), is where GUI is registered once and is displayed, \"retained\", on screen until it removes itself from rendering 5 . An IMGUI system relies on the developer placing GUI code within something like an OnGUIUpdate method which is called each render frame, where retained-mode requires the developer to hold references to objects created such as new Window , new Button , etc. IMGUI is known to be good for developer tools and usually quicker to develop, however, it isn't as nice for doing something similar to Unity's canvas system which is a retained model. Initially we thought of implementing both, however, we could see that this could cause confusion for developers using the GUI, as well as our retained-mode would be more akin to a wrapper over immediate mode because the library we are using (it wouldn't really be retained). Although the retained mode is nice to use and what we are used to with Unity, we knew we wouldn't be using a designer tool to create our GUI (nor have the time to implement one) and prioritized speed of development over designer ease to design. In addition, Dear ImGui is set up for immediate mode (as per the name) and we knew fighting the library would cause more headache than actual benefit. This decision was reaffirmed in our interview with Amandine , who said for a small scale/scope projects that immediate mode was the way to go. We also received similar advice from our conversation with Walt Destler , where he said that a retained-mode would be nice, but it wasn't worth the time to develop the tool to edit the UI (and he is making a game that will actually be published!). Wrapping All the Features \u00b6 With the mode decided on how game developers will interact with the GUI module, we then needed to develop a wrapper to wrap ImGui so ImGui code wouldn't infect the game's codebase. The reason this is useful is so you have the ability to replace ImGui with another GUI renderer of your choice without changing the game codebase. And although this was the aim, doing this isn't a trivial task because our GUI system was set up to imitate ImGui's; not a generic GUI. ImGui was the readily available example that we had to model after. Plus, our sample game may be devoid of ImGui, however, we didn't completely replicate all functionality of a GUI, only the features needed for our game, so another game that uses the engine (if GUI isn't expanded) may need to use ImGui functions/functionality. For developing the wrapper, again we looked at Unity's system where all GUI functions were positioning absolutely from the top-left corner, use Rect for positioning and size, as well as for what features were available and what features we have used in our development. We then combed through the ImGui demo, picking out the features from ImGui that we thought would be useful to have in our engine, which as you could imagine ballooned our GUI system (way beyond the listed needed features). Reading through the ImGui demo allowed us to learn ImGui, though it was ultimately a waste of time to collect features that weren't necessary to our game and we wouldn't have time to wrap. Instead of trying to wrap all functionality, even ones we wouldn't use, we simplified to be only the functionality we knew we would use as well as a handful of additional functionalities. We suggest starting with creating functionality that you need for this game. If your next game needs more functionality you can add it then, we aren't creating Unity and if you are, you are still better off iterating features as they emerge rather than trying to fully cover the system. In our GUI.h you will find commented out functions and TODO 's scattered throughout, which are remnants of trying to fully wrap ImGui and aren't required for our game. If you would like to try your hand at creating wrapper functionality, you can easily add functions throughout this file (even make a pull request, we'll most likely accept it). To go even further would be to combine/expand ImGui features in a single GUI feature that you would want to use in a game. When implementing our wrapper, the functions and functionality of ImGui didn't map one-to-one with how we would like to do our GUI. This is another reason to have a wrapper. For each ImGui function that we had decided was needed, we merged some of those functions together and replaced the ImGui classes (ImVec2/ImVec4) with the Isetta classes ( Math::Vector2 / Math::Vector4 / Color ). The biggest difference from our wrapper to ImGui (and Unity to an extent) is each GUI function requires a position and size for the element as well as an anchor pivot location and a element pivot location to determine where the element is located on the screen (by default, ImGui positions GUI in rows/columns and groups). Because our GUI and ImGui position elements differently, this is one of the reason we don't want to restrict the use of ImGui in the game code; they have different functionality which a game developer may want to take advantage of. Another difference between ImGui and our GUI implementations is the \"container\" windows (ie. window/modal/popup). Whereas ImGui relies on the game developer to remember to end the window context after beginning window, our GUI wraps this functionality with lambda functions. We made it this way because we think this wouldn't be as taxing on the developer. Sort of Related Lessons \u00b6 To reiterate the lessons from developing the GUI wrapper: Don't try to create/wrap all functionality for a system if it isn't needed for the game you are immediately making \u2192 make only what your game needs. For us: Buttons/Labels/Text/Input/Drawing Quads Don't think that what Unity or another engine has is what your engine needs. When you or a game developer needs a new feature, you can go back and add functionality. After completing the wrapper, or more accurately, while developing the wrapper, we continued to add functionality to the main engine loop as example code prove the features work and provide examples for everyone to see implementation. Here is a example of the UI running in the engine, with all implemented functionality displayed simultaneously. More on Horde3D \u00b6 In our week 1 blog , we mentioned that we chose Horde3D over Ogre 2.1 based on the fact that Horde3D has better documentation, is easier to build and is more lightweight. We thought it would give us good enough graphics with less cost importing the Horde3D library as our rendering engine. However, four weeks later, with integrating more modules into our game engine, we found that Horde3D might have been less than ideal than we initially thought. It is lightweight for external function calling (the reason why it has fewer and clearer API in the documentation), but hides too much detail for a deeper integration. GUI and Textures \u00b6 As mentioned before, as a part of graphics, we wanted to draw 2D overlays as in-game UI. However, Horde3D itself is such a \"pure\" rendering library that it isn't packaged with a UI utility. An extension can draw some UI, but because of the reasons listed above, we had to introduce one more third-party library. Stitching another library is not an easy task. We later found ImGui doesn't have its own texture loader, so we sought help from Horde3D since it can read an image from the disk as a texture. However, to our surprise, the texture decoding is hidden inside its TextureResource class instead of extracting it as a texture decoding 6 utility for reuse, since it is not supposed to be used by external users. Luckily, we later found h3dMapResStream , so that we can let Horde3D load and decode the texture for us and map the pixel stream 7 out later. Loading Nested Resources \u00b6 Loading rendering resources like meshes and animations is a time-consuming part of a game. To hold this resource loading by ourselves, we implemented a filesystem that directly calls Windows API. This week, we started on integrating it into the rendering module. Horde3D provides an h3dLoadResource API which allows us to read the resource file by ourselves and send the data to Horde3D. However, this API hides a lot of details which caused a bit of confusion. Horde3D supports .scene.xml file to specify a scene node with mesh, material and its shader. A simple example of it is like this: <Model name= \"sphere\" geometry= \"models/sphere/sphere.geo\" > <Mesh name= \"Sphere01\" material= \"models/sphere/stones.material.xml\" batchStart= \"0\" batchCount= \"2880\" vertRStart= \"0\" vertREnd= \"587\" /> </Model> As you can see, this file has nested containers and has multiple resources. This load resource function call will only load the \"meta\" file, and \"secretly\" add all nested resources into Horde3D's own resource manager but keep them unloaded, without notifying the developer. Even in its documentation, there is no mention about this: This function loads data for a resource that was previously added to the resource manager. If data is a NULL-pointer, the resource manager is told that the resource doesn't have any data (e.g. the corresponding file was not found). In this case, the resource remains in the unloaded state but is no longer returned when querying unloaded resources. When the specified resource is already loaded, the function returns false. As a user, we have to manually check if there are any unloaded resources remaining in a while loop. The way to achieve that is to do a query by h3dQueryUnloadedResource . After we dug deeper into this function, we found out the internal implementation of this function wasn't ideal. ResHandle ResourceManager :: queryUnloadedResource ( int index ) const { int j = 0 ; for ( uint32 i = 0 ; i < _resources . size (); ++ i ) { if ( _resources [ i ] != 0x0 && ! _resources [ i ] -> _loaded && ! _resources [ i ] -> _noQuery ) { if ( j == index ) return _resources [ i ] -> _handle ; else ++ j ; } } return 0 ; } What we want is a function that returns the newly nested resources added to the resource list (or the next unloaded resource in the list), while queryUnloadedResource goes through the whole list and returns the first one. It will be significantly slower if we have lots of resources to load. To fix this issue, we decided to assume that the resource handle is always increasing so that we can check the next resource by h3dGetNextResource and h3dIsResLoaded repeatedly until the handle reaches the end. Memory Anarchism \u00b6 After loading the resources, we also want to manage the memory taken by the resources by our own memory manager. However, Horde3D 1.0 doesn't support custom memory allocators (they do have it as a feature for Horde3D 2.0 which is still not released yet, if it is\u2014 check here \u2014at the time you're reading this it can be an exercise for you to replace Horde3D 1.0 with 2.0 and use custom memory allocation!). All the memory allocations are done by Horde3D internally with new s. We can't bring our own memory manager without heavily modifying the source code of Horde3D which is near impossible for our semester-long project (it would counteract the reason why we chose Horde3D before). It's unfair to say choosing Horde3D was a bad idea, but the lightweight-ness we expected from Horde3D to be a benefit is putting the burden onto us since we have so many customized systems, like filesystem and memory manager. This week we are implementing the scene graph and hopefully, we can still use Horde3D organically. Memory \u00b6 Starting from the last week, our team started integrating our own memory manager into the subsystems and 3 rd party libraries (yay!). It's a very exciting task, but it exposed many issues and limitations of our memory manager. As others were busy integrating the memory manager, one of our developers spent time gathering and analyzing incoming requirements so that we could make some updates to our memory manager. Freedom of The Free List Allocator \u00b6 The first thing is that the existing memory allocators are very limited and can't cover all of our usage scenarios. For example, our GUIModule 's StartUp() process involves a lot of dispersed allocation and deallocation of random sized memory, and is managed by ImGui so we can't easily change it. All we can give ImGui is an allocation callback and a freeing callback. As a stack allocator can only handle sequential alloc/free; pool allocator can only handle same sized alloc/free; dynamic arena can only be used with ObjectHandle s, there is no way to make this work without just wasting memory. In addition, the networking module needs to allocate memory for each client connected to the server at runtime for receiving messages, etc. That means the memory needs to be persistent, so the single frame and double buffers are out. Memory arena (refers to our defragmentated memory area as in last week's blog ) sounds like a good candidate for this, but the networking module needs to allocate memory for its ring buffer data structures, which holds an array buffer. Uh oh! Our ObjectHandle is not well-prepared to work with arrays. All of these signals are suggesting that we need a more versatile and flexible allocator. After some research, the free list allocator seems to be a good solution. Unlike stack allocators and pool allocators, free list allocators give flexibility to the user in terms of the size and order of allocations/frees. It works by: Keeping a linked list (sorted by memory address) of free memory chunks. Each time memory is requested, it goes through the linked list, find a large enough chunk and returns it to the caller. There are also options like find first fit (loop through the list and find the first big enough node, prioritize speed) and best fit (loop through the whole list, find those big enough ones, and choose the smallest one among them, prioritize low fragmentation) for free list allocators for different scenarios. When a memory is freed, the free list adds that chunk back to the free list for further requests and tries to consolidate it with adjacent chunks to reduce fragmentation. It doesn't try to defragment like the memory arena, because the chunks which are being used cannot be moved (the pointers remain static). As you can tell from the description, a free list can solve our speed and fragmentation problems to some degree. It's a bit slower than stack and pool allocators as we'll need to find memory chunk from the linked list and it may still cause some defragmentation, but the flexibility it brings is worth the tradeoffs. And here is how it works (you can take a look at the code if interested FreeListAllocator.h ): The class and Node struct are defined as below. We won't include alignment here so things are easier to explain. class FreeListAllocator { void * Alloc ( Size size ); void free ( void * memPtr ); Node * head ; void * memHead ; }; struct Node { Size size ; Node * next ; }; First, a free list allocator will be initialized like this when constructed. Here is what it looks like after allocating an object of size 200: The steps happened behind the scene are: Starting from head , find the first Node that's big enough to hold p1 and an AllocHeader Create a new Node after this allocation (in this case, at address 208) Remove the original Node from the linked list Construct an AllocHeader at the original Node 's address, this info will be used when freeing p1 After the AllocHeader , construct p1 and return this pointer The AllocHeader is defined like this: // sizeof(AllocHeader) is 8 struct AllocHeader { Size size ; }; Notice we just buried an AllocHeader in memory and didn't keep a pointer to it, it will be used during the freeing process. Then allocating another chunk will be similar to this one. Here is what it looks like after freeing p1: The freeing process is more interesting as here we mine out the AllocHeader buried before and use it to determine the size of the object to free. Here is what happened behind the scenes: 1. Starting from p1 's memory address, find the AllocHeader we buried before it 1. As we know the size now, create a new Node with that size and add it to the linked list (remember the linked list is always sorted by memory address) 1. Try to merge with the next and last Node . In this case, the new node is head and not adjacent to the next node, so we can't merge Let's see an example where we need to loop through the list to find a chunk that fits, and an example where we can merge the freed memory. ^ We used the second node to satisfy this request as the first node is not big enough ^ We are halfway through freeing p2, the new node is just created here. It's adjacent to the node on its left so we can merge them, like shown below. Our current implementation of the free list allocator is by no means optimal. Allocation and free are both of O(n) time complexity because of searching and keeping the list sorted. If time permits, we will optimize the underlying structure from using a linked list to a tree, so we can reduce the time complexity from O(n) to O(logn). The Almighty Alloc - Updates on Memory API Design \u00b6 We also updated our API to make it easier for the team to use. During the process, we had a big discussion on how to design one of the most fundamental APIs: How to alloc on the LSR and level data area (look at our week 2 memory section for a review!). As a recap, the difference between this two memory areas is that, level data memory will be cleared at the end of each level, and LSR will only be cleared when the game shuts down. We have two choices. One is to make things explicit and have both AllocOnLSR and AllocOnLevelData functions. These two functions will take care of their respective areas of the stack, and assert if called incorrectly. For example, if you try to allocate something on LSR after the level starts, it will pass an exception, as the main \"big stack\" is already in the stage of allocating level memory or other things. Similarly, if you try to allocate on the level before the level starts. This method prevents misuses of both functions by creating errors to ensure the developer knows what they are doing and when they are using a function incorrectly. However, the downside is that in the future, we may have more allocation types other than just LSR and level (sublevel for example) and it will be taxing for programmers to remember when to use each as well as tedious for module upkeep. The other way is to make things implicit and only have one Alloc function, which by itself determines where the new memory request should go. Different from our explicit approach, Alloc won't cause errors in any scenario. If a programmer allocated something in level data memory but assumed it's was located on LSR, then tried to use it in a later level, the game will probably crash. The advantage is that the API will be super clean and future additions of allocation types will be handled elegantly by this method. However, this approach assumes we have a team of programmers who know the API and understand the memory layout, if not, bugs would be very hard to catch. We chose the implicit solution for several reasons. First, we only have 4 programmers working in the same room so everyone is aware of how to properly use this API. Second, LSR memory allocation requests are mostly made in the StartUp methods of each module, and level data request will be made in the level's initialization function. The probably of misusing is low, if you are in StartUp you will be using LSR and if not you will be using level is simple enough to remember. Almighty Alloc for the win! Abandoning the \"all static\" fantasy \u00b6 In our week 2 blog , we said we wanted to achieve zero memory allocation after game StartUp and satisfy all memory requests with our memory manager. However, as fantastic as that sounds, it's been giving us a lot of headaches. We find it hard to predict how much memory we will need throughout the entire game, and how to make sure all systems run with enough memory. The audio system is a great example of this: there is no good way to determine its memory demand other than relentless iteration. Fortunately, we have our free list allocator now! Although not implemented yet, the free list will grow in size when it requires more memory. This method isn't as detrimental as new/delete or malloc/free because instead of getting more memory each call and switching between kernel and user mode, the free list will allocate chunks of memory to be used. Therefore, it naturally became our ideal solution to the problems mentioned before for the systems whose memory usage is hard to predict before runtime, we will use a free list allocator. This decision surely breaks our \"all static\" fantasy, but it seems to be more practical and still keeps us away from our two problems, again, they are speed and fragmentation. Networking \u00b6 An Overview of the Future \u00b6 Network programming has taken the back seat to other development recently, mainly because we hadn't nailed down our game object model yet. After all, unless we want to bake all of our systems code inside of our network code, then we need to have some sort of messaging protocol to make sure the right objects and functions are receiving the right packets. We also depend on object serialization and replication, but fortunately, yojimbo handles some of that for us and we can solely focus on the data flow through our engine. If you aren't using yojimbo or another networking library for your game, you can learn more about object serialization and world state replication in Chapters 4 and 5 of Multiplayer Game Programming by Joshua Glazer and Sanjay Madhav. With our final product being a game engine, it's important to abstract the network code as much as possible so that it's applicable across many games made using the engine. Some of the obvious use cases that our sample game highlights are object spawning, object destruction, object behavior change (targeting a different location), and data updates (changing health and score), just to name a few. Other potential needs might come in the form of remote procedure calls, where we can invoke functions across the network. Bearing those thoughts in mind, we've nailed down more of our game object model and have begun work on our scene graph system, so you'll see the network code of our engine start to build out these concepts more in the coming weeks. Reworking the Networking \u00b6 In the meantime, many of our systems are out of sync with each other, so we took the time this week to update our memory management within the networking module. Up until now we have been using the standard dynamic memory allocation of C++ with new and delete , but having full control of memory allocation, defragmentation, and access (as well as avoiding context switches!) is too great a benefit to a high-performance system like a game engine. Much of the networking system uses significant chunks of memory, such as the messaging buffers 8 and the socket-level packet queues 9 . And if you're hosting a server, then you need that much memory for each of your clients! As with some of our earlier development, we needed to alter the yojimbo library in order to comply with our own systems. This was only a couple of tweaks from pass-by-reference to pointer types instead, as our engine uses a lot of pointers so we can delay object initialization to our own execution order. Beyond this, we also got some more exposure to the packet serialization and streaming backend to the yojimbo library, so we may be more prepared to make adjustments at the low-level in the future if we need to. Beyond that, we need to allocate memory for a client and possibly a server on a single machine, plus the RingBuffer objects for queuing up messages with both. Everything within NetworkingModule 's StartUp function is allocated using the MemoryManager::AllocOnStack function, but it's less straightforward than if we were an entirely homebrew engine. yojimbo uses custom Allocator classes to divvy out memory to its Client and Server objects, so we are effectively pre-allocating a bunch of memory then passing pointers around until everyone is satisfied and on their merry way. One reason why it doesn't sound more complicated is that it, very luckily, isn't; yojimbo has a TLSF 10 allocator implementation that it uses for packet-level allocations, and that allocator can be restricted to the memory defined by our own custom allocator! So in the end, our memory management is only at the high level for the module, but we still get efficiency. We can have our cake and eat it, too! One of the quirks that we had to deal with when swapping out the memory allocators was how we are currently handling our memory chunks. We have a config file in which we specify how big our stack and list memory allocations are for each category of allocation, which is good when we want to do some regression testing 11 on our memory systems but bad for any type of general usage whatsoever! For instance, while we were refactoring our client and server memory to the new memory system, strange errors came from within the yojimbo library. After digging into it some more, we discovered that yojimbo was simply doing its best to allocate more memory for packets but we didn't give it enough memory! Everything was fine after updating the config file, but it took quite a bit of time to realize that the config held the solution. Afterwards, we had some discussions about allowing free growth of some stacks and heaps, but we running into this problem actually highlighted a bigger architecture problem for us to tackle. Mo' Players, Mo' Problems \u00b6 A lot of this memory is currently what we call LSR memory, so we allocate it onto our lowest level memory stack and just keep it there until we're done running the engine. Unfortunately, there are two problems with this: Game developers should be able to determine if they are running a server or not during runtime . Right now, since we are allocating the server memory at the lowest level of the stack, we need to know if they're running the server at startup. Game developers should be able to have single player modes and multiplayer modes within the same game. Our initial design was strictly online multiplayer, but it's not a stretch to think we could make a single player game with our engine. However, what if the developer wanted to allow gamers to go online after playing a single player mode? We currently assume that the developer always needs client memory (let's not even think about running a headless server), so that's allocated on the stack, but if they aren't receiving any networked messages then they shouldn't have to waste that memory! These problems exist solely because we're trying to be coy about our static memory allocation at startup. That's indeed a noble goal to shoot for, but there are appropriate cases and inappropriate cases. For networking, it's not unreasonable to expect the game developer to allocate and prepare the networking system during \"downtime\" in the game as opposed during important runtime when context switching and dynamic allocation would really pose a problem. This means that our solution is the simplest one: Just allocate any of our networking memory in our free list, a.k.a. our own custom heap space. This will give game developers the freedom to determine if they even need any networking memory at all, and if so, when it gets allocated. We expect to not have any significant memory fragmentation issues with this method either, since we just ask for a couple of large chunks of memory and will only ever remove the entire chunks. Patch Notes \u00b6 Explicit StartUp , Update , ShutDown \u00b6 Earlier this week a member of the team was merging code and ran into a problem. The build successfully compiled and worked until the window was closed but then it crashed. The crash was involving the AudioModule , but wasn't clear why it was broken. By stepping through the call stack, we saw the bug occured in the ShutDown function and the error was almost immediately noticeable. Here see if you can see the error: Were you able to? The AudioModule is attempting to ShutDown twice, which obviously won't work. This is just to show the added benefit of having the modules listed in this fashion rather than being put into a stack or list. To be fair this example might not have happened with a list because of the merge, but there is a likely scenario where a module gets added twice. Then you have the headache of searching and debugging rather than being able to visually spot it. Memory Updates \u00b6 We also made some other minor changes to the memory manager, and they are: Create new utilities for creating objects and arrays in different memory areas with variadic templates . For example, you can use MemoryManager::NewSingleFrame<MyObj>(param1, param2...) to create a new MyObj object on the single frame area, and use MemoryManager::NewArrOnFreeList<MyObj>(100) to create an array of 100 MyObj s. We used to have some \"magic numbers\"s in the memory manager to specify the default alignment of memory allocation. Those magic numbers are now abstracted out and shared by everyone using it. This will allow for easy changes in the future and prevent inconsistency. We used to have a slot in the config file that specifies the size of our LSR and level data memory area. We now allow each submodule to have a function that calculates their memory need. And the memory manager just call each of the functions and determine the total size needed. Some modules will need to read something from the config file in their calculation function, but our start up sequence perfectly supports this: the config file is read before anything else starts. The main reason why we did this was because it's very hard and tedious to calculate memory usage of each subsystem by hand and modifying the corresponding entry in the config file. This also makes each module's memory usage more transparent. Enough with our additions and patches. Now let's switch to one of our memory manager users and see what they have to say about our latest changes! RingBuffer gets Homegrown One of our anticipated tasks after fleshing out the memory module was to go through the rest of our systems and update their memory allocation to use our own memory allocators. The best place for us to start was, of course, our data structures! Our RingBuffer uses big contiguous chunks of memory for array allocation, so we swapped out any of our array new calls with MemoryManager::NewArrOnFreeList so that we could also avoid trying to defragment the big pieces of memory. This mimics the standard new way of doing things, but instead, we now get full knowledge and control of the memory. Our method also prevents half-baked usage of our system, because any memory of ours that is free'd using delete will crash the engine! Would recommend! Coming Soon \u00b6 Go check out our interview with Tommy Refenes from Team Meat! Last week we had a great conversation with Alice Ching from Funomena and will be publishing the advice she gave us soon as well! We are still working on editing Amandine Coget's interview, stay tuned for that. And this past week we also spoke with Martin Middleton, co-founder of Funomena, and will be working on transcribing and editing that in the coming weeks! Subscribe to our mailing list to get updates on all of these interviews as well as our blogs (we won't spam you, we promise)! Resources \u00b6 From development we are acquiring a lot of great resources from where we learned certain topics, and as much as we try to synthesize things in our blogs you may be interested in the originals, which is why we have a page full of links for you to browse. Originally Published October 5, 2018. Deterministic behaviour is a process whose resulting state is determined by the initial state and inputs. It is heavily reliant on having a fixed-time so each step is performing the same amount of \"work\". \u21a9 Data locality is essentially accessing data in as nearby of code as possible to utilize caches most effectively. Robert Nystrom covers it really well in this chapter from Game Programming Patterns . \u21a9 Branching in code is when the processor needs to evaluate something in order to determine what code to run next. It tends to be very wasteful of processing time because of the typical instruction pipeline on a computer. \u21a9 The window handle is a GLFW construct can be passed to objects and functions to allow them to hook into a particular window from the operating system. \u21a9 For more information on Retained Mode versus Immediate Mode . \u21a9 Typically, textures are stored in compressed formats like .png, to read the pixels from the image file, we need to decode the texture from compressed to pixels. \u21a9 A pixel stream is a stream of pixel data, typically as four floating point numbers or integers representing red, green, blue, and opacity/alpha. \u21a9 A messaging buffer is just a queue of messages that are buffered prior to being sent. Sending can be expensive and complicated, so buffering messages eases the messaging process. \u21a9 A packet queue is a queue of packets, which are small chunks of an original, bigger message. These are sent out in-batch because of packet size limitations over internet networks. \u21a9 Two-Level Segregate Fit (TLSF) is a memory allocation scheme meant for use in video games because of its fast and efficient memory usage. \u21a9 Regression testing is the process of testing changes in software to make sure functionality is not broken when updating the software. \u21a9","title":"[Week 5] Back in Business"},{"location":"blogs/week-5/#back-in-business","text":"","title":"Back in Business!"},{"location":"blogs/week-5/#byte-sized-updates","text":"Goodbye Module Manager : Removed the manager that handled the lifecycle of all of our modules because redundancy. Scene Graph Object Model : Determined to use a blend of data-oriented and object-oriented design for our scene graph object model for the rest of development (or until we change our minds). GUI : Created a module for the GUI system, and learned why having a sample game is important for engine development. More on Horde3D : Fixed the issues that Horde3D's lightweight-ness brought back to us. Memory : As the team is integrating memory manager to their modules, we got a better sense of how to do it and made some updates. We added a free list allocator, abandoned our \"all static\" fantasy and is updating our alloc API design. Networking : Replaced the memory management with our own system and discussed some big design decisions for the networking backend coming down the line. Although things look like they are not changing much, most of the current work is stemming from the need to rework systems after integration. The dependencies of a system are driving additional development on that system, almost like feature requests. We are pushing forward with an increased focus on the game's needs, and we're planning to start the game as soon as possible to find more cracks in our system.","title":"Byte-Sized Updates"},{"location":"blogs/week-5/#goodbye-module-manager","text":"You will not be missed. What ModuleManager became was a container that held all of the modules and dictated their lifecycle. It was originally meant to act as a protective barrier for the game developer, as described in Week 1 , to stop them from \"accidentally\" starting or stopping a module. So why are we removing something that is at the center of our engine, holding all of the module's lifecycle? Well, that's exactly the reason we are removing the manager. We could easily see the module manager becoming the central hub of the engine, i.e. everything needs to go through the module manager to work, which doesn't make sense. This decision stemmed from our conversation about the needs of the SceneGraph . The InputModule needs to be updated prior to the SceneGraph update, and the AudioModule needs to be updated after (see code below); the game developer should be able to use the input in their gameplay code and trigger audio that is played on the same frame as its triggered. ModuleManager :: Update () { inputModule -> Update (); // Need SceneGraph.Update() here audioModule -> Update (); } Since SceneGraph isn't a module, it shouldn't be part of ModuleManager and ModuleManager only has the Update function, so then how do you get SceneGraph update between them? There are numerous hacky ways: Pass functions as parameters to ModuleManager 's Update and RenderUpdate Have PreUpdate , Update , LateUpdate functions in ModuleManager , and the same three for the RenderUpdate Neither of these felt right and it kept begging the question: What was ModuleManager providing us that we couldn't just get by having the modules as member variables of the EngineLoop ? The answer was nothing. It was protecting them no more than the EngineLoop could, so we cut out the middleman. The decision also made us reconsider our update loops. Firstly, we were calling the 2 update functions SimulationUpdate and RenderUpdate , but realized those are inaccurate names. We've since renamed them and will be referencing them in the future as FixedUpdate and VariableUpdate . These names better portray the functionality and remove the misconception that RenderUpdate should only have rendering functions inside. With this out of the way, we started considering why we have our input, audio, gameplay, and all other update functions except network and physics inside FixedUpdate . By having these updates within FixedUpdate , the gameplay will appear to be stuttery and input won't be captured as frequently as rendering occurs. Networking and physics are the only 2 systems that are dependent on deterministic behavior 1 , everything else can use a variable delta time. So now our new update loop looks like: EngineLoop :: Update () { while (...) { for (...) { FixedUpdate ( intervalTime ); } VariableUpdate ( deltaTime ); } } EngineLoop :: VariableUpdate ( deltaTime ) { inputModule -> Update (); // should happen prior to gameplay updates sceneGraph -> Update (); sceneGraph -> LateUpdate (); // happens after the regular gameplay, but not after anything else so gameplay can still affect the current frame audioModule -> Update (); // happen after gameplay to play triggered audio and positioned based on gameplay renderModule -> Update (); // happen after gameplay for positioning and visibility guiModule -> Update (); // happen after rendering so it is displayed ontop of render windowModule -> Update (); // happen after render, gui, input to swap buffers because each depend on window handle memoryManager -> Update (); // happen at the end of the frame to clear single frame allocator and swap double frame allocators } EngineLoop :: FixedUpdate ( intervalTime ) { networking -> Update (); // happens first to receive, process, and send packets sceneGraph -> FixedUpdate (); // happens before physics so collisions can still be solved if one occurs physicsModule -> Update (); // after all gameplay to solve any collisions } Laying out our updates this way also let us reconcile some other conversations we were having. One of those conversations was how MemoryManager 's single and double frame allocators \"frames\" were peculiar. They were updated based on the simulation tick, but typically you associate a frame with rendering; this felt wrong. Since we weren't using the frame allocators at the time, we ignored the peculiarity. However, now the MemoryManager updates in the same frame as rendering, thus the frame allocators correspond to actual frames.","title":"Goodbye Module Manager"},{"location":"blogs/week-5/#scene-graph-object-model","text":"Now that we're getting to our high-level systems like the scene graph, we decided to sit down and determine what our object model would be. At its most basic, object models for game programming can be split into roughly two schools of design: data-oriented and object-oriented.","title":"Scene Graph Object Model"},{"location":"blogs/week-5/#its-all-in-the-data","text":"Data-oriented design is a very strongly - touted and thoroughly discussed philosophy that is essentially built around one concept: everything is data, and turning that data into something else is the end goal. More specifically, this manifests itself into a lot of thinking about data locality 2 and minimal branching 3 , and how to process similar data all together quickly. It came into the public eye around the seventh game console generation because CPUs began to be more powerful than necessary and memory was still slow, so game programmers did what they do best and yelled at the artists figured out how to best operate around the machine's memory accesses and data caches. If you don't come from video games, you very well may have never heard of data-oriented design. That's fair\u2014it's not really necessary unless you have a running-time limited system like video games. But in the video game world, data-oriented design has only gotten more and more popular, especially now that GPUs are becoming more generally programmable and CPUs are getting enough cores that they're effectively GPU-lites. And as game developers, we do want to use the best options available for our systems, but we face a conundrum: data-oriented programming tends to be very difficult and brittle in big, messy systems like a game engine .","title":"It's All in the Data"},{"location":"blogs/week-5/#to-oop-or-not-to-oop","text":"This is where object-oriented programming (or OOP) comes in. Everyone knows it, and programmers who have had as little as one programming class are already \"indoctrinated\" with OOP practices. It's very simple to think about conceptually, and its abstractions tend to make code cleaner when you having communicative or shared systems. However, OOP is almost on the opposite side of the spectrum from data-oriented programming, and it tends to prevent optimization at a high level. So you can imagine why game developers don't particularly like OOP. So which one do we go with? Data-oriented or object-oriented? Turns out, it's not a binary choice! Data-oriented concepts such as keeping similar data packed together or minding the data cache can still be put into practice with an object-oriented architecture, and the most significant blemish with OOP is inheritance ( this covers some of the misuses ), which we can specifically avoid while we're using other OOP concepts like composition and interfaces. We acknowledge that we don't have the time or experience to handle the difficulty of fully utilizing data-oriented design, and as a short yet ambitious project, we want to keep moving at top speed! So we settled on an object-oriented composition approach, where we can try to utilize data-oriented practices in cut-and-dry cases that won't cause more complication. Essentially, this means we're going to have objects very similar to Unity's GameObjects with their Components. Is it derivative? Slightly. But you know what? Convenient object models are used for a reason!","title":"To OOP or not to OOP"},{"location":"blogs/week-5/#gui","text":"","title":"GUI"},{"location":"blogs/week-5/#why-do-we-need-gui","text":"GUI, or Graphical User Interface, is one way a player is able to interact with the game and also allows the developer to display important information to the player. As mentioned in the week 3 blog , Horde3D isn't packaged directly with a user interface. However, our sample game needs some basic UI features. We had assumed any rendering library wouldn't be complete without a programmable GUI, which was our naive assumption from looking at Ogre3D as the tentpole. So with that, let's look at the demo to see what we need: The gameplay needs are fairly basic: static text for \"Health\" and \"Score\", dynamic text for the score value, and a filled rectangle to display the health bar. However, we realized that the gameplay demo isn't a complete picture of what the game needs (especially in the case of multiplayer). The actual game flow is depicted below, with needs of a menu to select single player or multiplayer and, at the very least, to be able to select whether the game client will be hosting or which host to connect to. These type of decisions require something like a button for selection. While thinking of the UI flow of the game, we did a quick mock-up of what the menu may look like. The menu would probably require a title and (as mentioned) buttons, as well as some way of choosing single player or multiplayer. For multiplayer, if you are a host, there would need to be feedback to alert you of the number of clients in your lobby as well as an additional button to start the game (the host holds all the authority in our game, because they will be acting as the server the clients communicate with). If connecting to a host, the most basic solution would be to have an input field for the host's IP address (a more complicated one would be to have a list of lobbies to connect to, which we may try doing with local addresses). \"But why stop here?\", we thought. What we have listed is only ~5 GUI features and most engines have way more than that. If we are already taking cues from Unity, why not have the full functionality of their GUI system , especially because we have found some of their features very handy in making games? Plus, a \"real\" game engine would have much more than this to accommodate their developers, we don't want to just do the bare minimum. And although doing slightly more than the bare minimum is a good idea, we quickly realized this was a very poor mindset to have.","title":"Why do we need GUI?"},{"location":"blogs/week-5/#so-many-gui-options","text":"So even before laying out the needs of the game, we went shopping for a library. Although Horde3D doesn't have GUI by default (we were realizing how Horde3D was able to remain so lightweight), we found a Horde extension that added 3D and 2D GUI to the rendering engine. When doing the initial research into the extension, we found a few Horde3D forums that pointed to an extension. To our dismay, there were replies stating that it wasn't the best solution. In addition, the extension was written in 2010 (8 years ago at the time of writing) which we thought didn't bode well since Horde3D has changed a lot in that time. The lack of maintenance of the extension could cause a serious headache. We then directed our attention to 2 other libraries: Dear ImGui (ImGui) and Qt . We swiftly made a decision to use ImGui because Qt has an open source/commercial policy that we didn't want to accidentally break. ImGui also had the added benefit of looking simple to integrate with only a few files, and there were ample resources for the library. Both the FAQ on the Github page and the repo owner are currently very active. Another benefit is it is in C++, which meant we wouldn't need an interface with another language. We also looked at Unity's GUI system to see what features a \"typical\" engine has which acted as a baseline of what we thought we needed. ImGui has overlapping functionality with Unity, so it passed that check.","title":"So Many GUI Options"},{"location":"blogs/week-5/#guimodule","text":"Like our other modules, the GUI module was broken into the module class, GUIModule and an interface class, GUI , for the game developer. Using ImGui's GLFW OpenGL example, we were able to have ImGui and our demo running simultaneous, but not really fully connected. When deciding our StartUp hierarchy, we decided to keep it as a flat structure. This meant the window module needed to be started prior to the rendering/input so the window handle 4 could be passed to those modules. This decision proved to be the right one when implementing the GUI system, because the GUI also needs the window handle which was easy to provide. The other benefit of doing the explicit ordering of the module's StartUp / ShutDown / Update is the GUI should be started after the window but also needs to be rendered (updated) after the render update otherwise the render module will clear what the GUI just displayed. The only hurdle which required redesign when integrating ImGui was that of the input system. ImGui handles its own input by setting the GLFW callbacks, however, our own input system also sets the GLFW callbacks. So depending on the startup order, one of the modules would clobber the other's callbacks in GLFW. ImGui was designed such that the callbacks could be optionally set on initialization, which allowed us to be able to add ImGui's callbacks to our own input system rather than on startup.","title":"GUIModule"},{"location":"blogs/week-5/#immediate-vs-retained-modes","text":"With ImGui minimally integrated the main discussion became on how we would like to use the GUI system (and in turn how the game developers would use it). We evaluated 2 options: IMGUI (not to be confused with ImGui) and retained-mode (although not knowing that's what it was called). IMGUI stands for immediate mode GUI which is a code-driven GUI system where on each rendering frame the application needs to issue the draw commands of the GUI (the GUI is not stored for multiple frames); where retained , also known as canvas/scene graph/object-based UI (this is how we originally labeled it), is where GUI is registered once and is displayed, \"retained\", on screen until it removes itself from rendering 5 . An IMGUI system relies on the developer placing GUI code within something like an OnGUIUpdate method which is called each render frame, where retained-mode requires the developer to hold references to objects created such as new Window , new Button , etc. IMGUI is known to be good for developer tools and usually quicker to develop, however, it isn't as nice for doing something similar to Unity's canvas system which is a retained model. Initially we thought of implementing both, however, we could see that this could cause confusion for developers using the GUI, as well as our retained-mode would be more akin to a wrapper over immediate mode because the library we are using (it wouldn't really be retained). Although the retained mode is nice to use and what we are used to with Unity, we knew we wouldn't be using a designer tool to create our GUI (nor have the time to implement one) and prioritized speed of development over designer ease to design. In addition, Dear ImGui is set up for immediate mode (as per the name) and we knew fighting the library would cause more headache than actual benefit. This decision was reaffirmed in our interview with Amandine , who said for a small scale/scope projects that immediate mode was the way to go. We also received similar advice from our conversation with Walt Destler , where he said that a retained-mode would be nice, but it wasn't worth the time to develop the tool to edit the UI (and he is making a game that will actually be published!).","title":"Immediate vs Retained Modes"},{"location":"blogs/week-5/#wrapping-all-the-features","text":"With the mode decided on how game developers will interact with the GUI module, we then needed to develop a wrapper to wrap ImGui so ImGui code wouldn't infect the game's codebase. The reason this is useful is so you have the ability to replace ImGui with another GUI renderer of your choice without changing the game codebase. And although this was the aim, doing this isn't a trivial task because our GUI system was set up to imitate ImGui's; not a generic GUI. ImGui was the readily available example that we had to model after. Plus, our sample game may be devoid of ImGui, however, we didn't completely replicate all functionality of a GUI, only the features needed for our game, so another game that uses the engine (if GUI isn't expanded) may need to use ImGui functions/functionality. For developing the wrapper, again we looked at Unity's system where all GUI functions were positioning absolutely from the top-left corner, use Rect for positioning and size, as well as for what features were available and what features we have used in our development. We then combed through the ImGui demo, picking out the features from ImGui that we thought would be useful to have in our engine, which as you could imagine ballooned our GUI system (way beyond the listed needed features). Reading through the ImGui demo allowed us to learn ImGui, though it was ultimately a waste of time to collect features that weren't necessary to our game and we wouldn't have time to wrap. Instead of trying to wrap all functionality, even ones we wouldn't use, we simplified to be only the functionality we knew we would use as well as a handful of additional functionalities. We suggest starting with creating functionality that you need for this game. If your next game needs more functionality you can add it then, we aren't creating Unity and if you are, you are still better off iterating features as they emerge rather than trying to fully cover the system. In our GUI.h you will find commented out functions and TODO 's scattered throughout, which are remnants of trying to fully wrap ImGui and aren't required for our game. If you would like to try your hand at creating wrapper functionality, you can easily add functions throughout this file (even make a pull request, we'll most likely accept it). To go even further would be to combine/expand ImGui features in a single GUI feature that you would want to use in a game. When implementing our wrapper, the functions and functionality of ImGui didn't map one-to-one with how we would like to do our GUI. This is another reason to have a wrapper. For each ImGui function that we had decided was needed, we merged some of those functions together and replaced the ImGui classes (ImVec2/ImVec4) with the Isetta classes ( Math::Vector2 / Math::Vector4 / Color ). The biggest difference from our wrapper to ImGui (and Unity to an extent) is each GUI function requires a position and size for the element as well as an anchor pivot location and a element pivot location to determine where the element is located on the screen (by default, ImGui positions GUI in rows/columns and groups). Because our GUI and ImGui position elements differently, this is one of the reason we don't want to restrict the use of ImGui in the game code; they have different functionality which a game developer may want to take advantage of. Another difference between ImGui and our GUI implementations is the \"container\" windows (ie. window/modal/popup). Whereas ImGui relies on the game developer to remember to end the window context after beginning window, our GUI wraps this functionality with lambda functions. We made it this way because we think this wouldn't be as taxing on the developer.","title":"Wrapping All the Features"},{"location":"blogs/week-5/#sort-of-related-lessons","text":"To reiterate the lessons from developing the GUI wrapper: Don't try to create/wrap all functionality for a system if it isn't needed for the game you are immediately making \u2192 make only what your game needs. For us: Buttons/Labels/Text/Input/Drawing Quads Don't think that what Unity or another engine has is what your engine needs. When you or a game developer needs a new feature, you can go back and add functionality. After completing the wrapper, or more accurately, while developing the wrapper, we continued to add functionality to the main engine loop as example code prove the features work and provide examples for everyone to see implementation. Here is a example of the UI running in the engine, with all implemented functionality displayed simultaneously.","title":"Sort of Related Lessons"},{"location":"blogs/week-5/#more-on-horde3d","text":"In our week 1 blog , we mentioned that we chose Horde3D over Ogre 2.1 based on the fact that Horde3D has better documentation, is easier to build and is more lightweight. We thought it would give us good enough graphics with less cost importing the Horde3D library as our rendering engine. However, four weeks later, with integrating more modules into our game engine, we found that Horde3D might have been less than ideal than we initially thought. It is lightweight for external function calling (the reason why it has fewer and clearer API in the documentation), but hides too much detail for a deeper integration.","title":"More on Horde3D"},{"location":"blogs/week-5/#gui-and-textures","text":"As mentioned before, as a part of graphics, we wanted to draw 2D overlays as in-game UI. However, Horde3D itself is such a \"pure\" rendering library that it isn't packaged with a UI utility. An extension can draw some UI, but because of the reasons listed above, we had to introduce one more third-party library. Stitching another library is not an easy task. We later found ImGui doesn't have its own texture loader, so we sought help from Horde3D since it can read an image from the disk as a texture. However, to our surprise, the texture decoding is hidden inside its TextureResource class instead of extracting it as a texture decoding 6 utility for reuse, since it is not supposed to be used by external users. Luckily, we later found h3dMapResStream , so that we can let Horde3D load and decode the texture for us and map the pixel stream 7 out later.","title":"GUI and Textures"},{"location":"blogs/week-5/#loading-nested-resources","text":"Loading rendering resources like meshes and animations is a time-consuming part of a game. To hold this resource loading by ourselves, we implemented a filesystem that directly calls Windows API. This week, we started on integrating it into the rendering module. Horde3D provides an h3dLoadResource API which allows us to read the resource file by ourselves and send the data to Horde3D. However, this API hides a lot of details which caused a bit of confusion. Horde3D supports .scene.xml file to specify a scene node with mesh, material and its shader. A simple example of it is like this: <Model name= \"sphere\" geometry= \"models/sphere/sphere.geo\" > <Mesh name= \"Sphere01\" material= \"models/sphere/stones.material.xml\" batchStart= \"0\" batchCount= \"2880\" vertRStart= \"0\" vertREnd= \"587\" /> </Model> As you can see, this file has nested containers and has multiple resources. This load resource function call will only load the \"meta\" file, and \"secretly\" add all nested resources into Horde3D's own resource manager but keep them unloaded, without notifying the developer. Even in its documentation, there is no mention about this: This function loads data for a resource that was previously added to the resource manager. If data is a NULL-pointer, the resource manager is told that the resource doesn't have any data (e.g. the corresponding file was not found). In this case, the resource remains in the unloaded state but is no longer returned when querying unloaded resources. When the specified resource is already loaded, the function returns false. As a user, we have to manually check if there are any unloaded resources remaining in a while loop. The way to achieve that is to do a query by h3dQueryUnloadedResource . After we dug deeper into this function, we found out the internal implementation of this function wasn't ideal. ResHandle ResourceManager :: queryUnloadedResource ( int index ) const { int j = 0 ; for ( uint32 i = 0 ; i < _resources . size (); ++ i ) { if ( _resources [ i ] != 0x0 && ! _resources [ i ] -> _loaded && ! _resources [ i ] -> _noQuery ) { if ( j == index ) return _resources [ i ] -> _handle ; else ++ j ; } } return 0 ; } What we want is a function that returns the newly nested resources added to the resource list (or the next unloaded resource in the list), while queryUnloadedResource goes through the whole list and returns the first one. It will be significantly slower if we have lots of resources to load. To fix this issue, we decided to assume that the resource handle is always increasing so that we can check the next resource by h3dGetNextResource and h3dIsResLoaded repeatedly until the handle reaches the end.","title":"Loading Nested Resources"},{"location":"blogs/week-5/#memory-anarchism","text":"After loading the resources, we also want to manage the memory taken by the resources by our own memory manager. However, Horde3D 1.0 doesn't support custom memory allocators (they do have it as a feature for Horde3D 2.0 which is still not released yet, if it is\u2014 check here \u2014at the time you're reading this it can be an exercise for you to replace Horde3D 1.0 with 2.0 and use custom memory allocation!). All the memory allocations are done by Horde3D internally with new s. We can't bring our own memory manager without heavily modifying the source code of Horde3D which is near impossible for our semester-long project (it would counteract the reason why we chose Horde3D before). It's unfair to say choosing Horde3D was a bad idea, but the lightweight-ness we expected from Horde3D to be a benefit is putting the burden onto us since we have so many customized systems, like filesystem and memory manager. This week we are implementing the scene graph and hopefully, we can still use Horde3D organically.","title":"Memory Anarchism"},{"location":"blogs/week-5/#memory","text":"Starting from the last week, our team started integrating our own memory manager into the subsystems and 3 rd party libraries (yay!). It's a very exciting task, but it exposed many issues and limitations of our memory manager. As others were busy integrating the memory manager, one of our developers spent time gathering and analyzing incoming requirements so that we could make some updates to our memory manager.","title":"Memory"},{"location":"blogs/week-5/#freedom-of-the-free-list-allocator","text":"The first thing is that the existing memory allocators are very limited and can't cover all of our usage scenarios. For example, our GUIModule 's StartUp() process involves a lot of dispersed allocation and deallocation of random sized memory, and is managed by ImGui so we can't easily change it. All we can give ImGui is an allocation callback and a freeing callback. As a stack allocator can only handle sequential alloc/free; pool allocator can only handle same sized alloc/free; dynamic arena can only be used with ObjectHandle s, there is no way to make this work without just wasting memory. In addition, the networking module needs to allocate memory for each client connected to the server at runtime for receiving messages, etc. That means the memory needs to be persistent, so the single frame and double buffers are out. Memory arena (refers to our defragmentated memory area as in last week's blog ) sounds like a good candidate for this, but the networking module needs to allocate memory for its ring buffer data structures, which holds an array buffer. Uh oh! Our ObjectHandle is not well-prepared to work with arrays. All of these signals are suggesting that we need a more versatile and flexible allocator. After some research, the free list allocator seems to be a good solution. Unlike stack allocators and pool allocators, free list allocators give flexibility to the user in terms of the size and order of allocations/frees. It works by: Keeping a linked list (sorted by memory address) of free memory chunks. Each time memory is requested, it goes through the linked list, find a large enough chunk and returns it to the caller. There are also options like find first fit (loop through the list and find the first big enough node, prioritize speed) and best fit (loop through the whole list, find those big enough ones, and choose the smallest one among them, prioritize low fragmentation) for free list allocators for different scenarios. When a memory is freed, the free list adds that chunk back to the free list for further requests and tries to consolidate it with adjacent chunks to reduce fragmentation. It doesn't try to defragment like the memory arena, because the chunks which are being used cannot be moved (the pointers remain static). As you can tell from the description, a free list can solve our speed and fragmentation problems to some degree. It's a bit slower than stack and pool allocators as we'll need to find memory chunk from the linked list and it may still cause some defragmentation, but the flexibility it brings is worth the tradeoffs. And here is how it works (you can take a look at the code if interested FreeListAllocator.h ): The class and Node struct are defined as below. We won't include alignment here so things are easier to explain. class FreeListAllocator { void * Alloc ( Size size ); void free ( void * memPtr ); Node * head ; void * memHead ; }; struct Node { Size size ; Node * next ; }; First, a free list allocator will be initialized like this when constructed. Here is what it looks like after allocating an object of size 200: The steps happened behind the scene are: Starting from head , find the first Node that's big enough to hold p1 and an AllocHeader Create a new Node after this allocation (in this case, at address 208) Remove the original Node from the linked list Construct an AllocHeader at the original Node 's address, this info will be used when freeing p1 After the AllocHeader , construct p1 and return this pointer The AllocHeader is defined like this: // sizeof(AllocHeader) is 8 struct AllocHeader { Size size ; }; Notice we just buried an AllocHeader in memory and didn't keep a pointer to it, it will be used during the freeing process. Then allocating another chunk will be similar to this one. Here is what it looks like after freeing p1: The freeing process is more interesting as here we mine out the AllocHeader buried before and use it to determine the size of the object to free. Here is what happened behind the scenes: 1. Starting from p1 's memory address, find the AllocHeader we buried before it 1. As we know the size now, create a new Node with that size and add it to the linked list (remember the linked list is always sorted by memory address) 1. Try to merge with the next and last Node . In this case, the new node is head and not adjacent to the next node, so we can't merge Let's see an example where we need to loop through the list to find a chunk that fits, and an example where we can merge the freed memory. ^ We used the second node to satisfy this request as the first node is not big enough ^ We are halfway through freeing p2, the new node is just created here. It's adjacent to the node on its left so we can merge them, like shown below. Our current implementation of the free list allocator is by no means optimal. Allocation and free are both of O(n) time complexity because of searching and keeping the list sorted. If time permits, we will optimize the underlying structure from using a linked list to a tree, so we can reduce the time complexity from O(n) to O(logn).","title":"Freedom of The Free List Allocator"},{"location":"blogs/week-5/#the-almighty-alloc-updates-on-memory-api-design","text":"We also updated our API to make it easier for the team to use. During the process, we had a big discussion on how to design one of the most fundamental APIs: How to alloc on the LSR and level data area (look at our week 2 memory section for a review!). As a recap, the difference between this two memory areas is that, level data memory will be cleared at the end of each level, and LSR will only be cleared when the game shuts down. We have two choices. One is to make things explicit and have both AllocOnLSR and AllocOnLevelData functions. These two functions will take care of their respective areas of the stack, and assert if called incorrectly. For example, if you try to allocate something on LSR after the level starts, it will pass an exception, as the main \"big stack\" is already in the stage of allocating level memory or other things. Similarly, if you try to allocate on the level before the level starts. This method prevents misuses of both functions by creating errors to ensure the developer knows what they are doing and when they are using a function incorrectly. However, the downside is that in the future, we may have more allocation types other than just LSR and level (sublevel for example) and it will be taxing for programmers to remember when to use each as well as tedious for module upkeep. The other way is to make things implicit and only have one Alloc function, which by itself determines where the new memory request should go. Different from our explicit approach, Alloc won't cause errors in any scenario. If a programmer allocated something in level data memory but assumed it's was located on LSR, then tried to use it in a later level, the game will probably crash. The advantage is that the API will be super clean and future additions of allocation types will be handled elegantly by this method. However, this approach assumes we have a team of programmers who know the API and understand the memory layout, if not, bugs would be very hard to catch. We chose the implicit solution for several reasons. First, we only have 4 programmers working in the same room so everyone is aware of how to properly use this API. Second, LSR memory allocation requests are mostly made in the StartUp methods of each module, and level data request will be made in the level's initialization function. The probably of misusing is low, if you are in StartUp you will be using LSR and if not you will be using level is simple enough to remember. Almighty Alloc for the win!","title":"The Almighty Alloc - Updates on Memory API Design"},{"location":"blogs/week-5/#abandoning-the-all-static-fantasy","text":"In our week 2 blog , we said we wanted to achieve zero memory allocation after game StartUp and satisfy all memory requests with our memory manager. However, as fantastic as that sounds, it's been giving us a lot of headaches. We find it hard to predict how much memory we will need throughout the entire game, and how to make sure all systems run with enough memory. The audio system is a great example of this: there is no good way to determine its memory demand other than relentless iteration. Fortunately, we have our free list allocator now! Although not implemented yet, the free list will grow in size when it requires more memory. This method isn't as detrimental as new/delete or malloc/free because instead of getting more memory each call and switching between kernel and user mode, the free list will allocate chunks of memory to be used. Therefore, it naturally became our ideal solution to the problems mentioned before for the systems whose memory usage is hard to predict before runtime, we will use a free list allocator. This decision surely breaks our \"all static\" fantasy, but it seems to be more practical and still keeps us away from our two problems, again, they are speed and fragmentation.","title":"Abandoning the \"all static\" fantasy"},{"location":"blogs/week-5/#networking","text":"","title":"Networking"},{"location":"blogs/week-5/#an-overview-of-the-future","text":"Network programming has taken the back seat to other development recently, mainly because we hadn't nailed down our game object model yet. After all, unless we want to bake all of our systems code inside of our network code, then we need to have some sort of messaging protocol to make sure the right objects and functions are receiving the right packets. We also depend on object serialization and replication, but fortunately, yojimbo handles some of that for us and we can solely focus on the data flow through our engine. If you aren't using yojimbo or another networking library for your game, you can learn more about object serialization and world state replication in Chapters 4 and 5 of Multiplayer Game Programming by Joshua Glazer and Sanjay Madhav. With our final product being a game engine, it's important to abstract the network code as much as possible so that it's applicable across many games made using the engine. Some of the obvious use cases that our sample game highlights are object spawning, object destruction, object behavior change (targeting a different location), and data updates (changing health and score), just to name a few. Other potential needs might come in the form of remote procedure calls, where we can invoke functions across the network. Bearing those thoughts in mind, we've nailed down more of our game object model and have begun work on our scene graph system, so you'll see the network code of our engine start to build out these concepts more in the coming weeks.","title":"An Overview of the Future"},{"location":"blogs/week-5/#reworking-the-networking","text":"In the meantime, many of our systems are out of sync with each other, so we took the time this week to update our memory management within the networking module. Up until now we have been using the standard dynamic memory allocation of C++ with new and delete , but having full control of memory allocation, defragmentation, and access (as well as avoiding context switches!) is too great a benefit to a high-performance system like a game engine. Much of the networking system uses significant chunks of memory, such as the messaging buffers 8 and the socket-level packet queues 9 . And if you're hosting a server, then you need that much memory for each of your clients! As with some of our earlier development, we needed to alter the yojimbo library in order to comply with our own systems. This was only a couple of tweaks from pass-by-reference to pointer types instead, as our engine uses a lot of pointers so we can delay object initialization to our own execution order. Beyond this, we also got some more exposure to the packet serialization and streaming backend to the yojimbo library, so we may be more prepared to make adjustments at the low-level in the future if we need to. Beyond that, we need to allocate memory for a client and possibly a server on a single machine, plus the RingBuffer objects for queuing up messages with both. Everything within NetworkingModule 's StartUp function is allocated using the MemoryManager::AllocOnStack function, but it's less straightforward than if we were an entirely homebrew engine. yojimbo uses custom Allocator classes to divvy out memory to its Client and Server objects, so we are effectively pre-allocating a bunch of memory then passing pointers around until everyone is satisfied and on their merry way. One reason why it doesn't sound more complicated is that it, very luckily, isn't; yojimbo has a TLSF 10 allocator implementation that it uses for packet-level allocations, and that allocator can be restricted to the memory defined by our own custom allocator! So in the end, our memory management is only at the high level for the module, but we still get efficiency. We can have our cake and eat it, too! One of the quirks that we had to deal with when swapping out the memory allocators was how we are currently handling our memory chunks. We have a config file in which we specify how big our stack and list memory allocations are for each category of allocation, which is good when we want to do some regression testing 11 on our memory systems but bad for any type of general usage whatsoever! For instance, while we were refactoring our client and server memory to the new memory system, strange errors came from within the yojimbo library. After digging into it some more, we discovered that yojimbo was simply doing its best to allocate more memory for packets but we didn't give it enough memory! Everything was fine after updating the config file, but it took quite a bit of time to realize that the config held the solution. Afterwards, we had some discussions about allowing free growth of some stacks and heaps, but we running into this problem actually highlighted a bigger architecture problem for us to tackle.","title":"Reworking the Networking"},{"location":"blogs/week-5/#mo-players-mo-problems","text":"A lot of this memory is currently what we call LSR memory, so we allocate it onto our lowest level memory stack and just keep it there until we're done running the engine. Unfortunately, there are two problems with this: Game developers should be able to determine if they are running a server or not during runtime . Right now, since we are allocating the server memory at the lowest level of the stack, we need to know if they're running the server at startup. Game developers should be able to have single player modes and multiplayer modes within the same game. Our initial design was strictly online multiplayer, but it's not a stretch to think we could make a single player game with our engine. However, what if the developer wanted to allow gamers to go online after playing a single player mode? We currently assume that the developer always needs client memory (let's not even think about running a headless server), so that's allocated on the stack, but if they aren't receiving any networked messages then they shouldn't have to waste that memory! These problems exist solely because we're trying to be coy about our static memory allocation at startup. That's indeed a noble goal to shoot for, but there are appropriate cases and inappropriate cases. For networking, it's not unreasonable to expect the game developer to allocate and prepare the networking system during \"downtime\" in the game as opposed during important runtime when context switching and dynamic allocation would really pose a problem. This means that our solution is the simplest one: Just allocate any of our networking memory in our free list, a.k.a. our own custom heap space. This will give game developers the freedom to determine if they even need any networking memory at all, and if so, when it gets allocated. We expect to not have any significant memory fragmentation issues with this method either, since we just ask for a couple of large chunks of memory and will only ever remove the entire chunks.","title":"Mo' Players, Mo' Problems"},{"location":"blogs/week-5/#patch-notes","text":"","title":"Patch Notes"},{"location":"blogs/week-5/#explicit-startup-update-shutdown","text":"Earlier this week a member of the team was merging code and ran into a problem. The build successfully compiled and worked until the window was closed but then it crashed. The crash was involving the AudioModule , but wasn't clear why it was broken. By stepping through the call stack, we saw the bug occured in the ShutDown function and the error was almost immediately noticeable. Here see if you can see the error: Were you able to? The AudioModule is attempting to ShutDown twice, which obviously won't work. This is just to show the added benefit of having the modules listed in this fashion rather than being put into a stack or list. To be fair this example might not have happened with a list because of the merge, but there is a likely scenario where a module gets added twice. Then you have the headache of searching and debugging rather than being able to visually spot it.","title":"Explicit StartUp, Update, ShutDown"},{"location":"blogs/week-5/#memory-updates","text":"We also made some other minor changes to the memory manager, and they are: Create new utilities for creating objects and arrays in different memory areas with variadic templates . For example, you can use MemoryManager::NewSingleFrame<MyObj>(param1, param2...) to create a new MyObj object on the single frame area, and use MemoryManager::NewArrOnFreeList<MyObj>(100) to create an array of 100 MyObj s. We used to have some \"magic numbers\"s in the memory manager to specify the default alignment of memory allocation. Those magic numbers are now abstracted out and shared by everyone using it. This will allow for easy changes in the future and prevent inconsistency. We used to have a slot in the config file that specifies the size of our LSR and level data memory area. We now allow each submodule to have a function that calculates their memory need. And the memory manager just call each of the functions and determine the total size needed. Some modules will need to read something from the config file in their calculation function, but our start up sequence perfectly supports this: the config file is read before anything else starts. The main reason why we did this was because it's very hard and tedious to calculate memory usage of each subsystem by hand and modifying the corresponding entry in the config file. This also makes each module's memory usage more transparent. Enough with our additions and patches. Now let's switch to one of our memory manager users and see what they have to say about our latest changes! RingBuffer gets Homegrown One of our anticipated tasks after fleshing out the memory module was to go through the rest of our systems and update their memory allocation to use our own memory allocators. The best place for us to start was, of course, our data structures! Our RingBuffer uses big contiguous chunks of memory for array allocation, so we swapped out any of our array new calls with MemoryManager::NewArrOnFreeList so that we could also avoid trying to defragment the big pieces of memory. This mimics the standard new way of doing things, but instead, we now get full knowledge and control of the memory. Our method also prevents half-baked usage of our system, because any memory of ours that is free'd using delete will crash the engine! Would recommend!","title":"Memory Updates"},{"location":"blogs/week-5/#coming-soon","text":"Go check out our interview with Tommy Refenes from Team Meat! Last week we had a great conversation with Alice Ching from Funomena and will be publishing the advice she gave us soon as well! We are still working on editing Amandine Coget's interview, stay tuned for that. And this past week we also spoke with Martin Middleton, co-founder of Funomena, and will be working on transcribing and editing that in the coming weeks! Subscribe to our mailing list to get updates on all of these interviews as well as our blogs (we won't spam you, we promise)!","title":"Coming Soon"},{"location":"blogs/week-5/#resources","text":"From development we are acquiring a lot of great resources from where we learned certain topics, and as much as we try to synthesize things in our blogs you may be interested in the originals, which is why we have a page full of links for you to browse. Originally Published October 5, 2018. Deterministic behaviour is a process whose resulting state is determined by the initial state and inputs. It is heavily reliant on having a fixed-time so each step is performing the same amount of \"work\". \u21a9 Data locality is essentially accessing data in as nearby of code as possible to utilize caches most effectively. Robert Nystrom covers it really well in this chapter from Game Programming Patterns . \u21a9 Branching in code is when the processor needs to evaluate something in order to determine what code to run next. It tends to be very wasteful of processing time because of the typical instruction pipeline on a computer. \u21a9 The window handle is a GLFW construct can be passed to objects and functions to allow them to hook into a particular window from the operating system. \u21a9 For more information on Retained Mode versus Immediate Mode . \u21a9 Typically, textures are stored in compressed formats like .png, to read the pixels from the image file, we need to decode the texture from compressed to pixels. \u21a9 A pixel stream is a stream of pixel data, typically as four floating point numbers or integers representing red, green, blue, and opacity/alpha. \u21a9 A messaging buffer is just a queue of messages that are buffered prior to being sent. Sending can be expensive and complicated, so buffering messages eases the messaging process. \u21a9 A packet queue is a queue of packets, which are small chunks of an original, bigger message. These are sent out in-batch because of packet size limitations over internet networks. \u21a9 Two-Level Segregate Fit (TLSF) is a memory allocation scheme meant for use in video games because of its fast and efficient memory usage. \u21a9 Regression testing is the process of testing changes in software to make sure functionality is not broken when updating the software. \u21a9","title":"Resources"},{"location":"blogs/week-6/","text":"I Think It's Playable \u00b6 Byte-Sized Updates \u00b6 Debug Drawing : Implemented our entire visual debug drawing system, which has already revealed a mistake! Level (Scene Graph) : Designed the scene graph architecture as Level-Entity-Component, and created a reflection system for loading user-defined levels. Transform : Began development on our transform hierarchy, and implemented some gameplay features with it like a flyby camera. DLL : The engine was built into a DLL (well, sort of), and we were able to run the engine in another project. Debug Drawing \u00b6 Debug drawing is the visual equivalent to a print statement, and it is a good immediate check on whether a system you've programmed \"looks\" right. It is much easier to debug something through drawing than it is through text output; in fact, it even helped us discover a problem with our assets! (We'll cover this in a bit) What we want is some primitive types: lines, rays, planes, cubes, and spheres. We initially looked into Horde3D's capabilities for drawing these primitives, but we thought it made no sense to import models for these simple drawings\u2014how would we even render a line model ?! As in the past weeks, Horde3D's lightweightedness gave us no immediate way of doing this. The Horde forums offered solutions for generating procedural geometry and using OpenGL 1 with Qt 2 , but they each had their own flaws, either using legacy functionality or bringing in libraries that mostly replicate our existing libraries. What we really wanted was to be able to write raw OpenGL code. Foray into OpenGL \u00b6 Getting OpenGL working with Horde required a decent understanding of Horde's render loop. Firstly, any OpenGL drawing needs to happen after Horde's h3dRender and h3dFinalizeFrame . In h3dRender , 'glClear' is called to blank the screen (clearing any drawings done prior to the functions), and between these two functions, OpenGL's state is being changed, and any changes between the two function calls could tamper with Horde's rendering. To begin using OpenGL, we followed some simple OpenGL tutorials on setting up a camera, viewport 3 , and rendering to the screen using vertex arrays and buffers 4 . This was done in isolation of the engine project as a primer to the updated OpenGL4.0. When bringing this into our engine, we removed the RenderModule and GUIModule 's StartUp , Update , and ShutDown so the only class utilizing OpenGL would our new system. With only the new system calling OpenGL code, we were able to get a triangle rendered in screen space in our engine relatively quickly. And it looks magnificent! Well, it's a start. From there, we implemented the element buffer 5 so that we could reuse the vertices, because although we are generating simple primitives, we want 2 versions: a solid and a wireframe. An element buffer allows us to avoid creating different lists of vertices for our primitives because we can just specify the ordering in which the same vertices are used to draw the primitive. The order of the vertices is particularly important for solid objects because it determines which \"side\" of the primitive is drawn, one of which is typically invisible! This vertex order can be clockwise or counterclockwise, so it's known as the winding order (kind of like how you wind a toy). With an understanding of OpenGL 4.0, proof that it was working in our solution, and a triangle in hand, we decided to break everything by reintroducing the RenderModule but not the GUIModule just yet\u2014we aren't that crazy! The reason to remove GUI to begin with was because it has OpenGL calls which could change the OpenGL state. We wanted to ensure the only system interfacing with OpenGL was DebugDraw. Although GUI happens after the DebugDraw , so the debug drawings don't appear over the UI, it was reintroduced to keep the errors only associated with debug drawer and not possible integration bugs. As you'd expect if you've done graphics work before, the screen was black, and our triangle was totally gone. After rigorous debugging (i.e. commenting things out to determine the source of the problem), we figured out that the element buffer was the source of the problem! The vertex buffer was able to render both wireframes and solid primitives, but the element buffer only rendered wireframes, as with the quad: Initially, we thought we didn't need the element buffer for debug drawing because it was so simple. However, being engineers, we also had the frustration of not knowing why it didn't work. After hours of more debugging, the answer was relatively simple: a reversed winding order. Rather than using the same winding order that the OpenGL test scene was using (clockwise), Horde had a reversed winding order (counterclockwise). So really, the back of the object was being \"rendered\", which actually means it was culled 6 instead of drawn! From here, implementing all the debug drawing just required accurate counting of vertex indices and lots of hand-drawn pictures with counter-clockwise swirls on the shapes. Our next problem was rendering things beyond the screen, in the 3D world. With our modest understanding of graphics, we knew we needed a model-view matrix 7 to translate the drawings. But before we got to that, we recognized that our row-major 15 array order could cause a problem because OpenGL is column-major 15 (which can be less intuitive initially to think about) and programmers typically create arrays as [row][column] . Looking at our Matrix4 class, we had done just that. This fix is to transpose 8 the matrix when we pass our information to OpenGL or Horde\u2014and we dodged a major bullet by figuring that out first! The drawings were now being placed at the right position (well, it was fairly hard to tell with not much of a frame of reference. Oh, the woes of initial graphics!). However, the squares on screen then became oddly rectangular, and by that we mean they were rectangles and not squares. The vertex positions are hardcoded, so we did a quick sanity check against the numbers and found that we do indeed have the vertices for a unit plane/cube. However, to move the hardcoded drawings throughout the scene, the vertex positions are shifted via a vertex shader 9 . The shader multiplies the vertex's homogenous 10 position with the object's local to world matrix, known as the model matrix , which allows the developer to specify position, rotation and scale. Then the shader multiplies the resultant Vector4 by the view matrix (that is, the position, rotation, and scale) of the camera. What was missing was the projection matrix , the matrix that defines the camera's frustum 11 and thus what the camera sees. This fix was as simple as grabbing the projection matrix from Horde (set by us on when initializing the camera) and passing it into the shader. The rectangle became square and is now positioned in the 3D world, rather than directly on the 2D screen! Model Asset Pipeline \u00b6 The rendering of the debug objects was nearing completion, but when the RenderModule reintroduced the \"pushing man\" animated model back to our scene, it gave us some pause. The camera had been put out at Z=600 and the man at Z=-100 just so that the man was properly rendered on the screen. Our initial thought was the model had an offset pivot 12 , so we imported him into Maya to check, but the pivot seemed okay. Luckily, we were smart enough to import him into Unity and compared him with a cube\u2014 and turns out, he was massive ! Pulling in an experienced artist to help us shrink the model without breaking the rig and animation, we were eventually able to export him at an appropriate scale (closer to the size of a unit cube and not a skyscraper). This also re-exposed us to our asset pipeline that requires the models to be pre-processed by Horde into .geo 13 models, which we expedited with a batch script. We brought the \"normal\"-sized man back into the scene and compared him to the other sized model we had originally: The one on the left is actually the giant, because he was pushed out to Z=-600 , the camera is at Z=2 , and the new model at Z=0 . The fact that the left model is visible at that distance depicts how large it really was. And so, our debug drawing was already proving to be valuable, as that problem could have taken much longer to solve if we didn't have something to immediately compare our model with. For convenience, we also implemented a grid, point, axis and gimble (3-orthogonal circles). Using debug drawing also revealed some error in our logic. Firstly, without RenderModule 's Update function, glClear was never being called. Therefore, once an object was rendered, it remained on the screen even when we rendered a new frame. The simple fix: reenable RenderModule . Next, once this was put into one of the other developer's hands to use within an actual level component, it became apparent that the debug drawing needed to be refactored. The original debug drawing code would draw the primitive, and if a duration was passed into the function, it was added to a list to be rendered in future frames. This worked with the debug drawing demo, which was only executed after the RenderModule in our execution order, but any debug drawing called before then such as within a Level 's Update function wouldn't be drawn. As we mentioned before, the h3dRender function in RenderModule 's Update function clears the screen before any drawing in Level 's Update function could appear. We refactored this to chambering all draw calls until after RenderModule 's Update function is called, then we execute each of the draw calls within DebugDraw 's Update function. Here is a sample of the debug drawings in action! Although this wrapped up nicely, getting debug drawing to work was taxing because debugging the drawing code wasn't easy (no pun intended). There was always an uncertainty of whether the object wasn't being rendered because of a Horde snafu or because it was 100 light years behind the camera. We relied on checking the error messages from OpenGL and making incremental steps so that the source of a problem was quick to suss out. As part of this development, we fleshed out the Matrix4 and Vector4 classes more, as well as added C++ union 14 s to them which caused issues when binding multiple elements to an array (turns out, you must wrap them in a struct within the union). Good Things to Know with regards to Horde3D/Graphics \u00b6 Check the winding order of your drawings Horde3D and OpenGL use column-major 15 matrices, our Matrix4 is row-major 15 , this distinction is important Horde3D renders down the negative z-axis, as do many computer graphics programs, but this isn't intuitive for developers used to Unity which is the opposite. This is something we may also decide to \"change\" by offsetting the camera's rotation by 180\u00b0 always, so it would effectively be looking down our positive z-axis. Have a good understanding of transformations (matrix and vector math) Level (Scene Graph) \u00b6 Level-Entity-Component Design \u00b6 Since all of us agreed on object-oriented composition 16 approach for our scene graph object model, this week we began development on the scene graph system. We designed the structure of the scene graph to be Level-Entity-Component (similar to Unity's Scene-GameObject-Component). LevelManager manages all the entities in this level and updates them one by one (the order depends on when the entity is spawned in this level) in its Update function. An entity is an object that exists in the current level. It's more like a data entry which only includes a name, pointers to its components, and a transform object (we spent a long time debating if the transform should own the children or the entity should own them; see this later section for details). Entity also has some lifetime-related functions like OnEnable , Update , and LateUpdate . Any user-declared component or built-in component (like MeshComponent ) inherits from Component class and will be activated, updated and destroyed by its entity. Loading a Level \u00b6 We are making this engine to support a twin-stick shooter, but we also want other game developers to be able to use it for other genres, so the engine definitely needs something for developers to customize the level for the game to load. Modern game engines all have their own level data files for the game developers to edit (by using the editor) and for the game to load, like .scene files for Unity and .umap files for Unreal. Knowing this, how should we load level in our engine? Sure, we can follow what modern engines do (again!) and design a formatted text file to hold all the information we need to load the level. However, it involves a parser and RTTI 17 (at least the ability to instantiate an object or a member variable from a string), which we think are quite costly to implement or find an appropriate library to import. Also, hooking the classes with RTTI can be very tedious (have you used UCLASS() and UPROPERTY() in Unreal Engine?). We decided to let the game developers write C++ startup scripts to load the level (similar to GameMode scripts in Unreal). Game developers need to create a class that inherits the Level class and implement the LoadLevel() function. The EngineLoop class will run the load level function after all module managers have started up and then run the actual game loop. This allows us the flexibility to be able to implement an actual scene file down the road, given enough time, without significant technical debt or concern for breaking older level files of the engine. Template Black Magic! \u00b6 Level Auto-Registration \u00b6 But this begs the question: now that we have a bunch of user-defined levels, how can the EngineLoop or the LevelManager know which class or which instance it should instantiate and load? Of course, we can have a string in the config file to specify which level to read, but it will still require some sort of RTTI. Luckily, we don't need the full information for the whole class including its member functions and member variables. All we really need is the name of the class and how to instantiate an instance of this class, so we can just use a static map with the name and the corresponding creating function inside of the LevelManager and let all user-defined levels register themselves. But we still haven't answered our question of how! How can we run a register function without having the instance of a class? Well, use static variables and static functions! The static variable initialization can call a static function which seems to be the best place to put our code for registering levels. Then, since each class needs some additional code, is there a way that we can handle it quietly and neatly? In fact, there is! Use macros and templates! Thanks to this article , we are finally able to expose the whole automatic registration with a single macro 18 : CREATE_LEVEL(NAME) . namespace Isetta { CREATE_LEVEL ( ExampleLevel ) void LoadLevel () override { InitializeCamera (); SpawnPlayer (); ... other level code }; }; Fake \"Concepts\" \u00b6 Just like The Boy Who Cried Wolf , \"concepts\" have been delayed from C++14 again(released in 2014) and are still not yet released with our version of C++17. However, for AddComponent<T> , we really need a way to constrain the template parameter to be a child class of Component . In Java, this is called Bounded Type Parameters . In the example from Oracle's 19 official tutorial (shown below), the template U is restricted to be a class that implements the Number interface. public < U extends Number > void inspect ( U u ){ System . out . println ( \"T: \" + t . getClass (). getName ()); System . out . println ( \"U: \" + u . getClass (). getName ()); } The good news is, now in C++17, we can use the built-in RTTI and constexpr if to achieve similar feature.: template < typename T , typename ... Args > T * Entity :: AddComponent ( bool isActive , Args && ... args ) { if constexpr ( ! std :: is_base_of < class Component , T >:: value ) { throw std :: logic_error ( \"%s is not a derived class from Component class\" , typeid ( T ). name ); } else { ... } } As we can see in this code snippet from Entity.h , the if statement is checking if Component class is the base class of the template typename T . Since all of the information can be calculated during compile time (templates are unfolded during the compilation), we can use the new constexpr keyword ( enable_if in C++14) to specify different behaviors in compile time. @mike_action : Templates are a slippery slope of insanity. Plus compile times are horrid. It's true that templates can largely increase the compile time. For a three-month project, though, templates are a convenient and handy tool to generate more code for special cases. To control the power of the templates, we should always be clear why we need a template and what the template is doing in our code. Transform \u00b6 As our scene graph and entity-component model take shape, we finally got the foundation for implementing our transform class and hierarchy. The transform class in game engines is pretty well established, so we didn't have to do a ton of research on how to do it\u2014we just need to implement a plain old transform class that supports hierarchy, translation, rotation, and scale. Oh, and we need to make sure it's not terribly slow or eats up 10 gigs of memory. If you want a review of vectors, matrices, and coordinate spaces, Game Engine Architecture 's chapter on 3D math and this book 's chapters 2-4 are good references. If you are not experienced with 3D math in game engines, here are some good APIs to look at as references: Unity's Transform API , Unreal's FTransform class , CryEngine's Entity class (different from Unity and Unreal, CryEngine doesn't have a dedicated transform class, its IEntity class takes care of transformation functions), and Godot Engine's transform class . The Functionality \u00b6 Since the birth of the first \"true 3D\" game, requirements for transform classes haven't really changed much. So this time we are not looking at our game to see what we need, rather, we extensively referenced the APIs listed above and added things that we think would be nice to have (like the ForChildren(Action<Transform*>) function that accepts a lambda and executes it on each child of a transform). Basically, we support: Setting and getting world position and local position Setting and getting world rotation and local rotation Setting and getting local scale, getting world scale (tentative): Local scale is pretty intuitive but world scale is kinda tricky, because if the parent is rotated arbitrarily, the \"world scale\" of the child is actually not well defined. We are still looking into it. Keeping track of hierarchy Setting and getting parent Looking at a certain point Converting a point/vector from local space to world space and the opposite The Matrix that Knows Everything \u00b6 Matrices are extremely helpful when representing transformations in 3D space, especially when you have a transformation hierarchy - when you want to transform a point from local space to world space, just multiply it with the parent's \"local to world matrix 4x4\" and tada! You've got the point in world space. Matrices also pack some other information in it, like world position of the object and its three basis vectors. Our implementation is following column-vector convention (explained down below ). So the world position of a transform is easily accessible by extracting the first three elements of its fourth column, and the first three elements of the first three columns are the local direction x, y, and z vectors respectively. The three direction vectors happen to be pointing to the left, forward, and up from the direction of the object (we are using a right-handed coordinate system, again, see explanation below ). Once you normalize the vectors, you can get the unit basis vectors, which are super useful when doing gameplay programming! This information can be visualized as below: Matrix for an object at world origin with no rotation and uniform scale of 1 The Conventions To Respect \u00b6 In 3D math, we have the boon (or bane?) to arbitrarily define and follow two conventions that are capable of making a huge difference in the code base: the handedness of our coordinate system, and whether or not we represent vectors as row vector or column vector . They don't change the underlying mathematical meaning of things, but we need to be aware of them to make things right. Handedness \u00b6 The handedness defines where the world basis vectors are pointing to, and affects the result of vector cross product. In a left-handed coordinate, the x-axis points to the right of the screen, y points up, and z points into the screen. Using your left hand, your pointer finger points in the direction of x (to the right of the screen), your middle finger points in y (to the top of the screen), then your thumb points into the screen. Well, on the other hand (literally!), keeping your index and middle fingers pointing in the same directions your thumb will point in the direction of the screen. By rotating your index to point to the left of the screen, rather than the right, your thumb (z-axis) now points into the screen again. See image below for a visualization. This convention also has implications on our gameplay code. In local space, let's define the z vector to be the forward vector, and the y vector to be the up vector. In a left-handed coordinate, the x vector will turn out to be the right vector. While in a right-handed coordinate, the x vector will be the left vector. This is why in the last section we said the first three elements of the matrix's first column is the left vector - it's always \"the x vector\", but the \"left\" or \"right\" is some meaning we impose on it. Column/Row Vector \u00b6 This is another convention we can arbitrarily define and follow and call it a day. The difference is that when we are multiplying a matrix and a vector together, we're not sure how to represent the vector in matrix form. We can either represent it with a 1xn matrix known as row vector or nx1 matrix known as column vector . This decision will impact which side of the equation we put the vector on when we multiply it with a 4x4 matrix. Recall that the multiplication between matrix A and matrix B is only defined when A's number of rows is equal to B's number of columns. Thus, column vector goes left while row vector goes right. Furthermore, this convention changes how we store data in a matrix. For example, for the elements that represent translation in a Matrix4 to be picked up properly during multiplication, they need to be put on the fourth row if we are using row vectors, and on the fourth column if we are using column vectors. You can do a multiplication manually to prove this. This is also why in an earlier section we explained how the first three elements of each column carry some meaning (base vector directions, and translation). If we choose to use column vectors, their meaning would change, as shown in the picture below. Another implied impact would be how we chain matrix multiplication together. If using row vector, matrix should be chained in the order in which transformations are applied, i.e., if we do Transformation #1 first, then Transformation #2 and Transformation #3, the way we express that in formula is V x M1 x M2 x M3 . While it's the opposite way if using column vector, which would be M3 x M2 x M1 x V . The Dirty Flag \u00b6 This section is about optimization rather than functionality. Dirty flag is trying to solve two problems with transformations: We need objects' latest \"local to world\" matrices every time we render those objects. To keep those matrices up-to-date, we can simply recalculate them every time we need them. But the recalculation can be expensive, especially when the object has a parent as it involves matrix multiplication. Also, a lot of the objects in the scene don't move at all or very seldom during their lifetime, like walls and terrains. Therefore, recalculating matrices every time can be a huge performance waste. Components of the matrix, like local position, local rotation, and local scale, may be updated very frequently, but the world matrix may be needed once a frame. For example, a developer can modify an object's local position, scale, and rotation in a single Update function, but the RenderUpdate only needs it once when it tries to draw the object. Therefore, if we recalculate the matrix everytime something changed, we may end up doing useless work (the matrix is only needed once but we recalculated it three times, the cost could be worse if doing multiple local updates in a frame). To solve these problems, we can introduce a \"dirty flag\" (which is simply a bool variable). The flag is set to dirty when something local is changed and the matrix needs to be updated. When the matrix is needed, if the dirty flag is set, it will perform the recalculation and clear the flag, otherwise just grab the matrix\u2014the fact that the dirty flag is not set implies that the matrix is up-to-date. By doing this, the matrix of those non-movable objects would only be calculated once in their lifetime versus calculated every frame. This can bring us a big performance boost. But the downside is that we need to keep a Matrix4 on each instance of the transform class, which obviously increases our memory usage. For a more in-depth view of dirty flag, you can read the chapter on it from the Game Programming Patterns . We had a stupid bug with dirty flags in the following code (modified for better display), can you spot it? void Transform :: SetParent ( Transform * transform ) { if ( parent == transform ) return ; if ( parent != nullptr ) parent -> RemoveChild ( this ); if ( transform != nullptr ) transform -> AddChild ( this ); parent = transform ; isDirty = true ; } Only this transform's isDirty flag is set to true, while its children's are not! When you set the parent of an object, its children's matrices will also become outdated, cause their hierarchy changed! So, what we should do instead, is something like this: ForSelfAndDescendents ([]( Transform * trans ) { trans -> isDirty = true ; }); The One Who Owns All Children \u00b6 We had a discussion (well, more like a war) on who should keep track of the hierarchy\u2014the Entity , or the Transform . The reason for letting Entity keep it is that the Entity needs functions like GetComponentsInChildren and GetComponentsInParent . The reason for letting Transform keep it is that it needs the hierarchy information more frequently as they are calculating matrices, and the Entity should really just keep a bunch of different components (including the Transform ) and let the components bring functionalities. For now, we decided to let the Transform class have it. This might be a wrong decision, but we'll figure that out down the line. There is actually another argument involved in this: Should the Transform class be just a data class? As in, it would just hold the data but not provide any functionality, and the Entity class would handle the functionality. This approach is actually adopted by CryEngine, but we decided to let Transform also hold the functionality. We really don't have a super strong reason for this, other than \"the entity should just be a component container\". It's more like a philosophy question. The Small Step! \u00b6 We implemented a fly camera control with our component and transform system, for both demoing and testing (and of course it's super buggy!). Here is what we got ( the gif is motion-sick prone, watch with caution )! That's all from Transform this week. We will spend the next week debugging and adding to it when we discover missing features. DLL \u00b6 So before this point, it might be fair to say we didn't really have an engine because all of our tests and code was hardcoded into the engine. With the scene graph ( Level and Transform ) updates to our engine, we are technically ready to start building a game! We could continue to build within the engine, but that doesn't seem like any fun. Instead, we want to make a DLL; that way, other people can just use the engine's DLL without needing to modify the engine source code. To build a DLL was fairly straightforward, we created a new configuration strictly for the DLL because we wanted to be able to still run/debug the engine from within the engine project without having to switch to the game project. Then within the engine, we defined a macro to export specific functions to dll which import the functions with the DLL. The only snag was with regards to starting the engine: The EngineLoop variable needs to be declared and the Run function must be called. So although the developer can declare variables and classes prior to calling Run , there is no way to hook those declarations into the engine, so we are fine with this for now. The game development can now officially begin in a separate project to the engine, but there is the added benefit that this also lets us start having more persistent test harnesses 20 (labeled IsettaTestbed within our engine). The tests can occur in different levels thus don't need to be removed to keep the main scene from becoming cluttered. Although this will be useful, there is a catch similar to how we had to go back and add comments to our code. We need to go back and add our new macro to all of the public functions that we want to be usable and exposed to the game developer! So in all honesty, the testbed is not ready to be tested in quite yet. Patch Notes \u00b6 Unit Testing Just Got a Bigger \"Unit\" \u00b6 This week, we decided to run our unit tests once again. ...two weeks after the last time. You might be able to guess the results: After some digging, we found that the failures were not because we're terrible engineers and broke an entire module, but rather that every unit test attempted to do something it could not: use the game engine. Particularly, we've begun refactoring all engine code to use its own utilities like memory, so for us to run unit tests on it, we need to run the engine first . We quickly looked online for a solution, and found that you can indeed run some code before each testing module by using the TEST_MODULE_INITIALIZE macro from the CppUnitTestFramework namespace. Our fix is literally about 30 lines: #include \"CppUnitTest.h\" #include \"EngineLoop.h\" using namespace Microsoft :: VisualStudio :: CppUnitTestFramework ; namespace Isetta { class TestInitialization { public : TestInitialization (); static void TestClassInitialize () { engineLoop . StartUp (); }; static void TestClassCleanup () { engineLoop . ShutDown (); }; private : static EngineLoop engineLoop ; }; TestInitialization :: TestInitialization (){}; EngineLoop TestInitialization :: engineLoop ; TEST_MODULE_INITIALIZE ( ModuleInitialize ) { TestInitialization :: TestClassInitialize (); } TEST_MODULE_CLEANUP ( ModuleCleanup ) { TestInitialization :: TestClassCleanup (); } } // namespace Isetta Once that was added, we ran the tests again and got flying colors! Unfortunately, this has left some heavy implications. From here on out, our unit tests will be dependent on the engine running beneath them unless we spent a lot of time on separating out that code. And because we use the engine, we need to bring in all of our engine code into the unit testing project, which can be a headache to maintain when we rename and move things around. As a result, we've decided to leave unit testing for something to consider further down the project timeline if we want to try it at all during the 15-week period. We'll still be developing testing harnesses to make sure our engine features don't regress, but a unit testing system is beginning to require an investment that we just can't provide. Hey, at least our current tests still pass! The jury may still be out on us not being terrible programmers, though... Coming Soon \u00b6 This week we released our conversation with Alice Ching of Funomena as well as the great advice she was able to give to us! We interviewed Aras Pranckevi\u010dius ( @aras_p ) from Unity's build team (as well as many other accomplishments) this week, and had a great talk about Unity's history and evolution. Resources \u00b6 There were lots of great resources gathered this week that we've chronicled on our resource page. If you're building your own engine and have deeper burning questions you may want to start there! Originally Published October 12, 2018. OpenGL is an open source graphics library for rendering 2D and 3D vector graphics. \u21a9 Qt is a framework for creating retained GUI applications. \u21a9 A viewport is the \"window\" which the camera will render content to the screen, it is specified with an (x,y) offset from the top-left in OpenGL and a width and height. \u21a9 Vertex arrays and buffers hold the vertex information such as vertex positions, normals, color, etc and are stored within the OpenGL state. \u21a9 Element buffers hold additional information regarding the vertices, specifically what index the pertinent information is located within the vertex array. \u21a9 Culling is the early rejection of objects being passed through the render pipeline , because they don't contribute to the final image. \u21a9 Model-view matrix refers to the matrix which transforms a position in local space to world space, then to camera space. \u21a9 The transpose of a matrix is when the entries on the diagonals are flipped about the center diagonal. \u21a9 A vertex shader is a graphics program that alters information associated to the vertices, it is one of the first stages in the graphics pipeline. \u21a9 Homogeneous coordinates differentiate points from vectors by expanding the traditional Vector3 to a Vector4 and placing a 0 in the 4 th element for vectors and 1 in the 4 th element for points. \u21a9 Frustum is the portion of the world which is viewable by a camera. It is typically shaped like a pyramid with near and far planes clipping the volume. What is rendered is the volume between the 2 planes. \u21a9 Pivot refers to the local position of the model which is the zero position. When transforming the model in the world space, all changes are relative to this point. An offset pivot is when the pivot is placed in a position that isn't about the model, for example offset in X=100 from the model. \u21a9 .geo files are Horde3D's processed file for model and animations, optimized for more efficient rendering. The file is processed through the Horde3DUtil library and done prior to runtime. \u21a9 In C++, a union is a special class type that can hold only one of its non-static data members at a time. Similar to a struct, you can declare multiple variables in a union, but only one is available at the same time. Another distinction is that the size of a struct is the sum of all of its members, but the size of a union is the size of the biggest member. The way the author understands it is that union gives you different ways to interpret the same memory values. \u21a9 Every Matrix4 has 16 numbers is it, and the 16 numbers are usually stored in a big array in a specific order. Column-major and row-major are two different orders of storing them. In Column-row matrices, numbers are put in the array \"column by column\", i.e., the 4 numbers in the first column occupy the first 4 slots in the array. While row-column matrices store them \"row by row\" - the first 4 slots in the array correspond to the first row in the matrix. \u21a9 \u21a9 \u21a9 \u21a9 In Object Oriented Programming, object composition is a way to combine simple objects or data types into more complex ones. The Component pattern in Game Programming Patterns book describes this in detail. \u21a9 RTTI stands for Runtime type information , which is a language feature that exposes information about an object's data type at runtime. For example, if you want to get the type name of some object as string , you would need RTTI. \u21a9 Macros are a way of automatically substituting text for some other during the compiling process. In C++, they are defined as #define TEXT_IN_CODE TEXT_TO_COPY OVER . For example, if you define #define SPEED 5 and write mySpeed = SPEED , SPEED will be substituted by 5 during compile time and the compiler will actually see mySpeed = 5 . \u21a9 Oracle is a computer technology corporation headquartered in Redwood Shores, California, who acquired Java from Sun Microsystems and is now maintaining it. \u21a9 Test harnesses is a test framework which can ensure the progression of the software. In the Isetta Engine case, they will act as sample levels to demo features of the engine and as versioning happens to ensure old features aren't broken on accident. \u21a9","title":"[Week 6] We Think It's Playable"},{"location":"blogs/week-6/#i-think-its-playable","text":"","title":"I Think It's Playable"},{"location":"blogs/week-6/#byte-sized-updates","text":"Debug Drawing : Implemented our entire visual debug drawing system, which has already revealed a mistake! Level (Scene Graph) : Designed the scene graph architecture as Level-Entity-Component, and created a reflection system for loading user-defined levels. Transform : Began development on our transform hierarchy, and implemented some gameplay features with it like a flyby camera. DLL : The engine was built into a DLL (well, sort of), and we were able to run the engine in another project.","title":"Byte-Sized Updates"},{"location":"blogs/week-6/#debug-drawing","text":"Debug drawing is the visual equivalent to a print statement, and it is a good immediate check on whether a system you've programmed \"looks\" right. It is much easier to debug something through drawing than it is through text output; in fact, it even helped us discover a problem with our assets! (We'll cover this in a bit) What we want is some primitive types: lines, rays, planes, cubes, and spheres. We initially looked into Horde3D's capabilities for drawing these primitives, but we thought it made no sense to import models for these simple drawings\u2014how would we even render a line model ?! As in the past weeks, Horde3D's lightweightedness gave us no immediate way of doing this. The Horde forums offered solutions for generating procedural geometry and using OpenGL 1 with Qt 2 , but they each had their own flaws, either using legacy functionality or bringing in libraries that mostly replicate our existing libraries. What we really wanted was to be able to write raw OpenGL code.","title":"Debug Drawing"},{"location":"blogs/week-6/#foray-into-opengl","text":"Getting OpenGL working with Horde required a decent understanding of Horde's render loop. Firstly, any OpenGL drawing needs to happen after Horde's h3dRender and h3dFinalizeFrame . In h3dRender , 'glClear' is called to blank the screen (clearing any drawings done prior to the functions), and between these two functions, OpenGL's state is being changed, and any changes between the two function calls could tamper with Horde's rendering. To begin using OpenGL, we followed some simple OpenGL tutorials on setting up a camera, viewport 3 , and rendering to the screen using vertex arrays and buffers 4 . This was done in isolation of the engine project as a primer to the updated OpenGL4.0. When bringing this into our engine, we removed the RenderModule and GUIModule 's StartUp , Update , and ShutDown so the only class utilizing OpenGL would our new system. With only the new system calling OpenGL code, we were able to get a triangle rendered in screen space in our engine relatively quickly. And it looks magnificent! Well, it's a start. From there, we implemented the element buffer 5 so that we could reuse the vertices, because although we are generating simple primitives, we want 2 versions: a solid and a wireframe. An element buffer allows us to avoid creating different lists of vertices for our primitives because we can just specify the ordering in which the same vertices are used to draw the primitive. The order of the vertices is particularly important for solid objects because it determines which \"side\" of the primitive is drawn, one of which is typically invisible! This vertex order can be clockwise or counterclockwise, so it's known as the winding order (kind of like how you wind a toy). With an understanding of OpenGL 4.0, proof that it was working in our solution, and a triangle in hand, we decided to break everything by reintroducing the RenderModule but not the GUIModule just yet\u2014we aren't that crazy! The reason to remove GUI to begin with was because it has OpenGL calls which could change the OpenGL state. We wanted to ensure the only system interfacing with OpenGL was DebugDraw. Although GUI happens after the DebugDraw , so the debug drawings don't appear over the UI, it was reintroduced to keep the errors only associated with debug drawer and not possible integration bugs. As you'd expect if you've done graphics work before, the screen was black, and our triangle was totally gone. After rigorous debugging (i.e. commenting things out to determine the source of the problem), we figured out that the element buffer was the source of the problem! The vertex buffer was able to render both wireframes and solid primitives, but the element buffer only rendered wireframes, as with the quad: Initially, we thought we didn't need the element buffer for debug drawing because it was so simple. However, being engineers, we also had the frustration of not knowing why it didn't work. After hours of more debugging, the answer was relatively simple: a reversed winding order. Rather than using the same winding order that the OpenGL test scene was using (clockwise), Horde had a reversed winding order (counterclockwise). So really, the back of the object was being \"rendered\", which actually means it was culled 6 instead of drawn! From here, implementing all the debug drawing just required accurate counting of vertex indices and lots of hand-drawn pictures with counter-clockwise swirls on the shapes. Our next problem was rendering things beyond the screen, in the 3D world. With our modest understanding of graphics, we knew we needed a model-view matrix 7 to translate the drawings. But before we got to that, we recognized that our row-major 15 array order could cause a problem because OpenGL is column-major 15 (which can be less intuitive initially to think about) and programmers typically create arrays as [row][column] . Looking at our Matrix4 class, we had done just that. This fix is to transpose 8 the matrix when we pass our information to OpenGL or Horde\u2014and we dodged a major bullet by figuring that out first! The drawings were now being placed at the right position (well, it was fairly hard to tell with not much of a frame of reference. Oh, the woes of initial graphics!). However, the squares on screen then became oddly rectangular, and by that we mean they were rectangles and not squares. The vertex positions are hardcoded, so we did a quick sanity check against the numbers and found that we do indeed have the vertices for a unit plane/cube. However, to move the hardcoded drawings throughout the scene, the vertex positions are shifted via a vertex shader 9 . The shader multiplies the vertex's homogenous 10 position with the object's local to world matrix, known as the model matrix , which allows the developer to specify position, rotation and scale. Then the shader multiplies the resultant Vector4 by the view matrix (that is, the position, rotation, and scale) of the camera. What was missing was the projection matrix , the matrix that defines the camera's frustum 11 and thus what the camera sees. This fix was as simple as grabbing the projection matrix from Horde (set by us on when initializing the camera) and passing it into the shader. The rectangle became square and is now positioned in the 3D world, rather than directly on the 2D screen!","title":"Foray into OpenGL"},{"location":"blogs/week-6/#model-asset-pipeline","text":"The rendering of the debug objects was nearing completion, but when the RenderModule reintroduced the \"pushing man\" animated model back to our scene, it gave us some pause. The camera had been put out at Z=600 and the man at Z=-100 just so that the man was properly rendered on the screen. Our initial thought was the model had an offset pivot 12 , so we imported him into Maya to check, but the pivot seemed okay. Luckily, we were smart enough to import him into Unity and compared him with a cube\u2014 and turns out, he was massive ! Pulling in an experienced artist to help us shrink the model without breaking the rig and animation, we were eventually able to export him at an appropriate scale (closer to the size of a unit cube and not a skyscraper). This also re-exposed us to our asset pipeline that requires the models to be pre-processed by Horde into .geo 13 models, which we expedited with a batch script. We brought the \"normal\"-sized man back into the scene and compared him to the other sized model we had originally: The one on the left is actually the giant, because he was pushed out to Z=-600 , the camera is at Z=2 , and the new model at Z=0 . The fact that the left model is visible at that distance depicts how large it really was. And so, our debug drawing was already proving to be valuable, as that problem could have taken much longer to solve if we didn't have something to immediately compare our model with. For convenience, we also implemented a grid, point, axis and gimble (3-orthogonal circles). Using debug drawing also revealed some error in our logic. Firstly, without RenderModule 's Update function, glClear was never being called. Therefore, once an object was rendered, it remained on the screen even when we rendered a new frame. The simple fix: reenable RenderModule . Next, once this was put into one of the other developer's hands to use within an actual level component, it became apparent that the debug drawing needed to be refactored. The original debug drawing code would draw the primitive, and if a duration was passed into the function, it was added to a list to be rendered in future frames. This worked with the debug drawing demo, which was only executed after the RenderModule in our execution order, but any debug drawing called before then such as within a Level 's Update function wouldn't be drawn. As we mentioned before, the h3dRender function in RenderModule 's Update function clears the screen before any drawing in Level 's Update function could appear. We refactored this to chambering all draw calls until after RenderModule 's Update function is called, then we execute each of the draw calls within DebugDraw 's Update function. Here is a sample of the debug drawings in action! Although this wrapped up nicely, getting debug drawing to work was taxing because debugging the drawing code wasn't easy (no pun intended). There was always an uncertainty of whether the object wasn't being rendered because of a Horde snafu or because it was 100 light years behind the camera. We relied on checking the error messages from OpenGL and making incremental steps so that the source of a problem was quick to suss out. As part of this development, we fleshed out the Matrix4 and Vector4 classes more, as well as added C++ union 14 s to them which caused issues when binding multiple elements to an array (turns out, you must wrap them in a struct within the union).","title":"Model Asset Pipeline"},{"location":"blogs/week-6/#good-things-to-know-with-regards-to-horde3dgraphics","text":"Check the winding order of your drawings Horde3D and OpenGL use column-major 15 matrices, our Matrix4 is row-major 15 , this distinction is important Horde3D renders down the negative z-axis, as do many computer graphics programs, but this isn't intuitive for developers used to Unity which is the opposite. This is something we may also decide to \"change\" by offsetting the camera's rotation by 180\u00b0 always, so it would effectively be looking down our positive z-axis. Have a good understanding of transformations (matrix and vector math)","title":"Good Things to Know with regards to Horde3D/Graphics"},{"location":"blogs/week-6/#level-scene-graph","text":"","title":"Level (Scene Graph)"},{"location":"blogs/week-6/#level-entity-component-design","text":"Since all of us agreed on object-oriented composition 16 approach for our scene graph object model, this week we began development on the scene graph system. We designed the structure of the scene graph to be Level-Entity-Component (similar to Unity's Scene-GameObject-Component). LevelManager manages all the entities in this level and updates them one by one (the order depends on when the entity is spawned in this level) in its Update function. An entity is an object that exists in the current level. It's more like a data entry which only includes a name, pointers to its components, and a transform object (we spent a long time debating if the transform should own the children or the entity should own them; see this later section for details). Entity also has some lifetime-related functions like OnEnable , Update , and LateUpdate . Any user-declared component or built-in component (like MeshComponent ) inherits from Component class and will be activated, updated and destroyed by its entity.","title":"Level-Entity-Component Design"},{"location":"blogs/week-6/#loading-a-level","text":"We are making this engine to support a twin-stick shooter, but we also want other game developers to be able to use it for other genres, so the engine definitely needs something for developers to customize the level for the game to load. Modern game engines all have their own level data files for the game developers to edit (by using the editor) and for the game to load, like .scene files for Unity and .umap files for Unreal. Knowing this, how should we load level in our engine? Sure, we can follow what modern engines do (again!) and design a formatted text file to hold all the information we need to load the level. However, it involves a parser and RTTI 17 (at least the ability to instantiate an object or a member variable from a string), which we think are quite costly to implement or find an appropriate library to import. Also, hooking the classes with RTTI can be very tedious (have you used UCLASS() and UPROPERTY() in Unreal Engine?). We decided to let the game developers write C++ startup scripts to load the level (similar to GameMode scripts in Unreal). Game developers need to create a class that inherits the Level class and implement the LoadLevel() function. The EngineLoop class will run the load level function after all module managers have started up and then run the actual game loop. This allows us the flexibility to be able to implement an actual scene file down the road, given enough time, without significant technical debt or concern for breaking older level files of the engine.","title":"Loading a Level"},{"location":"blogs/week-6/#template-black-magic","text":"","title":"Template Black Magic!"},{"location":"blogs/week-6/#level-auto-registration","text":"But this begs the question: now that we have a bunch of user-defined levels, how can the EngineLoop or the LevelManager know which class or which instance it should instantiate and load? Of course, we can have a string in the config file to specify which level to read, but it will still require some sort of RTTI. Luckily, we don't need the full information for the whole class including its member functions and member variables. All we really need is the name of the class and how to instantiate an instance of this class, so we can just use a static map with the name and the corresponding creating function inside of the LevelManager and let all user-defined levels register themselves. But we still haven't answered our question of how! How can we run a register function without having the instance of a class? Well, use static variables and static functions! The static variable initialization can call a static function which seems to be the best place to put our code for registering levels. Then, since each class needs some additional code, is there a way that we can handle it quietly and neatly? In fact, there is! Use macros and templates! Thanks to this article , we are finally able to expose the whole automatic registration with a single macro 18 : CREATE_LEVEL(NAME) . namespace Isetta { CREATE_LEVEL ( ExampleLevel ) void LoadLevel () override { InitializeCamera (); SpawnPlayer (); ... other level code }; };","title":"Level Auto-Registration"},{"location":"blogs/week-6/#fake-concepts","text":"Just like The Boy Who Cried Wolf , \"concepts\" have been delayed from C++14 again(released in 2014) and are still not yet released with our version of C++17. However, for AddComponent<T> , we really need a way to constrain the template parameter to be a child class of Component . In Java, this is called Bounded Type Parameters . In the example from Oracle's 19 official tutorial (shown below), the template U is restricted to be a class that implements the Number interface. public < U extends Number > void inspect ( U u ){ System . out . println ( \"T: \" + t . getClass (). getName ()); System . out . println ( \"U: \" + u . getClass (). getName ()); } The good news is, now in C++17, we can use the built-in RTTI and constexpr if to achieve similar feature.: template < typename T , typename ... Args > T * Entity :: AddComponent ( bool isActive , Args && ... args ) { if constexpr ( ! std :: is_base_of < class Component , T >:: value ) { throw std :: logic_error ( \"%s is not a derived class from Component class\" , typeid ( T ). name ); } else { ... } } As we can see in this code snippet from Entity.h , the if statement is checking if Component class is the base class of the template typename T . Since all of the information can be calculated during compile time (templates are unfolded during the compilation), we can use the new constexpr keyword ( enable_if in C++14) to specify different behaviors in compile time. @mike_action : Templates are a slippery slope of insanity. Plus compile times are horrid. It's true that templates can largely increase the compile time. For a three-month project, though, templates are a convenient and handy tool to generate more code for special cases. To control the power of the templates, we should always be clear why we need a template and what the template is doing in our code.","title":"Fake \"Concepts\""},{"location":"blogs/week-6/#transform","text":"As our scene graph and entity-component model take shape, we finally got the foundation for implementing our transform class and hierarchy. The transform class in game engines is pretty well established, so we didn't have to do a ton of research on how to do it\u2014we just need to implement a plain old transform class that supports hierarchy, translation, rotation, and scale. Oh, and we need to make sure it's not terribly slow or eats up 10 gigs of memory. If you want a review of vectors, matrices, and coordinate spaces, Game Engine Architecture 's chapter on 3D math and this book 's chapters 2-4 are good references. If you are not experienced with 3D math in game engines, here are some good APIs to look at as references: Unity's Transform API , Unreal's FTransform class , CryEngine's Entity class (different from Unity and Unreal, CryEngine doesn't have a dedicated transform class, its IEntity class takes care of transformation functions), and Godot Engine's transform class .","title":"Transform"},{"location":"blogs/week-6/#the-functionality","text":"Since the birth of the first \"true 3D\" game, requirements for transform classes haven't really changed much. So this time we are not looking at our game to see what we need, rather, we extensively referenced the APIs listed above and added things that we think would be nice to have (like the ForChildren(Action<Transform*>) function that accepts a lambda and executes it on each child of a transform). Basically, we support: Setting and getting world position and local position Setting and getting world rotation and local rotation Setting and getting local scale, getting world scale (tentative): Local scale is pretty intuitive but world scale is kinda tricky, because if the parent is rotated arbitrarily, the \"world scale\" of the child is actually not well defined. We are still looking into it. Keeping track of hierarchy Setting and getting parent Looking at a certain point Converting a point/vector from local space to world space and the opposite","title":"The Functionality"},{"location":"blogs/week-6/#the-matrix-that-knows-everything","text":"Matrices are extremely helpful when representing transformations in 3D space, especially when you have a transformation hierarchy - when you want to transform a point from local space to world space, just multiply it with the parent's \"local to world matrix 4x4\" and tada! You've got the point in world space. Matrices also pack some other information in it, like world position of the object and its three basis vectors. Our implementation is following column-vector convention (explained down below ). So the world position of a transform is easily accessible by extracting the first three elements of its fourth column, and the first three elements of the first three columns are the local direction x, y, and z vectors respectively. The three direction vectors happen to be pointing to the left, forward, and up from the direction of the object (we are using a right-handed coordinate system, again, see explanation below ). Once you normalize the vectors, you can get the unit basis vectors, which are super useful when doing gameplay programming! This information can be visualized as below: Matrix for an object at world origin with no rotation and uniform scale of 1","title":"The Matrix that Knows Everything"},{"location":"blogs/week-6/#the-conventions-to-respect","text":"In 3D math, we have the boon (or bane?) to arbitrarily define and follow two conventions that are capable of making a huge difference in the code base: the handedness of our coordinate system, and whether or not we represent vectors as row vector or column vector . They don't change the underlying mathematical meaning of things, but we need to be aware of them to make things right.","title":"The Conventions To Respect"},{"location":"blogs/week-6/#handedness","text":"The handedness defines where the world basis vectors are pointing to, and affects the result of vector cross product. In a left-handed coordinate, the x-axis points to the right of the screen, y points up, and z points into the screen. Using your left hand, your pointer finger points in the direction of x (to the right of the screen), your middle finger points in y (to the top of the screen), then your thumb points into the screen. Well, on the other hand (literally!), keeping your index and middle fingers pointing in the same directions your thumb will point in the direction of the screen. By rotating your index to point to the left of the screen, rather than the right, your thumb (z-axis) now points into the screen again. See image below for a visualization. This convention also has implications on our gameplay code. In local space, let's define the z vector to be the forward vector, and the y vector to be the up vector. In a left-handed coordinate, the x vector will turn out to be the right vector. While in a right-handed coordinate, the x vector will be the left vector. This is why in the last section we said the first three elements of the matrix's first column is the left vector - it's always \"the x vector\", but the \"left\" or \"right\" is some meaning we impose on it.","title":"Handedness"},{"location":"blogs/week-6/#columnrow-vector","text":"This is another convention we can arbitrarily define and follow and call it a day. The difference is that when we are multiplying a matrix and a vector together, we're not sure how to represent the vector in matrix form. We can either represent it with a 1xn matrix known as row vector or nx1 matrix known as column vector . This decision will impact which side of the equation we put the vector on when we multiply it with a 4x4 matrix. Recall that the multiplication between matrix A and matrix B is only defined when A's number of rows is equal to B's number of columns. Thus, column vector goes left while row vector goes right. Furthermore, this convention changes how we store data in a matrix. For example, for the elements that represent translation in a Matrix4 to be picked up properly during multiplication, they need to be put on the fourth row if we are using row vectors, and on the fourth column if we are using column vectors. You can do a multiplication manually to prove this. This is also why in an earlier section we explained how the first three elements of each column carry some meaning (base vector directions, and translation). If we choose to use column vectors, their meaning would change, as shown in the picture below. Another implied impact would be how we chain matrix multiplication together. If using row vector, matrix should be chained in the order in which transformations are applied, i.e., if we do Transformation #1 first, then Transformation #2 and Transformation #3, the way we express that in formula is V x M1 x M2 x M3 . While it's the opposite way if using column vector, which would be M3 x M2 x M1 x V .","title":"Column/Row Vector"},{"location":"blogs/week-6/#the-dirty-flag","text":"This section is about optimization rather than functionality. Dirty flag is trying to solve two problems with transformations: We need objects' latest \"local to world\" matrices every time we render those objects. To keep those matrices up-to-date, we can simply recalculate them every time we need them. But the recalculation can be expensive, especially when the object has a parent as it involves matrix multiplication. Also, a lot of the objects in the scene don't move at all or very seldom during their lifetime, like walls and terrains. Therefore, recalculating matrices every time can be a huge performance waste. Components of the matrix, like local position, local rotation, and local scale, may be updated very frequently, but the world matrix may be needed once a frame. For example, a developer can modify an object's local position, scale, and rotation in a single Update function, but the RenderUpdate only needs it once when it tries to draw the object. Therefore, if we recalculate the matrix everytime something changed, we may end up doing useless work (the matrix is only needed once but we recalculated it three times, the cost could be worse if doing multiple local updates in a frame). To solve these problems, we can introduce a \"dirty flag\" (which is simply a bool variable). The flag is set to dirty when something local is changed and the matrix needs to be updated. When the matrix is needed, if the dirty flag is set, it will perform the recalculation and clear the flag, otherwise just grab the matrix\u2014the fact that the dirty flag is not set implies that the matrix is up-to-date. By doing this, the matrix of those non-movable objects would only be calculated once in their lifetime versus calculated every frame. This can bring us a big performance boost. But the downside is that we need to keep a Matrix4 on each instance of the transform class, which obviously increases our memory usage. For a more in-depth view of dirty flag, you can read the chapter on it from the Game Programming Patterns . We had a stupid bug with dirty flags in the following code (modified for better display), can you spot it? void Transform :: SetParent ( Transform * transform ) { if ( parent == transform ) return ; if ( parent != nullptr ) parent -> RemoveChild ( this ); if ( transform != nullptr ) transform -> AddChild ( this ); parent = transform ; isDirty = true ; } Only this transform's isDirty flag is set to true, while its children's are not! When you set the parent of an object, its children's matrices will also become outdated, cause their hierarchy changed! So, what we should do instead, is something like this: ForSelfAndDescendents ([]( Transform * trans ) { trans -> isDirty = true ; });","title":"The Dirty Flag"},{"location":"blogs/week-6/#the-one-who-owns-all-children","text":"We had a discussion (well, more like a war) on who should keep track of the hierarchy\u2014the Entity , or the Transform . The reason for letting Entity keep it is that the Entity needs functions like GetComponentsInChildren and GetComponentsInParent . The reason for letting Transform keep it is that it needs the hierarchy information more frequently as they are calculating matrices, and the Entity should really just keep a bunch of different components (including the Transform ) and let the components bring functionalities. For now, we decided to let the Transform class have it. This might be a wrong decision, but we'll figure that out down the line. There is actually another argument involved in this: Should the Transform class be just a data class? As in, it would just hold the data but not provide any functionality, and the Entity class would handle the functionality. This approach is actually adopted by CryEngine, but we decided to let Transform also hold the functionality. We really don't have a super strong reason for this, other than \"the entity should just be a component container\". It's more like a philosophy question.","title":"The One Who Owns All Children"},{"location":"blogs/week-6/#the-small-step","text":"We implemented a fly camera control with our component and transform system, for both demoing and testing (and of course it's super buggy!). Here is what we got ( the gif is motion-sick prone, watch with caution )! That's all from Transform this week. We will spend the next week debugging and adding to it when we discover missing features.","title":"The Small Step!"},{"location":"blogs/week-6/#dll","text":"So before this point, it might be fair to say we didn't really have an engine because all of our tests and code was hardcoded into the engine. With the scene graph ( Level and Transform ) updates to our engine, we are technically ready to start building a game! We could continue to build within the engine, but that doesn't seem like any fun. Instead, we want to make a DLL; that way, other people can just use the engine's DLL without needing to modify the engine source code. To build a DLL was fairly straightforward, we created a new configuration strictly for the DLL because we wanted to be able to still run/debug the engine from within the engine project without having to switch to the game project. Then within the engine, we defined a macro to export specific functions to dll which import the functions with the DLL. The only snag was with regards to starting the engine: The EngineLoop variable needs to be declared and the Run function must be called. So although the developer can declare variables and classes prior to calling Run , there is no way to hook those declarations into the engine, so we are fine with this for now. The game development can now officially begin in a separate project to the engine, but there is the added benefit that this also lets us start having more persistent test harnesses 20 (labeled IsettaTestbed within our engine). The tests can occur in different levels thus don't need to be removed to keep the main scene from becoming cluttered. Although this will be useful, there is a catch similar to how we had to go back and add comments to our code. We need to go back and add our new macro to all of the public functions that we want to be usable and exposed to the game developer! So in all honesty, the testbed is not ready to be tested in quite yet.","title":"DLL"},{"location":"blogs/week-6/#patch-notes","text":"","title":"Patch Notes"},{"location":"blogs/week-6/#unit-testing-just-got-a-bigger-unit","text":"This week, we decided to run our unit tests once again. ...two weeks after the last time. You might be able to guess the results: After some digging, we found that the failures were not because we're terrible engineers and broke an entire module, but rather that every unit test attempted to do something it could not: use the game engine. Particularly, we've begun refactoring all engine code to use its own utilities like memory, so for us to run unit tests on it, we need to run the engine first . We quickly looked online for a solution, and found that you can indeed run some code before each testing module by using the TEST_MODULE_INITIALIZE macro from the CppUnitTestFramework namespace. Our fix is literally about 30 lines: #include \"CppUnitTest.h\" #include \"EngineLoop.h\" using namespace Microsoft :: VisualStudio :: CppUnitTestFramework ; namespace Isetta { class TestInitialization { public : TestInitialization (); static void TestClassInitialize () { engineLoop . StartUp (); }; static void TestClassCleanup () { engineLoop . ShutDown (); }; private : static EngineLoop engineLoop ; }; TestInitialization :: TestInitialization (){}; EngineLoop TestInitialization :: engineLoop ; TEST_MODULE_INITIALIZE ( ModuleInitialize ) { TestInitialization :: TestClassInitialize (); } TEST_MODULE_CLEANUP ( ModuleCleanup ) { TestInitialization :: TestClassCleanup (); } } // namespace Isetta Once that was added, we ran the tests again and got flying colors! Unfortunately, this has left some heavy implications. From here on out, our unit tests will be dependent on the engine running beneath them unless we spent a lot of time on separating out that code. And because we use the engine, we need to bring in all of our engine code into the unit testing project, which can be a headache to maintain when we rename and move things around. As a result, we've decided to leave unit testing for something to consider further down the project timeline if we want to try it at all during the 15-week period. We'll still be developing testing harnesses to make sure our engine features don't regress, but a unit testing system is beginning to require an investment that we just can't provide. Hey, at least our current tests still pass! The jury may still be out on us not being terrible programmers, though...","title":"Unit Testing Just Got a Bigger \"Unit\""},{"location":"blogs/week-6/#coming-soon","text":"This week we released our conversation with Alice Ching of Funomena as well as the great advice she was able to give to us! We interviewed Aras Pranckevi\u010dius ( @aras_p ) from Unity's build team (as well as many other accomplishments) this week, and had a great talk about Unity's history and evolution.","title":"Coming Soon"},{"location":"blogs/week-6/#resources","text":"There were lots of great resources gathered this week that we've chronicled on our resource page. If you're building your own engine and have deeper burning questions you may want to start there! Originally Published October 12, 2018. OpenGL is an open source graphics library for rendering 2D and 3D vector graphics. \u21a9 Qt is a framework for creating retained GUI applications. \u21a9 A viewport is the \"window\" which the camera will render content to the screen, it is specified with an (x,y) offset from the top-left in OpenGL and a width and height. \u21a9 Vertex arrays and buffers hold the vertex information such as vertex positions, normals, color, etc and are stored within the OpenGL state. \u21a9 Element buffers hold additional information regarding the vertices, specifically what index the pertinent information is located within the vertex array. \u21a9 Culling is the early rejection of objects being passed through the render pipeline , because they don't contribute to the final image. \u21a9 Model-view matrix refers to the matrix which transforms a position in local space to world space, then to camera space. \u21a9 The transpose of a matrix is when the entries on the diagonals are flipped about the center diagonal. \u21a9 A vertex shader is a graphics program that alters information associated to the vertices, it is one of the first stages in the graphics pipeline. \u21a9 Homogeneous coordinates differentiate points from vectors by expanding the traditional Vector3 to a Vector4 and placing a 0 in the 4 th element for vectors and 1 in the 4 th element for points. \u21a9 Frustum is the portion of the world which is viewable by a camera. It is typically shaped like a pyramid with near and far planes clipping the volume. What is rendered is the volume between the 2 planes. \u21a9 Pivot refers to the local position of the model which is the zero position. When transforming the model in the world space, all changes are relative to this point. An offset pivot is when the pivot is placed in a position that isn't about the model, for example offset in X=100 from the model. \u21a9 .geo files are Horde3D's processed file for model and animations, optimized for more efficient rendering. The file is processed through the Horde3DUtil library and done prior to runtime. \u21a9 In C++, a union is a special class type that can hold only one of its non-static data members at a time. Similar to a struct, you can declare multiple variables in a union, but only one is available at the same time. Another distinction is that the size of a struct is the sum of all of its members, but the size of a union is the size of the biggest member. The way the author understands it is that union gives you different ways to interpret the same memory values. \u21a9 Every Matrix4 has 16 numbers is it, and the 16 numbers are usually stored in a big array in a specific order. Column-major and row-major are two different orders of storing them. In Column-row matrices, numbers are put in the array \"column by column\", i.e., the 4 numbers in the first column occupy the first 4 slots in the array. While row-column matrices store them \"row by row\" - the first 4 slots in the array correspond to the first row in the matrix. \u21a9 \u21a9 \u21a9 \u21a9 In Object Oriented Programming, object composition is a way to combine simple objects or data types into more complex ones. The Component pattern in Game Programming Patterns book describes this in detail. \u21a9 RTTI stands for Runtime type information , which is a language feature that exposes information about an object's data type at runtime. For example, if you want to get the type name of some object as string , you would need RTTI. \u21a9 Macros are a way of automatically substituting text for some other during the compiling process. In C++, they are defined as #define TEXT_IN_CODE TEXT_TO_COPY OVER . For example, if you define #define SPEED 5 and write mySpeed = SPEED , SPEED will be substituted by 5 during compile time and the compiler will actually see mySpeed = 5 . \u21a9 Oracle is a computer technology corporation headquartered in Redwood Shores, California, who acquired Java from Sun Microsystems and is now maintaining it. \u21a9 Test harnesses is a test framework which can ensure the progression of the software. In the Isetta Engine case, they will act as sample levels to demo features of the engine and as versioning happens to ensure old features aren't broken on accident. \u21a9","title":"Resources"},{"location":"blogs/week-7/","text":"A Real Game Engine \u00b6 Byte-Sized Updates \u00b6 Collisions : Collision system is taking form, and the most basic intersection tests have been implemented, but there is still a lot of work to be done. Networking : Tons of things happened, including user-defined network messages, a new static initialization system, and network IDs for synchronizing objects across the network. Serialization : We discuss the state of serialization within our engine, and some problems we've faced with it. First Game : We made a game with our engine and it's not crashing and it's not dropping frames and it's fun to play! Collisions \u00b6 Collisions happen when an object touches, or intersects, another object. But bear in mind, there is a distinction between collisions and physics; physics is the interaction and reaction of objects by applying forces, such as gravity. Collisions are typically used in a physics system, but the opposite isn't true, so when books or developers mention a physics system they typically mean a physics-collision system. There is mostly no need to develop a physics engine nowadays (coming from the ones developing an engine\u2014ironic, right?), since even AAA companies are using 3 rd parties like Havok , PhysX , Bullet3D , and ODE . In our target game, we only need some basic collision detection but don't need physics. Also, since physics are fairly computationally expensive and don't play well with networking, we decided to only implement collision detection in our engine. The cases in our game which need collisions are player's bullets hitting enemies (may instantly hit or have some travel time), enemies attacking the player, and walling in the player. The responsibilities of our collision system then take shape from those features. We need detecting collisions between colliders of primitive shapes (sphere, box, and capsules), raycasting against colliders in the scene, collision callbacks for enter, stay, and exit, and collision-solving (determining where to place objects when they have collided). The collision-intersections for primitive shapes is a mostly solved-science within the games industry, and they cover most if not all use cases. And the primitive shapes of spheres, boxes, and capsules cover most game simulation needs pretty well, and very rarely would someone need a more advanced collider shape. The capsule may stand out since it isn't a primitive shape in some other contexts, such as computer graphics, but games usually include it because it wraps a humanoid character much more cleanly than a sphere or box would. The way our system was developed is similar to audio, in which there is a component class Collider that gets attached to entities, and a module class CollisionsModule which does management. The Collider class is just an abstract class that holds data and helper functions, and the actual colliders are BoxCollider , SphereCollider , and CapsuleCollider , which each contain three things: Parameters that define the shape For debug only: Update methods to draw the wireframe shape of the object Raycast method, since raycasting against different shapes is different A helper method for the intersection tests The actual intersection tests are on the CollisionsModule because an intersection test requires 2 colliders, which could be of differing types, so putting detection code on colliders would result in duplicating code in both colliders or letting one collider hold the method and the other call that method. Neither of those options seem clean or logical, so we took the data-oriented approach where the components only hold the data and the CollisionsModule holds all intersections tests. The intersection tests of primitives are fairly straightforward vector math, so we aren't going to try to explain that here. The most valuable resource on this subject was Real-Time Collision Detection by Christer Ericson , the book covers collisions much better than we can ever do. Some of our takeaways from it are: Box intersection tests in general are harder to understand than spheres or capsules Box-Capsule intersection test aren't simple, and at the time of publishing is the only test we have yet to complete (hopefully next week we'll have something for it...) Back to the CollisionsModule , again at the time of writing we are taking a naive approach to collision testing, which is to have an array of Collider s check against all other colliders in the array, and that ends up as an O(n^2) approach. We have a slight speedup by only checking dynamic entities against the static and other dynamic entities, but this speedup is probably negligible when compared to using a proper data structure for holding colliders, like a dynamic bounding volume tree (DBVT, see chapter 6 of the above book for details). The DBVT is a balanced tree that holds some primitive type, either axis-aligned bounding boxes or bounding spheres, that encompasses all elements held in the tree. As the tree is constructed, the parents expand to encompass its children and only the leaves are actual elements inserted into the tree. The typical balancing heuristic 1 is based on increasing the surface area of the parent as little as possible. It can also be based on volume. An example BVT is constructed below. Speeding it Up \u00b6 Constructing a tree allows us to split collision detection into multiple phases, rather than just iterating over the entire list. In the most basic cases you only have one phase\u2014the narrow phase\u2014and in the next step you'd have a broad phase and narrow phase. The narrow phase is checking individual colliders for intersection, such as checking if a sphere and capsule are colliding. The broad phase is used to pare down the number of objects to do collision tests with by adding more intersection tests. Wait what, more checks? Kind of. By walking the tree and checking for collisions against each node (i.e. a group of collisions), it can be determined which nodes to check against. As we haven't implemented this yet, we won't try to explain too much more of the implementation. You'll have to come back next week to find out how we did it! Sending Collision Events \u00b6 The main discussion our team had regarding the collision system was about how a game developer would be notified about the event of a collision. We knew we wanted to have callbacks for collision enter/stay/exit that could be on any component, but once the collision system has detected the collision, how do all the collision events get called? In addition, how do we avoid performing intersection tests on colliders attached to the same entities? There were a few solutions that we came up with, but none were perfect. One option was to have collision callback lists on each collider, and when the collider intersects with another, its callback list is iterated through and called. This allows for any component on any entity (including the collider's children) to subscribe to the collision callbacks, but that doesn't fix the problem of two colliders on the same object. It actually makes things slightly more confusing because now tracking which callback is called could become difficult, because it could be anything in the scene, not just the collider entity or its children. In addition, if an entity has two colliders, which list should other components subscribe to? Should it be both? Or just the first? It isn't clear. The next option solves one of these issues by having the collision callbacks be functions on the colliders themselves, so tracking bugs would be relatively straightforward. However, this is far too limiting and would cause tight coupling of the collider and other game logic. A third option would be that when a collision callback event is to occur, we loop through all components and call their collision callbacks. This is very performance-heavy because we would need to get all components currently on the entity and call the collision functions. The collision function would be declared virtual in the base Component class, so regardless of whether the component implements them, they would be called. There are some things we could do to stop this from being such a performance hit, similar to NEEDS_UPDATE in Entity . The final option would be to have a Collisions component which holds a list of colliders on the entity and a list of collision callbacks. Other components could then subscribe to the Collisions components list and the callback would be called for intersection of any collider within its list. This forces the addition of an additional component which ends up only being a container for lists, so it's not quite ideal. It also has the same problem as the first solution of allowing any entity to add to the list. We don't have a solution that we are confident in picking yet, and there is probably a better solution out there that might be a combination of these. Next week will talk about what we actually selected and why. No matter how we decide to do the collision callbacks or walk through the tree, the collision-intersection tests still need to occur. Here is our first test by looping over all the colliders in a vector: Your browser does not support the video tag. You'll probably notice the colliders remain red even after the colliders are no longer intersecting. This is because, at the time, there was no way to determine whether the colliders had exited yet; that required knowing they had collided before and were colliding the last frame. The solution to this is to have a collision pair set, where the IDs of the colliders (just the index in the vector at this point) are hashed into a set. It is important the hash ignores the order, since a collision of entity A with entity B is identical to a collision of entity B with entity A, so we can avoid doubling our collision checks. With a simple unordered set of pairs and the intersection test for most (excluding box-capsule), you'll get a result like this: Your browser does not support the video tag. There is still obviously a lot to do with the collision system such as testing raycasts and fixing the box-capsule intersections. More to come next week! Networking \u00b6 This has been a big week for the network! And this time, it's not from refactoring our memory allocators! (In fact, we haven't switched NetworkingModule over to using our free list allocator yet like we said we would two weeks ago, so it's definitely not that) To test our network previously, our messaging system was hard-coded into the engine loop. We didn't think much of this when we wrote it, but we were forced to take another look at it when designing a generic network message system that a game developer could use with our engine, and this is what we found: void NetworkingModule :: ProcessClientToServerMessages ( int clientIdx ) { const int channelIdx = 0 ; for (;;) { Message * message = server -> ReceiveMessage ( clientIdx , channelIdx ); if ( ! message ) { break ; } switch ( message -> GetType ()) { case HANDLE_MESSAGE : { // \u2026 more code here ... } break ; case STRING_MESSAGE : { // \u2026 more code here ... } break ; } server -> ReleaseMessage ( clientIdx , message ); } } As if this wasn't bad enough, in order to utilize these messages, the game programmer would need to fill out entries in an object factory 2 using some macros like the following (which also is not exposed to the game developer, being in our engine and all): MESSAGE_FACTORY_START ( IsettaMessageFactory , NUM_MESSAGE_TYPES ); DECLARE_MESSAGE_TYPE ( HANDLE_MESSAGE , HandleMessage ); DECLARE_MESSAGE_TYPE ( STRING_MESSAGE , StringMessage ); MESSAGE_FACTORY_FINISH (); It feels like we were thinking the game developer could just use our crappy generalized network messages for any of their networking needs! Unless the game programmers had access to our engine code, this would be impossible to work with (and even if they did have access to it, there has to be a more appropriate way to design network messages!). The first thing that we needed to do was to abstract out our messaging system so that someone could still use it without writing code directly into our NetworkingModule class. The Way Messages Are Made \u00b6 The basic problem we face is that we need to add references to each of our message types to some message object factory. There are a few other problems that we need to solve out of the gate as well: These message types also need callback functions for the client and the server that get run when a message is received. The memory used to instantiate the message objects needs to be managed by our low-level network library. The message object factory needs to know the total number of message types that it can send. This is so that we can send the minimum number of bits possible for encoding the message types inside of the message packets. Number 1 is fairly straightforward as far as C++ is concerned, since we just need to pass a function pointer around wherever we have to deal with messages. Number 2 is also straightforward because our networking library, yojimbo , already has a standard for allocating its memory, which we can just use when we instantiate a message object. So as you might guess, Number 3 is the real issue here. If we were making a single game with our code and we maintained access to all of the source code, Number 3 wouldn't be a big deal. We would probably handle it for quite some time in the same way we were originally handling it by hardcoding the message types in our message object factory, which would guarantee the number of messages we'll be sending. However, we're not just making one game with this code, and more importantly, we're trying to create a separation between the user code (a.k.a. game code) and the actual engine code, but good network messages tend to be highly customized and optimized for the relevant game's specific needs, and that can only happen if the game developer can create their own message objects. Creating custom message objects isn't a difficult problem in and of itself either if you utilize inheritance and typecasting 3 , but the custom message objects need to be known before we initialize our message factory, which happens during the initialization of our NetworkingModule . Our solution: A little bit of \"macro magic.\" Macros in C++ are very powerful tools, and one of the things that they excel at is automating boilerplate code 4 . The MESSAGE_FACTORY_START macros above are an example of that. But the question remains, how do you get a developer's code to be run by your code? Macros can't exactly take code from some part of code and inject it into something else. Or can they? Well, no, they can't. Macros are just named fragments of code that expand at compile time. However, the C language's love of pointers does allow us to essentially \"inject\" user code (functions, really) into our own code, it just tends to be convoluted and require a lot of copied code. And hiding all of that is what macros are good at! RPCs \u00b6 Before we go any further, let's make sure everyone's caught up on the newest lingo. In real-time networking, you have two basic types of networking: networked data and networked functions. Networked data is objects and data primitives that you sync across a network; one example of networked data would be an object's position being simulated by the server and sent out to every client. This data is much more appropriate for unreliable streamed network data because we can typically just forget about old, lost packets and take the newest ones as ground truth. Networked functions are also known as remote procedure calls , or RPCs . An RPC function is effectively just an integer handle and some data that is sent once from client to server or server to client, and once the receiver gets the message, they run a function that corresponds to the integer handle using the data that was sent as parameters. This is easier to keep track of in games because you can define the processing on either side of the delivery chain and you get to call it as frequently or infrequently as you want. However, you also have to be more careful about this data. Obviously if you lose the packets of an RPC call, you either need those packets to get sent again or you're losing out on whatever the other side was trying to tell you. An RPC is usually something like a \"pause menu\" event in a multiplayer game like Super Smash Bros. When one person pauses, the other screens also pause, but this is obviously a one-time event and not some synced variable (although it probably could be!). On the Isetta Engine team, we had a lengthy discussion about what we should use in our engine. These two types of networking aren't exclusive in any way, but with our inexperience with network programming, we wanted to ground ourselves with one implementation before extending into more use cases. The side that eventually won out was the RPC methods because RPCs can effectively emulate synced variables, just with some runtime and network bandwidth wasted, and RPCs are also a more straightforward API to design than a serialization standard that game developers would need to abide by in our engine! First Pass at RPC Messages \u00b6 The following implementation of RPC networked messages draws heavily from Tam\u00e1s Szelei's blog Emulating the static initialization blocks of Java in C++ , so if you're interested in the idea feel free to take a peek at his article (we also go beyond this implementation in our second pass similarly to how our LevelManager now handles static initialization). The main idea is that we can leverage C++'s static keyword along with a \"registry\" class to register our message types to the object factory at startup. If you do it right, this can be very effective, but the first implementation we landed on was pretty naive and didn't help very much. We created a macro for starting and ending a message object declaration, and inside of that macro we also packaged a special static \"helper\" class: #define RPC_START(MessageClass) \\ class MessageClass : public Message { \\ private: \\ static MessageClass* Create(void* memory) { \\ return new (memory) MessageClass(); \\ } \\ friend class RPCMessageFactory; \\ static RPCRegistryHelper MessageClass##_RPCRegistryHelper; ; The RPCRegistryHelper is a class that registers a given message type's information when its constructor is called. ...if you're at all familiar with C++, then you probably know that these static objects can't just be declared\u2014they need to be initialized. And the way that you do that is to define that variable somewhere in some other code, perhaps in NetworkingModule 's StartUp function maybe? In any case, we still have what should be external, user-defined code within our engine code at the end of the day, so more work will need to be done on that. Obviously this only got us halfway there as far as functionality goes, too. If we're sending RPC message objects, then we also need to define RPC message functions to be run when the objects are received! Just like with the object type declaration macro, these macros essentially just define a name that can be used throughout the code: #define RPC_CLIENT_FUNC \\ public: \\ static void RpcClient(Client* client, Message* message) \\ #define RPC_SERVER_FUNC \\ public: \\ static void RpcServer(int clientIdx, Server* server, \\ Message* message) \\ #define RPC_MESSAGE_FINISH(MessageClass, Tag) \\ public: \\ VIRTUAL_SERIALIZE_FUNCTIONS(); \\ } \\ ; #define RPC_MESSAGE_INIT(MessageClass, Tag) \\ NetworkRegistry::RegisterMessageType( \\ sizeof(MessageClass), MessageClass::Create, Tag, \\ MessageClass::RpcClient, MessageClass::RpcServer); With that, we technically had an RPC message system running, even if it still required a bit much with jumping through hoops. Once a message class had been defined like HandleMessage , the user could then generate those messages by calling a function on the NetworkManager and giving it the appropriate string tag, which in the case of HandleMessage was \"HNDL\" . With the generated message in-hand, the user can then populate its data however they like then send it on its merry way through the NetworkManager \u2014and on the other side of the network, someone would receive that message and run a function defined inside of the message class itself that has no scoping and no parameters! Sounds useful, huh? Second Pass at RPC Messages \u00b6 With what we had implemented essentially enforcing the use of global variables and poor scoping, we knew we had to do something better. There were several places for us to improve upon, but the first was simple: Why in the world did we need a NetworkRegistry static class and a NetworkManager static class? It turns out, there is no reason! We had originally implemented the NetworkRegistry as a globally accessible form of using messages, but the NetworkManager already effectively serves that purpose, so we just went ahead and combined those two things. (It was also at around this time that one of our developers refactored our codebase to use multiple projects, such as IsettaEngine, IsettaTestbed, and IsettaTest. Turns out, this can be a massive headache for anyone who was changing code when this happens! It really messed with our Git commits and was an overall bad time. Would not recommend.) Another improvement that we strove to get this time around was an actually usable message callback system. The previous one forced the developer to pair the functions that would be run when the RPC message object was received with the actual class of the message object itself, which obviously has very limited capability. Our immediate solution was to make a subscription-based callback system, much like our how Input class handles callback functions. Any message that is defined would get its own associated list of function pointers, and anybody in the world (who has the correct function parameters) can add their own function to that list. When that message object is received, every function in that callback list is called using that message object's data. This method does have a few drawbacks. Namely, you get more overhead either from having to check at runtime inside your callback functions whether you really want to be run due to a particular message that was received, or you get more overhead from creating more message objects that would be linked to sparser sets of callback functions. You also need to keep track of integer handles for every function that you link to the message objects, otherwise you won't be able to unsubscribe functions. However, as you might also imagine, this is an extremely user-friendly way of defining callback functions, and its usable anywhere in our code if you know the corresponding string tag! (Like \"HNDL\" for HandleMessage from before) To contrast the macros showed earlier that generate the singular message callback functions, this is an example of how a user might register a callback function at this point in development: exampleClientHandleId = NetworkManager :: RegisterClientCallback ( \"HNDL\" , []( Message * message ) { HandleMessage * handleMessage = static_cast < HandleMessage *> ( message ); // \u2026 use the handle somehow ... }); exampleServerHandleId = NetworkManager :: RegisterServerCallback ( \"HNDL\" , []( int clientIdx , Message * message ) { HandleMessage * handleMessage = reinterpret_cast < HandleMessage *> ( message ); // \u2026 use the handle somehow ... NetworkManager :: SendAllMessageFromServer ( \"HNDL\" , handleMessage ); }); Isn't that so much more refreshing? Also, since we're using C++ lambda functions for registering the callbacks, we can include any scoping that we want to be used by the function as well. So even if our only input parameter is essentially the message object itself, we can still influence the world outside of that callback function's scope! Aside on Message::Copy \u00b6 The NetworkManager::RegisterServerCallback example shows another cool feature that we implemented at this point. Before now, you would have to iterate through every single client and manually send them a message if you created a server callback function whose message you wanted to broadcast to all clients. This was sort of okay when the function was tied to the message object, but now that the functions can be defined anywhere, it gets a little more awkward to have to know about the individual clients that we're sending to. So we implemented a \"SendToAll\" type function instead, but we ran into an issue with our networking library: Every message that gets sent from the server to a client needs to be generated specifically for that client . It's wasteful, but it also allows us to track whether or not our messages have been received (which can be important!). But that was making message objects difficult to broadcast because we couldn't just send out the exact same message we received, we would have to copy its data to a bunch of new message objects and send those instead! And so our solution was to extend the requirements of an RPC message object. Now, not only do you need to define a serialization function, but you also need to define a copy function that can fill out its own data based on another message of the same type. With that one quick addition, we can now hide anything to do with connected clients or client numbers from the game developer! Network Identities \u00b6 So RPC networking is effectively implemented, what else is there even to do? We basically have everything we need, right? Not quite! Although RPC messages can account for most of the functionality of our engine (although we probably don't want to use them for everything ), there's one more very important piece that we need before this can actually be utilized: network identities. A network identity is an ID that can be attached to individual objects that will be correctly mapped across the network to the corresponding object on the other client and server machines. Typically the way that game programmers reference other objects is via their pointers, which works fine and dandy when you're on a single machine, but do you think those pointers will conveniently line up on someone else's computer? Without these IDs, the game would have no way of directing its message data to the correct objects! Now technically this is something that could be implemented on the game level and not the engine level if we stopped right here and only left our networking system as a bunch of RPCs, but we actually do need to have at least a networked transform in order to keep our characters in sync with one another on people's individual computers, and that's impossible to do without some sort of identifier that will point the transform data to the correct object. Fortunately, we've recently built up our component system and tested it out in the demo game, so we can implement our NetworkIdentity class as a component that will be held by individual game objects (or \"entities\") within the game. This way, the game developer can specify which objects in their game they want to have synced across the network so that they don't have to waste a bunch of networking bandwidth keeping a bunch of immobile, uninteresting things synced up with each other. Currently, the NetworkIdentity component is pretty much just a wrapper around some NetworkManager functionality that generates an identifier and maps it to the component that asked for it, but because we're developing a game engine and everything needs to be abstracted just enough to annoy the programming team, we also want the NetworkIdentity component to dictate the sync rate and authorities of the networked data that is associated with it. This will be more important when we talk about networked transforms though, so we'll leave that discussion for another day. Testing It All Out \u00b6 So now that we've mapped out a pretty robust system for sending networked messages, you can imagine our excitement to see some synced behavior between two computers across a network. When it came time to test, however, we faced a frustrating bug. For some reason the network IDs would not sync up! We thoroughly vetted the rest of our serialized data and saw that all of that was being correctly sent, but it seemed the network ID would never be properly sent from the client. With even more digging, we discovered that the ID was being properly sent from the client. So what in the world was happening between the two computers? As it turns out, we were storing the received network ID in the wrong place. The network IDs that we've implemented are unsigned integers for obvious reasons, but we don't have a serialization function for unsigned integers and the developer here was too lazy to implement that. So instead, we casted the unsigned integer to an integer and passed that to be serialized. The following is the macro from our networking library yojimbo that we use for serializing integers: int signedNetId = static_cast < int > ( netId ); serialize_int ( stream , signedNetId , 0 , 256 ); This expands to something like this: #define serialize_int(stream, value, min, max) \\ do { \\ yojimbo_assert(min < max); \\ int32_t int32_value = 0; \\ if (Stream::IsWriting) { \\ yojimbo_assert(int64_t(value) >= int64_t(min)); \\ yojimbo_assert(int64_t(value) <= int64_t(max)); \\ int32_value = (int32_t)value; \\ } \\ if (!stream->SerializeInteger(int32_value, min, max)) { \\ return false; \\ } \\ if (Stream::IsReading) { \\ value = int32_value; \\ if (int64_t(value) < int64_t(min) || int64_t(value) > int64_t(max)) { \\ return false; \\ } \\ } \\ } while (0) Depending on whether we're reading or writing from message data, the value either gets read or written to the variable that we're passing to the macro. Which means that, when we're reading from the stream (i.e. receiving a message), we write that data into the temporary signedNetId variable! And our netId variable is left at its initial value, 0. Acknowledging that we're very likely not to face the problem of too many network IDs, we changed our unsigned integer to a signed integer and left it at that. If we need to expand it to be unsigned, we can always write a serialize function. Oh, and one more bug popped up that we should note: As we alluded to above, each message object declaration requires a Serialize function. This function is where that serialize_int macro example comes from and is also what is used by yojimbo to generally serialize any subclass of its Message class. Well, one thing that we forgot when implementing the NetworkIdentity test messages was that you must return true at the end of the Serialize function. That's how yojimbo knows that the serialization succeeded. Unfortunately, the macro makes forgetting this piece of information non-fatal to the compile process, but then whenever you try to send a message that doesn't return true, your client will disconnect! So this is an incredibly important piece of tribal knowledge that we have to document somewhere to keep all of our future developers from tearing their hair out at our network messaging system. ...And a Third Pass at RPC Messages \u00b6 Oh, did you think we were done? We're just getting started! (Just kidding, don't worry, you're in the home stretch!) So after we finished the pre-code review version of our networking system, we had a bad taste left in our mouth. Why should the developer have to remember some weird string tag in order to send messages of a particular type, like \"HNDL\" ? And what happens if we have conflicting string tags or need longer strings? Well, as it turns out, we already utilize a system similar to this tagging but by using actual object types somewhere else in the code: Our component system! Specifically, inside of Entity.h we have a vector of component types that we use for checking whether or not the current entity has a component that's requested using the GetComponent function. Since the function is a template function, it can utilize C++'s std::type_index class which essentially enumerates the classes. By a similar method, we can just enumerate our user-defined messages by their type if we turn all of our message-based functions into template functions 5 ! Now, if you've watched any of Mike Acton's talks, you'll probably have a critical eye of templates. And we acknowledge that they do drive up compile time and wreck iteration speed, but in such a short notice, they can be a nice step around a lot of code that could be much harder to maintain. And because we're using it, we can get rid of our gross string tag method and do a very simple type check: template < typename T > int NetworkManager :: GetMessageTypeId () { return typeMap [ std :: type_index ( typeid ( T ))]; } So clean! This also gives us the added benefit of knowing our message types for several of the other message-based functions. Some of them already perform casts on the messages in order to access their data, and this would let us conveniently skip that step. Static Initialization 2.0 \u00b6 We're not done quite with the networking yet\u2014hang in there! There's still one more undesirable quality to our RPC message creation system. Remember the \"static initialization\" solution from before? Well, that technique hardly helps since the game developer is still required to initialize the static variables somewhere , and forgetting to do so can lead to a rabbit hole of debugging. Fortunately for us, we've already used an ever better system elsewhere in our code. In LevelManager.h , we define a static boolean variable whose initialization is also in the header file but is guarded by being a template function! So we don't have to worry about a multiple-definition error but we can also hide away the registration process from the game developer. You can read more about this technique in the section Template Black Magic! of our Week 6 blog. Making a Level \u00b6 As a nice bow to wrap around this whole week of intense networking development, we also took all of our networking code out of the engine loop (where it was hard-coded) and put it into NetworkLevel.cpp . In doing this, we were able to test a complete separation between the engine's networking logic and the game's, and guess what? It works! Check out that spawning and despawning synced up! Your browser does not support the video tag. This trippy zombie dance experience brought to you by the devs being too lazy to space out the spawns. That's it for this week! It might seem like a lot, but really we only worked through the fundamentals of a good networking system. In the next week or two, we will be introducing networked transforms into our system, which will let us have legitimate multiplayer going in our games! Serialization \u00b6 We realized during the writing of the networking section above that we haven't discussed serialization at all yet. Part of the reason for that is because we haven't really done any serialization ourselves; for the networking, we're simply using the networking library yojimbo 's own serialization functions, and beyond that we haven't really needed serialization because we're writing our game levels in C++ instead of encoded files. Serialization is the process of taking some object from our program into a stream of bytes that can be written somewhere, whether it's a file, a database, or even a network packet. It's a very important subject when looking at the big picture for game engines. It's not discussed very often because it's pretty disconnected from the game side of the engine, but a game engine's job is to move a lot of data through itself and process that data in some way. And when we say a lot, we mean a lot . Without serialization, your game engine would be at the whim of whatever other software saves out your resource files, and you would probably only write out simple text files that take ages to read and waste a lot of space. That being said, serialization itself is a difficult feature to tackle. How do you generate the serialized data in the first place (especially if you don't have good reflection data like in C++)? How do you guarantee backwards compatibility with the serialization, or do you even have it at all? These are all questions that we completely avoided by using big 3 rd party libraries for things like audio and graphics! And for the time being, we don't have to worry about serialization with our .cpp level files, so we won't worry about it. If you're interested in the topic, Jeff Preshing has a good introduction to the topic including links to several good resources to learn from. Serializing Our Own Data \u00b6 Our plan is to not serialize our level data, but we were still interested in serializing our basic data types to avoid rewriting code, such as for Vector3 and Matrix4x4. The main thing that this serialization would be used for (i.e. the only thing it would be used for) is networking, so we thought we could utilize yojimbo's serialize_object macro and write Serialize functions for each of our data containers that could then be sent across the network. But there's one big problem that we hit. The Serialize function used by yojimbo must be templated or else it won't work. Why is that an issue? Aren't we using a bunch of templates already? Well, the problem isn't the template itself per se\u2014it's the fact that a templated function must be defined inside of the header file. This means that, if we want to serialize a single class with yojimbo, we have to include the entire 6,000 lines of code in yojimbo.h to utilize the serialization macros. And including that much network code in our math data structures just didn't seem like a good idea. So for the time being, we'll leave serializing the data types to the game developer! We wish we could do more there, but it's currently not worth the time investment. First Game \u00b6 A major milestone is that we made a real game with our engine! It's running smoothly at ~60 fps with sound, gamepad input, models, lighting, textures, animation blending, movement, score counting, tweakable variables, an in-game entity inspector\u2026 All the beautiful things you can imagine in a game engine. The game is also built in a whole new project! That means we can now bundle our entire engine into a DLL 6 and pass it around for people to use. How exciting! Now, let's calm down and talk about how we did it. First, we built the entire engine into a DLL. Visual Studio is very helpful with this process by providing this document which covers the process. Basically, we just needed to add a new configuration, set its format from .exe to .dll , and add a bunch of modifiers like __declspec(dllexport) and __declspec(dllimport) to the classes we want to export and import to the game project. We made a macro for this, so we can easily get different behavior in different environments: #ifdef ISETTA_EXPORTS #define ISETTA_API __declspec(dllexport) #else #define ISETTA_API __declspec(dllimport) #endif The tricky thing is, the game project needs not only the DLLs, but also the following (if you are not sure how to do any of these, please refer to the document linked above): The header files in the engine. This allows the game project to include classes and functions from the engine and know what functionality they provide. The .lib files . They are generated together with the DLLs and tell the compiler where to find specific functions in the DLL. The header files and .lib files from external libraries, if needed. We are currently including everything from external libraries, which is certainly not optimal. We will try to find the cleanest way to include them, and post the update in a future blog If you see giant lists of \"unresolved external symbols\", don't panic! We had those too, so just check the above three things and make sure you have the appropriate ISETTA_API modifiers. Second, we downloaded some assets from Mixamo and made some game assets with Blender (programmer art, yay!), and converted them into a format that our engine can read. Luckily, this step is also not very difficult\u2014Horde3D comes with a \".dae to .geo converter\" 7 that, apparently, convert .dae files to .geo files, which is the file format Horde3D needs. But there are still two tricky things with it: The path to generated resource files still needs to be hard-coded into the game, so if the file structure changed, the game developer must remember to change those hard-coded value, too. Otherwise, the game will throw an exception. The units used by different software could be different. For example, in Maya, 1 unit is 1 centimeter by default, and in our engine, 1 unit is 1 meter. If we don't change the scale before exporting, the models will be huge in our engine. This can be fixed by changing the default scale in Maya, or by manually setting the object's scale in the game. If you don't, you may get a giant soldier, like we did: Your browser does not support the video tag. By the way, if you pass some code to DLL-imported function to find resources, it will try to find the resources relative to the game project, not the project where the DLL is from. This is pretty obvious, but took us a while to make it clear. Third, we started making the actual game and found out a ton of bugs in the engine, and fixed/worked around them. This experience firmly states that, making small test games is really a good way to test the engine ! Here are some bugs we found out: Entity::Destroy is not working properly. It does call the OnDestroy functions, but the entity itself is not removed from the entity list of the scene after that (we had a TODO there but no one remembered to fix it). Therefore, the OnDestroy function is called again and again and again, and the entity is never really \"destroyed\". This also revealed another issue in our free list allocator. As the OnDestroy functions release memory in the free list allocator, when the same memory is freed the second time, our allocator would freak out and throw an exception. We improved our free list allocator to make it safer. To avoid this issue, we chose to not dynamically delete anything in the game. Instead, we pooled all of them. The animation won't work correctly after turning the entity off and on again with Entity::SetActive . It's caused by our interface with Horde3d. We fixed it and it's working properly now, and we also added animation blending during the fixing process. You can see the version with broken animation here . As mentioned before, we had scaling problems with imported models, and we chose to set their scale in the game. This choice revealed a problem in our Transform class: when an entity's scale is small, calling TranslateWorld (the function that we use to move objects in world space) on the entity is very ineffective as the world translation is somehow affected by the object's local scale. We dug into the code and realized the reason is that when we were constructing the object's matrix from the local position, local rotation, and local scale, we applied local position before local scale. This would cause the local position to be \"scaled\" by local scale, which should not be. So we simply changed the order of constructing the matrix and it worked fine. We also found out some missing features during the making of this game: When we were implementing our GUI module, we only created some interfaces with ImGUI and left out some other that we thought wouldn't be used. Well, it turned out we didn't perfectly predict what we need. When making this game, we wanted to tweak some values at runtime with in-game GUI, and a \"float slider\" would be the best option for that. However, we didn't have that in our GUI interface, so we went ahead and added it to our engine. The list of audio files to be used in the game was hard-coded into the AudioModule. It's certainly not desirable and would destroy iteration time. So we moved that to the config file, thus allow specifying audio files to be used in the config. Finally, we got the game working! Yay! Patch Notes \u00b6 Editor \u00b6 After making our transform and GUI system, we spent 20 minutes hacking together a \"programmer UI\" in-game editor that's capable of displaying entity hierarchy, transform information, component list, gizmo drawing, and has a \"reset transform\" button. Being able to see the list of entities and their information made it so much easier for us to debug the game and engine. Our simple in-game inspector FreeListAllocator Alignment \u00b6 When we were working on the networking system this week, we attached NetworkIdentity components to game objects to synchronize them across the network. After doing this twice, we would get a memory reference error inside of FreeListAllocator ! The free list essentially tracks available nodes of memory in our own managed heap and tries to merge them if they're adjacent, so we looked into its allocation directly by stepping through the debugger to see what was going wrong. This is what we found: // \u2026 node = a custom data structure containing a size and a pointer to the next node intptr_t rawAddress = reinterpret_cast < intptr_t > ( node ); rawAddress += headerSize ; // leave size for header intptr_t misAlignment = rawAddress & ( alignment - 1 ); uint64_t adjustment = alignment - misAlignment ; intptr_t alignedAddress = rawAddress + adjustment ; size_t occupiedSize = headerSize + adjustment + size ; intptr_t headerAddress = alignedAddress - headerSize ; if ( node -> size >= occupiedSize + nodeSize ) { // enough space to put a node here auto * header = new ( reinterpret_cast < void *> ( headerAddress )) AllocHeader ( occupiedSize , adjustment ); InsertNodeAt ( node , new ( reinterpret_cast < void *> ( alignedAddress + size )) Node ( node -> size - occupiedSize )); } else // ... The problem in this code stems from one spot: Where auto* header is allocated using headerAddress . If headerAddress happens to be only 8 bytes away from the rawAddress of node , then the next pointer of node will be overwritten by the new header allocation! (This is one of the problems you may face when you write your own memory allocation system!) But when is headerAddress only 8 bytes away from node , especially when our default alignment is 16 bytes? The answer lies in where we started. If node happens to be misaligned, then the first block of code is not actually aligning our next address, it only puts it 8 bytes after our address because it's \"aligning\"! To fix this, we can increase our adjustment so that it always \"rounds up\", so to speak. This can be done in a short math expression: uint64_t adjustment = alignment - misAlignment ; adjustment += (( ~ alignment & adjustment ) << 1 ); intptr_t alignedAddress = rawAddress + adjustment ; There, no memory errors! \"Camera\" vs. \"CameraComponent\" \u00b6 Regarding our engine's naming conventions, the Isetta Team is usually pretty lax. But for our components, we decided to turn it up a notch and argue over which is better: \"Camera\" or \"CameraComponent.\" It remained an unresolved debate because one of our programmers was out at an interview over a couple days, but when he came back, we decided on a compromise. The component should include the word \"Component\" when the name of the component itself is only one word (e.g. \"CameraComponent\"), and it should not include \"Component\" when it already contains more than one word (e.g. \"NetworkIdentity\"). Coming Soon \u00b6 Go check out our interview with Amandine Coget , where we talk about the Frostbite engine, developing good usability, and the pros and cons of different UI models. We also interviewed Raymond Graham ( @wadarass ) this week, so expect his interview along with Martin Middleton's and Aras Pranckevicius's ( @aras_p ) in the coming weeks. Resources \u00b6 From development we are acquiring a lot of great resources from where we learned certain topics, and as much as we try to synthesize things in our blogs you may be interested in the originals, which is why we have a page full of links for you to browse. Originally Published October 19, 2018. A heuristic is a technique for solving a problem more quickly, often utilized in optimization problems for computer science. \u21a9 An object factory is a programming pattern where you decouple object creation and destruction with the actual usage of the object. An object factory typically creates objects based on passed parameters, which you would then receive and use. \u21a9 Typecasting , also known as type conversion , is a method of changing one data type to another. It helps ensure that variables are processed correctly by functions, but can also be dangerous with the way they handle the conversion. \u21a9 Boilerplate code is a section of code that has to be included in many places with little to no alteration. \u21a9 Function templates are functions definitions that, at compile time, generate individual function definitions for any types used with the template. This can be very useful for cutting out duplicate code, but greatly increases compile time and code size. \u21a9 A DLL is a dynamic-linked library which is Microsoft shared library concept which can be transported around easier than a project and contains information about the compiled project. \u21a9 The content pipeline of Horde3D is detailed on this page . Long story short, .dae, which stands for Digital Asset Exchange, is the file format that COLLADA (COLLAborative Design Activity) uses. COLLADA, in turn, supports portable representations for 3D assets. \".geo\" is the file format that Horde3D uses to load in assets at runtime. \u21a9","title":"[Week 7] A Real Game Engine"},{"location":"blogs/week-7/#a-real-game-engine","text":"","title":"A Real Game Engine"},{"location":"blogs/week-7/#byte-sized-updates","text":"Collisions : Collision system is taking form, and the most basic intersection tests have been implemented, but there is still a lot of work to be done. Networking : Tons of things happened, including user-defined network messages, a new static initialization system, and network IDs for synchronizing objects across the network. Serialization : We discuss the state of serialization within our engine, and some problems we've faced with it. First Game : We made a game with our engine and it's not crashing and it's not dropping frames and it's fun to play!","title":"Byte-Sized Updates"},{"location":"blogs/week-7/#collisions","text":"Collisions happen when an object touches, or intersects, another object. But bear in mind, there is a distinction between collisions and physics; physics is the interaction and reaction of objects by applying forces, such as gravity. Collisions are typically used in a physics system, but the opposite isn't true, so when books or developers mention a physics system they typically mean a physics-collision system. There is mostly no need to develop a physics engine nowadays (coming from the ones developing an engine\u2014ironic, right?), since even AAA companies are using 3 rd parties like Havok , PhysX , Bullet3D , and ODE . In our target game, we only need some basic collision detection but don't need physics. Also, since physics are fairly computationally expensive and don't play well with networking, we decided to only implement collision detection in our engine. The cases in our game which need collisions are player's bullets hitting enemies (may instantly hit or have some travel time), enemies attacking the player, and walling in the player. The responsibilities of our collision system then take shape from those features. We need detecting collisions between colliders of primitive shapes (sphere, box, and capsules), raycasting against colliders in the scene, collision callbacks for enter, stay, and exit, and collision-solving (determining where to place objects when they have collided). The collision-intersections for primitive shapes is a mostly solved-science within the games industry, and they cover most if not all use cases. And the primitive shapes of spheres, boxes, and capsules cover most game simulation needs pretty well, and very rarely would someone need a more advanced collider shape. The capsule may stand out since it isn't a primitive shape in some other contexts, such as computer graphics, but games usually include it because it wraps a humanoid character much more cleanly than a sphere or box would. The way our system was developed is similar to audio, in which there is a component class Collider that gets attached to entities, and a module class CollisionsModule which does management. The Collider class is just an abstract class that holds data and helper functions, and the actual colliders are BoxCollider , SphereCollider , and CapsuleCollider , which each contain three things: Parameters that define the shape For debug only: Update methods to draw the wireframe shape of the object Raycast method, since raycasting against different shapes is different A helper method for the intersection tests The actual intersection tests are on the CollisionsModule because an intersection test requires 2 colliders, which could be of differing types, so putting detection code on colliders would result in duplicating code in both colliders or letting one collider hold the method and the other call that method. Neither of those options seem clean or logical, so we took the data-oriented approach where the components only hold the data and the CollisionsModule holds all intersections tests. The intersection tests of primitives are fairly straightforward vector math, so we aren't going to try to explain that here. The most valuable resource on this subject was Real-Time Collision Detection by Christer Ericson , the book covers collisions much better than we can ever do. Some of our takeaways from it are: Box intersection tests in general are harder to understand than spheres or capsules Box-Capsule intersection test aren't simple, and at the time of publishing is the only test we have yet to complete (hopefully next week we'll have something for it...) Back to the CollisionsModule , again at the time of writing we are taking a naive approach to collision testing, which is to have an array of Collider s check against all other colliders in the array, and that ends up as an O(n^2) approach. We have a slight speedup by only checking dynamic entities against the static and other dynamic entities, but this speedup is probably negligible when compared to using a proper data structure for holding colliders, like a dynamic bounding volume tree (DBVT, see chapter 6 of the above book for details). The DBVT is a balanced tree that holds some primitive type, either axis-aligned bounding boxes or bounding spheres, that encompasses all elements held in the tree. As the tree is constructed, the parents expand to encompass its children and only the leaves are actual elements inserted into the tree. The typical balancing heuristic 1 is based on increasing the surface area of the parent as little as possible. It can also be based on volume. An example BVT is constructed below.","title":"Collisions"},{"location":"blogs/week-7/#speeding-it-up","text":"Constructing a tree allows us to split collision detection into multiple phases, rather than just iterating over the entire list. In the most basic cases you only have one phase\u2014the narrow phase\u2014and in the next step you'd have a broad phase and narrow phase. The narrow phase is checking individual colliders for intersection, such as checking if a sphere and capsule are colliding. The broad phase is used to pare down the number of objects to do collision tests with by adding more intersection tests. Wait what, more checks? Kind of. By walking the tree and checking for collisions against each node (i.e. a group of collisions), it can be determined which nodes to check against. As we haven't implemented this yet, we won't try to explain too much more of the implementation. You'll have to come back next week to find out how we did it!","title":"Speeding it Up"},{"location":"blogs/week-7/#sending-collision-events","text":"The main discussion our team had regarding the collision system was about how a game developer would be notified about the event of a collision. We knew we wanted to have callbacks for collision enter/stay/exit that could be on any component, but once the collision system has detected the collision, how do all the collision events get called? In addition, how do we avoid performing intersection tests on colliders attached to the same entities? There were a few solutions that we came up with, but none were perfect. One option was to have collision callback lists on each collider, and when the collider intersects with another, its callback list is iterated through and called. This allows for any component on any entity (including the collider's children) to subscribe to the collision callbacks, but that doesn't fix the problem of two colliders on the same object. It actually makes things slightly more confusing because now tracking which callback is called could become difficult, because it could be anything in the scene, not just the collider entity or its children. In addition, if an entity has two colliders, which list should other components subscribe to? Should it be both? Or just the first? It isn't clear. The next option solves one of these issues by having the collision callbacks be functions on the colliders themselves, so tracking bugs would be relatively straightforward. However, this is far too limiting and would cause tight coupling of the collider and other game logic. A third option would be that when a collision callback event is to occur, we loop through all components and call their collision callbacks. This is very performance-heavy because we would need to get all components currently on the entity and call the collision functions. The collision function would be declared virtual in the base Component class, so regardless of whether the component implements them, they would be called. There are some things we could do to stop this from being such a performance hit, similar to NEEDS_UPDATE in Entity . The final option would be to have a Collisions component which holds a list of colliders on the entity and a list of collision callbacks. Other components could then subscribe to the Collisions components list and the callback would be called for intersection of any collider within its list. This forces the addition of an additional component which ends up only being a container for lists, so it's not quite ideal. It also has the same problem as the first solution of allowing any entity to add to the list. We don't have a solution that we are confident in picking yet, and there is probably a better solution out there that might be a combination of these. Next week will talk about what we actually selected and why. No matter how we decide to do the collision callbacks or walk through the tree, the collision-intersection tests still need to occur. Here is our first test by looping over all the colliders in a vector: Your browser does not support the video tag. You'll probably notice the colliders remain red even after the colliders are no longer intersecting. This is because, at the time, there was no way to determine whether the colliders had exited yet; that required knowing they had collided before and were colliding the last frame. The solution to this is to have a collision pair set, where the IDs of the colliders (just the index in the vector at this point) are hashed into a set. It is important the hash ignores the order, since a collision of entity A with entity B is identical to a collision of entity B with entity A, so we can avoid doubling our collision checks. With a simple unordered set of pairs and the intersection test for most (excluding box-capsule), you'll get a result like this: Your browser does not support the video tag. There is still obviously a lot to do with the collision system such as testing raycasts and fixing the box-capsule intersections. More to come next week!","title":"Sending Collision Events"},{"location":"blogs/week-7/#networking","text":"This has been a big week for the network! And this time, it's not from refactoring our memory allocators! (In fact, we haven't switched NetworkingModule over to using our free list allocator yet like we said we would two weeks ago, so it's definitely not that) To test our network previously, our messaging system was hard-coded into the engine loop. We didn't think much of this when we wrote it, but we were forced to take another look at it when designing a generic network message system that a game developer could use with our engine, and this is what we found: void NetworkingModule :: ProcessClientToServerMessages ( int clientIdx ) { const int channelIdx = 0 ; for (;;) { Message * message = server -> ReceiveMessage ( clientIdx , channelIdx ); if ( ! message ) { break ; } switch ( message -> GetType ()) { case HANDLE_MESSAGE : { // \u2026 more code here ... } break ; case STRING_MESSAGE : { // \u2026 more code here ... } break ; } server -> ReleaseMessage ( clientIdx , message ); } } As if this wasn't bad enough, in order to utilize these messages, the game programmer would need to fill out entries in an object factory 2 using some macros like the following (which also is not exposed to the game developer, being in our engine and all): MESSAGE_FACTORY_START ( IsettaMessageFactory , NUM_MESSAGE_TYPES ); DECLARE_MESSAGE_TYPE ( HANDLE_MESSAGE , HandleMessage ); DECLARE_MESSAGE_TYPE ( STRING_MESSAGE , StringMessage ); MESSAGE_FACTORY_FINISH (); It feels like we were thinking the game developer could just use our crappy generalized network messages for any of their networking needs! Unless the game programmers had access to our engine code, this would be impossible to work with (and even if they did have access to it, there has to be a more appropriate way to design network messages!). The first thing that we needed to do was to abstract out our messaging system so that someone could still use it without writing code directly into our NetworkingModule class.","title":"Networking"},{"location":"blogs/week-7/#the-way-messages-are-made","text":"The basic problem we face is that we need to add references to each of our message types to some message object factory. There are a few other problems that we need to solve out of the gate as well: These message types also need callback functions for the client and the server that get run when a message is received. The memory used to instantiate the message objects needs to be managed by our low-level network library. The message object factory needs to know the total number of message types that it can send. This is so that we can send the minimum number of bits possible for encoding the message types inside of the message packets. Number 1 is fairly straightforward as far as C++ is concerned, since we just need to pass a function pointer around wherever we have to deal with messages. Number 2 is also straightforward because our networking library, yojimbo , already has a standard for allocating its memory, which we can just use when we instantiate a message object. So as you might guess, Number 3 is the real issue here. If we were making a single game with our code and we maintained access to all of the source code, Number 3 wouldn't be a big deal. We would probably handle it for quite some time in the same way we were originally handling it by hardcoding the message types in our message object factory, which would guarantee the number of messages we'll be sending. However, we're not just making one game with this code, and more importantly, we're trying to create a separation between the user code (a.k.a. game code) and the actual engine code, but good network messages tend to be highly customized and optimized for the relevant game's specific needs, and that can only happen if the game developer can create their own message objects. Creating custom message objects isn't a difficult problem in and of itself either if you utilize inheritance and typecasting 3 , but the custom message objects need to be known before we initialize our message factory, which happens during the initialization of our NetworkingModule . Our solution: A little bit of \"macro magic.\" Macros in C++ are very powerful tools, and one of the things that they excel at is automating boilerplate code 4 . The MESSAGE_FACTORY_START macros above are an example of that. But the question remains, how do you get a developer's code to be run by your code? Macros can't exactly take code from some part of code and inject it into something else. Or can they? Well, no, they can't. Macros are just named fragments of code that expand at compile time. However, the C language's love of pointers does allow us to essentially \"inject\" user code (functions, really) into our own code, it just tends to be convoluted and require a lot of copied code. And hiding all of that is what macros are good at!","title":"The Way Messages Are Made"},{"location":"blogs/week-7/#rpcs","text":"Before we go any further, let's make sure everyone's caught up on the newest lingo. In real-time networking, you have two basic types of networking: networked data and networked functions. Networked data is objects and data primitives that you sync across a network; one example of networked data would be an object's position being simulated by the server and sent out to every client. This data is much more appropriate for unreliable streamed network data because we can typically just forget about old, lost packets and take the newest ones as ground truth. Networked functions are also known as remote procedure calls , or RPCs . An RPC function is effectively just an integer handle and some data that is sent once from client to server or server to client, and once the receiver gets the message, they run a function that corresponds to the integer handle using the data that was sent as parameters. This is easier to keep track of in games because you can define the processing on either side of the delivery chain and you get to call it as frequently or infrequently as you want. However, you also have to be more careful about this data. Obviously if you lose the packets of an RPC call, you either need those packets to get sent again or you're losing out on whatever the other side was trying to tell you. An RPC is usually something like a \"pause menu\" event in a multiplayer game like Super Smash Bros. When one person pauses, the other screens also pause, but this is obviously a one-time event and not some synced variable (although it probably could be!). On the Isetta Engine team, we had a lengthy discussion about what we should use in our engine. These two types of networking aren't exclusive in any way, but with our inexperience with network programming, we wanted to ground ourselves with one implementation before extending into more use cases. The side that eventually won out was the RPC methods because RPCs can effectively emulate synced variables, just with some runtime and network bandwidth wasted, and RPCs are also a more straightforward API to design than a serialization standard that game developers would need to abide by in our engine!","title":"RPCs"},{"location":"blogs/week-7/#first-pass-at-rpc-messages","text":"The following implementation of RPC networked messages draws heavily from Tam\u00e1s Szelei's blog Emulating the static initialization blocks of Java in C++ , so if you're interested in the idea feel free to take a peek at his article (we also go beyond this implementation in our second pass similarly to how our LevelManager now handles static initialization). The main idea is that we can leverage C++'s static keyword along with a \"registry\" class to register our message types to the object factory at startup. If you do it right, this can be very effective, but the first implementation we landed on was pretty naive and didn't help very much. We created a macro for starting and ending a message object declaration, and inside of that macro we also packaged a special static \"helper\" class: #define RPC_START(MessageClass) \\ class MessageClass : public Message { \\ private: \\ static MessageClass* Create(void* memory) { \\ return new (memory) MessageClass(); \\ } \\ friend class RPCMessageFactory; \\ static RPCRegistryHelper MessageClass##_RPCRegistryHelper; ; The RPCRegistryHelper is a class that registers a given message type's information when its constructor is called. ...if you're at all familiar with C++, then you probably know that these static objects can't just be declared\u2014they need to be initialized. And the way that you do that is to define that variable somewhere in some other code, perhaps in NetworkingModule 's StartUp function maybe? In any case, we still have what should be external, user-defined code within our engine code at the end of the day, so more work will need to be done on that. Obviously this only got us halfway there as far as functionality goes, too. If we're sending RPC message objects, then we also need to define RPC message functions to be run when the objects are received! Just like with the object type declaration macro, these macros essentially just define a name that can be used throughout the code: #define RPC_CLIENT_FUNC \\ public: \\ static void RpcClient(Client* client, Message* message) \\ #define RPC_SERVER_FUNC \\ public: \\ static void RpcServer(int clientIdx, Server* server, \\ Message* message) \\ #define RPC_MESSAGE_FINISH(MessageClass, Tag) \\ public: \\ VIRTUAL_SERIALIZE_FUNCTIONS(); \\ } \\ ; #define RPC_MESSAGE_INIT(MessageClass, Tag) \\ NetworkRegistry::RegisterMessageType( \\ sizeof(MessageClass), MessageClass::Create, Tag, \\ MessageClass::RpcClient, MessageClass::RpcServer); With that, we technically had an RPC message system running, even if it still required a bit much with jumping through hoops. Once a message class had been defined like HandleMessage , the user could then generate those messages by calling a function on the NetworkManager and giving it the appropriate string tag, which in the case of HandleMessage was \"HNDL\" . With the generated message in-hand, the user can then populate its data however they like then send it on its merry way through the NetworkManager \u2014and on the other side of the network, someone would receive that message and run a function defined inside of the message class itself that has no scoping and no parameters! Sounds useful, huh?","title":"First Pass at RPC Messages"},{"location":"blogs/week-7/#second-pass-at-rpc-messages","text":"With what we had implemented essentially enforcing the use of global variables and poor scoping, we knew we had to do something better. There were several places for us to improve upon, but the first was simple: Why in the world did we need a NetworkRegistry static class and a NetworkManager static class? It turns out, there is no reason! We had originally implemented the NetworkRegistry as a globally accessible form of using messages, but the NetworkManager already effectively serves that purpose, so we just went ahead and combined those two things. (It was also at around this time that one of our developers refactored our codebase to use multiple projects, such as IsettaEngine, IsettaTestbed, and IsettaTest. Turns out, this can be a massive headache for anyone who was changing code when this happens! It really messed with our Git commits and was an overall bad time. Would not recommend.) Another improvement that we strove to get this time around was an actually usable message callback system. The previous one forced the developer to pair the functions that would be run when the RPC message object was received with the actual class of the message object itself, which obviously has very limited capability. Our immediate solution was to make a subscription-based callback system, much like our how Input class handles callback functions. Any message that is defined would get its own associated list of function pointers, and anybody in the world (who has the correct function parameters) can add their own function to that list. When that message object is received, every function in that callback list is called using that message object's data. This method does have a few drawbacks. Namely, you get more overhead either from having to check at runtime inside your callback functions whether you really want to be run due to a particular message that was received, or you get more overhead from creating more message objects that would be linked to sparser sets of callback functions. You also need to keep track of integer handles for every function that you link to the message objects, otherwise you won't be able to unsubscribe functions. However, as you might also imagine, this is an extremely user-friendly way of defining callback functions, and its usable anywhere in our code if you know the corresponding string tag! (Like \"HNDL\" for HandleMessage from before) To contrast the macros showed earlier that generate the singular message callback functions, this is an example of how a user might register a callback function at this point in development: exampleClientHandleId = NetworkManager :: RegisterClientCallback ( \"HNDL\" , []( Message * message ) { HandleMessage * handleMessage = static_cast < HandleMessage *> ( message ); // \u2026 use the handle somehow ... }); exampleServerHandleId = NetworkManager :: RegisterServerCallback ( \"HNDL\" , []( int clientIdx , Message * message ) { HandleMessage * handleMessage = reinterpret_cast < HandleMessage *> ( message ); // \u2026 use the handle somehow ... NetworkManager :: SendAllMessageFromServer ( \"HNDL\" , handleMessage ); }); Isn't that so much more refreshing? Also, since we're using C++ lambda functions for registering the callbacks, we can include any scoping that we want to be used by the function as well. So even if our only input parameter is essentially the message object itself, we can still influence the world outside of that callback function's scope!","title":"Second Pass at RPC Messages"},{"location":"blogs/week-7/#aside-on-messagecopy","text":"The NetworkManager::RegisterServerCallback example shows another cool feature that we implemented at this point. Before now, you would have to iterate through every single client and manually send them a message if you created a server callback function whose message you wanted to broadcast to all clients. This was sort of okay when the function was tied to the message object, but now that the functions can be defined anywhere, it gets a little more awkward to have to know about the individual clients that we're sending to. So we implemented a \"SendToAll\" type function instead, but we ran into an issue with our networking library: Every message that gets sent from the server to a client needs to be generated specifically for that client . It's wasteful, but it also allows us to track whether or not our messages have been received (which can be important!). But that was making message objects difficult to broadcast because we couldn't just send out the exact same message we received, we would have to copy its data to a bunch of new message objects and send those instead! And so our solution was to extend the requirements of an RPC message object. Now, not only do you need to define a serialization function, but you also need to define a copy function that can fill out its own data based on another message of the same type. With that one quick addition, we can now hide anything to do with connected clients or client numbers from the game developer!","title":"Aside on Message::Copy"},{"location":"blogs/week-7/#network-identities","text":"So RPC networking is effectively implemented, what else is there even to do? We basically have everything we need, right? Not quite! Although RPC messages can account for most of the functionality of our engine (although we probably don't want to use them for everything ), there's one more very important piece that we need before this can actually be utilized: network identities. A network identity is an ID that can be attached to individual objects that will be correctly mapped across the network to the corresponding object on the other client and server machines. Typically the way that game programmers reference other objects is via their pointers, which works fine and dandy when you're on a single machine, but do you think those pointers will conveniently line up on someone else's computer? Without these IDs, the game would have no way of directing its message data to the correct objects! Now technically this is something that could be implemented on the game level and not the engine level if we stopped right here and only left our networking system as a bunch of RPCs, but we actually do need to have at least a networked transform in order to keep our characters in sync with one another on people's individual computers, and that's impossible to do without some sort of identifier that will point the transform data to the correct object. Fortunately, we've recently built up our component system and tested it out in the demo game, so we can implement our NetworkIdentity class as a component that will be held by individual game objects (or \"entities\") within the game. This way, the game developer can specify which objects in their game they want to have synced across the network so that they don't have to waste a bunch of networking bandwidth keeping a bunch of immobile, uninteresting things synced up with each other. Currently, the NetworkIdentity component is pretty much just a wrapper around some NetworkManager functionality that generates an identifier and maps it to the component that asked for it, but because we're developing a game engine and everything needs to be abstracted just enough to annoy the programming team, we also want the NetworkIdentity component to dictate the sync rate and authorities of the networked data that is associated with it. This will be more important when we talk about networked transforms though, so we'll leave that discussion for another day.","title":"Network Identities"},{"location":"blogs/week-7/#testing-it-all-out","text":"So now that we've mapped out a pretty robust system for sending networked messages, you can imagine our excitement to see some synced behavior between two computers across a network. When it came time to test, however, we faced a frustrating bug. For some reason the network IDs would not sync up! We thoroughly vetted the rest of our serialized data and saw that all of that was being correctly sent, but it seemed the network ID would never be properly sent from the client. With even more digging, we discovered that the ID was being properly sent from the client. So what in the world was happening between the two computers? As it turns out, we were storing the received network ID in the wrong place. The network IDs that we've implemented are unsigned integers for obvious reasons, but we don't have a serialization function for unsigned integers and the developer here was too lazy to implement that. So instead, we casted the unsigned integer to an integer and passed that to be serialized. The following is the macro from our networking library yojimbo that we use for serializing integers: int signedNetId = static_cast < int > ( netId ); serialize_int ( stream , signedNetId , 0 , 256 ); This expands to something like this: #define serialize_int(stream, value, min, max) \\ do { \\ yojimbo_assert(min < max); \\ int32_t int32_value = 0; \\ if (Stream::IsWriting) { \\ yojimbo_assert(int64_t(value) >= int64_t(min)); \\ yojimbo_assert(int64_t(value) <= int64_t(max)); \\ int32_value = (int32_t)value; \\ } \\ if (!stream->SerializeInteger(int32_value, min, max)) { \\ return false; \\ } \\ if (Stream::IsReading) { \\ value = int32_value; \\ if (int64_t(value) < int64_t(min) || int64_t(value) > int64_t(max)) { \\ return false; \\ } \\ } \\ } while (0) Depending on whether we're reading or writing from message data, the value either gets read or written to the variable that we're passing to the macro. Which means that, when we're reading from the stream (i.e. receiving a message), we write that data into the temporary signedNetId variable! And our netId variable is left at its initial value, 0. Acknowledging that we're very likely not to face the problem of too many network IDs, we changed our unsigned integer to a signed integer and left it at that. If we need to expand it to be unsigned, we can always write a serialize function. Oh, and one more bug popped up that we should note: As we alluded to above, each message object declaration requires a Serialize function. This function is where that serialize_int macro example comes from and is also what is used by yojimbo to generally serialize any subclass of its Message class. Well, one thing that we forgot when implementing the NetworkIdentity test messages was that you must return true at the end of the Serialize function. That's how yojimbo knows that the serialization succeeded. Unfortunately, the macro makes forgetting this piece of information non-fatal to the compile process, but then whenever you try to send a message that doesn't return true, your client will disconnect! So this is an incredibly important piece of tribal knowledge that we have to document somewhere to keep all of our future developers from tearing their hair out at our network messaging system.","title":"Testing It All Out"},{"location":"blogs/week-7/#and-a-third-pass-at-rpc-messages","text":"Oh, did you think we were done? We're just getting started! (Just kidding, don't worry, you're in the home stretch!) So after we finished the pre-code review version of our networking system, we had a bad taste left in our mouth. Why should the developer have to remember some weird string tag in order to send messages of a particular type, like \"HNDL\" ? And what happens if we have conflicting string tags or need longer strings? Well, as it turns out, we already utilize a system similar to this tagging but by using actual object types somewhere else in the code: Our component system! Specifically, inside of Entity.h we have a vector of component types that we use for checking whether or not the current entity has a component that's requested using the GetComponent function. Since the function is a template function, it can utilize C++'s std::type_index class which essentially enumerates the classes. By a similar method, we can just enumerate our user-defined messages by their type if we turn all of our message-based functions into template functions 5 ! Now, if you've watched any of Mike Acton's talks, you'll probably have a critical eye of templates. And we acknowledge that they do drive up compile time and wreck iteration speed, but in such a short notice, they can be a nice step around a lot of code that could be much harder to maintain. And because we're using it, we can get rid of our gross string tag method and do a very simple type check: template < typename T > int NetworkManager :: GetMessageTypeId () { return typeMap [ std :: type_index ( typeid ( T ))]; } So clean! This also gives us the added benefit of knowing our message types for several of the other message-based functions. Some of them already perform casts on the messages in order to access their data, and this would let us conveniently skip that step.","title":"...And a Third Pass at RPC Messages"},{"location":"blogs/week-7/#static-initialization-20","text":"We're not done quite with the networking yet\u2014hang in there! There's still one more undesirable quality to our RPC message creation system. Remember the \"static initialization\" solution from before? Well, that technique hardly helps since the game developer is still required to initialize the static variables somewhere , and forgetting to do so can lead to a rabbit hole of debugging. Fortunately for us, we've already used an ever better system elsewhere in our code. In LevelManager.h , we define a static boolean variable whose initialization is also in the header file but is guarded by being a template function! So we don't have to worry about a multiple-definition error but we can also hide away the registration process from the game developer. You can read more about this technique in the section Template Black Magic! of our Week 6 blog.","title":"Static Initialization 2.0"},{"location":"blogs/week-7/#making-a-level","text":"As a nice bow to wrap around this whole week of intense networking development, we also took all of our networking code out of the engine loop (where it was hard-coded) and put it into NetworkLevel.cpp . In doing this, we were able to test a complete separation between the engine's networking logic and the game's, and guess what? It works! Check out that spawning and despawning synced up! Your browser does not support the video tag. This trippy zombie dance experience brought to you by the devs being too lazy to space out the spawns. That's it for this week! It might seem like a lot, but really we only worked through the fundamentals of a good networking system. In the next week or two, we will be introducing networked transforms into our system, which will let us have legitimate multiplayer going in our games!","title":"Making a Level"},{"location":"blogs/week-7/#serialization","text":"We realized during the writing of the networking section above that we haven't discussed serialization at all yet. Part of the reason for that is because we haven't really done any serialization ourselves; for the networking, we're simply using the networking library yojimbo 's own serialization functions, and beyond that we haven't really needed serialization because we're writing our game levels in C++ instead of encoded files. Serialization is the process of taking some object from our program into a stream of bytes that can be written somewhere, whether it's a file, a database, or even a network packet. It's a very important subject when looking at the big picture for game engines. It's not discussed very often because it's pretty disconnected from the game side of the engine, but a game engine's job is to move a lot of data through itself and process that data in some way. And when we say a lot, we mean a lot . Without serialization, your game engine would be at the whim of whatever other software saves out your resource files, and you would probably only write out simple text files that take ages to read and waste a lot of space. That being said, serialization itself is a difficult feature to tackle. How do you generate the serialized data in the first place (especially if you don't have good reflection data like in C++)? How do you guarantee backwards compatibility with the serialization, or do you even have it at all? These are all questions that we completely avoided by using big 3 rd party libraries for things like audio and graphics! And for the time being, we don't have to worry about serialization with our .cpp level files, so we won't worry about it. If you're interested in the topic, Jeff Preshing has a good introduction to the topic including links to several good resources to learn from.","title":"Serialization"},{"location":"blogs/week-7/#serializing-our-own-data","text":"Our plan is to not serialize our level data, but we were still interested in serializing our basic data types to avoid rewriting code, such as for Vector3 and Matrix4x4. The main thing that this serialization would be used for (i.e. the only thing it would be used for) is networking, so we thought we could utilize yojimbo's serialize_object macro and write Serialize functions for each of our data containers that could then be sent across the network. But there's one big problem that we hit. The Serialize function used by yojimbo must be templated or else it won't work. Why is that an issue? Aren't we using a bunch of templates already? Well, the problem isn't the template itself per se\u2014it's the fact that a templated function must be defined inside of the header file. This means that, if we want to serialize a single class with yojimbo, we have to include the entire 6,000 lines of code in yojimbo.h to utilize the serialization macros. And including that much network code in our math data structures just didn't seem like a good idea. So for the time being, we'll leave serializing the data types to the game developer! We wish we could do more there, but it's currently not worth the time investment.","title":"Serializing Our Own Data"},{"location":"blogs/week-7/#first-game","text":"A major milestone is that we made a real game with our engine! It's running smoothly at ~60 fps with sound, gamepad input, models, lighting, textures, animation blending, movement, score counting, tweakable variables, an in-game entity inspector\u2026 All the beautiful things you can imagine in a game engine. The game is also built in a whole new project! That means we can now bundle our entire engine into a DLL 6 and pass it around for people to use. How exciting! Now, let's calm down and talk about how we did it. First, we built the entire engine into a DLL. Visual Studio is very helpful with this process by providing this document which covers the process. Basically, we just needed to add a new configuration, set its format from .exe to .dll , and add a bunch of modifiers like __declspec(dllexport) and __declspec(dllimport) to the classes we want to export and import to the game project. We made a macro for this, so we can easily get different behavior in different environments: #ifdef ISETTA_EXPORTS #define ISETTA_API __declspec(dllexport) #else #define ISETTA_API __declspec(dllimport) #endif The tricky thing is, the game project needs not only the DLLs, but also the following (if you are not sure how to do any of these, please refer to the document linked above): The header files in the engine. This allows the game project to include classes and functions from the engine and know what functionality they provide. The .lib files . They are generated together with the DLLs and tell the compiler where to find specific functions in the DLL. The header files and .lib files from external libraries, if needed. We are currently including everything from external libraries, which is certainly not optimal. We will try to find the cleanest way to include them, and post the update in a future blog If you see giant lists of \"unresolved external symbols\", don't panic! We had those too, so just check the above three things and make sure you have the appropriate ISETTA_API modifiers. Second, we downloaded some assets from Mixamo and made some game assets with Blender (programmer art, yay!), and converted them into a format that our engine can read. Luckily, this step is also not very difficult\u2014Horde3D comes with a \".dae to .geo converter\" 7 that, apparently, convert .dae files to .geo files, which is the file format Horde3D needs. But there are still two tricky things with it: The path to generated resource files still needs to be hard-coded into the game, so if the file structure changed, the game developer must remember to change those hard-coded value, too. Otherwise, the game will throw an exception. The units used by different software could be different. For example, in Maya, 1 unit is 1 centimeter by default, and in our engine, 1 unit is 1 meter. If we don't change the scale before exporting, the models will be huge in our engine. This can be fixed by changing the default scale in Maya, or by manually setting the object's scale in the game. If you don't, you may get a giant soldier, like we did: Your browser does not support the video tag. By the way, if you pass some code to DLL-imported function to find resources, it will try to find the resources relative to the game project, not the project where the DLL is from. This is pretty obvious, but took us a while to make it clear. Third, we started making the actual game and found out a ton of bugs in the engine, and fixed/worked around them. This experience firmly states that, making small test games is really a good way to test the engine ! Here are some bugs we found out: Entity::Destroy is not working properly. It does call the OnDestroy functions, but the entity itself is not removed from the entity list of the scene after that (we had a TODO there but no one remembered to fix it). Therefore, the OnDestroy function is called again and again and again, and the entity is never really \"destroyed\". This also revealed another issue in our free list allocator. As the OnDestroy functions release memory in the free list allocator, when the same memory is freed the second time, our allocator would freak out and throw an exception. We improved our free list allocator to make it safer. To avoid this issue, we chose to not dynamically delete anything in the game. Instead, we pooled all of them. The animation won't work correctly after turning the entity off and on again with Entity::SetActive . It's caused by our interface with Horde3d. We fixed it and it's working properly now, and we also added animation blending during the fixing process. You can see the version with broken animation here . As mentioned before, we had scaling problems with imported models, and we chose to set their scale in the game. This choice revealed a problem in our Transform class: when an entity's scale is small, calling TranslateWorld (the function that we use to move objects in world space) on the entity is very ineffective as the world translation is somehow affected by the object's local scale. We dug into the code and realized the reason is that when we were constructing the object's matrix from the local position, local rotation, and local scale, we applied local position before local scale. This would cause the local position to be \"scaled\" by local scale, which should not be. So we simply changed the order of constructing the matrix and it worked fine. We also found out some missing features during the making of this game: When we were implementing our GUI module, we only created some interfaces with ImGUI and left out some other that we thought wouldn't be used. Well, it turned out we didn't perfectly predict what we need. When making this game, we wanted to tweak some values at runtime with in-game GUI, and a \"float slider\" would be the best option for that. However, we didn't have that in our GUI interface, so we went ahead and added it to our engine. The list of audio files to be used in the game was hard-coded into the AudioModule. It's certainly not desirable and would destroy iteration time. So we moved that to the config file, thus allow specifying audio files to be used in the config. Finally, we got the game working! Yay!","title":"First Game"},{"location":"blogs/week-7/#patch-notes","text":"","title":"Patch Notes"},{"location":"blogs/week-7/#editor","text":"After making our transform and GUI system, we spent 20 minutes hacking together a \"programmer UI\" in-game editor that's capable of displaying entity hierarchy, transform information, component list, gizmo drawing, and has a \"reset transform\" button. Being able to see the list of entities and their information made it so much easier for us to debug the game and engine. Our simple in-game inspector","title":"Editor"},{"location":"blogs/week-7/#freelistallocator-alignment","text":"When we were working on the networking system this week, we attached NetworkIdentity components to game objects to synchronize them across the network. After doing this twice, we would get a memory reference error inside of FreeListAllocator ! The free list essentially tracks available nodes of memory in our own managed heap and tries to merge them if they're adjacent, so we looked into its allocation directly by stepping through the debugger to see what was going wrong. This is what we found: // \u2026 node = a custom data structure containing a size and a pointer to the next node intptr_t rawAddress = reinterpret_cast < intptr_t > ( node ); rawAddress += headerSize ; // leave size for header intptr_t misAlignment = rawAddress & ( alignment - 1 ); uint64_t adjustment = alignment - misAlignment ; intptr_t alignedAddress = rawAddress + adjustment ; size_t occupiedSize = headerSize + adjustment + size ; intptr_t headerAddress = alignedAddress - headerSize ; if ( node -> size >= occupiedSize + nodeSize ) { // enough space to put a node here auto * header = new ( reinterpret_cast < void *> ( headerAddress )) AllocHeader ( occupiedSize , adjustment ); InsertNodeAt ( node , new ( reinterpret_cast < void *> ( alignedAddress + size )) Node ( node -> size - occupiedSize )); } else // ... The problem in this code stems from one spot: Where auto* header is allocated using headerAddress . If headerAddress happens to be only 8 bytes away from the rawAddress of node , then the next pointer of node will be overwritten by the new header allocation! (This is one of the problems you may face when you write your own memory allocation system!) But when is headerAddress only 8 bytes away from node , especially when our default alignment is 16 bytes? The answer lies in where we started. If node happens to be misaligned, then the first block of code is not actually aligning our next address, it only puts it 8 bytes after our address because it's \"aligning\"! To fix this, we can increase our adjustment so that it always \"rounds up\", so to speak. This can be done in a short math expression: uint64_t adjustment = alignment - misAlignment ; adjustment += (( ~ alignment & adjustment ) << 1 ); intptr_t alignedAddress = rawAddress + adjustment ; There, no memory errors!","title":"FreeListAllocator Alignment"},{"location":"blogs/week-7/#camera-vs-cameracomponent","text":"Regarding our engine's naming conventions, the Isetta Team is usually pretty lax. But for our components, we decided to turn it up a notch and argue over which is better: \"Camera\" or \"CameraComponent.\" It remained an unresolved debate because one of our programmers was out at an interview over a couple days, but when he came back, we decided on a compromise. The component should include the word \"Component\" when the name of the component itself is only one word (e.g. \"CameraComponent\"), and it should not include \"Component\" when it already contains more than one word (e.g. \"NetworkIdentity\").","title":"\"Camera\" vs. \"CameraComponent\""},{"location":"blogs/week-7/#coming-soon","text":"Go check out our interview with Amandine Coget , where we talk about the Frostbite engine, developing good usability, and the pros and cons of different UI models. We also interviewed Raymond Graham ( @wadarass ) this week, so expect his interview along with Martin Middleton's and Aras Pranckevicius's ( @aras_p ) in the coming weeks.","title":"Coming Soon"},{"location":"blogs/week-7/#resources","text":"From development we are acquiring a lot of great resources from where we learned certain topics, and as much as we try to synthesize things in our blogs you may be interested in the originals, which is why we have a page full of links for you to browse. Originally Published October 19, 2018. A heuristic is a technique for solving a problem more quickly, often utilized in optimization problems for computer science. \u21a9 An object factory is a programming pattern where you decouple object creation and destruction with the actual usage of the object. An object factory typically creates objects based on passed parameters, which you would then receive and use. \u21a9 Typecasting , also known as type conversion , is a method of changing one data type to another. It helps ensure that variables are processed correctly by functions, but can also be dangerous with the way they handle the conversion. \u21a9 Boilerplate code is a section of code that has to be included in many places with little to no alteration. \u21a9 Function templates are functions definitions that, at compile time, generate individual function definitions for any types used with the template. This can be very useful for cutting out duplicate code, but greatly increases compile time and code size. \u21a9 A DLL is a dynamic-linked library which is Microsoft shared library concept which can be transported around easier than a project and contains information about the compiled project. \u21a9 The content pipeline of Horde3D is detailed on this page . Long story short, .dae, which stands for Digital Asset Exchange, is the file format that COLLADA (COLLAborative Design Activity) uses. COLLADA, in turn, supports portable representations for 3D assets. \".geo\" is the file format that Horde3D uses to load in assets at runtime. \u21a9","title":"Resources"},{"location":"blogs/week-8/","text":"Patching Holes \u00b6 Byte-Sized Updates \u00b6 Component Registry : To add component inheritance support, we added a component registration mechanism for the components to store their type hierarchy. Collisions : Third week working with collisions and things seem to be wrapping up, main focus was on how to handle collision events when one occurs. You may not notice a difference from last week...as it turns out, high-level architecture diagrams aren't great at showing progress after the early stages of development! Component Registry \u00b6 Why Do We Need a Component Registry? \u00b6 When we were implementing the entity class, we saved the std::type_index for every component added to the entity by a array to allow the game developer to call GetComponent<T>() without dynamic casting the type each call (which is quite expensive). This meant that inside of an entity, we had two array to save the information for a component: an array for the type_index as mentioned, and another array for the pointers to the components themselves. When the game developer calls GetComponent<T> , the function first searches through the type index array to see if the array contains this specific type index. If so, it returns the corresponding component pointer (sharing the same index). However, it became problematic when we were implementing the collider components. We are supporting multiple types of colliders, like BoxCollider , SphereCollider , and CapsuleCollider . Being different shapes, these colliders require different operations and calculations, but they have the same interface as colliders; they all have functions like RayCast or CollisionCheck . To avoid repeating ourselves, we created an abstract Collider component for the specific types of colliders to inherit from. For the same purpose, whenever we call GetComponent<Collider> , we also expect the function to return either a BoxCollider , a SphereCollider or a CapsuleCollider since they are all children classes of the Collider class. However, the std::type_index of those classes are different based on the fact that they are actually different classes. Thus, the GetComponent<Collider> function cannot find its corresponding type index from the type index array, since no exact Collider component is ever attached to the entity. This will happen for any type that inherits from another class of component. We tried to convince our collision programmer that this was fine, but he wasn't having it\u2014we had to find a way to fix it. Approaching a Solution \u00b6 After lengthy discussion, we decided that we want to make a similar system like the level registration. If you remember from Template Black Magic! in Week 6 , we used a template with a static variable that calls a static register function to register the level into the engine so that the level could be loaded by name later. We wanted to use the same technique to save the components' hierarchy metadata before the engine is started. To do that, we need a key-value map 1 whose key is a type index of a component and the value is a list of type indices that indicate the subclass components (including itself) of the key component. We are using a linked list instead of an array here for two reasons: one, we don't need random access for the list, and two, we can simply use push_front to make sure the component itself is always the first one of its subcomponent list. The registration process seems straightforward, since it's just adding the type index to the corresponding list. But how do we call the register function before the engine/game starts? Again, we wanted to make use of the static initialization phase. First Try: Another Registration Pattern \u00b6 template < typename Curr , typename Base > class ComponentRegistry { protected : static bool registered ; }; class MeshComponent : public Component , public ComponentRegistry < MeshComponent , Component > { static bool IsRegistered () { return registered ; } // required ... } template < typename Curr , typename Base > bool ComponentRegistry < Curr , Base >:: registered = Component :: Register ( std :: type_index { typeof ( Curr )}, std :: type_index { typeof ( Base )}); Just like what we did for the level registration, we created a template to run the static initialization process. Each component inherited from the template will have a static registered variable which registers the component to the type hierarchy map. It works perfectly well with levels and most components. However, it breaks when a component is inheriting an existing component like this: class Collider : public Component , public ComponentRegistry < Collider , Component > {...} class BoxCollider : public Component , public ComponentRegistry < BoxCollider , Collider > {...} Yes, it can still register the type to the type hierarchy map, but the BoxCollider now contains two registered variables\u2014 one from its base class Collider , and one from the template class ComponentRegistery<BoxCollider, Collider> . There's no way to tell the compiler which is which, and this results in a compilation error! Second Try: Manual Template Specialization \u00b6 So let's recap the problem: Every template specialization has the same variable name, registered , and this leads to the problem of having an ambiguous symbol. Since we were using macros in level registration, why not utilize macros further to provide a unique variable name for each component? Or specifically, for each template specialization? #define CREATE_COMPONENT_BEGIN(NAME, BASE) \\ template <> \\ class ISETTA_API_DECLARE \\ ComponentRegistry<class NAME, BASE> { \\ protected: \\ static bool NAME##Registered; \\ }; \\ class ISETTA_API_DECLARE NAME \\ : public BASE, \\ public ComponentRegistry<NAME, BASE> { \\ protected: \\ static bool isRegistered() { return NAME##Registered; } \\ \\ private: #define CREATE_COMPONENT_END(NAME, BASE) \\ } \\ ; \\ bool ComponentRegistry<NAME, BASE>::NAME##Registered = \\ Component::RegisterComponent(std::type_index(typeid(NAME)), \\ std::type_index(typeid(BASE))); Now we are manually specializing the template and defining different registered variables with different names by macros. The ambiguous symbol issue should be all good and ironed out now. But actually, it brought us another issue! When defining the value of the static variable of a fully specialized template, the definition statement 2 should actually be written in the .cpp file instead of the header file. However, this would requires game developers to write two different macros in two different files, which is easily forgettable and could cause quite a bit of headache if forgotten. So we had to find a way to keep the definition statement in the header file. Third Try: Introducing \"Dummy\" Template Parameter \u00b6 Since the problem is mainly caused by fully specialized templates, we thought, \"What about adding a dummy parameter to keep the templates partially specialized?\" As long as the template is not fully specialized, the definition statement can be kept in the header file with the other macros! #define CREATE_COMPONENT_BEGIN(NAME, BASE) \\ template <typename Dummy> \\ class ISETTA_API_DECLARE \\ ComponentRegistry<class NAME, BASE, Dummy> { \\ protected: \\ static bool NAME##Registered; \\ }; \\ class ISETTA_API_DECLARE NAME \\ : public BASE, \\ public ComponentRegistry<NAME, BASE, void> { \\ protected: \\ static bool isRegistered() { return NAME##Registered; } \\ \\ private: #define CREATE_COMPONENT_END(NAME, BASE) \\ } \\ ; \\ template <typename Dummy> \\ bool ComponentRegistry<NAME, BASE, Exclude, Dummy>::NAME##Registered = \\ Component::RegisterComponent(std::type_index(typeid(NAME)), \\ std::type_index(typeid(BASE))); This idea might just be stupid enough to work...and it does! With this trick, we can now declare a new component like this: CREATE_COMPONENT_BEGIN ( GUIComponent , Component ) private : ... public : ... CREATE_COMPONENT_END ( GUIComponent , Component ) As we move forward with our engine development, we're becoming more and more of C++ Magicians\u2122! Collisions \u00b6 Collision Handling \u00b6 So, as promised, we can provide an update on the collision system's development! Unfortunately, it isn't quite as exciting as what we promised last week . We are still working on the BVTree (see last week for more details); since it's just an optimization (although a necessary one!), we put it off again this week to work on other bug fixes that were needed more immediately. The other decision we left from last week was how to handle the collisions once they happen, and we will be answering that this week. The collisions system architecture that remains unchanged from last week is how we determine the event from collisions, i.e. the collision just began, it's continued, or it just ended. After the broad- and narrow-phase 3 determine a intersection between colliders has occurred, the collision-pair is searched for in a hashed collision-set to determine whether the colliders just entered one another, are still intersecting from a previous frame, or, in the case the pair exists in the set but are no longer intersecting, the collision has ended. What wasn't clear from last week, however, was how the collision callbacks (on enter/stay/exit) were called in the respective event. For details on the options we considered, you can read last week 's blog, but our actual implementation now is a variation of having a collisions handler component. The collisions handler component, labeled CollisionHandler , holds the callbacks which other components can subscribe/unsubscribe from. Rather than the handler holding a list of collider pointers like suggested, the Collider component now holds a reference to the CollisionHandler . This is advantageous as compared to a list of colliders because, when checking intersections, the CollisionModule explicitly has Collider pointers, not CollisionHandler pointers. And in comparison with the solution proposed last week, where the list was publicly available for colliders to subscribe to, this system stops colliders that aren't on the same entity or its children from subscribing to the CollisionHandler . So no weird sibling-entity colliders. The CollisionHandler , when enabled, walks down the hierarchy tree looking for Collider and CollisionHandler components; when a CollisionHandler component is found, the depth-first search down that path of children is stopped, and in the case a Collider component is found, that collider's handler is assigned to be the collision handler that was just enabled. When the CollisionHandler is disabled, it walks up the tree to get a reference of some ancestor CollisionHandler , which is then assigned to the children colliders in the same fashion as we describe above for enabling. A couple of other decisions had to be made regarding the collision handling design, as well. When the CollisionModule is iterating through the list of colliders for intersection tests, it now needs to check the CollisionHandler for each collider because detecting collisions between colliders of the same handler seems like a bad day just waiting to happen! This also allows for child colliders to create a complex collision shape with only the basic primitives, which is a pretty neat benefit. While writing about this topic, our team actually broke out into an in-depth discussion about the behaviour when a collider's ancestors are changed. In the case that an entity above the root CollisionHandler (the collision handler that the collider has reference to) in the scene hierarchy was moved, no additional behaviour is needed because the collider reference to the collision handler remains unchanged. But in the case the handler isn't moved but the collider is, the collider now holds a reference to an invalid handler, one not in its ancestor tree! To fix this, we have a callback event for when a parent is changed, OnHierarchyChanged , that the Collider components subscribe to. When that event is fired, each of the colliders walks up the hierarchy tree looking for a CollisionHandler to store a reference of. We even got some early optimizations going with this solution, too: this process can be short circuited by also looking at Collider s while we're walking up the tree and using their reference to the CollisionHandler instead. This system is also still in development because it is tied with the event-messaging system. Box-Capsule Collisions Will be the Death of Us \u00b6 On another note, the box-capsule collision intersection test was iterated on this week. This particular test was by far the most complicated to rationalize in 3D, and is the scariest with regards to performance. All intersection tests were done geometrically rather than using an algorithm like GJK 4 , which might have been a mistake on our part. Most of the intersection tests had readable/followable code, however some like box-box and box-capsule quickly became large and difficult to follow. By trying to brute-force the box-capsule test with geometry rather than with an algorithm (or maybe just a better understanding of the topic), we were only able to produce a intersection test that works when the capsule line-segment is perpendicular to at least one of the local axes of the box. If none of the three axes of the capsule line-segment are perpendicular to any of the local axes of the box, the test simply doesn't work. This was left as an exercise to the reader\u2026well that's not really fair, but we decided that wasting additional time right now on this case isn't worth it, especially because we don't expect to have this case appear in our target game. It's pretty disappointing for us and you might write off this engine as unusable now (it took you this long to figure that out?), but because of strict time commitment we have to keep moving. It mostly works (see below!) and will work for our game, because the show must go on. Your browser does not support the video tag. This was when we finally got the capsule with 2 non-perpendicular axes working! Your browser does not support the video tag. Raycasting Functionality \u00b6 The other system developed this week was raycasting. A benefit of doing this after the intersection testing was a lot of the code used in testing intersection was able to be reused for the raycasting system. Something that also arose from the raycasting system was the need of a way to convert a mouse click point (or any point on screen) to a ray, which got us sidetracked from actually adding in raycasting functionality! (Some TODO s for other features were also added to the CameraComponent that would be nice) This wasn't necessary in the basic sense, but it made testing the raycasting functionality much easier and is a nice feature that can be used in other places as well. Your browser does not support the video tag. Ignoring Collisions \u00b6 The final feature we started working on this week regarding our collisions system was ignoring collisions and collision layers. Ignoring collisions was a straightforward addition to the CollisionModule by adding a hash set of collider pointers. The rationale of keeping the hash set on the CollisionModule versus the Collider components themselves was to avoid having to search on the components. Since we already have the pair, it is simpler to store on the module. We will also have a collision layer system used to allow groups of entities, or \"layers\", to ignore collisions of entities on another layer rather than having to specify and store collisions of all the other colliders. The actual implementation of layers is still in the works, so before we say too much more, we'll hold off for future weeks for additional updates. Wrap-up \u00b6 This was the third week developing the collisions system, and the rationale to choose an existing collision/physics system is becoming more clear. This is a mentally complicated system which can have a serious impact on performance; it's not that just the math that has difficult and complicated parts, but the entire system that can be a complex mess (we are trying our best to avoid this). We can already tell that this system will require more profiling than other systems in the engine. We will be able to better tell once its integrated into our game\u2014more to come in future weeks! \"What TODO to die today, at a minute or two 'til two\" \u00b6 Well, we are over halfway through the project and have no plans on slowing (even if we are a little ). We were talking as a team about how we are leaving a lot of work behind to keep things moving forward, sticking with the motto: If we don't need it now, don't do it now. At some point, we are going to need to go back and revisit our TODO s and fix/clean things up as well as add that functionality because why not. Some TODO s are naturally getting resolved because we needed the functionality, whereas others, as you will see, aren't. We did a grep 5 for TODO s to see how much we are going to have to do later. Here are the stats: We also did some lines-of-code analysis for a recent presentation, and we have to say, the numbers aren't too shabby: ...well, this is if you also take in the external libraries. It looks a lot more like an actual engine when you consider this much code. Ours looks a little more like this: There, that's a little more accurate. We have to admit, 15,000 lines of code isn't all that many for eight weeks, but what's more remarkable about this number is that this is actually an appropriate amount for a legitimate game engine . Like, one that could be used by an indie studio. A pretty significant chunk of what happens in commercial engines nowadays is external libraries and software, and there isn't an exception at the smaller scale. So if you're ever wondering if you should be \"writing your own engine\" when you want to make a game, or just go and use Unity again: Maybe give brewing one up a shot! Patch Notes \u00b6 Exiting the Engine Loop \u00b6 Inside of our engine loop, we hard-code an input binding of the Escape key to setting an isGameRunning flag to false . This allows us to exit games by simply hitting the Escape key. However, this also means that if we remove the InputModule from our engine (or possibly even change it), our engine crashes immediately! (And more importantly, the game developer is stuck with an Escape key that exits their game) We removed that hardcoding and obviously found ourselves unable to close any of our games. We anticipated that would happen. But what we didn't anticipate is that the we didn't have any interface for closing our games via the game developer code, and that's a bit of a problem. The solution was straightforward: Add some functionality that the game developer can use for shutting down the engine loop. But as programmers, it's often not the solution that we debate about\u2014it's the implementation. We had a heated debate about the architecture of our engine loop exit interface, in which we covered two options. Option #1 was to make EngineLoop a singleton with a twist; the singleton would not be given to anyone but the first thing that asked for it. Anything that requested the singleton after the first access would receive nothing, almost like a \"first-come first-serve\" methodology. Option #2 was to make EngineLoop a globally accessible singleton 6 , but do validation checks against the functions being run like Start in order to prevent the user from double-starting the engine. To go beyond a simple static singleton class, we added another layer of separation to the problem. You see, our engine developers (i.e. us) need straight-to-the-source access to information about our engine loop. But our game developers (i.e. also us but with arbitrary restrictions) should only have a limited range of \"engine-level\" control, such as starting the entire engine and shutting it down. So following Unity and Unreal 's examples, we created a class Application that has static class functions for this game developer functionality. We then chose Option #2 for our EngineLoop 's implementation due to the fact that running a DLL's main function is much more annoying than just exposing some \"start\" function in that DLL's header files. Hunting Memory Leaks \u00b6 Our project has had a small chunk of persistent memory leaks for the past couple of weeks. We never bothered to suss them out due to being too busy with other parts of the project, but recently we gained a bit of time to investigate the issue. Our most recent memory leak patch involved some new learnings about how our memory management system was freeing memory\u2014that is, our memory allocator only frees the memory you tell it to free, it does not call the destructor. So anything in our system that allocates memory within itself needs to be destructed before the memory is freed, otherwise those inner memory allocations will never be released! This could have been fixed by making the memory freeing function a template function, but even in our template frenzy, we could recognize that that would balloon our compile time astronomically, so we decided to just be explicit about object destruction instead. And so, we began our hunt for memory leaks as one typically does: by pulling systems out of our engine until a memory leak disappears! This was actually very effective to a point, and it even got us to realize that we had an insufficient engine exiting scheme, which you can read above . We found that upon exiting a level, the Level object was not getting destructed, and within that Level object, we also have a levelRoot object for the uppermost transform in the scene graph hierarchy that was also not getting destructed. That cleared up most of our memory leaks, but there were still a couple left. Via more extracting of systems in our engine, we discovered that our networking module contained the rest of our memory leaks. The first chunk was straightforward to fix; we simply didn't destruct our custom networking allocators (used by our networking library) before shutting down NetworkingModule . But we still had 80 bytes of memory leak going on somewhere , and we couldn't find out where. Our code was clean, as far as we could tell. Well, it actually was. The memory leak was found within our networking library yojimbo , and in the most ironic place possible: yojimbo has memory leak detection functionality, and enabling it caused the memory leak. That functionality has not been useful to us yet, and the fact that it actually caused a problem made it pretty easy to just disable it permanently and rely on our own devices. So we learned a pretty important lesson that day\u2014the programmers behind your libraries aren't perfect either! Coming Soon \u00b6 We just released our interview with Aras Pranckevicius , where we had a great talk about the history and design of the Unity game engine. This past week we interviewed with Jeff Preshing ( @wadarass and talked about some key skills for engine developers, so keep your eye out for that and the other interviews coming soon. Resources \u00b6 From development we are acquiring a lot of great resources from where we learned certain topics, and as much as we try to synthesize things in our blogs you may be interested in the originals, which is why we have a page full of links for you to browse. Originally Published October 26, 2018. A key-value map (or a dictionary) is an abstract data type composed of a collection of (key, value) pairs. \u21a9 Since typeid() doesn't work with incomplete classes, we had to declare this function first in the class and define it after the component class is completed. \u21a9 The broad-phase detection is performed by constructing a tree of simple primitives (either sphere or box) where the complex collider shapes are simplified into these shapes. Then iterating through these checks to determine if the simple shapes are colliding (this is a quicker calculation). The narrow-phase happens after the broad phase by checking the exact shape of the collider which are typically more complex shapes, such as capsules and object-bounding boxes (obb). \u21a9 Gilbert-Johnson-Keerthi ( GJK ) is a distance algorithm method of determing the minimum distance between convex sets. It is used heavily in collision detection systems. Casey Muratori has a great blog on Implementing GJK . \u21a9 grep is a command-line utility for searching plain-text data sets for lines that match a regular expression. Very useful for searching an entire codebase for certain things. \u21a9 The singleton pattern is a software design pattern that restricts the instantiation of a class to one object. This is useful when exactly one object is needed to coordinate actions across the system. \u21a9","title":"[Week 8] Patching Holes"},{"location":"blogs/week-8/#patching-holes","text":"","title":"Patching Holes"},{"location":"blogs/week-8/#byte-sized-updates","text":"Component Registry : To add component inheritance support, we added a component registration mechanism for the components to store their type hierarchy. Collisions : Third week working with collisions and things seem to be wrapping up, main focus was on how to handle collision events when one occurs. You may not notice a difference from last week...as it turns out, high-level architecture diagrams aren't great at showing progress after the early stages of development!","title":"Byte-Sized Updates"},{"location":"blogs/week-8/#component-registry","text":"","title":"Component Registry"},{"location":"blogs/week-8/#why-do-we-need-a-component-registry","text":"When we were implementing the entity class, we saved the std::type_index for every component added to the entity by a array to allow the game developer to call GetComponent<T>() without dynamic casting the type each call (which is quite expensive). This meant that inside of an entity, we had two array to save the information for a component: an array for the type_index as mentioned, and another array for the pointers to the components themselves. When the game developer calls GetComponent<T> , the function first searches through the type index array to see if the array contains this specific type index. If so, it returns the corresponding component pointer (sharing the same index). However, it became problematic when we were implementing the collider components. We are supporting multiple types of colliders, like BoxCollider , SphereCollider , and CapsuleCollider . Being different shapes, these colliders require different operations and calculations, but they have the same interface as colliders; they all have functions like RayCast or CollisionCheck . To avoid repeating ourselves, we created an abstract Collider component for the specific types of colliders to inherit from. For the same purpose, whenever we call GetComponent<Collider> , we also expect the function to return either a BoxCollider , a SphereCollider or a CapsuleCollider since they are all children classes of the Collider class. However, the std::type_index of those classes are different based on the fact that they are actually different classes. Thus, the GetComponent<Collider> function cannot find its corresponding type index from the type index array, since no exact Collider component is ever attached to the entity. This will happen for any type that inherits from another class of component. We tried to convince our collision programmer that this was fine, but he wasn't having it\u2014we had to find a way to fix it.","title":"Why Do We Need a Component Registry?"},{"location":"blogs/week-8/#approaching-a-solution","text":"After lengthy discussion, we decided that we want to make a similar system like the level registration. If you remember from Template Black Magic! in Week 6 , we used a template with a static variable that calls a static register function to register the level into the engine so that the level could be loaded by name later. We wanted to use the same technique to save the components' hierarchy metadata before the engine is started. To do that, we need a key-value map 1 whose key is a type index of a component and the value is a list of type indices that indicate the subclass components (including itself) of the key component. We are using a linked list instead of an array here for two reasons: one, we don't need random access for the list, and two, we can simply use push_front to make sure the component itself is always the first one of its subcomponent list. The registration process seems straightforward, since it's just adding the type index to the corresponding list. But how do we call the register function before the engine/game starts? Again, we wanted to make use of the static initialization phase.","title":"Approaching a Solution"},{"location":"blogs/week-8/#first-try-another-registration-pattern","text":"template < typename Curr , typename Base > class ComponentRegistry { protected : static bool registered ; }; class MeshComponent : public Component , public ComponentRegistry < MeshComponent , Component > { static bool IsRegistered () { return registered ; } // required ... } template < typename Curr , typename Base > bool ComponentRegistry < Curr , Base >:: registered = Component :: Register ( std :: type_index { typeof ( Curr )}, std :: type_index { typeof ( Base )}); Just like what we did for the level registration, we created a template to run the static initialization process. Each component inherited from the template will have a static registered variable which registers the component to the type hierarchy map. It works perfectly well with levels and most components. However, it breaks when a component is inheriting an existing component like this: class Collider : public Component , public ComponentRegistry < Collider , Component > {...} class BoxCollider : public Component , public ComponentRegistry < BoxCollider , Collider > {...} Yes, it can still register the type to the type hierarchy map, but the BoxCollider now contains two registered variables\u2014 one from its base class Collider , and one from the template class ComponentRegistery<BoxCollider, Collider> . There's no way to tell the compiler which is which, and this results in a compilation error!","title":"First Try: Another Registration Pattern"},{"location":"blogs/week-8/#second-try-manual-template-specialization","text":"So let's recap the problem: Every template specialization has the same variable name, registered , and this leads to the problem of having an ambiguous symbol. Since we were using macros in level registration, why not utilize macros further to provide a unique variable name for each component? Or specifically, for each template specialization? #define CREATE_COMPONENT_BEGIN(NAME, BASE) \\ template <> \\ class ISETTA_API_DECLARE \\ ComponentRegistry<class NAME, BASE> { \\ protected: \\ static bool NAME##Registered; \\ }; \\ class ISETTA_API_DECLARE NAME \\ : public BASE, \\ public ComponentRegistry<NAME, BASE> { \\ protected: \\ static bool isRegistered() { return NAME##Registered; } \\ \\ private: #define CREATE_COMPONENT_END(NAME, BASE) \\ } \\ ; \\ bool ComponentRegistry<NAME, BASE>::NAME##Registered = \\ Component::RegisterComponent(std::type_index(typeid(NAME)), \\ std::type_index(typeid(BASE))); Now we are manually specializing the template and defining different registered variables with different names by macros. The ambiguous symbol issue should be all good and ironed out now. But actually, it brought us another issue! When defining the value of the static variable of a fully specialized template, the definition statement 2 should actually be written in the .cpp file instead of the header file. However, this would requires game developers to write two different macros in two different files, which is easily forgettable and could cause quite a bit of headache if forgotten. So we had to find a way to keep the definition statement in the header file.","title":"Second Try: Manual Template Specialization"},{"location":"blogs/week-8/#third-try-introducing-dummy-template-parameter","text":"Since the problem is mainly caused by fully specialized templates, we thought, \"What about adding a dummy parameter to keep the templates partially specialized?\" As long as the template is not fully specialized, the definition statement can be kept in the header file with the other macros! #define CREATE_COMPONENT_BEGIN(NAME, BASE) \\ template <typename Dummy> \\ class ISETTA_API_DECLARE \\ ComponentRegistry<class NAME, BASE, Dummy> { \\ protected: \\ static bool NAME##Registered; \\ }; \\ class ISETTA_API_DECLARE NAME \\ : public BASE, \\ public ComponentRegistry<NAME, BASE, void> { \\ protected: \\ static bool isRegistered() { return NAME##Registered; } \\ \\ private: #define CREATE_COMPONENT_END(NAME, BASE) \\ } \\ ; \\ template <typename Dummy> \\ bool ComponentRegistry<NAME, BASE, Exclude, Dummy>::NAME##Registered = \\ Component::RegisterComponent(std::type_index(typeid(NAME)), \\ std::type_index(typeid(BASE))); This idea might just be stupid enough to work...and it does! With this trick, we can now declare a new component like this: CREATE_COMPONENT_BEGIN ( GUIComponent , Component ) private : ... public : ... CREATE_COMPONENT_END ( GUIComponent , Component ) As we move forward with our engine development, we're becoming more and more of C++ Magicians\u2122!","title":"Third Try: Introducing \"Dummy\" Template Parameter"},{"location":"blogs/week-8/#collisions","text":"","title":"Collisions"},{"location":"blogs/week-8/#collision-handling","text":"So, as promised, we can provide an update on the collision system's development! Unfortunately, it isn't quite as exciting as what we promised last week . We are still working on the BVTree (see last week for more details); since it's just an optimization (although a necessary one!), we put it off again this week to work on other bug fixes that were needed more immediately. The other decision we left from last week was how to handle the collisions once they happen, and we will be answering that this week. The collisions system architecture that remains unchanged from last week is how we determine the event from collisions, i.e. the collision just began, it's continued, or it just ended. After the broad- and narrow-phase 3 determine a intersection between colliders has occurred, the collision-pair is searched for in a hashed collision-set to determine whether the colliders just entered one another, are still intersecting from a previous frame, or, in the case the pair exists in the set but are no longer intersecting, the collision has ended. What wasn't clear from last week, however, was how the collision callbacks (on enter/stay/exit) were called in the respective event. For details on the options we considered, you can read last week 's blog, but our actual implementation now is a variation of having a collisions handler component. The collisions handler component, labeled CollisionHandler , holds the callbacks which other components can subscribe/unsubscribe from. Rather than the handler holding a list of collider pointers like suggested, the Collider component now holds a reference to the CollisionHandler . This is advantageous as compared to a list of colliders because, when checking intersections, the CollisionModule explicitly has Collider pointers, not CollisionHandler pointers. And in comparison with the solution proposed last week, where the list was publicly available for colliders to subscribe to, this system stops colliders that aren't on the same entity or its children from subscribing to the CollisionHandler . So no weird sibling-entity colliders. The CollisionHandler , when enabled, walks down the hierarchy tree looking for Collider and CollisionHandler components; when a CollisionHandler component is found, the depth-first search down that path of children is stopped, and in the case a Collider component is found, that collider's handler is assigned to be the collision handler that was just enabled. When the CollisionHandler is disabled, it walks up the tree to get a reference of some ancestor CollisionHandler , which is then assigned to the children colliders in the same fashion as we describe above for enabling. A couple of other decisions had to be made regarding the collision handling design, as well. When the CollisionModule is iterating through the list of colliders for intersection tests, it now needs to check the CollisionHandler for each collider because detecting collisions between colliders of the same handler seems like a bad day just waiting to happen! This also allows for child colliders to create a complex collision shape with only the basic primitives, which is a pretty neat benefit. While writing about this topic, our team actually broke out into an in-depth discussion about the behaviour when a collider's ancestors are changed. In the case that an entity above the root CollisionHandler (the collision handler that the collider has reference to) in the scene hierarchy was moved, no additional behaviour is needed because the collider reference to the collision handler remains unchanged. But in the case the handler isn't moved but the collider is, the collider now holds a reference to an invalid handler, one not in its ancestor tree! To fix this, we have a callback event for when a parent is changed, OnHierarchyChanged , that the Collider components subscribe to. When that event is fired, each of the colliders walks up the hierarchy tree looking for a CollisionHandler to store a reference of. We even got some early optimizations going with this solution, too: this process can be short circuited by also looking at Collider s while we're walking up the tree and using their reference to the CollisionHandler instead. This system is also still in development because it is tied with the event-messaging system.","title":"Collision Handling"},{"location":"blogs/week-8/#box-capsule-collisions-will-be-the-death-of-us","text":"On another note, the box-capsule collision intersection test was iterated on this week. This particular test was by far the most complicated to rationalize in 3D, and is the scariest with regards to performance. All intersection tests were done geometrically rather than using an algorithm like GJK 4 , which might have been a mistake on our part. Most of the intersection tests had readable/followable code, however some like box-box and box-capsule quickly became large and difficult to follow. By trying to brute-force the box-capsule test with geometry rather than with an algorithm (or maybe just a better understanding of the topic), we were only able to produce a intersection test that works when the capsule line-segment is perpendicular to at least one of the local axes of the box. If none of the three axes of the capsule line-segment are perpendicular to any of the local axes of the box, the test simply doesn't work. This was left as an exercise to the reader\u2026well that's not really fair, but we decided that wasting additional time right now on this case isn't worth it, especially because we don't expect to have this case appear in our target game. It's pretty disappointing for us and you might write off this engine as unusable now (it took you this long to figure that out?), but because of strict time commitment we have to keep moving. It mostly works (see below!) and will work for our game, because the show must go on. Your browser does not support the video tag. This was when we finally got the capsule with 2 non-perpendicular axes working! Your browser does not support the video tag.","title":"Box-Capsule Collisions Will be the Death of Us"},{"location":"blogs/week-8/#raycasting-functionality","text":"The other system developed this week was raycasting. A benefit of doing this after the intersection testing was a lot of the code used in testing intersection was able to be reused for the raycasting system. Something that also arose from the raycasting system was the need of a way to convert a mouse click point (or any point on screen) to a ray, which got us sidetracked from actually adding in raycasting functionality! (Some TODO s for other features were also added to the CameraComponent that would be nice) This wasn't necessary in the basic sense, but it made testing the raycasting functionality much easier and is a nice feature that can be used in other places as well. Your browser does not support the video tag.","title":"Raycasting Functionality"},{"location":"blogs/week-8/#ignoring-collisions","text":"The final feature we started working on this week regarding our collisions system was ignoring collisions and collision layers. Ignoring collisions was a straightforward addition to the CollisionModule by adding a hash set of collider pointers. The rationale of keeping the hash set on the CollisionModule versus the Collider components themselves was to avoid having to search on the components. Since we already have the pair, it is simpler to store on the module. We will also have a collision layer system used to allow groups of entities, or \"layers\", to ignore collisions of entities on another layer rather than having to specify and store collisions of all the other colliders. The actual implementation of layers is still in the works, so before we say too much more, we'll hold off for future weeks for additional updates.","title":"Ignoring Collisions"},{"location":"blogs/week-8/#wrap-up","text":"This was the third week developing the collisions system, and the rationale to choose an existing collision/physics system is becoming more clear. This is a mentally complicated system which can have a serious impact on performance; it's not that just the math that has difficult and complicated parts, but the entire system that can be a complex mess (we are trying our best to avoid this). We can already tell that this system will require more profiling than other systems in the engine. We will be able to better tell once its integrated into our game\u2014more to come in future weeks!","title":"Wrap-up"},{"location":"blogs/week-8/#what-todo-to-die-today-at-a-minute-or-two-til-two","text":"Well, we are over halfway through the project and have no plans on slowing (even if we are a little ). We were talking as a team about how we are leaving a lot of work behind to keep things moving forward, sticking with the motto: If we don't need it now, don't do it now. At some point, we are going to need to go back and revisit our TODO s and fix/clean things up as well as add that functionality because why not. Some TODO s are naturally getting resolved because we needed the functionality, whereas others, as you will see, aren't. We did a grep 5 for TODO s to see how much we are going to have to do later. Here are the stats: We also did some lines-of-code analysis for a recent presentation, and we have to say, the numbers aren't too shabby: ...well, this is if you also take in the external libraries. It looks a lot more like an actual engine when you consider this much code. Ours looks a little more like this: There, that's a little more accurate. We have to admit, 15,000 lines of code isn't all that many for eight weeks, but what's more remarkable about this number is that this is actually an appropriate amount for a legitimate game engine . Like, one that could be used by an indie studio. A pretty significant chunk of what happens in commercial engines nowadays is external libraries and software, and there isn't an exception at the smaller scale. So if you're ever wondering if you should be \"writing your own engine\" when you want to make a game, or just go and use Unity again: Maybe give brewing one up a shot!","title":"\"What TODO to die today, at a minute or two 'til two\""},{"location":"blogs/week-8/#patch-notes","text":"","title":"Patch Notes"},{"location":"blogs/week-8/#exiting-the-engine-loop","text":"Inside of our engine loop, we hard-code an input binding of the Escape key to setting an isGameRunning flag to false . This allows us to exit games by simply hitting the Escape key. However, this also means that if we remove the InputModule from our engine (or possibly even change it), our engine crashes immediately! (And more importantly, the game developer is stuck with an Escape key that exits their game) We removed that hardcoding and obviously found ourselves unable to close any of our games. We anticipated that would happen. But what we didn't anticipate is that the we didn't have any interface for closing our games via the game developer code, and that's a bit of a problem. The solution was straightforward: Add some functionality that the game developer can use for shutting down the engine loop. But as programmers, it's often not the solution that we debate about\u2014it's the implementation. We had a heated debate about the architecture of our engine loop exit interface, in which we covered two options. Option #1 was to make EngineLoop a singleton with a twist; the singleton would not be given to anyone but the first thing that asked for it. Anything that requested the singleton after the first access would receive nothing, almost like a \"first-come first-serve\" methodology. Option #2 was to make EngineLoop a globally accessible singleton 6 , but do validation checks against the functions being run like Start in order to prevent the user from double-starting the engine. To go beyond a simple static singleton class, we added another layer of separation to the problem. You see, our engine developers (i.e. us) need straight-to-the-source access to information about our engine loop. But our game developers (i.e. also us but with arbitrary restrictions) should only have a limited range of \"engine-level\" control, such as starting the entire engine and shutting it down. So following Unity and Unreal 's examples, we created a class Application that has static class functions for this game developer functionality. We then chose Option #2 for our EngineLoop 's implementation due to the fact that running a DLL's main function is much more annoying than just exposing some \"start\" function in that DLL's header files.","title":"Exiting the Engine Loop"},{"location":"blogs/week-8/#hunting-memory-leaks","text":"Our project has had a small chunk of persistent memory leaks for the past couple of weeks. We never bothered to suss them out due to being too busy with other parts of the project, but recently we gained a bit of time to investigate the issue. Our most recent memory leak patch involved some new learnings about how our memory management system was freeing memory\u2014that is, our memory allocator only frees the memory you tell it to free, it does not call the destructor. So anything in our system that allocates memory within itself needs to be destructed before the memory is freed, otherwise those inner memory allocations will never be released! This could have been fixed by making the memory freeing function a template function, but even in our template frenzy, we could recognize that that would balloon our compile time astronomically, so we decided to just be explicit about object destruction instead. And so, we began our hunt for memory leaks as one typically does: by pulling systems out of our engine until a memory leak disappears! This was actually very effective to a point, and it even got us to realize that we had an insufficient engine exiting scheme, which you can read above . We found that upon exiting a level, the Level object was not getting destructed, and within that Level object, we also have a levelRoot object for the uppermost transform in the scene graph hierarchy that was also not getting destructed. That cleared up most of our memory leaks, but there were still a couple left. Via more extracting of systems in our engine, we discovered that our networking module contained the rest of our memory leaks. The first chunk was straightforward to fix; we simply didn't destruct our custom networking allocators (used by our networking library) before shutting down NetworkingModule . But we still had 80 bytes of memory leak going on somewhere , and we couldn't find out where. Our code was clean, as far as we could tell. Well, it actually was. The memory leak was found within our networking library yojimbo , and in the most ironic place possible: yojimbo has memory leak detection functionality, and enabling it caused the memory leak. That functionality has not been useful to us yet, and the fact that it actually caused a problem made it pretty easy to just disable it permanently and rely on our own devices. So we learned a pretty important lesson that day\u2014the programmers behind your libraries aren't perfect either!","title":"Hunting Memory Leaks"},{"location":"blogs/week-8/#coming-soon","text":"We just released our interview with Aras Pranckevicius , where we had a great talk about the history and design of the Unity game engine. This past week we interviewed with Jeff Preshing ( @wadarass and talked about some key skills for engine developers, so keep your eye out for that and the other interviews coming soon.","title":"Coming Soon"},{"location":"blogs/week-8/#resources","text":"From development we are acquiring a lot of great resources from where we learned certain topics, and as much as we try to synthesize things in our blogs you may be interested in the originals, which is why we have a page full of links for you to browse. Originally Published October 26, 2018. A key-value map (or a dictionary) is an abstract data type composed of a collection of (key, value) pairs. \u21a9 Since typeid() doesn't work with incomplete classes, we had to declare this function first in the class and define it after the component class is completed. \u21a9 The broad-phase detection is performed by constructing a tree of simple primitives (either sphere or box) where the complex collider shapes are simplified into these shapes. Then iterating through these checks to determine if the simple shapes are colliding (this is a quicker calculation). The narrow-phase happens after the broad phase by checking the exact shape of the collider which are typically more complex shapes, such as capsules and object-bounding boxes (obb). \u21a9 Gilbert-Johnson-Keerthi ( GJK ) is a distance algorithm method of determing the minimum distance between convex sets. It is used heavily in collision detection systems. Casey Muratori has a great blog on Implementing GJK . \u21a9 grep is a command-line utility for searching plain-text data sets for lines that match a regular expression. Very useful for searching an entire codebase for certain things. \u21a9 The singleton pattern is a software design pattern that restricts the instantiation of a class to one object. This is useful when exactly one object is needed to coordinate actions across the system. \u21a9","title":"Resources"},{"location":"blogs/week-9/","text":"Hello from the West Coast \u00b6 Before we get started, we apologize for the delay with publishing Week 9. The Isetta Team was in Los Angeles this past week trying to wrangle some engine developers for interviews and meetings, so we naturally fell a little behind on our usually strict schedule! That being said, we got a couple of interviews while we were out galavanting in LA, and we hope you anticipate them in the coming weeks! Special thanks to Cort Stratton , Sunil Nayak, and Jibran Khan for helping us figure things out while we were out on the west coast! Byte-Sized Updates \u00b6 Network Transform : Built up the entire NetworkTransform class, including client authority and transform interpolation. Event Messaging System : Implemented the event messaging system with both queued callbacks and immediate callbacks. Collisions - Dynamic AABB Tree : Finally got to work on optimizing the collision tree, and the speed up on the detection was better than expected! Console : Developed a console window for easier debugging and information display. Components : Added several more features to our Component class family, including much of what Unity has for theirs (which is beginning to make sense to us). Network Transform \u00b6 We may have finished our generic network messaging framework, but being able to send messages across the network is only half the battle. The other half is doing cool things with those messages\u2014and in our case this week, synchronizing game state. Networked transform data is just like any other data that you send across some network, except it needs to be sent quite frequently and also happens to be needed on practically every networked object in a game. So with that in mind, we need to focus on a few things when we implement networked transforms in our engine: How much data are we sending? How frequently are we sending that data? How do we keep the right people in control of their own objects, e.g. player characters? There are a few other things to think about, like how do we make all of the data we receive seem smooth (e.g. characters aren't just jumping around all over the place) and what do we really need to send at any given moment, but those are optimizations for the above three questions, so we can consider solutions to those once we already have solutions for the big three. Sending Our First Transform \u00b6 Now that we have our snazzy new components system and our network messaging system, why not use them to our advantage? We can create a message called TransformMessage that carries four things: the position, rotation, and scale of the object, and the object's network ID. The reasoning for the first three should be obvious, but the reasoning for the ID might not be. A couple weeks ago in Network Identities , we created a NetworkId class (previously named NetworkIdentity ) that could be used to identify objects that correspond to one another between two separate computers, since you can't really use pointers over the network. We had only used NetworkId for a contrived spawning/despawning example, but networked transforms were the real motivation behind the network IDs. By sending that across the network along with the transform data, we can figure out who should be receiving the data once the message is received by the other computers. Using our components system, we can also generate a component for our networked transforms, conveniently called NetworkTransform . This will be what's responsible for updating the entity's actual transform with whatever we receive from the network, or on the flip side, sending out the most recent transform to the network. To do so, we need to register some network message handlers that will handle any incoming TransformMessage traffic\u2014we can register these message handlers within the first Start function that runs for any NetworkTransform components, and doing so will create a couple of callbacks: // Client callback code Entity * entity = NetworkManager :: Instance . GetNetworkEntity ( transformMessage -> netId ); if ( entity ) { Transform t = entity -> GetTransform ; t . SetLocalPos ( transformMessage -> localPos ); t . SetLocalScale ( transformMessage -> localScale ); t . SetLocalRot ( transformMessage -> localRot ); } // \u2026 // Server callback code NetworkManager :: Instance . SendAllMessageFromServer < TransformMessage > ( transformMessage ); // ... Pretty basic, right? This is essentially all you need to get started with networking transform data (although you might notice a problem in this code that we'll delve into later ). The final bit is sending out the actual TransformMessage objects. We could just send a new TransformMessage every time we update the component, but we need to start thinking less about the functionality and more about the data now. Each of these TransformMessage objects contains two Vector3 s, one Quaternion , and an int for the network ID. That amounts to 4 bytes per float and int, and 11 floats/ints in total, which is total of 44 bytes. Messages will typically also have some amount of overhead in them, which we can say will be 16 bytes for a round 60 bytes total per message (the actual overhead depends on your packet and message implementation!). Now imagine, if we're sending a TransformMessage every update and our game updates 60 times every second, then we're sending 3.5KB of data every second for every networked object in the game . That doesn't seem like an incredible amount of data, but network bandwidth is a shared resource not only in the game, but across entire local networks. Also, the amount of data you send across the network can often correspond with the latency of the response on other player's machines. With this in mind, we should always try to be practical with the data that we send. The first simple way to do this would be to only send TransformMessage s every N updates, because let's be honest, most of our game objects are not all that important to the feel or activity of the game, and only a couple key objects may ever need fast-as-possible updates for their transforms. An assumption our team makes here is that we won't want to control the network update frequency of individual networked components, so our updateInterval is a class variable of NetworkId which can then be referenced by everything else that uses the networking. Fortunately, this assumption is easy to fix refactor later if we actually do need different update frequencies. Who Owns What \u00b6 Above, we mentioned that there was a problem with the code for the TransformMessage handling. Did you spot it? We'll give you a hint: What happens if everyone sends TransformMessage s for every one of their networked objects? Answer: Nothing good, that's for sure. One of the problems that you can pretty much ignore with just the generic RPC 1 messages is one of client authority , which is the idea that certain clients should have authority over certain networked data. This has multiple levels to it, too. The most basic form is what we'll be doing, where one client gets to \"own\" a particular object and send messages related to that object. Another form of client authority is for information contention, where one player might have successfully shot another player on their local machine, but on the other player's machine, the shot was a miss. Usually what you can default to is that the server is always right (unless you're doing peer-to-peer connections), but in some games you may want to defer to the shooter or the shootee, depending on the game design. Our client authority is very straightforward; every time a networked object is instantiated (i.e. it has a NetworkId component), the NetworkId is also assigned a client index. That client index can then be used to affirm whether or not some machine is the authority over that object before we apply any of the data. That can be done on both the client's side, where the client can check against their own index, or the server's side, where the server can validate the index of the client requesting that the action be performed. This is a trivial check when handling messages: NetworkId * netId = NetworkManager :: Instance . GetNetworkId ( transformMessage -> netId ); if ( ! netId || netId -> HasClientAuthority ) { return ; } To keep ourselves from sending way more data than is needed, we also perform this check inside of NetworkTransform to ensure that we own the networked object and we should indeed be sending this object's transform to the server. With that, we've answered the big three questions and our NetworkTransform s are ready to go! Your browser does not support the video tag. ...it's a little choppy, isn't it? Uh, does this mean we need to increase the number of TransformMessage s we send? Interpolation, or Doing a Lot with a Little \u00b6 Enter interpolation. Interpolation is when you take some data, for example two endpoints, and extrapolate the data between that, such as creating a line with the two endpoints. With interpolated data, you can effectively \"smooth\" out whatever data you have without increasing the amount of data you're receiving or processing. In the two endpoints example, we can smoothly walk from one endpoint to the other thanks to the line that we created. Interpolation is used everywhere in game development because computers are machines of a discrete, 1-or-0 nature but humans operate more continuously (only technically\u2014 humans are more like machines than most of us think ). Graphics and animation use it, audio uses it, and of course, networking uses it. Interpolation is the first optimization that we are going to apply to our solution of the big three questions from above. There are multiple kinds of interpolation that we'll need to know about because of the nature of our data. Most data can be nicely smoothed by linear interpolation , or lerp , which is pretty much the same as the line example from above. Another kind that we'll need to use for our rotations is spherical linear interpolation , or slerp . We won't get into the math ( reference if you're interested), but it's effectively a constant speed movement like lerp but along the edge of a circle's arc. Functions for getting a data point along that range have already been dealt with all the way back in Week 1 , though. What we need to do for our NetworkTransform is the \"over time\" part. The Heftiness of Network Interpolation \u00b6 An important thing to remember is that oftentimes optimizations are really tradeoffs. For speed we may sacrifice space via caching, or for readability we may sacrifice performance. Network optimizations are no different. When we decided to only send infrequent transform data across the network, we also decided to do the work of faking a smooth network experience, and that comes with a significant price. In order to properly interpolate the transform data over time, we need to keep track of additional information. Particularly, we need the previous and target transforms of our networked object. The reason why we need both of them is because our actual transform will be somewhere between the previous and target, and getting rid of one makes it impossible to know where the actual transform should go in that range. For the time being, we'll only focus on positional interpolation as that's the most obvious and straightforward of the trio. We can update our TransformMessage client handler to register these previous and target positions: // \u2026 client callback boilerplate here Transform & t = entity -> GetTransform ; targetPos = t . GetParent -> GetWorldPos + transformMessage -> localPos ; prevPos = t . GetWorldPos ; And we can add another section to the NetworkTransform 's FixedUpdate function that uses a new interpolation float value to determine how far along it is between the previous and target positions: if ( netId -> HasClientAuthority ) { // \u2026 send out the TransformMessage } else if ( interpolation < 1 ) { Transform & t = entity -> GetTransform ; interpolation = max ( interpolation + 1.0 / netId -> updateInterval , 1 ); t . SetLocalPos ( Math :: Vector3 :: Lerp ( prevPos , targetPos , interpolation )); } Every frame, interpolation is being incremented (in this case, by enough that it will be finished with the interpolation by the time the next TransformMessage should be due), then the entity is set to some corresponding position based on interpolation 's new value. If we receive a new TransformMessage but we haven't finished our interpolation, then we simply reset the previous position to exactly where we're currently at and interpolate from there! With this, we can immediately see an improvement in the networked movement (although you may notice the latency being a bit higher): Your browser does not support the video tag. A little developer strife to note here: When we were testing network interpolation, we eventually got to interpolation rotation and ended up hitting a ton of bugs. We thought this was the network code behaving strangely with quaternions, but as it turns out, our Quaternion class was heavily bugged! You can read more about this in the What's Wrong with Our Quaternions section of the Patch Notes. Fewer Messages, Fewer Problems \u00b6 Now that our results are looking pretty good, let's make it more optimal. A flaw with our update frequency optimization is that we're just blindly updating every N frames and asking no questions. Sometimes, this is desirable behavior! But if network bandwidth is precious (and for a lot of gamers, it is), then we should try to make our updating a little more intelligent. An important question to ask ourselves is, how precise do our transforms really need to be across the network? If you're running a networked physics simulation game (God help you), then high precision might be very important. But for most cases, things can be off a little bit and nobody will be the wiser\u2014especially if you factor that into your gameplay code and correct anything that's off when it becomes important (like the direction a character is shooting). A very common and significant network optimization is to add an update distance to your networked transforms. An update distance is just like an update frequency, but for space. If we know that 0.1 units of spatial precision is all we need, then we can check if our current position is 0.1 units away from the last position that we sent across the network, and if it's not, then we just don't send a message yet. On the opposite side of an update distance, there's another tricky fault to consider. What if our game interpolates an object slower than it's receiving TransformMessage s? By our interpolation scheme described above, the object will perpetually fall behind its target position, eventually being egregiously far away from where it actually is on the server. This can be solved by introducing a snapping distance . Whenever we receive a TransformMessage , we check if our current position is further away from the target position than the snapping distance, and if so, we just snap right to the target position. It's not graceful, but it prevents accumulative error in case the interpolation isn't quite fast enough. The Band Breaks Up \u00b6 We reduced the frequency of our TransformMessage s, but they're still kind of big. And by that, what we really mean is that we're sending a lot of useless data with each message. Sure, we could do some technical optimizations like dropping a value in the quaternion since we can reconstruct that part of it , but that's for the hardcore network programmer. No, we're novice network programmers, so we're gonna solve novice problems. For example, breaking up our TransformMessage object. Within a transform, you have the three components of position, rotation, and scale. Each of those is important in representing anything visually, but they don't necessarily change as often as one another. In some games, you may be changing position constantly but never rotation and scale, and in others, you could be changing position and rotation pretty consistently (like a twin-stick shooter\u2014hint, hint). The problem child in almost every game, though, is scale: When is that even changed ? If we remove scale from our TransformMessage object, the message's size now becomes 48 bytes. That reduces our bandwidth usage by 20%! But what do we do about the scale? We can't just leave it the same all the time, because sometimes games really do need to change scale. In this case, we can create a new message class for it called ScaleMessage , which only contains the Vector3 for the scale and an int for the network ID, and all it updates is the scale. Wait, so if our TransformMessage is now only 48 bytes, why don't we also separate position and rotation into PositionMessage and RotationMessage ? Well, we have done that for convenience sake, but it's not necessarily a good idea. Don't forget that every message we create adds a (blindly estimated) 16 bytes of overhead to the total plus any other necessary information, so our ScaleMessage object isn't just the 12 bytes of the scale's Vector3 , it's 32 bytes (because of the message overhead plus the network ID). And if you're developing a twin-stick shooter and you know that position and rotation are almost glued to the hip, then by separating the position and rotation messages, you're adding almost 20 bytes of extra data to the network stream for every two messages! Not to mention you'll have to perform extra processing for each individual message as well. This gets even worse when you need more information in every message! So in the end, we did indeed break TransformMessage into three different message classes to keep things simple, but it's important to consider the usage of these messages and try to optimize by packing things together. Out-of-Order \u00b6 In the midst of all of this serious network interpolation work that we did, we faced some stuttering issues when the networked objects would update their positions; objects would mostly move like they were supposed to, they would sometimes go slightly back for a hot moment then continue as usual. While the interpolation code did still need some work, the stuttering wasn't of the same sort that we were facing there. As it turns out, keeping track of message order can be pretty important. Our assumption was that the most recently received TransformMessage is the most recent message. However, if a TransformMessage is delayed by a second or so due to router magic, and then the local client finally receives that message, that \"new\" message would send our character waaay out of place! So we need to identify which messages were sent later rather than what was received later. Our networking library yojimbo does this with its ordered-reliable messaging, but that messaging system is really meant to be used for things that you need to receive in the correct order so that you can properly process the information. We just need to know whether or not we should be ignoring a given message, so our solution can be simpler and lighter-weight. Each of the transform message classes now contains a timestamp float that indicates when the message was sent last, so all we have to do is check that against our most recently used timestamp to see if it's valid: // Inside the client position callback code NetworkTransform * nt = entity -> GetComponent < NetworkTransform > ; \\ if ( nt && positionMessage -> updateTime < nt -> lastPosMessage ) { \\ return ; \\ } // ... go on to process the message if it's valid This does introduce an extra 4 bytes of size to each of the transform messages, but it's important for correctness. Another thing that can be done to prevent bogus transform messages (which we did do) is to keep track of the most recent timestamps for each networked object and throw out any messages that fail before even sending them to the clients. This can take a good chunk of memory on the server depending on how many networked objects you have, but it will reduce unnecessary network bandwidth even further. Interpolating the Whole Three Yards \u00b6 So we've covered almost everything when it comes to basic networked transforms, but there's one more detail that we haven't gotten to yet. So far, we've only discussed positional interpolation and how that plays out in the code. Rotational and scalar interpolation aren't all the much different when it comes to the math, but a question comes to mind now that we have three different values to interpolate: Who dictates the interpolation? The most simple interpolation scheme would be to keep one interpolation float value and interpolate all of the transform components by that. That's straightforward and lean, as far as memory is concerned. Unfortunately, that poses a problem when you want to \"restart\" the interpolation. Recall from above that when we receive a new transform message, we reset interpolation and then interpolate between the current transform and the received transform. What if we were 80% of the way through interpolating the position, then we got an updated rotation? If we were to reset interpolation and go from there, the last 20% of the positional interpolation would take longer than the first 80% did! To avoid this, we need to track interpolation on a piecewise basis. Again, that makes the NetworkTransform class even heftier, but heftiness is sometimes okay in the name of quality. Now, we have three different interpolation float values that are all completely independent of each other, so if we receive a late rotation update, our character will just do the rotation while its completing its translation in due time: Your browser does not support the video tag. And so, NetworkTransform is finished! We're sure that it will still take a lot of refinement and iteration, but it works as well as we need it to in order to start making true online multiplayer games. We actually have one more note that we actually neglected until the end of its development as well. The speed at which the transform interpolates doesn't have to match the frequency of the messages being sent out, and in fact, sometimes you don't want this to be the case. For that, we have the interpolationFactor variable on our NetworkId class. If set to greater than 1, the interpolation will happen faster, and less than 1 will make it slower. Less than or equal to 0 really should make the interpolation instant, but we can update that next week! Event Messaging System \u00b6 What is an Event Messaging System? \u00b6 Games are inherently event-driven. \u2014 Jason Gregory, Game Engine Architecture, 3 rd Ed. When we are playing games, the progress is often driven by events like killing an enemy or collecting coins. Once the event is raised or triggered, multiple game entities need to respond to that. For example, after you kill an enemy, your score will increase and enemy AI will locate your position. Since there is clearly a variety of in-game events, it's near impossible to keep them as function APIs in the base class like OnExplosion . This Statically Typed Late Function Binding introduces inflexibility by baking the event types into the engine code and by creating an enormous base entity class which contains all possible callbacks, even most entities only override a few of them. The former one also leads to the failure of data-driven design, since events are not extendable. To deal with this problem, we need a more flexible solution, which can dynamically add new events and register corresponding events to them. This is what an event messaging system is meant to be. It is a hub that contains all the events, maps them to the callbacks and handles the raised events together. Storing All Event Data \u00b6 Since it is a hub that stores and handles the data centrally, the first question is: How do we store the data? From our experience, an event is more like a behavior than an object that can be stored somewhere for later queries. Luckily, in the programming world, we can make a behavior like an object. The Command Pattern comes to rescue! In the Gang of Four , the authors describe the Command Pattern as encapsulating a request as an object, thereby letting users parameterize clients with different requests, queue or log requests, and supporting undoable operations. To contextualize it, the Command Pattern is a technique that we use to encapsulate the event behaviors as event objects so that we can store them, queue them, and log them. The two basic factors of an event are its name and its parameters. The name is quite straightforward; it's a string used to tell which event is raised, such as an \"Explosion Event\" or a \"Collecting Event\". The parameters are the tricky part. They can be of various types and the amount is also changeable. Usually, people use union to handle the various types and use a string to indicate which type is this parameter: struct EventParam { std :: string type ; union { int , float , ... } value ; } This old technique is no longer encouraged by modern C++, mostly because of its un-safeness, where safeness is referring to type safety 2 . Any developer using this EventParam can get messy values if they try to read the value as an different type than it is. C++17 provides a safe alternative to use\u2014 std::variant \u2014which will generate a compile time error if the developer fetches data of the wrong type. So now, we have all the data we need to encapsulate an event object, what about the command behavior? This is an easy answer: We are also introducing Observer Pattern into the event object. As mentioned above, one single event can affect a lot of game objects. Due to the need to decouple the massive game objects from the event object, we have to use the Observer Pattern. Also, the nature of this pattern fits the event-driven system perfectly. Because we have already extensively used the Observer Pattern in our game engine, such as with the input, network, and collision modules, we won't talk too much about it again. If you want to know more about this pattern and how it works in the game engine, this article will be very helpful. Queued Callbacks vs Immediate Callbacks \u00b6 After an event is raised, we can easily look up all the callbacks observing this event. But when should we handle the callbacks? Should we handle the event immediately and call all the callbacks, or should we put it into a queue so that we can handle all the events in a specific phase? We can see the use for both of them. For the immediate callbacks, they are simple and responsive. It runs the expected code right after something happens. For queued callbacks, they are less responsive than the immediate ones, but allow for the frame to complete so all objects have a chance to update before the callbacks are called. There is a special queue called a priority queue (or heap) which sorts all of the elements that are inserted into it. This sorting property allows us to add two more parameters: The priority and the frame count when the event is triggered. When the \"Main Character Dies\" event comes with an \"NPC Goes to Sleep\" event, we definitely think the former one is more important to handle. In this case, we should have different priorities for those two events and let the priority queue sort the events for us. Also, with the frame count parameter, we can now say \"raise this event ten frames later\". Since we are not implementing true multi-threading or coroutines, this can help the engine with some asynchronous 3 features. We could do something similar with delaying based on time, however because our update is variable there is no guarantee the event would be raised exactly N seconds later. With this type of inconsistency we decided to hold off from implementing events based on time and to just use the frame count instead. After all that, we have both queued and immediate callbacks in our engine. We provide two different API's so that the developers can choose whatever fits their needs best. Collisions - Dynamic AABB Tree \u00b6 Introducing the Tree \u00b6 In Week 7 , we talked about accelerating our collision system using a DBVT (Dynamic Bounding Volume Tree; this will be referred to as BV Tree in the blog). This week, we finally got the chance to implement it in our engine and the improvement is phenomenal! Your browser does not support the video tag. Without BV Tree, having 80 sphere colliders in the scene results in an average frame time of ~35 ms, or 28 FPS, which is okay but certainly not enough given that we will have lots of zombies in the game. With BV Tree, we can handle more than 200 dynamic colliders in ~15 ms. Alright, alright, enough bragging\u2014how did we do it? First, let's make clear what we are trying to do. Detecting collisions by comparing each possible object pair results in O(n^2) time complexity, but in reality, the chance that all pairs collide with each other is really close to zero. As it might be too hard to reduce the time of each single collision detection (\"Although definitely possible with our implementation,\" says the person who programmed them), what we want is to efficiently find those pairs that can possibly collide with each other. Basically, we want determine if a pair could collide before doing a math intensive check. Our dear friend, BV Tree, can help us with it. With a BV Tree, when we want to find all the other colliders that are colliding with collider A, we just need to walk down from the tree root where we can possibly find another collider that collides with it\u2014more specifically, where collider A is spatially located in the tree. We will talk about how we can do this in detail later. In theory, it reduces time complexity from O(n^2) to O(n logn) . The BV Tree uses bounding volumes to do this, as stated in weeks prior, so we need to decide on the type of bounding volume we wanted to use for the tree. The choices are usually between axis-aligned bounding boxes (AABB) and bounding spheres. An AABB is the smallest box whose edges are lined up with the world's X, Y, and Z axes and can enclose the specified object (in our case, collider primitives). A bounding sphere is similar, just being a sphere instead of a box. They are both pretty easy to construct and detect collisions between, but bounding spheres require additional volume when encapsulating oblong shapes, such as humanoid characters, which we will have more of than anything in our game, so we chose AABB's as our bounding objects. We also found AABB's to be more widely used than bounding spheres through our research (You can look as our references here ! Making the Tree \u00b6 The next step is to design our BV Tree and decide the operations it should support. First, of course, we need to construct the tree. When we were looking at online resources, two of the most common tree construction techniques were the top-down method and the bottom-up method. They apply the best to scenarios where the colliders are known before runtime and won't change a lot. But in our case, the colliders will change their positions a lot, and we won't know where the colliders will be before they get instantiated. So we chose the third popular method: Constructing the tree by inserting colliders into it one by one when their respective OnEnable function is called. And this leads us to the second operation\u2014insertion. Before we dive into how insertion works, let's look at the two most important properties of a BV Tree: Only leaves can contain actual colliders Every branch is guaranteed to have two valid children And so a BV Tree node looks like this: struct Node { Node * parent ; Node * left ; Node * right ; // we know a node is leaf when the collider is null, // or the left/right is null Collider * const collider ; // The leaves' AABB are just the AABB of primitive colliders, // and the branches' AABB are the AABB of both its children. AABB aabb ; }; These two properties make it so easy for us to add and remove colliders from the tree. And thanks to these properties, the logic of adding a collider to the tree is the following (you can refer to our Github repo for actual implementation): Walk down the tree along the direction that causes minimum surface area change (as mentioned in Week 7 blog , we should keep the tree balanced, and a typical balancing heuristic 4 is surface area). Each step is done by trying to enclose the new collider's AABB with the left and right children, and comparing their surface area change after the enclosing operation. After arriving at a leaf, construct a new branch node at that leaf's position, and set the original leaf and the new collider as the two children of the new branch node. Also do some other pointer bookkeeping to make the tree valid. We also need to remove nodes from the tree when collider components are removed or disabled. Removing is also a pretty straightforward operation\u2014just find the node's sibling, and pop it up to the position of its parent. A thing to note is that everytime we modify something at the leaf level, we should pop up level by level to make sure the parent's AABB still contains both children. The last operation for us to cover is updating the tree. When a collider moves, its AABB will also change. We can keep the collider where it is in the tree, in which case the tree still works, but would become unbalanced pretty quickly. Therefore, we remove a collider from the tree when its AABB gets invalidated and then add it back, so we can always have a balanced tree. However in a game where everything is moving all the time, doing this means we need to basically remove all colliders and add them back every frame, because their AABB's get invalidated frequently. To solve this problem, we introduced margins in our AABB, or called with a cute name, \"fat AABB's\". Fat AABBs don't tightly enclose the collision primitive, instead, they leave some margins around the collider. This allows the colliders to move within their fat AABB's for a while before the fat AABB's get invalidated, and thus reduces the need of re-adding colliders. It's worth noting that there is usually not a \"silver bullet\" margin value for AABB's, and we should experiment with it to get the best performance. So we made the margin (or \"fat factor\") a config value. Using the Tree \u00b6 Finally, we have a BV Tree we can use! The final step is to just generate a list of pairs that can possibly collide with one another, and pass that list to our \"narrow phase\" to detect actual collisions. We did this by looking at every collider, walking down the tree, and adding possible collisions to a unordered_set of collider-collider pairs. However, if we do this naively, we may end up having duplicate pairs in the set\u2014for example, we may have both <Collider A, Collider B> and <Collider B, Collider A> . The way we solved this problem is to have a customized hash function for the set like this: struct UnorderedPairHash { template < typename T > std :: size_t operator ( std :: pair < T , T > const & p ) const { return ( std :: hash < T > ( p . first ) ^ std :: hash < T > ( p . second )); } }; So <Collider A, Collider B> and <Collider B, Collider A> will be hashed to the same value and the set will help us prevent the duplicate. Hooking into CollisionModule \u00b6 The clean setup of CollisionModule allowed us to easily integrate the BV Tree into our engine. We just created a BV Tree member variable in CollisionModule , so it can take care of calling the Update and GetPossibleCollisionPairs on the tree. On the other hand, the colliders can add/remove themselves to/from the tree easily as they are already friended to the CollisionModule . So yeah! We got a working dynamic AABB tree! In summary, the operations we needed on the tree are: Add colliders Remove colliders Update colliders Get possible collision pairs What's Next \u00b6 As the tree is integrated into the engine and has a well-defined interface, we can easily iterate on it to improve performance and provide more functionality. We did a little bit of profiling on the tree, and to our surprise, updating the tree took a very small percentage of the time, while getting possible collision pairs took around 80% of the CollisionModule 's update time. Therefore, the next step for us is to optimize the \"getting pairs\" process by either reducing the constant time of each operation or even improving our algorithm to reduce the big O time complexity 5 . Console \u00b6 A console, in-game command-line, logger...the system has many names, but none of them can be seen on our engine architecture diagram. That's because it (sort of) isn't a system in the engine. The console is a great editor tool. Wait, I remember you said you weren't going to write an editor! Well, we still aren't, but as alluded to back in Week 2 , the console is something that will be easy to develop and speed our debugging process. What the console needs to be able to do is output the same messages that are sent to the Visual Studio output window, and filter those messages based on verbosity level (info, warning, error) and channel (general, graphics, networking, etc.). So as part of this first requirement, since all the log messages pass through the Logger class, and we don't want to change how we do the logging, the Logger needs knowledge of the console. But this doesn't seem right, since the Logger is one of our core systems and the console is just a feature; our solution, which is admittedly probably not the best, is to store a function pointer 6 which is called each time the Logger write method is called, and with the same string that is given to the Logger write method. It doesn't use the event messaging system because that requires the Logger , so in the case that nothing is listening to the event, a warning message is sent; this creates an infinitely recursive loop, which is a commonly known allergen for most programmers. It also isn't using a registration system like our input system because we weren't sure any other system would need to be listening to the Logger messages, and why over complicate something when we could just keep it simple? The function pointer stops the Logger system being coupled with anything else, such as the console. So with that, we set off in building out the GUI of the console, which ended up looking like this: The console started as another way for us to flex our abstraction of the GUI system, and in the end it required adding in an additional implementation, GUI::ButtonDropDown (displayed below), that wasn't included with the imported GUI library. Getting the functionality we wanted for filtering messages required us to use the variables already set up in the Logger , verbosity and channel masks. First, when displaying a message, the verbosity level is searched within the string to alter the color of the text, and the index of the message's verbosity is stored. The channel name is searched and stored as well, so based on whether the mask allows that channel and verbosity, the message will be displayed. The console component itself also alters the state of the Logger through config variables, so if info messages aren't displayed in the console, they also won't be displayed in the output window of Visual Studio. However, unlike the output window, whose history can't be changed, i.e. messages that were filtered out at an earlier time cannot be inserted into the output window, messages within the console can. So the entire history of messages can be seen as well as filtered based on the two criteria. As shown: The other functionality of the console is to be able to change configuration variables during runtime. Because of how we had done our CVar system from Week 2 , this implementation was far easier than we had expected. The names of all the CVar s were already stored in a map which can be grabbed on initialization of the console, which happens long after the CVar s have been registered into the CVarRegistry . Then on pattern match, if anything is found followed by an equal sign ( cvar_name=1 ), the configuration system can be called to set the variable. This variable is then changed throughout the game, wherever referenced! But why stop there? We could make it even easier for the developer! To start, let's list out all the commands available with an additional command. This requires the ability for users to define their own commands and callbacks for the console, so the user can create a command by having a keyword associated with a callback function that takes the console instance and string that follows their command, which in turn can be used as input parameters and parsed based on function need. So a command followed by a user delimiter 7 (the pipe symbol \"|\" for the Isetta Engine) denotes a user command and a command followed by a config delimiter \"=\" denotes a config command\u2014this stops users from overriding config commands with their own. The reason to also pass an instance of the console to the callback is so the console's AddLog function can be called within the callback, which will only display information on that console in the case the developer has multiple consoles. The history of the commands are also stored, so a developer can cycle back through old commands with the Up Arrow key. Okay\u2014all of this is neat, with config and user commands and all, but there are so many commands, and even with the feature to list them all out, they can just be tiring in general. So we added an autocomplete feature on pressing the Tab key! The autocomplete works by comparing the characters since the last space with the same number of characters in all the commands. Once all the possible matches are found, the callback then determines if it can fully complete the word in the case of only one match, or it can complete a few more letters until the input word matches all the possible candidate commands, for example the commands \"help\", \"hello\" would be matched up to \"he\" if \"h\" was typed. The commands are currently stored in a vector and map, but the best data structure for this type of comparison would be a trie 8 , so we may think of a way to use that for storage. Luckily, the console isn't a runtime feature so optimization isn't of the utmost importance. Components \u00b6 Awake Myth \u00b6 When we were designing the functions of the engine loop, we decided to have OnEnable , Start , Update , FixedUpdate , OnDisable and OnDestroy functions for the entity and the components. This is the smallest number of functions we thought we would need at that time. The reason we have OnEnable and Start is that some code should be run every time when a component is set active, while some should only be run once throughout its lifetime. Same logic applies to the OnDisable and OnDestroy functions. If you've used Unity before, you will find that it's quite similar with Unity's MonoBehaviour , but with one function missing: The Awake function. We didn't understand before why both Awake and Start are needed. Is that simply because of Unity is providing one additional phase for you to control? The answer is yes and no. This week, we finally got to the point that we thought we should add an Awake into the entities' lifecycle. It's all because of how the Start function works. As discussed in a previous post , we are using a loading script to load the level, in which it creates the entities and attaches components. In that case, all the components attached on one single entity are not attached simultaneously. This leads to the fact that the GetComponent will largely depend on the attachment order if it is called in a function that is called when the component has just been added to one entity, since the component you are querying might not have been added yet. To avoid that ambiguity, we didn't call the Start function when the component is added; instead, we just marked it as NeedStart . After all the components are added, in the beginning of next frame, the engine loop will check if the component needs to be started. If so, the Start function will be run. It works perfectly! Well, it does in most cases. It still introduces delaying frames when we try to add more components inside of the Start function. As the graph shows above, every time we are adding a component in the Start function, it will be brought in one frame late because the Start function can only be called by the CheckStart function, which is only called at the beginning of next frame. How do we deal with this issue? On one hand, we need a function called uniformly after all components are added so that we can correctly call GetComponent , but on the other hand, we want to eliminate the unnecessary AddComponent delay. We also cannot just use the OnEnable function, since it will be called every time the component is set active. The solution, surprisingly, turns out to be the missing Awake function! With the Awake function, we can now easily use the functions to achieve the two features we listed above. In addition, we also understand what might be the reason why Unity has these two \"redundant\" functions! Start Your Engines! \u00b6 Originally, our component startup ordering was this: A component is added to an entity, then when Update is called on the entity, our system checks if Start needs to be run on the component. It's not a terrible waste of time to check that before every Update , so it hasn't been an issue until now. However, when we were working on our NetworkTransform component, we were adding the component to an entity and immediately getting an exception. Digging through the code revealed that one of the other components that we grab within Start was not getting grabbed, and some more digging showed us the error of our ways: The check to call Start was not being run in FixedUpdate . We could have just put the start check at the beginning of FixedUpdate and called it a day, but at that point we would be checking every component if it was added to its entity many times per frame . So we came up with a new scheme where components no longer check to be started inside of Update , but instead are added to a stack of entitiesToStart on the current Level object that is popped from every Update and FixedUpdate . This way, we gain two benefits: We are no longer checking every component if they need to be started every frame, and the entity's Update function no longer controls the Start of components because the Level handles it instead. Preprocessing the Component Hierarchy Tree \u00b6 Last week , we registered the components' hierarchy tree by a static registration function. After the registration, we have the unordered_map that maps every component type to its direct children component types (like mapping Collider to BoxCollider , SphereCollider , and CapsuleCollider ). When GetComponent<T> is called, we do a lookup into the map and find all descendant component types by digging down into the tree and matching the types of components already attached to current entity. This is straightforward but quite expensive, because digging down involves multiple function calls. Since the type hierarchy tree is constructed during the static initialization time, we can always preprocess the tree to meet our needs after static initialization and before it is actually used. The way we optimized the hierarchy tree is to flatten the tree so that the parent component is not only mapped to its direct children components but also mapped to all the descendent components. This preprocessing can greatly reduce the lookup time to find all the descendent while it also increases the space the tree takes up. Thanks to our experience in using Unity, we know that functions like GetComponent are called quite frequently, so we finally decided to sacrifice some of the memory space to contain the type tree. void Component :: FlattenHelper ( std :: type_index parent , std :: type_index curr ) { std :: unordered_map < std :: type_index , std :: list < std :: type_index >>& children = childrenTypes ; std :: list < std :: type_index >* parentList = & children . at ( parent ); std :: list < std :: type_index >* componentList = & children . at ( curr ); for ( auto & childList : * componentList ) { if ( childList != curr ) { parentList -> push_back ( childList ); FlattenHelper ( curr , childList ); } } } The algorithm we are using is depth-first search. The helper function iterates through the direct children list and appends it to its parent component's children list. Then it calls the helper function recursively with itself as the parent component. The result is like this. Your browser does not support the video tag. Checking Components' Uniqueness \u00b6 When we were working on the collision handler, which contains all the collision callbacks and is queried by the collision module for collision tests, we realized that it should be a unique component on each entity. This means that if one single entity has multiple collision handles, it might behave strangely. It's not a good rule to be enforced by the game developer, since it will be hard to debug if the developer happens to have multiple collision handlers on one entity. Thus, we want to enforce this uniqueness requirement from our engine by providing one more option when creating a new component. With that unique option parameter in the template, we now check if the component should be unique on an entity in the registration function. If so, we add said component to a set that contains all the unique components. We also changed the AddComponent function so that it now checks whether the component to be added is inside the unique component set or not. If so, it will do a GetComponent call to see if this type of component has already added to the entity. This uniqueness check can prevent the game developers from having unexpected behavior. Some might ask, \" GetComponent not only returns the template component itself but also its descendent components, so how should the uniqueness work with this inheritance situation?\" To simplify this question, let's say components B and C both inherited from component A; then, in theory, can components B and C both be on the same entity? What about components A and C? Does our solution work for this case? The main point to think about here is, what is inheritance in computer science? It's a is-a relationship! Every child component is of the same type as its base class, but not the same as its sibling class. Now, if we revisit the question, the answer should be straightforward. Since component C is-a component of the same type as A, they should not appear on the same entity, while component B is not is-a (it's not quite as graceful this way) component of the same type as C, so they can both exist on one entity. Does the solution I described before work for this case? You bet it does. If you can go over how we construct the type hierarchy tree and how we check the uniqueness, you will see it works out naturally! The downside of providing this uniqueness check is that it's kind of expensive when there is a deep type-hierarchy tree. This is something that is inevitable, even though we are still trying keep the performance acceptable. The solution for now is to use the parallel algorithm 9 from C++17. The way we are checking if any component attached to the entity has an equal type with the component adding to the entity is to use any_of function. Since the predicator is read-only, it's quite safe to use a parallel_policy to execute this algorithm. Patch Notes \u00b6 What's Wrong with Our Quaternions \u00b6 While we were doing our NetworkTransform work this past week, we needed to do some quaternion math in order to interpolate rotations. When we started that part of the work, we were befuddled; the interpolation was always ridiculously broken, to the extent of having NaN 10 and non-unitary values! Well, as it turns out, it had nothing to do with our networking code and had everything to do with our Quaternion code. You see, when people say that you should unit test, you don't need to necessarily unit test everything \u2014but you really should unit test your most basic classes! This week, we fixed most of our Quaternion math functions because they were incorrectly ordering their elements ( w was placed inconsistently throughout the code), as well as fixed the Quaternion::Slerp function to properly handle interpolations between the exact same rotations. We also had to fix the AngleDeg function due to it calling itself recursively and overflowing the stack. A while ago, we also fixed the AngleAxis function which was using radians instead of degrees. Lesson learned: Test your math libraries! Logger File I/O \u00b6 This week, when one of the team was looking into their log file, they noticed an issue: The log files were all 0kB. Most log files ended up being empty, and very few had actual log lines in them. The answer to why this was is fortunately fairly straightforward. When \"optimizing\" the Logger , the system waits for a config-specified number of bytes to be written to the buffer prior to writing the information to a file. This is to avoid flushing the buffer, opening, and writing to a file each and every log message, which is most certainly costly. However, what this didn't take into account is what happens if you don't reach that buffer limit before ending your session, or worse, before you crash. The fix was to add a shutdown function to the Logger which, regardless of bytes in the buffer, writes to the respective output file. This is then called at the end of our shutdown sequence to ensure each system is able to write to the Logger as needed. The shutdown solved this particular issue, but we admit isn't perfect. The log file still won't be written to when the game crashes the log file won't be written to. We suspect that we can solve this by using a master try-catch statement in our engine loop, but we'll report on that in the coming weeks. Coming Soon \u00b6 This past week we were out in Los Angeles, California interviewing with Jeet Shroff , Florian Strauss , and Elan Ruskin which we will be posting as soon as we can! This also leads perfectly into our other deliverable during this period, which is publishing these interviews into a book through CMU's ETC Press. More details to come soon! With regards to future work, we are coming close to the final moments of the project but still have a lot of work to do... Luckily, we only have 2 major systems left to develop as well as the final sample demo game we showed back in week 0 . Resources \u00b6 The resources page is still going strong, we are trying to link any resource we have found useful throughout our time in development. This can be a great resource to better understand any of the topics we are discussing or a jumpstart to help with your development. Let us know if there is anything with this page we could better organize to help you! Originally Published November 7, 2018. A remote procedure call or RPC is a function that can be called on a computer by another computer across a network. \u21a9 Type safety is when the compiler can check whether the written code is using the right types and whether the language prevents or discourages type errors, errors from mismatching types. \u21a9 Asynchronous is typically associated with parallel programming, and is when a task runs and completes separately from the main application/thread. \u21a9 A heuristic is similar to a rule that an algorithm or program follows as an optimization technique; it is used to arrive at the result with fewer iterations. \u21a9 Big O notation is a mathematical notation that describes the limiting behavior of a function when its argument approaches a value, typically infinity. It's usually used to evaluate the performance of an algorithm. \u21a9 A function pointer is a 4/8 byte pointer, depending on your computer architecture, that points to a virtual table holding a reference to a function; instead of pointing to a variable the pointer points to a function. \u21a9 A delimiter is a sequence of one or more characters used to specify the boundary between separate regions in text or other data streams. An example would be the comma character for comma-separated values. \u21a9 A trie , also known as a radix tree, digital tree, or prefix tree, is a kind of search tree\u2014an ordered tree data used to store a dynamic set or associative array where the keys are usually strings. \u21a9 Parallel programming is when code can be run on multiple threads rather than a linear process, therefore speeding up the execution. A parallel algorithm is an algorithm that can be executed as multiple pieces at a time, being split among multiple threads, then joined at the end to get the result. \u21a9 NaN (not a number) is a numeric data type value that represents an undefined or unrepresentable value, especially in floating-point calculations. \u21a9","title":"[Week 9] Hello from the West Coast"},{"location":"blogs/week-9/#hello-from-the-west-coast","text":"Before we get started, we apologize for the delay with publishing Week 9. The Isetta Team was in Los Angeles this past week trying to wrangle some engine developers for interviews and meetings, so we naturally fell a little behind on our usually strict schedule! That being said, we got a couple of interviews while we were out galavanting in LA, and we hope you anticipate them in the coming weeks! Special thanks to Cort Stratton , Sunil Nayak, and Jibran Khan for helping us figure things out while we were out on the west coast!","title":"Hello from the West Coast"},{"location":"blogs/week-9/#byte-sized-updates","text":"Network Transform : Built up the entire NetworkTransform class, including client authority and transform interpolation. Event Messaging System : Implemented the event messaging system with both queued callbacks and immediate callbacks. Collisions - Dynamic AABB Tree : Finally got to work on optimizing the collision tree, and the speed up on the detection was better than expected! Console : Developed a console window for easier debugging and information display. Components : Added several more features to our Component class family, including much of what Unity has for theirs (which is beginning to make sense to us).","title":"Byte-Sized Updates"},{"location":"blogs/week-9/#network-transform","text":"We may have finished our generic network messaging framework, but being able to send messages across the network is only half the battle. The other half is doing cool things with those messages\u2014and in our case this week, synchronizing game state. Networked transform data is just like any other data that you send across some network, except it needs to be sent quite frequently and also happens to be needed on practically every networked object in a game. So with that in mind, we need to focus on a few things when we implement networked transforms in our engine: How much data are we sending? How frequently are we sending that data? How do we keep the right people in control of their own objects, e.g. player characters? There are a few other things to think about, like how do we make all of the data we receive seem smooth (e.g. characters aren't just jumping around all over the place) and what do we really need to send at any given moment, but those are optimizations for the above three questions, so we can consider solutions to those once we already have solutions for the big three.","title":"Network Transform"},{"location":"blogs/week-9/#sending-our-first-transform","text":"Now that we have our snazzy new components system and our network messaging system, why not use them to our advantage? We can create a message called TransformMessage that carries four things: the position, rotation, and scale of the object, and the object's network ID. The reasoning for the first three should be obvious, but the reasoning for the ID might not be. A couple weeks ago in Network Identities , we created a NetworkId class (previously named NetworkIdentity ) that could be used to identify objects that correspond to one another between two separate computers, since you can't really use pointers over the network. We had only used NetworkId for a contrived spawning/despawning example, but networked transforms were the real motivation behind the network IDs. By sending that across the network along with the transform data, we can figure out who should be receiving the data once the message is received by the other computers. Using our components system, we can also generate a component for our networked transforms, conveniently called NetworkTransform . This will be what's responsible for updating the entity's actual transform with whatever we receive from the network, or on the flip side, sending out the most recent transform to the network. To do so, we need to register some network message handlers that will handle any incoming TransformMessage traffic\u2014we can register these message handlers within the first Start function that runs for any NetworkTransform components, and doing so will create a couple of callbacks: // Client callback code Entity * entity = NetworkManager :: Instance . GetNetworkEntity ( transformMessage -> netId ); if ( entity ) { Transform t = entity -> GetTransform ; t . SetLocalPos ( transformMessage -> localPos ); t . SetLocalScale ( transformMessage -> localScale ); t . SetLocalRot ( transformMessage -> localRot ); } // \u2026 // Server callback code NetworkManager :: Instance . SendAllMessageFromServer < TransformMessage > ( transformMessage ); // ... Pretty basic, right? This is essentially all you need to get started with networking transform data (although you might notice a problem in this code that we'll delve into later ). The final bit is sending out the actual TransformMessage objects. We could just send a new TransformMessage every time we update the component, but we need to start thinking less about the functionality and more about the data now. Each of these TransformMessage objects contains two Vector3 s, one Quaternion , and an int for the network ID. That amounts to 4 bytes per float and int, and 11 floats/ints in total, which is total of 44 bytes. Messages will typically also have some amount of overhead in them, which we can say will be 16 bytes for a round 60 bytes total per message (the actual overhead depends on your packet and message implementation!). Now imagine, if we're sending a TransformMessage every update and our game updates 60 times every second, then we're sending 3.5KB of data every second for every networked object in the game . That doesn't seem like an incredible amount of data, but network bandwidth is a shared resource not only in the game, but across entire local networks. Also, the amount of data you send across the network can often correspond with the latency of the response on other player's machines. With this in mind, we should always try to be practical with the data that we send. The first simple way to do this would be to only send TransformMessage s every N updates, because let's be honest, most of our game objects are not all that important to the feel or activity of the game, and only a couple key objects may ever need fast-as-possible updates for their transforms. An assumption our team makes here is that we won't want to control the network update frequency of individual networked components, so our updateInterval is a class variable of NetworkId which can then be referenced by everything else that uses the networking. Fortunately, this assumption is easy to fix refactor later if we actually do need different update frequencies.","title":"Sending Our First Transform"},{"location":"blogs/week-9/#who-owns-what","text":"Above, we mentioned that there was a problem with the code for the TransformMessage handling. Did you spot it? We'll give you a hint: What happens if everyone sends TransformMessage s for every one of their networked objects? Answer: Nothing good, that's for sure. One of the problems that you can pretty much ignore with just the generic RPC 1 messages is one of client authority , which is the idea that certain clients should have authority over certain networked data. This has multiple levels to it, too. The most basic form is what we'll be doing, where one client gets to \"own\" a particular object and send messages related to that object. Another form of client authority is for information contention, where one player might have successfully shot another player on their local machine, but on the other player's machine, the shot was a miss. Usually what you can default to is that the server is always right (unless you're doing peer-to-peer connections), but in some games you may want to defer to the shooter or the shootee, depending on the game design. Our client authority is very straightforward; every time a networked object is instantiated (i.e. it has a NetworkId component), the NetworkId is also assigned a client index. That client index can then be used to affirm whether or not some machine is the authority over that object before we apply any of the data. That can be done on both the client's side, where the client can check against their own index, or the server's side, where the server can validate the index of the client requesting that the action be performed. This is a trivial check when handling messages: NetworkId * netId = NetworkManager :: Instance . GetNetworkId ( transformMessage -> netId ); if ( ! netId || netId -> HasClientAuthority ) { return ; } To keep ourselves from sending way more data than is needed, we also perform this check inside of NetworkTransform to ensure that we own the networked object and we should indeed be sending this object's transform to the server. With that, we've answered the big three questions and our NetworkTransform s are ready to go! Your browser does not support the video tag. ...it's a little choppy, isn't it? Uh, does this mean we need to increase the number of TransformMessage s we send?","title":"Who Owns What"},{"location":"blogs/week-9/#interpolation-or-doing-a-lot-with-a-little","text":"Enter interpolation. Interpolation is when you take some data, for example two endpoints, and extrapolate the data between that, such as creating a line with the two endpoints. With interpolated data, you can effectively \"smooth\" out whatever data you have without increasing the amount of data you're receiving or processing. In the two endpoints example, we can smoothly walk from one endpoint to the other thanks to the line that we created. Interpolation is used everywhere in game development because computers are machines of a discrete, 1-or-0 nature but humans operate more continuously (only technically\u2014 humans are more like machines than most of us think ). Graphics and animation use it, audio uses it, and of course, networking uses it. Interpolation is the first optimization that we are going to apply to our solution of the big three questions from above. There are multiple kinds of interpolation that we'll need to know about because of the nature of our data. Most data can be nicely smoothed by linear interpolation , or lerp , which is pretty much the same as the line example from above. Another kind that we'll need to use for our rotations is spherical linear interpolation , or slerp . We won't get into the math ( reference if you're interested), but it's effectively a constant speed movement like lerp but along the edge of a circle's arc. Functions for getting a data point along that range have already been dealt with all the way back in Week 1 , though. What we need to do for our NetworkTransform is the \"over time\" part.","title":"Interpolation, or Doing a Lot with a Little"},{"location":"blogs/week-9/#the-heftiness-of-network-interpolation","text":"An important thing to remember is that oftentimes optimizations are really tradeoffs. For speed we may sacrifice space via caching, or for readability we may sacrifice performance. Network optimizations are no different. When we decided to only send infrequent transform data across the network, we also decided to do the work of faking a smooth network experience, and that comes with a significant price. In order to properly interpolate the transform data over time, we need to keep track of additional information. Particularly, we need the previous and target transforms of our networked object. The reason why we need both of them is because our actual transform will be somewhere between the previous and target, and getting rid of one makes it impossible to know where the actual transform should go in that range. For the time being, we'll only focus on positional interpolation as that's the most obvious and straightforward of the trio. We can update our TransformMessage client handler to register these previous and target positions: // \u2026 client callback boilerplate here Transform & t = entity -> GetTransform ; targetPos = t . GetParent -> GetWorldPos + transformMessage -> localPos ; prevPos = t . GetWorldPos ; And we can add another section to the NetworkTransform 's FixedUpdate function that uses a new interpolation float value to determine how far along it is between the previous and target positions: if ( netId -> HasClientAuthority ) { // \u2026 send out the TransformMessage } else if ( interpolation < 1 ) { Transform & t = entity -> GetTransform ; interpolation = max ( interpolation + 1.0 / netId -> updateInterval , 1 ); t . SetLocalPos ( Math :: Vector3 :: Lerp ( prevPos , targetPos , interpolation )); } Every frame, interpolation is being incremented (in this case, by enough that it will be finished with the interpolation by the time the next TransformMessage should be due), then the entity is set to some corresponding position based on interpolation 's new value. If we receive a new TransformMessage but we haven't finished our interpolation, then we simply reset the previous position to exactly where we're currently at and interpolate from there! With this, we can immediately see an improvement in the networked movement (although you may notice the latency being a bit higher): Your browser does not support the video tag. A little developer strife to note here: When we were testing network interpolation, we eventually got to interpolation rotation and ended up hitting a ton of bugs. We thought this was the network code behaving strangely with quaternions, but as it turns out, our Quaternion class was heavily bugged! You can read more about this in the What's Wrong with Our Quaternions section of the Patch Notes.","title":"The Heftiness of Network Interpolation"},{"location":"blogs/week-9/#fewer-messages-fewer-problems","text":"Now that our results are looking pretty good, let's make it more optimal. A flaw with our update frequency optimization is that we're just blindly updating every N frames and asking no questions. Sometimes, this is desirable behavior! But if network bandwidth is precious (and for a lot of gamers, it is), then we should try to make our updating a little more intelligent. An important question to ask ourselves is, how precise do our transforms really need to be across the network? If you're running a networked physics simulation game (God help you), then high precision might be very important. But for most cases, things can be off a little bit and nobody will be the wiser\u2014especially if you factor that into your gameplay code and correct anything that's off when it becomes important (like the direction a character is shooting). A very common and significant network optimization is to add an update distance to your networked transforms. An update distance is just like an update frequency, but for space. If we know that 0.1 units of spatial precision is all we need, then we can check if our current position is 0.1 units away from the last position that we sent across the network, and if it's not, then we just don't send a message yet. On the opposite side of an update distance, there's another tricky fault to consider. What if our game interpolates an object slower than it's receiving TransformMessage s? By our interpolation scheme described above, the object will perpetually fall behind its target position, eventually being egregiously far away from where it actually is on the server. This can be solved by introducing a snapping distance . Whenever we receive a TransformMessage , we check if our current position is further away from the target position than the snapping distance, and if so, we just snap right to the target position. It's not graceful, but it prevents accumulative error in case the interpolation isn't quite fast enough.","title":"Fewer Messages, Fewer Problems"},{"location":"blogs/week-9/#the-band-breaks-up","text":"We reduced the frequency of our TransformMessage s, but they're still kind of big. And by that, what we really mean is that we're sending a lot of useless data with each message. Sure, we could do some technical optimizations like dropping a value in the quaternion since we can reconstruct that part of it , but that's for the hardcore network programmer. No, we're novice network programmers, so we're gonna solve novice problems. For example, breaking up our TransformMessage object. Within a transform, you have the three components of position, rotation, and scale. Each of those is important in representing anything visually, but they don't necessarily change as often as one another. In some games, you may be changing position constantly but never rotation and scale, and in others, you could be changing position and rotation pretty consistently (like a twin-stick shooter\u2014hint, hint). The problem child in almost every game, though, is scale: When is that even changed ? If we remove scale from our TransformMessage object, the message's size now becomes 48 bytes. That reduces our bandwidth usage by 20%! But what do we do about the scale? We can't just leave it the same all the time, because sometimes games really do need to change scale. In this case, we can create a new message class for it called ScaleMessage , which only contains the Vector3 for the scale and an int for the network ID, and all it updates is the scale. Wait, so if our TransformMessage is now only 48 bytes, why don't we also separate position and rotation into PositionMessage and RotationMessage ? Well, we have done that for convenience sake, but it's not necessarily a good idea. Don't forget that every message we create adds a (blindly estimated) 16 bytes of overhead to the total plus any other necessary information, so our ScaleMessage object isn't just the 12 bytes of the scale's Vector3 , it's 32 bytes (because of the message overhead plus the network ID). And if you're developing a twin-stick shooter and you know that position and rotation are almost glued to the hip, then by separating the position and rotation messages, you're adding almost 20 bytes of extra data to the network stream for every two messages! Not to mention you'll have to perform extra processing for each individual message as well. This gets even worse when you need more information in every message! So in the end, we did indeed break TransformMessage into three different message classes to keep things simple, but it's important to consider the usage of these messages and try to optimize by packing things together.","title":"The Band Breaks Up"},{"location":"blogs/week-9/#out-of-order","text":"In the midst of all of this serious network interpolation work that we did, we faced some stuttering issues when the networked objects would update their positions; objects would mostly move like they were supposed to, they would sometimes go slightly back for a hot moment then continue as usual. While the interpolation code did still need some work, the stuttering wasn't of the same sort that we were facing there. As it turns out, keeping track of message order can be pretty important. Our assumption was that the most recently received TransformMessage is the most recent message. However, if a TransformMessage is delayed by a second or so due to router magic, and then the local client finally receives that message, that \"new\" message would send our character waaay out of place! So we need to identify which messages were sent later rather than what was received later. Our networking library yojimbo does this with its ordered-reliable messaging, but that messaging system is really meant to be used for things that you need to receive in the correct order so that you can properly process the information. We just need to know whether or not we should be ignoring a given message, so our solution can be simpler and lighter-weight. Each of the transform message classes now contains a timestamp float that indicates when the message was sent last, so all we have to do is check that against our most recently used timestamp to see if it's valid: // Inside the client position callback code NetworkTransform * nt = entity -> GetComponent < NetworkTransform > ; \\ if ( nt && positionMessage -> updateTime < nt -> lastPosMessage ) { \\ return ; \\ } // ... go on to process the message if it's valid This does introduce an extra 4 bytes of size to each of the transform messages, but it's important for correctness. Another thing that can be done to prevent bogus transform messages (which we did do) is to keep track of the most recent timestamps for each networked object and throw out any messages that fail before even sending them to the clients. This can take a good chunk of memory on the server depending on how many networked objects you have, but it will reduce unnecessary network bandwidth even further.","title":"Out-of-Order"},{"location":"blogs/week-9/#interpolating-the-whole-three-yards","text":"So we've covered almost everything when it comes to basic networked transforms, but there's one more detail that we haven't gotten to yet. So far, we've only discussed positional interpolation and how that plays out in the code. Rotational and scalar interpolation aren't all the much different when it comes to the math, but a question comes to mind now that we have three different values to interpolate: Who dictates the interpolation? The most simple interpolation scheme would be to keep one interpolation float value and interpolate all of the transform components by that. That's straightforward and lean, as far as memory is concerned. Unfortunately, that poses a problem when you want to \"restart\" the interpolation. Recall from above that when we receive a new transform message, we reset interpolation and then interpolate between the current transform and the received transform. What if we were 80% of the way through interpolating the position, then we got an updated rotation? If we were to reset interpolation and go from there, the last 20% of the positional interpolation would take longer than the first 80% did! To avoid this, we need to track interpolation on a piecewise basis. Again, that makes the NetworkTransform class even heftier, but heftiness is sometimes okay in the name of quality. Now, we have three different interpolation float values that are all completely independent of each other, so if we receive a late rotation update, our character will just do the rotation while its completing its translation in due time: Your browser does not support the video tag. And so, NetworkTransform is finished! We're sure that it will still take a lot of refinement and iteration, but it works as well as we need it to in order to start making true online multiplayer games. We actually have one more note that we actually neglected until the end of its development as well. The speed at which the transform interpolates doesn't have to match the frequency of the messages being sent out, and in fact, sometimes you don't want this to be the case. For that, we have the interpolationFactor variable on our NetworkId class. If set to greater than 1, the interpolation will happen faster, and less than 1 will make it slower. Less than or equal to 0 really should make the interpolation instant, but we can update that next week!","title":"Interpolating the Whole Three Yards"},{"location":"blogs/week-9/#event-messaging-system","text":"","title":"Event Messaging System"},{"location":"blogs/week-9/#what-is-an-event-messaging-system","text":"Games are inherently event-driven. \u2014 Jason Gregory, Game Engine Architecture, 3 rd Ed. When we are playing games, the progress is often driven by events like killing an enemy or collecting coins. Once the event is raised or triggered, multiple game entities need to respond to that. For example, after you kill an enemy, your score will increase and enemy AI will locate your position. Since there is clearly a variety of in-game events, it's near impossible to keep them as function APIs in the base class like OnExplosion . This Statically Typed Late Function Binding introduces inflexibility by baking the event types into the engine code and by creating an enormous base entity class which contains all possible callbacks, even most entities only override a few of them. The former one also leads to the failure of data-driven design, since events are not extendable. To deal with this problem, we need a more flexible solution, which can dynamically add new events and register corresponding events to them. This is what an event messaging system is meant to be. It is a hub that contains all the events, maps them to the callbacks and handles the raised events together.","title":"What is an Event Messaging System?"},{"location":"blogs/week-9/#storing-all-event-data","text":"Since it is a hub that stores and handles the data centrally, the first question is: How do we store the data? From our experience, an event is more like a behavior than an object that can be stored somewhere for later queries. Luckily, in the programming world, we can make a behavior like an object. The Command Pattern comes to rescue! In the Gang of Four , the authors describe the Command Pattern as encapsulating a request as an object, thereby letting users parameterize clients with different requests, queue or log requests, and supporting undoable operations. To contextualize it, the Command Pattern is a technique that we use to encapsulate the event behaviors as event objects so that we can store them, queue them, and log them. The two basic factors of an event are its name and its parameters. The name is quite straightforward; it's a string used to tell which event is raised, such as an \"Explosion Event\" or a \"Collecting Event\". The parameters are the tricky part. They can be of various types and the amount is also changeable. Usually, people use union to handle the various types and use a string to indicate which type is this parameter: struct EventParam { std :: string type ; union { int , float , ... } value ; } This old technique is no longer encouraged by modern C++, mostly because of its un-safeness, where safeness is referring to type safety 2 . Any developer using this EventParam can get messy values if they try to read the value as an different type than it is. C++17 provides a safe alternative to use\u2014 std::variant \u2014which will generate a compile time error if the developer fetches data of the wrong type. So now, we have all the data we need to encapsulate an event object, what about the command behavior? This is an easy answer: We are also introducing Observer Pattern into the event object. As mentioned above, one single event can affect a lot of game objects. Due to the need to decouple the massive game objects from the event object, we have to use the Observer Pattern. Also, the nature of this pattern fits the event-driven system perfectly. Because we have already extensively used the Observer Pattern in our game engine, such as with the input, network, and collision modules, we won't talk too much about it again. If you want to know more about this pattern and how it works in the game engine, this article will be very helpful.","title":"Storing All Event Data"},{"location":"blogs/week-9/#queued-callbacks-vs-immediate-callbacks","text":"After an event is raised, we can easily look up all the callbacks observing this event. But when should we handle the callbacks? Should we handle the event immediately and call all the callbacks, or should we put it into a queue so that we can handle all the events in a specific phase? We can see the use for both of them. For the immediate callbacks, they are simple and responsive. It runs the expected code right after something happens. For queued callbacks, they are less responsive than the immediate ones, but allow for the frame to complete so all objects have a chance to update before the callbacks are called. There is a special queue called a priority queue (or heap) which sorts all of the elements that are inserted into it. This sorting property allows us to add two more parameters: The priority and the frame count when the event is triggered. When the \"Main Character Dies\" event comes with an \"NPC Goes to Sleep\" event, we definitely think the former one is more important to handle. In this case, we should have different priorities for those two events and let the priority queue sort the events for us. Also, with the frame count parameter, we can now say \"raise this event ten frames later\". Since we are not implementing true multi-threading or coroutines, this can help the engine with some asynchronous 3 features. We could do something similar with delaying based on time, however because our update is variable there is no guarantee the event would be raised exactly N seconds later. With this type of inconsistency we decided to hold off from implementing events based on time and to just use the frame count instead. After all that, we have both queued and immediate callbacks in our engine. We provide two different API's so that the developers can choose whatever fits their needs best.","title":"Queued Callbacks vs Immediate Callbacks"},{"location":"blogs/week-9/#collisions-dynamic-aabb-tree","text":"","title":"Collisions - Dynamic AABB Tree"},{"location":"blogs/week-9/#introducing-the-tree","text":"In Week 7 , we talked about accelerating our collision system using a DBVT (Dynamic Bounding Volume Tree; this will be referred to as BV Tree in the blog). This week, we finally got the chance to implement it in our engine and the improvement is phenomenal! Your browser does not support the video tag. Without BV Tree, having 80 sphere colliders in the scene results in an average frame time of ~35 ms, or 28 FPS, which is okay but certainly not enough given that we will have lots of zombies in the game. With BV Tree, we can handle more than 200 dynamic colliders in ~15 ms. Alright, alright, enough bragging\u2014how did we do it? First, let's make clear what we are trying to do. Detecting collisions by comparing each possible object pair results in O(n^2) time complexity, but in reality, the chance that all pairs collide with each other is really close to zero. As it might be too hard to reduce the time of each single collision detection (\"Although definitely possible with our implementation,\" says the person who programmed them), what we want is to efficiently find those pairs that can possibly collide with each other. Basically, we want determine if a pair could collide before doing a math intensive check. Our dear friend, BV Tree, can help us with it. With a BV Tree, when we want to find all the other colliders that are colliding with collider A, we just need to walk down from the tree root where we can possibly find another collider that collides with it\u2014more specifically, where collider A is spatially located in the tree. We will talk about how we can do this in detail later. In theory, it reduces time complexity from O(n^2) to O(n logn) . The BV Tree uses bounding volumes to do this, as stated in weeks prior, so we need to decide on the type of bounding volume we wanted to use for the tree. The choices are usually between axis-aligned bounding boxes (AABB) and bounding spheres. An AABB is the smallest box whose edges are lined up with the world's X, Y, and Z axes and can enclose the specified object (in our case, collider primitives). A bounding sphere is similar, just being a sphere instead of a box. They are both pretty easy to construct and detect collisions between, but bounding spheres require additional volume when encapsulating oblong shapes, such as humanoid characters, which we will have more of than anything in our game, so we chose AABB's as our bounding objects. We also found AABB's to be more widely used than bounding spheres through our research (You can look as our references here !","title":"Introducing the Tree"},{"location":"blogs/week-9/#making-the-tree","text":"The next step is to design our BV Tree and decide the operations it should support. First, of course, we need to construct the tree. When we were looking at online resources, two of the most common tree construction techniques were the top-down method and the bottom-up method. They apply the best to scenarios where the colliders are known before runtime and won't change a lot. But in our case, the colliders will change their positions a lot, and we won't know where the colliders will be before they get instantiated. So we chose the third popular method: Constructing the tree by inserting colliders into it one by one when their respective OnEnable function is called. And this leads us to the second operation\u2014insertion. Before we dive into how insertion works, let's look at the two most important properties of a BV Tree: Only leaves can contain actual colliders Every branch is guaranteed to have two valid children And so a BV Tree node looks like this: struct Node { Node * parent ; Node * left ; Node * right ; // we know a node is leaf when the collider is null, // or the left/right is null Collider * const collider ; // The leaves' AABB are just the AABB of primitive colliders, // and the branches' AABB are the AABB of both its children. AABB aabb ; }; These two properties make it so easy for us to add and remove colliders from the tree. And thanks to these properties, the logic of adding a collider to the tree is the following (you can refer to our Github repo for actual implementation): Walk down the tree along the direction that causes minimum surface area change (as mentioned in Week 7 blog , we should keep the tree balanced, and a typical balancing heuristic 4 is surface area). Each step is done by trying to enclose the new collider's AABB with the left and right children, and comparing their surface area change after the enclosing operation. After arriving at a leaf, construct a new branch node at that leaf's position, and set the original leaf and the new collider as the two children of the new branch node. Also do some other pointer bookkeeping to make the tree valid. We also need to remove nodes from the tree when collider components are removed or disabled. Removing is also a pretty straightforward operation\u2014just find the node's sibling, and pop it up to the position of its parent. A thing to note is that everytime we modify something at the leaf level, we should pop up level by level to make sure the parent's AABB still contains both children. The last operation for us to cover is updating the tree. When a collider moves, its AABB will also change. We can keep the collider where it is in the tree, in which case the tree still works, but would become unbalanced pretty quickly. Therefore, we remove a collider from the tree when its AABB gets invalidated and then add it back, so we can always have a balanced tree. However in a game where everything is moving all the time, doing this means we need to basically remove all colliders and add them back every frame, because their AABB's get invalidated frequently. To solve this problem, we introduced margins in our AABB, or called with a cute name, \"fat AABB's\". Fat AABBs don't tightly enclose the collision primitive, instead, they leave some margins around the collider. This allows the colliders to move within their fat AABB's for a while before the fat AABB's get invalidated, and thus reduces the need of re-adding colliders. It's worth noting that there is usually not a \"silver bullet\" margin value for AABB's, and we should experiment with it to get the best performance. So we made the margin (or \"fat factor\") a config value.","title":"Making the Tree"},{"location":"blogs/week-9/#using-the-tree","text":"Finally, we have a BV Tree we can use! The final step is to just generate a list of pairs that can possibly collide with one another, and pass that list to our \"narrow phase\" to detect actual collisions. We did this by looking at every collider, walking down the tree, and adding possible collisions to a unordered_set of collider-collider pairs. However, if we do this naively, we may end up having duplicate pairs in the set\u2014for example, we may have both <Collider A, Collider B> and <Collider B, Collider A> . The way we solved this problem is to have a customized hash function for the set like this: struct UnorderedPairHash { template < typename T > std :: size_t operator ( std :: pair < T , T > const & p ) const { return ( std :: hash < T > ( p . first ) ^ std :: hash < T > ( p . second )); } }; So <Collider A, Collider B> and <Collider B, Collider A> will be hashed to the same value and the set will help us prevent the duplicate.","title":"Using the Tree"},{"location":"blogs/week-9/#hooking-into-collisionmodule","text":"The clean setup of CollisionModule allowed us to easily integrate the BV Tree into our engine. We just created a BV Tree member variable in CollisionModule , so it can take care of calling the Update and GetPossibleCollisionPairs on the tree. On the other hand, the colliders can add/remove themselves to/from the tree easily as they are already friended to the CollisionModule . So yeah! We got a working dynamic AABB tree! In summary, the operations we needed on the tree are: Add colliders Remove colliders Update colliders Get possible collision pairs","title":"Hooking into CollisionModule"},{"location":"blogs/week-9/#whats-next","text":"As the tree is integrated into the engine and has a well-defined interface, we can easily iterate on it to improve performance and provide more functionality. We did a little bit of profiling on the tree, and to our surprise, updating the tree took a very small percentage of the time, while getting possible collision pairs took around 80% of the CollisionModule 's update time. Therefore, the next step for us is to optimize the \"getting pairs\" process by either reducing the constant time of each operation or even improving our algorithm to reduce the big O time complexity 5 .","title":"What's Next"},{"location":"blogs/week-9/#console","text":"A console, in-game command-line, logger...the system has many names, but none of them can be seen on our engine architecture diagram. That's because it (sort of) isn't a system in the engine. The console is a great editor tool. Wait, I remember you said you weren't going to write an editor! Well, we still aren't, but as alluded to back in Week 2 , the console is something that will be easy to develop and speed our debugging process. What the console needs to be able to do is output the same messages that are sent to the Visual Studio output window, and filter those messages based on verbosity level (info, warning, error) and channel (general, graphics, networking, etc.). So as part of this first requirement, since all the log messages pass through the Logger class, and we don't want to change how we do the logging, the Logger needs knowledge of the console. But this doesn't seem right, since the Logger is one of our core systems and the console is just a feature; our solution, which is admittedly probably not the best, is to store a function pointer 6 which is called each time the Logger write method is called, and with the same string that is given to the Logger write method. It doesn't use the event messaging system because that requires the Logger , so in the case that nothing is listening to the event, a warning message is sent; this creates an infinitely recursive loop, which is a commonly known allergen for most programmers. It also isn't using a registration system like our input system because we weren't sure any other system would need to be listening to the Logger messages, and why over complicate something when we could just keep it simple? The function pointer stops the Logger system being coupled with anything else, such as the console. So with that, we set off in building out the GUI of the console, which ended up looking like this: The console started as another way for us to flex our abstraction of the GUI system, and in the end it required adding in an additional implementation, GUI::ButtonDropDown (displayed below), that wasn't included with the imported GUI library. Getting the functionality we wanted for filtering messages required us to use the variables already set up in the Logger , verbosity and channel masks. First, when displaying a message, the verbosity level is searched within the string to alter the color of the text, and the index of the message's verbosity is stored. The channel name is searched and stored as well, so based on whether the mask allows that channel and verbosity, the message will be displayed. The console component itself also alters the state of the Logger through config variables, so if info messages aren't displayed in the console, they also won't be displayed in the output window of Visual Studio. However, unlike the output window, whose history can't be changed, i.e. messages that were filtered out at an earlier time cannot be inserted into the output window, messages within the console can. So the entire history of messages can be seen as well as filtered based on the two criteria. As shown: The other functionality of the console is to be able to change configuration variables during runtime. Because of how we had done our CVar system from Week 2 , this implementation was far easier than we had expected. The names of all the CVar s were already stored in a map which can be grabbed on initialization of the console, which happens long after the CVar s have been registered into the CVarRegistry . Then on pattern match, if anything is found followed by an equal sign ( cvar_name=1 ), the configuration system can be called to set the variable. This variable is then changed throughout the game, wherever referenced! But why stop there? We could make it even easier for the developer! To start, let's list out all the commands available with an additional command. This requires the ability for users to define their own commands and callbacks for the console, so the user can create a command by having a keyword associated with a callback function that takes the console instance and string that follows their command, which in turn can be used as input parameters and parsed based on function need. So a command followed by a user delimiter 7 (the pipe symbol \"|\" for the Isetta Engine) denotes a user command and a command followed by a config delimiter \"=\" denotes a config command\u2014this stops users from overriding config commands with their own. The reason to also pass an instance of the console to the callback is so the console's AddLog function can be called within the callback, which will only display information on that console in the case the developer has multiple consoles. The history of the commands are also stored, so a developer can cycle back through old commands with the Up Arrow key. Okay\u2014all of this is neat, with config and user commands and all, but there are so many commands, and even with the feature to list them all out, they can just be tiring in general. So we added an autocomplete feature on pressing the Tab key! The autocomplete works by comparing the characters since the last space with the same number of characters in all the commands. Once all the possible matches are found, the callback then determines if it can fully complete the word in the case of only one match, or it can complete a few more letters until the input word matches all the possible candidate commands, for example the commands \"help\", \"hello\" would be matched up to \"he\" if \"h\" was typed. The commands are currently stored in a vector and map, but the best data structure for this type of comparison would be a trie 8 , so we may think of a way to use that for storage. Luckily, the console isn't a runtime feature so optimization isn't of the utmost importance.","title":"Console"},{"location":"blogs/week-9/#components","text":"","title":"Components"},{"location":"blogs/week-9/#awake-myth","text":"When we were designing the functions of the engine loop, we decided to have OnEnable , Start , Update , FixedUpdate , OnDisable and OnDestroy functions for the entity and the components. This is the smallest number of functions we thought we would need at that time. The reason we have OnEnable and Start is that some code should be run every time when a component is set active, while some should only be run once throughout its lifetime. Same logic applies to the OnDisable and OnDestroy functions. If you've used Unity before, you will find that it's quite similar with Unity's MonoBehaviour , but with one function missing: The Awake function. We didn't understand before why both Awake and Start are needed. Is that simply because of Unity is providing one additional phase for you to control? The answer is yes and no. This week, we finally got to the point that we thought we should add an Awake into the entities' lifecycle. It's all because of how the Start function works. As discussed in a previous post , we are using a loading script to load the level, in which it creates the entities and attaches components. In that case, all the components attached on one single entity are not attached simultaneously. This leads to the fact that the GetComponent will largely depend on the attachment order if it is called in a function that is called when the component has just been added to one entity, since the component you are querying might not have been added yet. To avoid that ambiguity, we didn't call the Start function when the component is added; instead, we just marked it as NeedStart . After all the components are added, in the beginning of next frame, the engine loop will check if the component needs to be started. If so, the Start function will be run. It works perfectly! Well, it does in most cases. It still introduces delaying frames when we try to add more components inside of the Start function. As the graph shows above, every time we are adding a component in the Start function, it will be brought in one frame late because the Start function can only be called by the CheckStart function, which is only called at the beginning of next frame. How do we deal with this issue? On one hand, we need a function called uniformly after all components are added so that we can correctly call GetComponent , but on the other hand, we want to eliminate the unnecessary AddComponent delay. We also cannot just use the OnEnable function, since it will be called every time the component is set active. The solution, surprisingly, turns out to be the missing Awake function! With the Awake function, we can now easily use the functions to achieve the two features we listed above. In addition, we also understand what might be the reason why Unity has these two \"redundant\" functions!","title":"Awake Myth"},{"location":"blogs/week-9/#start-your-engines","text":"Originally, our component startup ordering was this: A component is added to an entity, then when Update is called on the entity, our system checks if Start needs to be run on the component. It's not a terrible waste of time to check that before every Update , so it hasn't been an issue until now. However, when we were working on our NetworkTransform component, we were adding the component to an entity and immediately getting an exception. Digging through the code revealed that one of the other components that we grab within Start was not getting grabbed, and some more digging showed us the error of our ways: The check to call Start was not being run in FixedUpdate . We could have just put the start check at the beginning of FixedUpdate and called it a day, but at that point we would be checking every component if it was added to its entity many times per frame . So we came up with a new scheme where components no longer check to be started inside of Update , but instead are added to a stack of entitiesToStart on the current Level object that is popped from every Update and FixedUpdate . This way, we gain two benefits: We are no longer checking every component if they need to be started every frame, and the entity's Update function no longer controls the Start of components because the Level handles it instead.","title":"Start Your Engines!"},{"location":"blogs/week-9/#preprocessing-the-component-hierarchy-tree","text":"Last week , we registered the components' hierarchy tree by a static registration function. After the registration, we have the unordered_map that maps every component type to its direct children component types (like mapping Collider to BoxCollider , SphereCollider , and CapsuleCollider ). When GetComponent<T> is called, we do a lookup into the map and find all descendant component types by digging down into the tree and matching the types of components already attached to current entity. This is straightforward but quite expensive, because digging down involves multiple function calls. Since the type hierarchy tree is constructed during the static initialization time, we can always preprocess the tree to meet our needs after static initialization and before it is actually used. The way we optimized the hierarchy tree is to flatten the tree so that the parent component is not only mapped to its direct children components but also mapped to all the descendent components. This preprocessing can greatly reduce the lookup time to find all the descendent while it also increases the space the tree takes up. Thanks to our experience in using Unity, we know that functions like GetComponent are called quite frequently, so we finally decided to sacrifice some of the memory space to contain the type tree. void Component :: FlattenHelper ( std :: type_index parent , std :: type_index curr ) { std :: unordered_map < std :: type_index , std :: list < std :: type_index >>& children = childrenTypes ; std :: list < std :: type_index >* parentList = & children . at ( parent ); std :: list < std :: type_index >* componentList = & children . at ( curr ); for ( auto & childList : * componentList ) { if ( childList != curr ) { parentList -> push_back ( childList ); FlattenHelper ( curr , childList ); } } } The algorithm we are using is depth-first search. The helper function iterates through the direct children list and appends it to its parent component's children list. Then it calls the helper function recursively with itself as the parent component. The result is like this. Your browser does not support the video tag.","title":"Preprocessing the Component Hierarchy Tree"},{"location":"blogs/week-9/#checking-components-uniqueness","text":"When we were working on the collision handler, which contains all the collision callbacks and is queried by the collision module for collision tests, we realized that it should be a unique component on each entity. This means that if one single entity has multiple collision handles, it might behave strangely. It's not a good rule to be enforced by the game developer, since it will be hard to debug if the developer happens to have multiple collision handlers on one entity. Thus, we want to enforce this uniqueness requirement from our engine by providing one more option when creating a new component. With that unique option parameter in the template, we now check if the component should be unique on an entity in the registration function. If so, we add said component to a set that contains all the unique components. We also changed the AddComponent function so that it now checks whether the component to be added is inside the unique component set or not. If so, it will do a GetComponent call to see if this type of component has already added to the entity. This uniqueness check can prevent the game developers from having unexpected behavior. Some might ask, \" GetComponent not only returns the template component itself but also its descendent components, so how should the uniqueness work with this inheritance situation?\" To simplify this question, let's say components B and C both inherited from component A; then, in theory, can components B and C both be on the same entity? What about components A and C? Does our solution work for this case? The main point to think about here is, what is inheritance in computer science? It's a is-a relationship! Every child component is of the same type as its base class, but not the same as its sibling class. Now, if we revisit the question, the answer should be straightforward. Since component C is-a component of the same type as A, they should not appear on the same entity, while component B is not is-a (it's not quite as graceful this way) component of the same type as C, so they can both exist on one entity. Does the solution I described before work for this case? You bet it does. If you can go over how we construct the type hierarchy tree and how we check the uniqueness, you will see it works out naturally! The downside of providing this uniqueness check is that it's kind of expensive when there is a deep type-hierarchy tree. This is something that is inevitable, even though we are still trying keep the performance acceptable. The solution for now is to use the parallel algorithm 9 from C++17. The way we are checking if any component attached to the entity has an equal type with the component adding to the entity is to use any_of function. Since the predicator is read-only, it's quite safe to use a parallel_policy to execute this algorithm.","title":"Checking Components' Uniqueness"},{"location":"blogs/week-9/#patch-notes","text":"","title":"Patch Notes"},{"location":"blogs/week-9/#whats-wrong-with-our-quaternions","text":"While we were doing our NetworkTransform work this past week, we needed to do some quaternion math in order to interpolate rotations. When we started that part of the work, we were befuddled; the interpolation was always ridiculously broken, to the extent of having NaN 10 and non-unitary values! Well, as it turns out, it had nothing to do with our networking code and had everything to do with our Quaternion code. You see, when people say that you should unit test, you don't need to necessarily unit test everything \u2014but you really should unit test your most basic classes! This week, we fixed most of our Quaternion math functions because they were incorrectly ordering their elements ( w was placed inconsistently throughout the code), as well as fixed the Quaternion::Slerp function to properly handle interpolations between the exact same rotations. We also had to fix the AngleDeg function due to it calling itself recursively and overflowing the stack. A while ago, we also fixed the AngleAxis function which was using radians instead of degrees. Lesson learned: Test your math libraries!","title":"What's Wrong with Our Quaternions"},{"location":"blogs/week-9/#logger-file-io","text":"This week, when one of the team was looking into their log file, they noticed an issue: The log files were all 0kB. Most log files ended up being empty, and very few had actual log lines in them. The answer to why this was is fortunately fairly straightforward. When \"optimizing\" the Logger , the system waits for a config-specified number of bytes to be written to the buffer prior to writing the information to a file. This is to avoid flushing the buffer, opening, and writing to a file each and every log message, which is most certainly costly. However, what this didn't take into account is what happens if you don't reach that buffer limit before ending your session, or worse, before you crash. The fix was to add a shutdown function to the Logger which, regardless of bytes in the buffer, writes to the respective output file. This is then called at the end of our shutdown sequence to ensure each system is able to write to the Logger as needed. The shutdown solved this particular issue, but we admit isn't perfect. The log file still won't be written to when the game crashes the log file won't be written to. We suspect that we can solve this by using a master try-catch statement in our engine loop, but we'll report on that in the coming weeks.","title":"Logger File I/O"},{"location":"blogs/week-9/#coming-soon","text":"This past week we were out in Los Angeles, California interviewing with Jeet Shroff , Florian Strauss , and Elan Ruskin which we will be posting as soon as we can! This also leads perfectly into our other deliverable during this period, which is publishing these interviews into a book through CMU's ETC Press. More details to come soon! With regards to future work, we are coming close to the final moments of the project but still have a lot of work to do... Luckily, we only have 2 major systems left to develop as well as the final sample demo game we showed back in week 0 .","title":"Coming Soon"},{"location":"blogs/week-9/#resources","text":"The resources page is still going strong, we are trying to link any resource we have found useful throughout our time in development. This can be a great resource to better understand any of the topics we are discussing or a jumpstart to help with your development. Let us know if there is anything with this page we could better organize to help you! Originally Published November 7, 2018. A remote procedure call or RPC is a function that can be called on a computer by another computer across a network. \u21a9 Type safety is when the compiler can check whether the written code is using the right types and whether the language prevents or discourages type errors, errors from mismatching types. \u21a9 Asynchronous is typically associated with parallel programming, and is when a task runs and completes separately from the main application/thread. \u21a9 A heuristic is similar to a rule that an algorithm or program follows as an optimization technique; it is used to arrive at the result with fewer iterations. \u21a9 Big O notation is a mathematical notation that describes the limiting behavior of a function when its argument approaches a value, typically infinity. It's usually used to evaluate the performance of an algorithm. \u21a9 A function pointer is a 4/8 byte pointer, depending on your computer architecture, that points to a virtual table holding a reference to a function; instead of pointing to a variable the pointer points to a function. \u21a9 A delimiter is a sequence of one or more characters used to specify the boundary between separate regions in text or other data streams. An example would be the comma character for comma-separated values. \u21a9 A trie , also known as a radix tree, digital tree, or prefix tree, is a kind of search tree\u2014an ordered tree data used to store a dynamic set or associative array where the keys are usually strings. \u21a9 Parallel programming is when code can be run on multiple threads rather than a linear process, therefore speeding up the execution. A parallel algorithm is an algorithm that can be executed as multiple pieces at a time, being split among multiple threads, then joined at the end to get the result. \u21a9 NaN (not a number) is a numeric data type value that represents an undefined or unrepresentable value, especially in floating-point calculations. \u21a9","title":"Resources"},{"location":"compendium/","text":"What is a \"Compendium\"? \u00b6 A compendium is a collection of concise but detailed information about a particular subject, like a comprehensive summary. For example, the encyclopedia can be referred to as a compendium of all human knowledge. The Isetta Compendium is composed of all of our different domains of work, ranging from our Core systems all the way up through our high-level Gameplay systems. It includes the information covered in each of our weekly blogs for these domains, all of the relevant interviewee discussions related to the domain, and a postmortem from the team where we look back on our work and talk about our key learnings. If you're interested in a particular subsystem, and especially if you're hoping to implement that system yourself, then this compendium may be useful to you. This section also serves as an index into the rest of our website's content depending on the engine subsystem.","title":"Introduction"},{"location":"compendium/#what-is-a-compendium","text":"A compendium is a collection of concise but detailed information about a particular subject, like a comprehensive summary. For example, the encyclopedia can be referred to as a compendium of all human knowledge. The Isetta Compendium is composed of all of our different domains of work, ranging from our Core systems all the way up through our high-level Gameplay systems. It includes the information covered in each of our weekly blogs for these domains, all of the relevant interviewee discussions related to the domain, and a postmortem from the team where we look back on our work and talk about our key learnings. If you're interested in a particular subsystem, and especially if you're hoping to implement that system yourself, then this compendium may be useful to you. This section also serves as an index into the rest of our website's content depending on the engine subsystem.","title":"What is a \"Compendium\"?"},{"location":"compendium/3rdParty/","text":"3 rd Party \u00b6 Timeline \u00b6 Week 1 \u00b6 Rendering: Ogre vs Horde3D : We narrowed down to two choices, Ogre and Horde3D . While Ogre is powerful and well-known, we decided on Horde3D due to the ease of building it and its lightweight design. Then we made a demo to prove we could! Audio: OpenAL Soft vs FMOD : For audio, our two choices were OpenAL Soft and FMOD . We tried out OpenAL Soft due to it being open source, but we were confused with it even three hours after working at it. On the other hand, FMOD was a breeze to use and had ample documentation. So we jumped right into it and implemented our audio engine using FMOD! Networking: GameNetworkingSockets vs yojimbo : See the Networking Compendium for more details! String ID : String hashing is a very helpful utility that allows strings to use less memory and serve better as keys for other values, so we found an open-source string ID library to bring in. Week 2 \u00b6 Making our Rendering API : After settling on Horde3D, we wrapped it up inside our engine as RenderModule , and for windows , we wrapped GLFW as WindowModule . Our initial module design was a bust, but we learned to keep things flat and managed in one place early on. Week 3 \u00b6 Input Implementation : We implemented our InputModule by wrapping the basic GLFW functionality with a couple of our own interfaces, namely a polling method and a callback registration method. Week 5 \u00b6 Ooey GUI : As it turns out, Horde3D has no GUI solutions, so we looked into ImGui and Qt . ImGui had a more lenient usage policy, and we could tell it was in very active development, so we went ahead with that. ImGui Stands for Something? : After choosing a library, we needed to decide on how developers would use our GUI system. We took advice from Amandine Coget and Walt Destler and went with a immediate mode GUI implementation, then quickly realized how important making a game is for proving which GUI features you actually need! Horde3D Turns Out to Be a Pain : We discovered that \"lightweight\" sometimes means \"missing features\" when Horde3D revealed that it does not expose its texture loading functionality (needed by our GUI system), attempts to load some resources during runtime , and won't let us control its memory . Week 10 \u00b6 We Expose Textures! : We complained about textures five weeks ago, and we felt that had dealt with Horde3D's oppressive control of the textures for long enough, so we whipped up our own OpenGL texture loader and hooked that in! Patch Notes: Modifying Input with Modifier Keys : We originally didn't have modifier keys like Shift and Ctrl implemented with our input system, but when we started to add debug features, we quickly realized the necessity of specialized key combinations for input. Week 11 \u00b6 Font Frenzy : Up until now, our text was always tiny and boring, so we decided to replace it with some better font. As it turns out, we didn't have enough font functionality in our engine! We reworked most of font implementation and ended up with some pretty results. Week 12 \u00b6 How Can Audio Be 3D If Our Ears Are 2D? : Audio is often left until the end on game projects, and the Isetta Engine is no different! We found that our audio system wasn't quite enough, so we implemented spatialized audio and reworked the rest of our module to be more effective and usable. Relevant Interviews \u00b6 Jeff Preshing \u00b6 Creating Your First Engine Systems Integration Martin Middleton \u00b6 The Spectrum of Engine Development Amandine Coget \u00b6 GUI: Immediate vs Retained Modes Tommy Refenes \u00b6 Using 3 rd Party Libraries Adam Serdar \u00b6 Integrated Libraries into an Engine Postmortem \u00b6 Research the libraries you'd consider using, then have a night's sleep before choosing to use one. Or even better, take some time to use the library ( not integrate it) in a separate test environment! Many of the libraries that we used for our engine were paired with some test projects, so we were able to go in and rummage around for a while before we made the leap and brought them into the engine. In doing so, we got to see (1) what are the most commonly used parts of the library, (2) what are the quirky or strange parts of the library, and (3) how comprehensive is the documentation. This stage was great for us to scour the internet for forums or documentation pages related to what we wanted out of the library, and if we struggled to find what we needed, then it was a good sign that the library might not be right for us. However, even after doing all of this, don't feel like you're stuck with a library that you've chosen just because you've spent some time with it. Long-term usage of a library will reveal much more of the negatives than short-term usage will. In our case, we first tried using the GameNetworkingSockets library for our networking solution, and we thought that it mostly had what we wanted after we ran the test project. Later, we discovered a lot of commented-out code and grew much more suspicious of missing functionality that we might need, so we jumped ship over to yojimbo . We can't say for sure whether GameNetworkingSockets would have been a poor choice, but yojimbo has turned out nicely for our engine, so we certainly didn't lose anything by being cautious like this! Create an abstraction layer for your game developers\u2014don't just expose the library. Most libraries that you use will not follow the same semantics, conventions, and guidelines as your own engine's API, and keeping that consistent is important for the usability of the engine. This will still be useful even if you have to do it for over 100 functions; in our case for Dear, Imgui, we had to abstract out quite a few functions and classes just to keep them within our own style. The result was a seamless GUI API from a 3 rd party library that we brought into our engine! Without that, we likely would have had a much harder time utilizing our engine's GUI functionality since its API conventions would differ from the rest of the engine's API. Your first abstraction will look a lot like the library that you're abstracting, which is okay; it doesn't need to look different until you bring in another library for that system as well. This is another good reason to create an abstraction layer: You can much more easily pull out your previous 3 rd party library without affecting the games that are using that module. You won't be able to change the return values or function signatures, of course, but you can change anything about the implementation and the game developer should hardly be able to tell the difference! More things to know: Integrating multiple rendering libraries isn't pleasant, but it may be necessary depending on what you use. The best steps you can follow would be to get each library working separately then slowly integrate one into another in small, testable chunks. Your engine doesn't need all of the features a library provides, so don't try to expose everything that you think is necessary; only expose things that you see as immediately being needed for the engine. We made this mistake with our GUI library, and regretted the time wasted on it. Another key benefit of an abstraction layer above your library would be that you can more easily expose more functionality when you find that you need it. Our opinions on the libraries that we used: Dear, Imgui : Recommended . Dear, Imgui is a robust and consistently-improving library for immediate mode GUI rendering. The only negatives we found in using it were that we needed to provide the textures for it to render and Horde3D was not providing them, and it required access to our windows, which was another Horde3D annoyance. Horde3D : Not recommended . Horde3D is definitely a lightweight rendering engine, but that was more of a hindrance than a help to us. We wanted something that would be quick to integrate into the engine, but we made the mistake of overlooking missing features until it was too late. The asset pipeline of Horde is also messy and slow, which got very annoying when we were developing games later in the project timeline. yojimbo : Recommended . yojimbo does pretty much what it says it does: Real-time networking messages and client-server communications. We had to inject a lot of our own functionality, like connection callbacks and an abstracted method of declaring message types, but the base functionality was all there and good for us to build off of. FMOD : Recommended . FMOD delivers simple audio functionality to a game engine, which is precisely what we needed. It possibly doesn't have the advanced functionality of its competitors, and controlling its memory accesses requires heavy modification, but we were able to get it running and integrated in record time, which means it's good in our book. Brofiler : No opinion . Brofiler was sufficient for us to profile out our systems and even specific functions within the engine, but we had to make a fork of the repository to fix some things in it before we could even use a functional build. If we spent more time on this aspect of development, we may have formed a good opinion of the tool, but we didn't, so we haven't!","title":"3rd Party"},{"location":"compendium/3rdParty/#3rd-party","text":"","title":"3rd Party"},{"location":"compendium/3rdParty/#timeline","text":"","title":"Timeline"},{"location":"compendium/3rdParty/#week-1","text":"Rendering: Ogre vs Horde3D : We narrowed down to two choices, Ogre and Horde3D . While Ogre is powerful and well-known, we decided on Horde3D due to the ease of building it and its lightweight design. Then we made a demo to prove we could! Audio: OpenAL Soft vs FMOD : For audio, our two choices were OpenAL Soft and FMOD . We tried out OpenAL Soft due to it being open source, but we were confused with it even three hours after working at it. On the other hand, FMOD was a breeze to use and had ample documentation. So we jumped right into it and implemented our audio engine using FMOD! Networking: GameNetworkingSockets vs yojimbo : See the Networking Compendium for more details! String ID : String hashing is a very helpful utility that allows strings to use less memory and serve better as keys for other values, so we found an open-source string ID library to bring in.","title":"Week 1"},{"location":"compendium/3rdParty/#week-2","text":"Making our Rendering API : After settling on Horde3D, we wrapped it up inside our engine as RenderModule , and for windows , we wrapped GLFW as WindowModule . Our initial module design was a bust, but we learned to keep things flat and managed in one place early on.","title":"Week 2"},{"location":"compendium/3rdParty/#week-3","text":"Input Implementation : We implemented our InputModule by wrapping the basic GLFW functionality with a couple of our own interfaces, namely a polling method and a callback registration method.","title":"Week 3"},{"location":"compendium/3rdParty/#week-5","text":"Ooey GUI : As it turns out, Horde3D has no GUI solutions, so we looked into ImGui and Qt . ImGui had a more lenient usage policy, and we could tell it was in very active development, so we went ahead with that. ImGui Stands for Something? : After choosing a library, we needed to decide on how developers would use our GUI system. We took advice from Amandine Coget and Walt Destler and went with a immediate mode GUI implementation, then quickly realized how important making a game is for proving which GUI features you actually need! Horde3D Turns Out to Be a Pain : We discovered that \"lightweight\" sometimes means \"missing features\" when Horde3D revealed that it does not expose its texture loading functionality (needed by our GUI system), attempts to load some resources during runtime , and won't let us control its memory .","title":"Week 5"},{"location":"compendium/3rdParty/#week-10","text":"We Expose Textures! : We complained about textures five weeks ago, and we felt that had dealt with Horde3D's oppressive control of the textures for long enough, so we whipped up our own OpenGL texture loader and hooked that in! Patch Notes: Modifying Input with Modifier Keys : We originally didn't have modifier keys like Shift and Ctrl implemented with our input system, but when we started to add debug features, we quickly realized the necessity of specialized key combinations for input.","title":"Week 10"},{"location":"compendium/3rdParty/#week-11","text":"Font Frenzy : Up until now, our text was always tiny and boring, so we decided to replace it with some better font. As it turns out, we didn't have enough font functionality in our engine! We reworked most of font implementation and ended up with some pretty results.","title":"Week 11"},{"location":"compendium/3rdParty/#week-12","text":"How Can Audio Be 3D If Our Ears Are 2D? : Audio is often left until the end on game projects, and the Isetta Engine is no different! We found that our audio system wasn't quite enough, so we implemented spatialized audio and reworked the rest of our module to be more effective and usable.","title":"Week 12"},{"location":"compendium/3rdParty/#relevant-interviews","text":"","title":"Relevant Interviews"},{"location":"compendium/3rdParty/#jeff-preshing","text":"Creating Your First Engine Systems Integration","title":"Jeff Preshing"},{"location":"compendium/3rdParty/#martin-middleton","text":"The Spectrum of Engine Development","title":"Martin Middleton"},{"location":"compendium/3rdParty/#amandine-coget","text":"GUI: Immediate vs Retained Modes","title":"Amandine Coget"},{"location":"compendium/3rdParty/#tommy-refenes","text":"Using 3 rd Party Libraries","title":"Tommy Refenes"},{"location":"compendium/3rdParty/#adam-serdar","text":"Integrated Libraries into an Engine","title":"Adam Serdar"},{"location":"compendium/3rdParty/#postmortem","text":"Research the libraries you'd consider using, then have a night's sleep before choosing to use one. Or even better, take some time to use the library ( not integrate it) in a separate test environment! Many of the libraries that we used for our engine were paired with some test projects, so we were able to go in and rummage around for a while before we made the leap and brought them into the engine. In doing so, we got to see (1) what are the most commonly used parts of the library, (2) what are the quirky or strange parts of the library, and (3) how comprehensive is the documentation. This stage was great for us to scour the internet for forums or documentation pages related to what we wanted out of the library, and if we struggled to find what we needed, then it was a good sign that the library might not be right for us. However, even after doing all of this, don't feel like you're stuck with a library that you've chosen just because you've spent some time with it. Long-term usage of a library will reveal much more of the negatives than short-term usage will. In our case, we first tried using the GameNetworkingSockets library for our networking solution, and we thought that it mostly had what we wanted after we ran the test project. Later, we discovered a lot of commented-out code and grew much more suspicious of missing functionality that we might need, so we jumped ship over to yojimbo . We can't say for sure whether GameNetworkingSockets would have been a poor choice, but yojimbo has turned out nicely for our engine, so we certainly didn't lose anything by being cautious like this! Create an abstraction layer for your game developers\u2014don't just expose the library. Most libraries that you use will not follow the same semantics, conventions, and guidelines as your own engine's API, and keeping that consistent is important for the usability of the engine. This will still be useful even if you have to do it for over 100 functions; in our case for Dear, Imgui, we had to abstract out quite a few functions and classes just to keep them within our own style. The result was a seamless GUI API from a 3 rd party library that we brought into our engine! Without that, we likely would have had a much harder time utilizing our engine's GUI functionality since its API conventions would differ from the rest of the engine's API. Your first abstraction will look a lot like the library that you're abstracting, which is okay; it doesn't need to look different until you bring in another library for that system as well. This is another good reason to create an abstraction layer: You can much more easily pull out your previous 3 rd party library without affecting the games that are using that module. You won't be able to change the return values or function signatures, of course, but you can change anything about the implementation and the game developer should hardly be able to tell the difference! More things to know: Integrating multiple rendering libraries isn't pleasant, but it may be necessary depending on what you use. The best steps you can follow would be to get each library working separately then slowly integrate one into another in small, testable chunks. Your engine doesn't need all of the features a library provides, so don't try to expose everything that you think is necessary; only expose things that you see as immediately being needed for the engine. We made this mistake with our GUI library, and regretted the time wasted on it. Another key benefit of an abstraction layer above your library would be that you can more easily expose more functionality when you find that you need it. Our opinions on the libraries that we used: Dear, Imgui : Recommended . Dear, Imgui is a robust and consistently-improving library for immediate mode GUI rendering. The only negatives we found in using it were that we needed to provide the textures for it to render and Horde3D was not providing them, and it required access to our windows, which was another Horde3D annoyance. Horde3D : Not recommended . Horde3D is definitely a lightweight rendering engine, but that was more of a hindrance than a help to us. We wanted something that would be quick to integrate into the engine, but we made the mistake of overlooking missing features until it was too late. The asset pipeline of Horde is also messy and slow, which got very annoying when we were developing games later in the project timeline. yojimbo : Recommended . yojimbo does pretty much what it says it does: Real-time networking messages and client-server communications. We had to inject a lot of our own functionality, like connection callbacks and an abstracted method of declaring message types, but the base functionality was all there and good for us to build off of. FMOD : Recommended . FMOD delivers simple audio functionality to a game engine, which is precisely what we needed. It possibly doesn't have the advanced functionality of its competitors, and controlling its memory accesses requires heavy modification, but we were able to get it running and integrated in record time, which means it's good in our book. Brofiler : No opinion . Brofiler was sufficient for us to profile out our systems and even specific functions within the engine, but we had to make a fork of the repository to fix some things in it before we could even use a functional build. If we spent more time on this aspect of development, we may have formed a good opinion of the tool, but we didn't, so we haven't!","title":"Postmortem"},{"location":"compendium/Build-ResourceManagement/","text":"Build & Resource Management \u00b6 Timeline \u00b6 Week 2 \u00b6 Engine Config : By analyzing our needs and referencing established engines , we designed and implemented a surprisingly easy to use configuration system. Week 5 \u00b6 Loading Nested Resources with Horde3D : Many of the Horde3D resources that we use have resources nested within them like materials and geometry. Horde3D would mark these down but not load them when asked to, so we poked our way in to get a better handle on whether resources were unloaded or not. Week 6 \u00b6 DLLifying the Engine : Eventually our engine will need to be a black box, as is the way of game engines. We started this journey by building our engine into a Dynamic Linked Library to be used by a separate project, and after marking all of our code with the correct intrinsics, we created a separate testbed project and made our first official external project! Week 11 \u00b6 Asset Processing Tool : With more game development happening than ever, processing our game assets was becoming more and more annoying. To remedy this, we built an asset processing tool to process our COLLADA (.dae) files all in one simple software package. Week 12 \u00b6 Exporting the Engine : Now that we set up our whole DLL build configuration and we were developing several games, we needed a way to get all of the files together conveniently. So we created a tool for exporting our header files along with our build and resource files into one packaged location, which could then be passed along to any project using the engine. Week 13 \u00b6 Building a Build Pipeline : Our previous build tools were all functioning well and good, but we found that building the engine was taking more of our time with every day. We already had a build pipeline for our website set up in Jenkins, so we drafted a couple of build scripts to be used with our header exporter from Week 12 and gave them a test drive. ...After many more test drives, we nailed down the last details and had a smart, effective build pipeline on our hands! Relevant Interviews \u00b6 Jeff Preshing \u00b6 System Integration Aras Pranckevi\u010dius \u00b6 From Graphics to Plumbing Casey Muratori \u00b6 Builds: Keep 'em Simple Postmortem \u00b6 Optimizing the build pipeline not only saves time, but it also keeps things clear among teammates. We've heard varying advice from professionals on how engine builds should be managed, but from our own experience, we can definitely say that it's a topic worth putting serious investment into. The first reason is obviously build times, which can balloon to greatly annoying proportions if you liberally use C++ features like templates and have lots of separated files and dense headers. The second, less known reason is work friction. When we started developing the engine, we could make builds quickly and cleanly almost always because there was nothing in it and no systems touched each other, but over time, our systems began to intermingle, then we began to organize our Visual Studio files into different project structures, then we began using a DLL for actual game development. Next thing you know, our developers are frustrated with having to wait almost an hour just to iterate on one system within our testbed project simply because that's the way things are set up! So optimizing your build process can mean a lot more than just saving time on the build. In the latter half of the project, we set up an automated process on a shared team machine that would regularly build the engine and push that to a remote repository, and this gave us two swanky benefits: We could make gradual changes to the engine and eventually those would propagate out to others naturally, and we would know as soon as the engine breaks without having to build it ourselves and be greeted by a nasty error message. Spending more time on our build pipeline became more rewarding during the project because of how much it influenced our development time, so it's definitely something to be watching and improving from the start. The asset pipeline of your engine needs to be clearly defined and easy for the game developer. Our asset pipeline was unfortunately mostly determined by Horde3D, our graphics library, which made things a mess for us to manage. Processing our resources was difficult because we needed to use particular file types, so we bit the bullet and invested time in some small tools that automated a lot of that process for us. By the end of the project, we were able to mostly ignore the asset pipeline thanks to this, but for future projects we'll definitely consider automating this pipeline from the beginning. More things to know: Some predefined things are necessary, like a \"Resources\" folder. To make everything modular also makes your engine brittle, and the last thing that you want your probably bug-ridden engine to be is more brittle. Use configuration files, and start them from early on. We noticed very early in our development that we were hardcoding some dangerous configurations, and by implementing this system early, we also established good practices for exposing our engines variables to the game developer. Changing the structure of your files mid-project should be avoided if possible. Refactors of the project can cause immediate productivity issues as well as lasting issues through the remainder of the project, like version control messiness. Debug , Release , and ReleaseWithDebugFeatures configurations should be tested regularly together. Bugs across configurations happen more often than you'd hope, especially when using 3 rd party libraries. Test your build pipeline on a clean computer, including the setup steps. This will ensure that you don't have any assumptions about making the build from your own computer.","title":"Build & Resource Management"},{"location":"compendium/Build-ResourceManagement/#build-resource-management","text":"","title":"Build &amp; Resource Management"},{"location":"compendium/Build-ResourceManagement/#timeline","text":"","title":"Timeline"},{"location":"compendium/Build-ResourceManagement/#week-2","text":"Engine Config : By analyzing our needs and referencing established engines , we designed and implemented a surprisingly easy to use configuration system.","title":"Week 2"},{"location":"compendium/Build-ResourceManagement/#week-5","text":"Loading Nested Resources with Horde3D : Many of the Horde3D resources that we use have resources nested within them like materials and geometry. Horde3D would mark these down but not load them when asked to, so we poked our way in to get a better handle on whether resources were unloaded or not.","title":"Week 5"},{"location":"compendium/Build-ResourceManagement/#week-6","text":"DLLifying the Engine : Eventually our engine will need to be a black box, as is the way of game engines. We started this journey by building our engine into a Dynamic Linked Library to be used by a separate project, and after marking all of our code with the correct intrinsics, we created a separate testbed project and made our first official external project!","title":"Week 6"},{"location":"compendium/Build-ResourceManagement/#week-11","text":"Asset Processing Tool : With more game development happening than ever, processing our game assets was becoming more and more annoying. To remedy this, we built an asset processing tool to process our COLLADA (.dae) files all in one simple software package.","title":"Week 11"},{"location":"compendium/Build-ResourceManagement/#week-12","text":"Exporting the Engine : Now that we set up our whole DLL build configuration and we were developing several games, we needed a way to get all of the files together conveniently. So we created a tool for exporting our header files along with our build and resource files into one packaged location, which could then be passed along to any project using the engine.","title":"Week 12"},{"location":"compendium/Build-ResourceManagement/#week-13","text":"Building a Build Pipeline : Our previous build tools were all functioning well and good, but we found that building the engine was taking more of our time with every day. We already had a build pipeline for our website set up in Jenkins, so we drafted a couple of build scripts to be used with our header exporter from Week 12 and gave them a test drive. ...After many more test drives, we nailed down the last details and had a smart, effective build pipeline on our hands!","title":"Week 13"},{"location":"compendium/Build-ResourceManagement/#relevant-interviews","text":"","title":"Relevant Interviews"},{"location":"compendium/Build-ResourceManagement/#jeff-preshing","text":"System Integration","title":"Jeff Preshing"},{"location":"compendium/Build-ResourceManagement/#aras-pranckevicius","text":"From Graphics to Plumbing","title":"Aras Pranckevi\u010dius"},{"location":"compendium/Build-ResourceManagement/#casey-muratori","text":"Builds: Keep 'em Simple","title":"Casey Muratori"},{"location":"compendium/Build-ResourceManagement/#postmortem","text":"Optimizing the build pipeline not only saves time, but it also keeps things clear among teammates. We've heard varying advice from professionals on how engine builds should be managed, but from our own experience, we can definitely say that it's a topic worth putting serious investment into. The first reason is obviously build times, which can balloon to greatly annoying proportions if you liberally use C++ features like templates and have lots of separated files and dense headers. The second, less known reason is work friction. When we started developing the engine, we could make builds quickly and cleanly almost always because there was nothing in it and no systems touched each other, but over time, our systems began to intermingle, then we began to organize our Visual Studio files into different project structures, then we began using a DLL for actual game development. Next thing you know, our developers are frustrated with having to wait almost an hour just to iterate on one system within our testbed project simply because that's the way things are set up! So optimizing your build process can mean a lot more than just saving time on the build. In the latter half of the project, we set up an automated process on a shared team machine that would regularly build the engine and push that to a remote repository, and this gave us two swanky benefits: We could make gradual changes to the engine and eventually those would propagate out to others naturally, and we would know as soon as the engine breaks without having to build it ourselves and be greeted by a nasty error message. Spending more time on our build pipeline became more rewarding during the project because of how much it influenced our development time, so it's definitely something to be watching and improving from the start. The asset pipeline of your engine needs to be clearly defined and easy for the game developer. Our asset pipeline was unfortunately mostly determined by Horde3D, our graphics library, which made things a mess for us to manage. Processing our resources was difficult because we needed to use particular file types, so we bit the bullet and invested time in some small tools that automated a lot of that process for us. By the end of the project, we were able to mostly ignore the asset pipeline thanks to this, but for future projects we'll definitely consider automating this pipeline from the beginning. More things to know: Some predefined things are necessary, like a \"Resources\" folder. To make everything modular also makes your engine brittle, and the last thing that you want your probably bug-ridden engine to be is more brittle. Use configuration files, and start them from early on. We noticed very early in our development that we were hardcoding some dangerous configurations, and by implementing this system early, we also established good practices for exposing our engines variables to the game developer. Changing the structure of your files mid-project should be avoided if possible. Refactors of the project can cause immediate productivity issues as well as lasting issues through the remainder of the project, like version control messiness. Debug , Release , and ReleaseWithDebugFeatures configurations should be tested regularly together. Bugs across configurations happen more often than you'd hope, especially when using 3 rd party libraries. Test your build pipeline on a clean computer, including the setup steps. This will ensure that you don't have any assumptions about making the build from your own computer.","title":"Postmortem"},{"location":"compendium/Collisions/","text":"Collisions \u00b6 Timeline \u00b6 Week 7 \u00b6 Intersection Testing : Collisions were needed for a few key cases in our game (bullets hitting enemies, enemies hitting players, players hitting walls), and each of these things can be encapsulated by primitive shapes like spheres, capsules, and boxes. We referenced Real-Time Collision Detection to get our collision system started with some of the simpler collision scenarios, like box-sphere and sphere-capsule. How Should Collisions Be Handled? : Detecting collisions is only half the battle. We considered multiple options for signaling when a collision occurs in our engine, which include callbacks on every collider, callbacks on every component, and a specific component that houses collision handling logic. Week 8 \u00b6 How We Ended Up Handling Collisions : After weighing our decisions, we went the CollisionHandler component route where a single component holds the callbacks and the colliders reference that component. Some messy stuff can happen with that, such as when you move colliders away from their handlers, but we came up with some basic fallbacks for those scenarios. Box-Capsule is the Worst : Most of our intersection testing code worked out after our first pass, but box-capsule collisions continued to best us. We got it working in most cases, but didn't have the time to vet it for all cases (and we believe that GJK would be a better alternative for any future attempts at a collisions system). Extra Features : Luckily, collider-collider interactions aren't the only thing that collisions systems are useful for, so we spent some time on features like raycasting that would provide us a lot of utility for games and tests in the future. Week 9 \u00b6 The Dynamic Bounding Volume Tree : When we started the collision system, we discussed a method for speeding up the intersection tests by culling out big groups of colliders with a single test. Well, that's DBVT! Our investment was validated by the results, with an over 4x performance increase! Week 11 \u00b6 And Now We Solve the Collisions : The next step for our collisions system was to keep our colliders from intersecting, a.k.a. collision-solving. We leveraged our existing collision system to start and made a pretty flaky collision solver , then we took a closer look at how Unity's collision system behaved and took another swing at it in a case-by-case fasion . Week 13 \u00b6 Solving More Correctly : Our box collision solving still had some slippery kinks that we worked out by baking the collision solving into the intersection test phase of our algorithm, then we topped it off with a little nudge in the right direction to prevent indefinitely prolonged intersections. We also added \"mass\" into an engine with no physics so that colliders could seem heavier! Relevant Interviews \u00b6 Tommy Refenes \u00b6 Using 3 rd Party Libraries Postmortem \u00b6 Use a physics library. Seriously. Well, if you want to implement your own physics engine, then maybe you should dig into an existing one and see how they do it, but if you just want physics or collisions in your engine, then for real\u2014 don't implement your own! We'll just talk about collisions from here since that's all we created in our own engine. Collisions will be one of the most error-prone systems in your engine because even when your math is correct, you may still be missing edge cases, or your scene hierarchy update is at odds with how your collision system processes its spatial data structure. This is not to mention that how you implement the collisions is up to you, but the ways that you understand better might be a bad choice in the long run. We decided to implement our collisions geometrically which meant we had pretty and fast equations that would determine intersections, but once we got to the oh-so-complex case of a capsule and a box colliding, we found that neatness faded away and we were stuck with a long and very case-based intersection test algorithm. You might think we're being overly pessimistic by recommending against building your own collision system, but one of the nice parts of a collision system is that you can easily replace an existing one with your own later down the line. Most of the features in a collision system are ones that you find in any implementation, so swapping them out doesn't tend to affect other systems unless you made assumptions about specific parts of the collision system, like how collisions are resolved. Handling collisions is half the battle. Our first inclination for the collision system was that we needed to mathematically solve all of our intersection tests and robustly implement that, which is definitely something that we needed to do eventually, but we quickly realized that our collision system meant nothing if we didn't have any collision handling system on top of it. And the collision handling system will be more intertwined with the rest of your engine depending on how you implement it. Ours relies on our scene hierarchy to match collisions to their collision handlers, but you could also just point collisions directly at their handlers and be done with it. Because we chose the more automated fashion, we also had to handle more cases such as when collisions or collision handlers are moved in the hierarchy. More things to know: GJK is the industry standard for a reason. Detecting collisions geometrically tends to break down into case-by-case tests for anything other than spheres, and even then you'll find a lot of bugs and inaccuracies because of the breadth of the code at that point. Raycasting will be a widely-used feature of your collisions system. Obviously colliders are good for things like trigger zones or physics simulations, but any game with the concept of \"shooting\" or \"pointing\" will need raycasting for many of its features. Spatial partitioning can be very expensive when used incorrectly. Gathering results from a spatial partition (like for our dynamic bounding volume tree) more than once a frame will impact your performance significantly, so using heuristics to prevent over-calculation and sharing the data with other systems is important to keep things running smoothly. For collision solving, accuracy is more important than performance. In the worst case, you can solve with only one iteration of collision resolution per frame and just let the solution work itself out over multiple frames, whereas if the solution is inaccurate, that will propagate to a totally incorrect behavior. The ordering of intersection testing, the collision resolution, and your collision callbacks matter. In our engine, we use our intersection tests to determine if we even need to perform collision resolution, then we move our colliders to where they should be, then we run the collision callback functions. If the callbacks are run earlier, then any translation they do on entities would be overwritten by the collision resolution! Test different sizes and scales of your shapes along with different rotations. Your code may be built on assumptions of uniform transformations, and it will definitely break as soon as something is not uniformly transformed!","title":"Collisions"},{"location":"compendium/Collisions/#collisions","text":"","title":"Collisions"},{"location":"compendium/Collisions/#timeline","text":"","title":"Timeline"},{"location":"compendium/Collisions/#week-7","text":"Intersection Testing : Collisions were needed for a few key cases in our game (bullets hitting enemies, enemies hitting players, players hitting walls), and each of these things can be encapsulated by primitive shapes like spheres, capsules, and boxes. We referenced Real-Time Collision Detection to get our collision system started with some of the simpler collision scenarios, like box-sphere and sphere-capsule. How Should Collisions Be Handled? : Detecting collisions is only half the battle. We considered multiple options for signaling when a collision occurs in our engine, which include callbacks on every collider, callbacks on every component, and a specific component that houses collision handling logic.","title":"Week 7"},{"location":"compendium/Collisions/#week-8","text":"How We Ended Up Handling Collisions : After weighing our decisions, we went the CollisionHandler component route where a single component holds the callbacks and the colliders reference that component. Some messy stuff can happen with that, such as when you move colliders away from their handlers, but we came up with some basic fallbacks for those scenarios. Box-Capsule is the Worst : Most of our intersection testing code worked out after our first pass, but box-capsule collisions continued to best us. We got it working in most cases, but didn't have the time to vet it for all cases (and we believe that GJK would be a better alternative for any future attempts at a collisions system). Extra Features : Luckily, collider-collider interactions aren't the only thing that collisions systems are useful for, so we spent some time on features like raycasting that would provide us a lot of utility for games and tests in the future.","title":"Week 8"},{"location":"compendium/Collisions/#week-9","text":"The Dynamic Bounding Volume Tree : When we started the collision system, we discussed a method for speeding up the intersection tests by culling out big groups of colliders with a single test. Well, that's DBVT! Our investment was validated by the results, with an over 4x performance increase!","title":"Week 9"},{"location":"compendium/Collisions/#week-11","text":"And Now We Solve the Collisions : The next step for our collisions system was to keep our colliders from intersecting, a.k.a. collision-solving. We leveraged our existing collision system to start and made a pretty flaky collision solver , then we took a closer look at how Unity's collision system behaved and took another swing at it in a case-by-case fasion .","title":"Week 11"},{"location":"compendium/Collisions/#week-13","text":"Solving More Correctly : Our box collision solving still had some slippery kinks that we worked out by baking the collision solving into the intersection test phase of our algorithm, then we topped it off with a little nudge in the right direction to prevent indefinitely prolonged intersections. We also added \"mass\" into an engine with no physics so that colliders could seem heavier!","title":"Week 13"},{"location":"compendium/Collisions/#relevant-interviews","text":"","title":"Relevant Interviews"},{"location":"compendium/Collisions/#tommy-refenes","text":"Using 3 rd Party Libraries","title":"Tommy Refenes"},{"location":"compendium/Collisions/#postmortem","text":"Use a physics library. Seriously. Well, if you want to implement your own physics engine, then maybe you should dig into an existing one and see how they do it, but if you just want physics or collisions in your engine, then for real\u2014 don't implement your own! We'll just talk about collisions from here since that's all we created in our own engine. Collisions will be one of the most error-prone systems in your engine because even when your math is correct, you may still be missing edge cases, or your scene hierarchy update is at odds with how your collision system processes its spatial data structure. This is not to mention that how you implement the collisions is up to you, but the ways that you understand better might be a bad choice in the long run. We decided to implement our collisions geometrically which meant we had pretty and fast equations that would determine intersections, but once we got to the oh-so-complex case of a capsule and a box colliding, we found that neatness faded away and we were stuck with a long and very case-based intersection test algorithm. You might think we're being overly pessimistic by recommending against building your own collision system, but one of the nice parts of a collision system is that you can easily replace an existing one with your own later down the line. Most of the features in a collision system are ones that you find in any implementation, so swapping them out doesn't tend to affect other systems unless you made assumptions about specific parts of the collision system, like how collisions are resolved. Handling collisions is half the battle. Our first inclination for the collision system was that we needed to mathematically solve all of our intersection tests and robustly implement that, which is definitely something that we needed to do eventually, but we quickly realized that our collision system meant nothing if we didn't have any collision handling system on top of it. And the collision handling system will be more intertwined with the rest of your engine depending on how you implement it. Ours relies on our scene hierarchy to match collisions to their collision handlers, but you could also just point collisions directly at their handlers and be done with it. Because we chose the more automated fashion, we also had to handle more cases such as when collisions or collision handlers are moved in the hierarchy. More things to know: GJK is the industry standard for a reason. Detecting collisions geometrically tends to break down into case-by-case tests for anything other than spheres, and even then you'll find a lot of bugs and inaccuracies because of the breadth of the code at that point. Raycasting will be a widely-used feature of your collisions system. Obviously colliders are good for things like trigger zones or physics simulations, but any game with the concept of \"shooting\" or \"pointing\" will need raycasting for many of its features. Spatial partitioning can be very expensive when used incorrectly. Gathering results from a spatial partition (like for our dynamic bounding volume tree) more than once a frame will impact your performance significantly, so using heuristics to prevent over-calculation and sharing the data with other systems is important to keep things running smoothly. For collision solving, accuracy is more important than performance. In the worst case, you can solve with only one iteration of collision resolution per frame and just let the solution work itself out over multiple frames, whereas if the solution is inaccurate, that will propagate to a totally incorrect behavior. The ordering of intersection testing, the collision resolution, and your collision callbacks matter. In our engine, we use our intersection tests to determine if we even need to perform collision resolution, then we move our colliders to where they should be, then we run the collision callback functions. If the callbacks are run earlier, then any translation they do on entities would be overwritten by the collision resolution! Test different sizes and scales of your shapes along with different rotations. Your code may be built on assumptions of uniform transformations, and it will definitely break as soon as something is not uniformly transformed!","title":"Postmortem"},{"location":"compendium/Core/","text":"Core \u00b6 Timeline \u00b6 Week 1 \u00b6 The Whole Core : At the start of our journey, we layed out the roadmap of our core libraries and introduced each of them \u2014 even though some were changed or removed later on. Module Manager : Our module manager takes care of module startup, shutdown and update sequence. We thought it would be trivial but actually fell into a 2-hour discussion about the fundamental design of the modules... Error Handling : We agreed on the usages of exceptions and asserts, and hesitantly brought in the Microsoft Foundation Class library to help with asserts. Math : The first version of our Math library was quickly implemented by referencing Unity's Math API and various online 3D math resources. String ID : We didn't think too much and brought in an open-source string ID library. Week 2 \u00b6 Patch Notes: Asserts : Our premonition was right \u2014 the Microsoft Foundation Class library was indeed not the best way to implement assertions for us, so we removed it and made our own. Week 3 \u00b6 Serialization : We originally included serialization in our architecture diagram because we thought networking would need it. But the networking library we brought in already has its own! As much as we wanted to do serialization, we decided to drop it to save headaches and limit the scope. STL Wrapper : As the development went on, we realized an STL wrapper is not necessary and would be tedious and trivial to implement, we dropped it. We still had IsettaAlias.h though, so we can write U8 instead of uint8_t every time. Data Structures : We realized we need some custom data structures like RingBuffer for our other systems, so we implemented that. At this point, we also got ourselves into the mindset that we could implement our data strucures on-demand throughout the project. Timer : We implemented our Timer class using C++'s built-in <chrono> library \u2014 it was as simple as that. File System : We made a file system that's capable of async file manipulation using Microsoft's API. It was a lot of circular documentation, and knowing the right terminology was really important to figuring out what to learn. Week 4 \u00b6 Patch Notes: Someone's Checking Our Math! : Someone found a bug in our Matrix4 class and opened a pull request on Github! Week 5 \u00b6 Module Manager, Removed : ModuleManager was becoming an obvious central hub (a.k.a. choke point) for our engine's modules. Once we had introduced systems other than modules in our engine, the module manager became restrictice, so we said good bye to it and moved its functionality to Engine Loop. This also allowed us to revisit our different update loops, now named FixedUpdate and VariableUpdate . Week 7 \u00b6 Serialization : We discussed what our need for serialization is within the engine and rationalized why sticking with our networking library's serialization was best for the time being. Week 9 \u00b6 Patch Notes: Quaternions : We fixed some nasty Quaternion bugs in our Math library during the process of making NetworkTransform . Logger File I/O : We discovered a problem with our logging output caused by optimization in our file system and made a non-perfect fix. Week 10 \u00b6 Our Own Dynamic Array : We used std::vector extensively in our engine and discovered the need for managing its memory, so we implmented our own dynamic array class. The process revealed more problems in our memory management system! Week 13 \u00b6 Delegates : We made a new data structure that takes care of managing callbacks and handles to unify our system-to-system event structure. Patch Notes: Moving the Dynamic Array : Our custom dynamic array had a bug that completely broke any move operations on them. Relevant Interviews \u00b6 Elan Ruskin \u00b6 Synchronization of Time Engines Can Change, Little by Little Shanee Nishry \u00b6 Keeping it Minimal, Decoupled & Data-Oriented Developing for an iOS Engine Jeff Preshing \u00b6 The Arc80 Engine Architecture Systems Integration Low-level Debugging Reflection and Serialization Console Development Experiences Raymond Graham \u00b6 The Console Evolution and Engine Implications Porting between Non-Compatible Architectures Amandine Coget \u00b6 Parallelism and Data-Oriented Design Tommy Refenes \u00b6 Basic Principles of Abstraction Stealth Loading in Super Meat Boy Alice Ching \u00b6 Jason Gregory \u00b6 Oliver Franzke \u00b6 Walt Destler \u00b6 Postmortem \u00b6 Unit test your math library. Even if it's hard to create test cases for things like Matrix4 and Quaternion , this is crucial. If you don't unit test your math, something in the engine will definitely break. If you have your own memory manager, be wary of creating your own standard library classes as well. We assumed that making an equivalent of the std::vector class would be straightforward since it's so ubiquitous, but there's a lot going under the hood for the commonly used classes and you'll likely do some of it incorrectly, which will propagate errors throughout your engine. Implementing your own data structures is wise, though, because you will often need it in more than one place, and duplicating the code will likely cause problems down the road.","title":"Core"},{"location":"compendium/Core/#core","text":"","title":"Core"},{"location":"compendium/Core/#timeline","text":"","title":"Timeline"},{"location":"compendium/Core/#week-1","text":"The Whole Core : At the start of our journey, we layed out the roadmap of our core libraries and introduced each of them \u2014 even though some were changed or removed later on. Module Manager : Our module manager takes care of module startup, shutdown and update sequence. We thought it would be trivial but actually fell into a 2-hour discussion about the fundamental design of the modules... Error Handling : We agreed on the usages of exceptions and asserts, and hesitantly brought in the Microsoft Foundation Class library to help with asserts. Math : The first version of our Math library was quickly implemented by referencing Unity's Math API and various online 3D math resources. String ID : We didn't think too much and brought in an open-source string ID library.","title":"Week 1"},{"location":"compendium/Core/#week-2","text":"Patch Notes: Asserts : Our premonition was right \u2014 the Microsoft Foundation Class library was indeed not the best way to implement assertions for us, so we removed it and made our own.","title":"Week 2"},{"location":"compendium/Core/#week-3","text":"Serialization : We originally included serialization in our architecture diagram because we thought networking would need it. But the networking library we brought in already has its own! As much as we wanted to do serialization, we decided to drop it to save headaches and limit the scope. STL Wrapper : As the development went on, we realized an STL wrapper is not necessary and would be tedious and trivial to implement, we dropped it. We still had IsettaAlias.h though, so we can write U8 instead of uint8_t every time. Data Structures : We realized we need some custom data structures like RingBuffer for our other systems, so we implemented that. At this point, we also got ourselves into the mindset that we could implement our data strucures on-demand throughout the project. Timer : We implemented our Timer class using C++'s built-in <chrono> library \u2014 it was as simple as that. File System : We made a file system that's capable of async file manipulation using Microsoft's API. It was a lot of circular documentation, and knowing the right terminology was really important to figuring out what to learn.","title":"Week 3"},{"location":"compendium/Core/#week-4","text":"Patch Notes: Someone's Checking Our Math! : Someone found a bug in our Matrix4 class and opened a pull request on Github!","title":"Week 4"},{"location":"compendium/Core/#week-5","text":"Module Manager, Removed : ModuleManager was becoming an obvious central hub (a.k.a. choke point) for our engine's modules. Once we had introduced systems other than modules in our engine, the module manager became restrictice, so we said good bye to it and moved its functionality to Engine Loop. This also allowed us to revisit our different update loops, now named FixedUpdate and VariableUpdate .","title":"Week 5"},{"location":"compendium/Core/#week-7","text":"Serialization : We discussed what our need for serialization is within the engine and rationalized why sticking with our networking library's serialization was best for the time being.","title":"Week 7"},{"location":"compendium/Core/#week-9","text":"Patch Notes: Quaternions : We fixed some nasty Quaternion bugs in our Math library during the process of making NetworkTransform . Logger File I/O : We discovered a problem with our logging output caused by optimization in our file system and made a non-perfect fix.","title":"Week 9"},{"location":"compendium/Core/#week-10","text":"Our Own Dynamic Array : We used std::vector extensively in our engine and discovered the need for managing its memory, so we implmented our own dynamic array class. The process revealed more problems in our memory management system!","title":"Week 10"},{"location":"compendium/Core/#week-13","text":"Delegates : We made a new data structure that takes care of managing callbacks and handles to unify our system-to-system event structure. Patch Notes: Moving the Dynamic Array : Our custom dynamic array had a bug that completely broke any move operations on them.","title":"Week 13"},{"location":"compendium/Core/#relevant-interviews","text":"","title":"Relevant Interviews"},{"location":"compendium/Core/#elan-ruskin","text":"Synchronization of Time Engines Can Change, Little by Little","title":"Elan Ruskin"},{"location":"compendium/Core/#shanee-nishry","text":"Keeping it Minimal, Decoupled & Data-Oriented Developing for an iOS Engine","title":"Shanee Nishry"},{"location":"compendium/Core/#jeff-preshing","text":"The Arc80 Engine Architecture Systems Integration Low-level Debugging Reflection and Serialization Console Development Experiences","title":"Jeff Preshing"},{"location":"compendium/Core/#raymond-graham","text":"The Console Evolution and Engine Implications Porting between Non-Compatible Architectures","title":"Raymond Graham"},{"location":"compendium/Core/#amandine-coget","text":"Parallelism and Data-Oriented Design","title":"Amandine Coget"},{"location":"compendium/Core/#tommy-refenes","text":"Basic Principles of Abstraction Stealth Loading in Super Meat Boy","title":"Tommy Refenes"},{"location":"compendium/Core/#alice-ching","text":"","title":"Alice Ching"},{"location":"compendium/Core/#jason-gregory","text":"","title":"Jason Gregory"},{"location":"compendium/Core/#oliver-franzke","text":"","title":"Oliver Franzke"},{"location":"compendium/Core/#walt-destler","text":"","title":"Walt Destler"},{"location":"compendium/Core/#postmortem","text":"Unit test your math library. Even if it's hard to create test cases for things like Matrix4 and Quaternion , this is crucial. If you don't unit test your math, something in the engine will definitely break. If you have your own memory manager, be wary of creating your own standard library classes as well. We assumed that making an equivalent of the std::vector class would be straightforward since it's so ubiquitous, but there's a lot going under the hood for the commonly used classes and you'll likely do some of it incorrectly, which will propagate errors throughout your engine. Implementing your own data structures is wise, though, because you will often need it in more than one place, and duplicating the code will likely cause problems down the road.","title":"Postmortem"},{"location":"compendium/Gameplay/","text":"Gameplay \u00b6 Timeline \u00b6 Week 0 \u00b6 Our Example Game : Some advice that we received from many game engine developers was to target our engine at a specific game, so we designed and implemented a game in the Unreal engine that we could try to build in our own engine as our goal. Week 7 \u00b6 Our First Game : Seven weeks after starting, we finally developed a game with our engine in an entirely different Visual Studio project than our game engine's! We discovered a lot about how to use a DLL, the importance of header files, and the asset pipeline of our engine and Horde3D. But most importantly, we found many bugs, including several on our Entity and Transform classes. Week 9 \u00b6 Event Messaging System : We developed an event messaging system for the sake of communicating between systems in a much more decoupled and flexible fashion. We implemented a generic method of attaching to events via string s for names and EventParam s for parameters, and we also differentiated queued and immediate callbacks for more precise control between our systems. Week 10 \u00b6 Our Second Game : Our first game was a simple twin stick shooter, so we wanted our second game to be a departure from that. We wanted to leverage our networking features including NetworkTransform along with some UI elements. Again, we found that developing a game in the engine reveals all of the bugs\u2014especially fundamental ones! Week 11 \u00b6 AI Pathfinding : Although AI isn't necessarily part of game engines, we wanted to implement it as a high-level piece of the engine to be generally usable by games. For twin stick shooters specifically, we weighed our options between A* and vector field implementations and eventually landed on using vector flow fields with steering behaviors in the end. Oh, Right, the Example Game : We finally began development for the target demo game of our engine by building up our menu system and in-game UI. Week 13 \u00b6 Hitscan : To get the most out of both our engine and our demo game, we decided to use hitscan for our gunplay implementation. We mostly leveraged our existing collision system and thought carefully about the data flow through our game . The results proved our investment was worthwhile! Our Third Non-Demo Game : Keeping with tradition, we developed a game that was not our target demo game in order to test out a conglomerate of our engine's features. This time, we pushed our collisions system to the maximum which, as you can probably guess, revealed lots of bugs with it! Relevant Interviews \u00b6 Tommy Refenes \u00b6 \"Garbage Physics\" Adam Serdar \u00b6 The Engine Dictates the Game Amit Patel & Rob Shillingsburg \u00b6 Jason Gregory \u00b6 Postmortem \u00b6 Build games as soon and as often as you possibly can. And if you can't do a \"game\" yet, then do something that you would be proud of as a starting programmers like a moving cube! We initially thought that we would just focus on engine development with some tech demos here and there, but games ended up being the stars during our development. Games test engine features in their natural environment against correct use cases and, most importantly, with other engine features! Games will reveal more bugs in your engine than tech demos will. Don't get us wrong, tech demos will prove whether or not your feature is working correctly, but games evoke a usage of your engine that just happens to break it a lot. Thankfully, our team had a lot of game engine programmers tell us from the get-go that we should be building games with our engine. We initially thought that they were just being overzealous with game development, but as it turns out, they were just being wiser than us. A good way to look at it is this: If you want your car to be safe and effective to drive on the road, would you want it to only be driven in the small testing course in the manufacturing building? At least at the start, target your engine at a specific game. Do not try to build a generic engine from the get-go. When beginning to plan out our engine, we didn't weigh what we wanted to make with it very seriously because, if we're honest, a lot of different genres of games share most of their tech. But again, thanks to the wisdom of professionals, we were taught that what tech is used is different from how that tech is used. Having a target game (or, at the very least, genre) lets you create tests, demos, and other features that coalesce more easily, and it also makes features a lot easier to cull from your engine when you have a deadline. We started our development with a target game in mind, and it allowed us to refer back to a \"ground truth\" whenever we were implementing features. That ground truth was rarely where we ended development, though, but because we had that target, we could appropriately determine when we needed to focus on the target game's features and when we could expand upon existing features. This ended in a more feature-full engine than we anticipated having at the beginning, which is pretty contradictory to the action of targeting a single game for development! More things to know: The genre that your engine is targeting isn't that important. It can help you narrow down the systems that you believe your engine needs, but the features will be better dictated by a specific game. Tech demos will effectively reproduce bugs, help you resolve them, and keep you from regressing any features, so leverage that whenever you run into bugs that deserve the investment. Hold onto past tech demo and game code as reference of how you want people to use your engine. Half of the battle of a game engine is its API and learnability, and you've got perfect examples right at your fingertips!","title":"Gameplay"},{"location":"compendium/Gameplay/#gameplay","text":"","title":"Gameplay"},{"location":"compendium/Gameplay/#timeline","text":"","title":"Timeline"},{"location":"compendium/Gameplay/#week-0","text":"Our Example Game : Some advice that we received from many game engine developers was to target our engine at a specific game, so we designed and implemented a game in the Unreal engine that we could try to build in our own engine as our goal.","title":"Week 0"},{"location":"compendium/Gameplay/#week-7","text":"Our First Game : Seven weeks after starting, we finally developed a game with our engine in an entirely different Visual Studio project than our game engine's! We discovered a lot about how to use a DLL, the importance of header files, and the asset pipeline of our engine and Horde3D. But most importantly, we found many bugs, including several on our Entity and Transform classes.","title":"Week 7"},{"location":"compendium/Gameplay/#week-9","text":"Event Messaging System : We developed an event messaging system for the sake of communicating between systems in a much more decoupled and flexible fashion. We implemented a generic method of attaching to events via string s for names and EventParam s for parameters, and we also differentiated queued and immediate callbacks for more precise control between our systems.","title":"Week 9"},{"location":"compendium/Gameplay/#week-10","text":"Our Second Game : Our first game was a simple twin stick shooter, so we wanted our second game to be a departure from that. We wanted to leverage our networking features including NetworkTransform along with some UI elements. Again, we found that developing a game in the engine reveals all of the bugs\u2014especially fundamental ones!","title":"Week 10"},{"location":"compendium/Gameplay/#week-11","text":"AI Pathfinding : Although AI isn't necessarily part of game engines, we wanted to implement it as a high-level piece of the engine to be generally usable by games. For twin stick shooters specifically, we weighed our options between A* and vector field implementations and eventually landed on using vector flow fields with steering behaviors in the end. Oh, Right, the Example Game : We finally began development for the target demo game of our engine by building up our menu system and in-game UI.","title":"Week 11"},{"location":"compendium/Gameplay/#week-13","text":"Hitscan : To get the most out of both our engine and our demo game, we decided to use hitscan for our gunplay implementation. We mostly leveraged our existing collision system and thought carefully about the data flow through our game . The results proved our investment was worthwhile! Our Third Non-Demo Game : Keeping with tradition, we developed a game that was not our target demo game in order to test out a conglomerate of our engine's features. This time, we pushed our collisions system to the maximum which, as you can probably guess, revealed lots of bugs with it!","title":"Week 13"},{"location":"compendium/Gameplay/#relevant-interviews","text":"","title":"Relevant Interviews"},{"location":"compendium/Gameplay/#tommy-refenes","text":"\"Garbage Physics\"","title":"Tommy Refenes"},{"location":"compendium/Gameplay/#adam-serdar","text":"The Engine Dictates the Game","title":"Adam Serdar"},{"location":"compendium/Gameplay/#amit-patel-rob-shillingsburg","text":"","title":"Amit Patel &amp; Rob Shillingsburg"},{"location":"compendium/Gameplay/#jason-gregory","text":"","title":"Jason Gregory"},{"location":"compendium/Gameplay/#postmortem","text":"Build games as soon and as often as you possibly can. And if you can't do a \"game\" yet, then do something that you would be proud of as a starting programmers like a moving cube! We initially thought that we would just focus on engine development with some tech demos here and there, but games ended up being the stars during our development. Games test engine features in their natural environment against correct use cases and, most importantly, with other engine features! Games will reveal more bugs in your engine than tech demos will. Don't get us wrong, tech demos will prove whether or not your feature is working correctly, but games evoke a usage of your engine that just happens to break it a lot. Thankfully, our team had a lot of game engine programmers tell us from the get-go that we should be building games with our engine. We initially thought that they were just being overzealous with game development, but as it turns out, they were just being wiser than us. A good way to look at it is this: If you want your car to be safe and effective to drive on the road, would you want it to only be driven in the small testing course in the manufacturing building? At least at the start, target your engine at a specific game. Do not try to build a generic engine from the get-go. When beginning to plan out our engine, we didn't weigh what we wanted to make with it very seriously because, if we're honest, a lot of different genres of games share most of their tech. But again, thanks to the wisdom of professionals, we were taught that what tech is used is different from how that tech is used. Having a target game (or, at the very least, genre) lets you create tests, demos, and other features that coalesce more easily, and it also makes features a lot easier to cull from your engine when you have a deadline. We started our development with a target game in mind, and it allowed us to refer back to a \"ground truth\" whenever we were implementing features. That ground truth was rarely where we ended development, though, but because we had that target, we could appropriately determine when we needed to focus on the target game's features and when we could expand upon existing features. This ended in a more feature-full engine than we anticipated having at the beginning, which is pretty contradictory to the action of targeting a single game for development! More things to know: The genre that your engine is targeting isn't that important. It can help you narrow down the systems that you believe your engine needs, but the features will be better dictated by a specific game. Tech demos will effectively reproduce bugs, help you resolve them, and keep you from regressing any features, so leverage that whenever you run into bugs that deserve the investment. Hold onto past tech demo and game code as reference of how you want people to use your engine. Half of the battle of a game engine is its API and learnability, and you've got perfect examples right at your fingertips!","title":"Postmortem"},{"location":"compendium/Interviews/","text":"Postmortem (IN-PROGRESS) \u00b6 Our decision to start interviewing professional engine developers was one that came about early in the planning process. Since we wanted to prove the worth of our project as a learning tool for other novice programmers, we needed to talk to the pros to understand what they thought was necessary. We were very fortunate to receive feedback from multiple veteran engine programmers including Jason Gregory, Amit Patel, Rob Schillingsburg, and Jonathan Franzke. These talks helped us to win our case for pitching the project, though we thought there could be more to it than that. Talking to professionals regularly throughout the semester could be a great way to give our audience multiple perspectives on the engine development process. Over the summer, we compiled a list of over 100 game engine programmers, from big name CEOs to unknown indies. Once the semester started, we filtered that list based on what topic of engine development the professional specialized in. Our goal with this was to conduct an interview on a different topic each week. This turned out to be a complex, multi-step process. We began by sending emails to the professionals on our list. If they responded, we'd schedule an interview time with them. On the days leading up to the interview, the team would work together to heavily research the professional (almost to a point of stalking) and come up with a list of questions most relevant to their knowledge, experience, and background. After the interview was done, we'd take the recording and transcribe it for clarity and pass our edits on to the professional for their input. With the interviewee's approval, we'd publish the transcribed interview on our site. At first, our process was naturally a little rough. We went the route of transcribing the entire interviews by hand, which ate heavily into our limited time frame. To remedy this, we decided to use YouTube's free auto-transcription service to get a full script of what was said. While it doesn't do an especially accurate job, it made things a lot easier for one team member to go through and correct the text. What's more, we consulted with ETC Press Editor-in-Chief Brad King to hone our formatting skills, as none of us were sure how a transcribed interview should look. When it came to reviewing our transcriptions with our interviewees, we noticed that there were lot of extremes between interviewees. We dealt with industry corporations who will clamp down hard on any publications and try to completely control what was said in the interview. We talked to some very driven and enthusiastic individuals who would consistently comment and chime in on our edits. Conversely, we also spoke to professionals who were completely lax about the review process. Lastly, there were some developers who simply didn't get back to us at all after the interview. On that last note, one of the most harrowing problems we faced in the interview process was when we were going out to Los Angeles to interview a number of people face-to-face. They all had agreed to interviews beforehand, but when we reached out to set up logistics a couple weeks before the trip, three of our biggest interviewees only gave us silence. We were in a frenzy trying to pick up the pieces and make the most of our trip. Working with these different kinds of personalities and scenarios added some more complexity to this part of our project that none of us saw coming. The interviewees' thoughts and stories on engine development often gave us the perfect solution for problems we were having on our own engine. They would even give us vocabulary we didn't have before. In one of our early meetings with Brad, he raised the idea of compiling our interviews into a printed book at the end of the semester to get it in the hands of as many people as possible. We thought about the idea, and it stuck with us. Before we knew it, we were published authors(or editors).","title":"Interviews"},{"location":"compendium/Interviews/#postmortem-in-progress","text":"Our decision to start interviewing professional engine developers was one that came about early in the planning process. Since we wanted to prove the worth of our project as a learning tool for other novice programmers, we needed to talk to the pros to understand what they thought was necessary. We were very fortunate to receive feedback from multiple veteran engine programmers including Jason Gregory, Amit Patel, Rob Schillingsburg, and Jonathan Franzke. These talks helped us to win our case for pitching the project, though we thought there could be more to it than that. Talking to professionals regularly throughout the semester could be a great way to give our audience multiple perspectives on the engine development process. Over the summer, we compiled a list of over 100 game engine programmers, from big name CEOs to unknown indies. Once the semester started, we filtered that list based on what topic of engine development the professional specialized in. Our goal with this was to conduct an interview on a different topic each week. This turned out to be a complex, multi-step process. We began by sending emails to the professionals on our list. If they responded, we'd schedule an interview time with them. On the days leading up to the interview, the team would work together to heavily research the professional (almost to a point of stalking) and come up with a list of questions most relevant to their knowledge, experience, and background. After the interview was done, we'd take the recording and transcribe it for clarity and pass our edits on to the professional for their input. With the interviewee's approval, we'd publish the transcribed interview on our site. At first, our process was naturally a little rough. We went the route of transcribing the entire interviews by hand, which ate heavily into our limited time frame. To remedy this, we decided to use YouTube's free auto-transcription service to get a full script of what was said. While it doesn't do an especially accurate job, it made things a lot easier for one team member to go through and correct the text. What's more, we consulted with ETC Press Editor-in-Chief Brad King to hone our formatting skills, as none of us were sure how a transcribed interview should look. When it came to reviewing our transcriptions with our interviewees, we noticed that there were lot of extremes between interviewees. We dealt with industry corporations who will clamp down hard on any publications and try to completely control what was said in the interview. We talked to some very driven and enthusiastic individuals who would consistently comment and chime in on our edits. Conversely, we also spoke to professionals who were completely lax about the review process. Lastly, there were some developers who simply didn't get back to us at all after the interview. On that last note, one of the most harrowing problems we faced in the interview process was when we were going out to Los Angeles to interview a number of people face-to-face. They all had agreed to interviews beforehand, but when we reached out to set up logistics a couple weeks before the trip, three of our biggest interviewees only gave us silence. We were in a frenzy trying to pick up the pieces and make the most of our trip. Working with these different kinds of personalities and scenarios added some more complexity to this part of our project that none of us saw coming. The interviewees' thoughts and stories on engine development often gave us the perfect solution for problems we were having on our own engine. They would even give us vocabulary we didn't have before. In one of our early meetings with Brad, he raised the idea of compiling our interviews into a printed book at the end of the semester to get it in the hands of as many people as possible. We thought about the idea, and it stuck with us. Before we knew it, we were published authors(or editors).","title":"Postmortem (IN-PROGRESS)"},{"location":"compendium/Memory/","text":"Memory \u00b6 Timeline \u00b6 Week 2 \u00b6 Memory Management, Part 1 : We designed and implemented our initial naive version of stack and pool allocators, whose performance was proven to be phenomenal through benchmarking . We also proposed a memory management pattern in our game, without knowing how big a boon or bane it would be in the future. Week 3 \u00b6 Memory Management, Part 2 : We implemented a double-buffer allocator , a pool allocator , and our very first version of defragmentation with custom object handles instead of regular pointers. Week 4 \u00b6 Memory Defragmentation : A naive and unoptimized defragmentation system was born. Week 5 \u00b6 Memory Management, Part 3 : Our previous allocators weren't enough for our needs, so we implemented an almighty free list allocator that enabled a lot of versatility in our memory management systems. Unfortunately, this is effectively dynamic allocation, so we had to abandon our dream of solely static allocations in the engine . Our API was changed after all of this to improve usability for our developers. Week 7 \u00b6 Patch Notes: Free List Alignment Fix : We found an alignment bug in our freelist and fixed it\u2014temporarily. We really fixed it a week or two later by changing the execution order of when we access a node on the free list and when we overwrite it. Week 10 \u00b6 Our Own Dynamic Array : We used std::vector extensively in our engine and discovered the need for managing its memory, so we implmented our own dynamic array class. The process revealed more problems in our memory management system, which includes: A draw back to preallocating all of our memory , which caused us to miss when we would make out-of-array accesses. Our memory management can only be used after a certain point in our initialization sequence , so we needed to weigh some architecture decisions in order to use static variables. Placement new for array is undefined , and we had no clue. This one wasn't on us, but we did need to account for it by iterating over all of our allocated elements individually and initializing them instead of using array new . Week 12 \u00b6 Memory Management, Initialization Edition : Our free list allocator had been useful so far, but we kept hitting a problem whenever we would use more than we allotted at startup. So we implemented some allocator expansion features! And since our memory manager isn't a module within our engine, we refactored it to follow RAII . Performance Optimization : Some of our systems heavily used our free list allocator but could have been using our pool allocators, so we switched them over and saw some modest performance increases. Week 13 \u00b6 Monitoring Memory Leaks : At long last, we implemented a system that can monitor leaks on our own memory manager. Relevant Interviews \u00b6 Elan Ruskin \u00b6 Allocate, Allocate, Reallocate Shanee Nishry \u00b6 Developing for an iOS Engine Caching in on Memory Jeff Preshing \u00b6 The Arc80 Engine Architecture Raymond Graham \u00b6 Porting between Non-Compatible Architectures Amandine Coget \u00b6 Parallelism and Data-Oriented Design Postmortem \u00b6 Writing your own memory manager will be a good use of time if you're trying to learn low-level programming. It requires a wide breadth of low-level topics, including how malloc and free work; the performance implications of memory allocation (and garbage collectors); the layout of memory and details regarding that like cache, alignment, and structure padding; and a deeper understanding of C++, like how to use placement new , how to manually call destructors on objects, and where different types of objects are allocated (stack, heap, data segment). This might seem intimidating on first glance, but it's not if you give yourself plenty of time to research, implement, and reimplement. We didn't even come close to getting it right the first time\u2014in fact, we probably still have a couple fatal bugs within our memory management system that we haven't sussed out yet. But because the memory manager was used by practically everything in the engine, we had plenty of opportunities to test and iterate on it, which resulted in a more robust and functional memory manager than if we had tried to design a good one outright. Writing your own memory manager may not be a good use of time if you need results quick. We appreciated developing our own as part of our project because it definitely yielded growth to our programmers, and we did see notable performance gains as a result of making one ourselves, but we had to spend a large amount of our time developing the memory manager or fixing it, and that portion of time was even more significant due to how short our project was. On a project that has the same timeline that we had for this engine (about 15 weeks), we would probably opt to not make a memory manager so that we could just go straight into developing higher level systems sooner. If you're targeting modern PCs, then you won't even really have to worry about stringent memory requirements either, so optimizing the development will likely still be more important than optimizing your memory accesses. More things to know: Be comprehensive with performance benchmarking for memory management systems. We benchmarked our allocators individually, but we failed to benchmark the performance of the entire system in use in a game so we couldn't see if our time investments were worthwhile. It will be tempting to see that individual allocators do well and just believe that results in overall better performance, but overhead can easily be introduced when you have multiple different subsystems to manage in a system. Be careful of using templates in a memory manager, including variadic templates. These not only utterly destroyed our compile time and broke Intellisense when using new (we couldn't tell what parameters were used in a constructor because they were hidden by the variadic templates), but they also made tracking down errors excruciatingly difficult because the error message would come from the memory allocator implementation and we wouldn't know which templated usage of that allocator was really failing. The impact of different allocation patterns (like pool, single-frame, double-frame, etc.) is more important than blindly trying to make every memory allocation in your system \"statically allocated.\" For example, in our engine, using a pool allocator for our entities improved our performance dramatically. The pool allocator also gives us more flexibility regarding how that memory is used, and we can reason about other optimizations we can make on it in the future as well.","title":"Memory"},{"location":"compendium/Memory/#memory","text":"","title":"Memory"},{"location":"compendium/Memory/#timeline","text":"","title":"Timeline"},{"location":"compendium/Memory/#week-2","text":"Memory Management, Part 1 : We designed and implemented our initial naive version of stack and pool allocators, whose performance was proven to be phenomenal through benchmarking . We also proposed a memory management pattern in our game, without knowing how big a boon or bane it would be in the future.","title":"Week 2"},{"location":"compendium/Memory/#week-3","text":"Memory Management, Part 2 : We implemented a double-buffer allocator , a pool allocator , and our very first version of defragmentation with custom object handles instead of regular pointers.","title":"Week 3"},{"location":"compendium/Memory/#week-4","text":"Memory Defragmentation : A naive and unoptimized defragmentation system was born.","title":"Week 4"},{"location":"compendium/Memory/#week-5","text":"Memory Management, Part 3 : Our previous allocators weren't enough for our needs, so we implemented an almighty free list allocator that enabled a lot of versatility in our memory management systems. Unfortunately, this is effectively dynamic allocation, so we had to abandon our dream of solely static allocations in the engine . Our API was changed after all of this to improve usability for our developers.","title":"Week 5"},{"location":"compendium/Memory/#week-7","text":"Patch Notes: Free List Alignment Fix : We found an alignment bug in our freelist and fixed it\u2014temporarily. We really fixed it a week or two later by changing the execution order of when we access a node on the free list and when we overwrite it.","title":"Week 7"},{"location":"compendium/Memory/#week-10","text":"Our Own Dynamic Array : We used std::vector extensively in our engine and discovered the need for managing its memory, so we implmented our own dynamic array class. The process revealed more problems in our memory management system, which includes: A draw back to preallocating all of our memory , which caused us to miss when we would make out-of-array accesses. Our memory management can only be used after a certain point in our initialization sequence , so we needed to weigh some architecture decisions in order to use static variables. Placement new for array is undefined , and we had no clue. This one wasn't on us, but we did need to account for it by iterating over all of our allocated elements individually and initializing them instead of using array new .","title":"Week 10"},{"location":"compendium/Memory/#week-12","text":"Memory Management, Initialization Edition : Our free list allocator had been useful so far, but we kept hitting a problem whenever we would use more than we allotted at startup. So we implemented some allocator expansion features! And since our memory manager isn't a module within our engine, we refactored it to follow RAII . Performance Optimization : Some of our systems heavily used our free list allocator but could have been using our pool allocators, so we switched them over and saw some modest performance increases.","title":"Week 12"},{"location":"compendium/Memory/#week-13","text":"Monitoring Memory Leaks : At long last, we implemented a system that can monitor leaks on our own memory manager.","title":"Week 13"},{"location":"compendium/Memory/#relevant-interviews","text":"","title":"Relevant Interviews"},{"location":"compendium/Memory/#elan-ruskin","text":"Allocate, Allocate, Reallocate","title":"Elan Ruskin"},{"location":"compendium/Memory/#shanee-nishry","text":"Developing for an iOS Engine Caching in on Memory","title":"Shanee Nishry"},{"location":"compendium/Memory/#jeff-preshing","text":"The Arc80 Engine Architecture","title":"Jeff Preshing"},{"location":"compendium/Memory/#raymond-graham","text":"Porting between Non-Compatible Architectures","title":"Raymond Graham"},{"location":"compendium/Memory/#amandine-coget","text":"Parallelism and Data-Oriented Design","title":"Amandine Coget"},{"location":"compendium/Memory/#postmortem","text":"Writing your own memory manager will be a good use of time if you're trying to learn low-level programming. It requires a wide breadth of low-level topics, including how malloc and free work; the performance implications of memory allocation (and garbage collectors); the layout of memory and details regarding that like cache, alignment, and structure padding; and a deeper understanding of C++, like how to use placement new , how to manually call destructors on objects, and where different types of objects are allocated (stack, heap, data segment). This might seem intimidating on first glance, but it's not if you give yourself plenty of time to research, implement, and reimplement. We didn't even come close to getting it right the first time\u2014in fact, we probably still have a couple fatal bugs within our memory management system that we haven't sussed out yet. But because the memory manager was used by practically everything in the engine, we had plenty of opportunities to test and iterate on it, which resulted in a more robust and functional memory manager than if we had tried to design a good one outright. Writing your own memory manager may not be a good use of time if you need results quick. We appreciated developing our own as part of our project because it definitely yielded growth to our programmers, and we did see notable performance gains as a result of making one ourselves, but we had to spend a large amount of our time developing the memory manager or fixing it, and that portion of time was even more significant due to how short our project was. On a project that has the same timeline that we had for this engine (about 15 weeks), we would probably opt to not make a memory manager so that we could just go straight into developing higher level systems sooner. If you're targeting modern PCs, then you won't even really have to worry about stringent memory requirements either, so optimizing the development will likely still be more important than optimizing your memory accesses. More things to know: Be comprehensive with performance benchmarking for memory management systems. We benchmarked our allocators individually, but we failed to benchmark the performance of the entire system in use in a game so we couldn't see if our time investments were worthwhile. It will be tempting to see that individual allocators do well and just believe that results in overall better performance, but overhead can easily be introduced when you have multiple different subsystems to manage in a system. Be careful of using templates in a memory manager, including variadic templates. These not only utterly destroyed our compile time and broke Intellisense when using new (we couldn't tell what parameters were used in a constructor because they were hidden by the variadic templates), but they also made tracking down errors excruciatingly difficult because the error message would come from the memory allocator implementation and we wouldn't know which templated usage of that allocator was really failing. The impact of different allocation patterns (like pool, single-frame, double-frame, etc.) is more important than blindly trying to make every memory allocation in your system \"statically allocated.\" For example, in our engine, using a pool allocator for our entities improved our performance dramatically. The pool allocator also gives us more flexibility regarding how that memory is used, and we can reason about other optimizations we can make on it in the future as well.","title":"Postmortem"},{"location":"compendium/Networking/","text":"Networking \u00b6 Timeline \u00b6 Week 1 \u00b6 Investigating Libraries : We built and ran Valve's GameNetworkingSockets library, but found it was too messy and half-baked for us to comfortably use, especially regarding documentation. We found yojimbo as an alternative, which seemed cleaner and more prepared for us to use. Week 2 \u00b6 The Decision : Based on documentation and apparently feature richness, we chose yojimbo for our low-level networking library. We extracted the test code from yojimbo's own test project and integrated it into our testbed. Week 3 \u00b6 Connecting our Computers : We developed a basic client-server model that we (for the time being) assumed would always be on, then cleared it on a single-machine setup. We became thirsty for progress and pushed forward to test a computer-computer setup , and we got sounds playing across the network! Week 5 \u00b6 Memory for the Network : We mapped out some future plans regarding client and server needs, then we transitioned the networking systems onto our own managed memory, which involved some important high-level decisions . Week 7 \u00b6 Messages to Our Future Selves : With our base network functionality out of the way, we began implementing the system that would support most if not all of our networking functionality: Remote procedural call messages . First and Second Versions of Network Messages : We made some attempts at a robust, convenient message creation system... Network IDs : ...then we established a system for tracking entities across the network using those messages... Third Version of Network Messages : ...and we redid the remote procedural call messages again! This time, we got it to utilize templating and classes instead of gross string identifiers like \"HNDL\" . Week 9 \u00b6 Transform-ing the Network : We used our previous RPC message framework to develop our networked transformation synchronization system! After the initial pass, we basically had functional network transforms, so we spent the rest of our time on nice things to have like interpolation and optimized network usage . Week 10 \u00b6 Missing Features : When developing the second game in our engine, we realized that we were missing some key things like parenting and snapping over the network. So we implemented them! Patch Notes: Making the Network \"Local\" : Our NetworkTransform class originally performed all of its calculations in world space to keep things uniform, but that meant sending way more synchronization messages than needed when parenting! As a result, we swapped world space out for local space. Week 13 \u00b6 Networked Game Management : Although all of our in-game needs were pretty much covered, we still lacked many out-of-game things, such as connection callbacks and networked level loading . Network Discovery : Another big out-of-game feature we were missing was a lobby system! Our previous systems required the player to know the IP of their host, but we implemented a LAN broadcasting system using some basic socket programming. Relevant Interviews \u00b6 Martin Middleton \u00b6 Technology from Flower to Journey Journey's Peer Networking System Amit Patel & Rob Shillingsburg \u00b6 Walt Destler \u00b6 Postmortem \u00b6 Short disclaimer: We never dove deeply into game networking for various reasons, so we can't speak much towards advanced network programming or any beyond-basic features like client-side prediction or world state delta encoding; if you're interested in those things, there are some great papers from games like TRIBES and Doom III that have good information about them. Instead, what we can write about is how one would even start doing network programming for games, especially as someone who hasn't done any network programming before. First, just because you're doing network programming for a game does not mean that you have to use the TRIBES Engine networking model \u2014especially if you're not even making a first-person shooter! We mistakenly believed that we needed to encode the world state and intelligently map out all of our entities as serializable objects when we were starting our networking features, largely due to Multiplayer Game Programming by Glazer and Madhav (which is a great introduction to network programming by the way!). We didn't know what all of that meant, or even what the data was supposed to look like, but we believed that we needed to create a robust, effective, and optimized network model for our engine, so you can imagine what we looked like when we were trying to design an architecture for something we had no idea about! Fortunately, one of our programmers had some higher level networking experience and wisely suggested that we start small: All we need are network messages. We were building on top of yojimbo so we avoided having to program below the session layer , and we were able to implement some rudimentary network messages very quickly. The moment we got messages going across the network, we were ecstatic! This was the cornerstone of anything that we would need for game networking, and we managed to make it happen! We only added one more advanced feature to our network repertoire beyond network messages because we never really needed anything else. Fundamentally, all you need in a networked game is a signal that will tell other computers to run some code, which is what those messages can do. By getting over this bump in the road, we were able to move onto doing networked spawning, networked object transforms, and more, and we didn't waste much of our time trying to over engineer the networking simply because we didn't understand it better. The networking features of a game engine cannot be predicted. Even if you're targeting a specific genre with that engine (like twin-stick shooters or something weird like that), you'll be adding in features that you think games will need and not the ones they actually need. We spent time on things that we thought would be important, such as optimizing our network transforms to take up less bandwidth, and it turned out that we were missing much more important features, like an API for parenting objects across the network. The former wasn't important because in the end, our networking was never able to extend across a LAN setup, whereas the latter was needed for the immediately next game that we made after finishing our network manager API. But we believed that optimizations would be really important because that's stressed a lot by network programmers who are far more experienced than anyone on our team. We talk about this elsewhere, but what's important when developing any game engine system (networking included) is to actually make games when you're doing that . A game engine in isolation would yield networked tech demos, and tech demos test what you already know you have. What's more is that making games also tests the real use cases of the games; as it turns out, a LAN doesn't really need packet optimization, but it does need gameplay quality of life features regarding networking like snapping objects into position. Knowing that requires stepping up and making something real with the system, not just setting up test suites. More things to know: Every new feature that you get working over a network will be an exciting moment, and you should celebrate it with as much of your team as possible . Building your networking system on top of a 3 rd party library will get you most of the way there, especially if you don't have great needs from your networking. Our network message system is flexible and effective, but it requires a lot of duplicated code, which makes it a hassle to set up for new functionalities over the network. Know your end goal before designing your network system architecture. LAN networking should prioritize different things than long-distance networking, like responsiveness over packet reduction.","title":"Networking"},{"location":"compendium/Networking/#networking","text":"","title":"Networking"},{"location":"compendium/Networking/#timeline","text":"","title":"Timeline"},{"location":"compendium/Networking/#week-1","text":"Investigating Libraries : We built and ran Valve's GameNetworkingSockets library, but found it was too messy and half-baked for us to comfortably use, especially regarding documentation. We found yojimbo as an alternative, which seemed cleaner and more prepared for us to use.","title":"Week 1"},{"location":"compendium/Networking/#week-2","text":"The Decision : Based on documentation and apparently feature richness, we chose yojimbo for our low-level networking library. We extracted the test code from yojimbo's own test project and integrated it into our testbed.","title":"Week 2"},{"location":"compendium/Networking/#week-3","text":"Connecting our Computers : We developed a basic client-server model that we (for the time being) assumed would always be on, then cleared it on a single-machine setup. We became thirsty for progress and pushed forward to test a computer-computer setup , and we got sounds playing across the network!","title":"Week 3"},{"location":"compendium/Networking/#week-5","text":"Memory for the Network : We mapped out some future plans regarding client and server needs, then we transitioned the networking systems onto our own managed memory, which involved some important high-level decisions .","title":"Week 5"},{"location":"compendium/Networking/#week-7","text":"Messages to Our Future Selves : With our base network functionality out of the way, we began implementing the system that would support most if not all of our networking functionality: Remote procedural call messages . First and Second Versions of Network Messages : We made some attempts at a robust, convenient message creation system... Network IDs : ...then we established a system for tracking entities across the network using those messages... Third Version of Network Messages : ...and we redid the remote procedural call messages again! This time, we got it to utilize templating and classes instead of gross string identifiers like \"HNDL\" .","title":"Week 7"},{"location":"compendium/Networking/#week-9","text":"Transform-ing the Network : We used our previous RPC message framework to develop our networked transformation synchronization system! After the initial pass, we basically had functional network transforms, so we spent the rest of our time on nice things to have like interpolation and optimized network usage .","title":"Week 9"},{"location":"compendium/Networking/#week-10","text":"Missing Features : When developing the second game in our engine, we realized that we were missing some key things like parenting and snapping over the network. So we implemented them! Patch Notes: Making the Network \"Local\" : Our NetworkTransform class originally performed all of its calculations in world space to keep things uniform, but that meant sending way more synchronization messages than needed when parenting! As a result, we swapped world space out for local space.","title":"Week 10"},{"location":"compendium/Networking/#week-13","text":"Networked Game Management : Although all of our in-game needs were pretty much covered, we still lacked many out-of-game things, such as connection callbacks and networked level loading . Network Discovery : Another big out-of-game feature we were missing was a lobby system! Our previous systems required the player to know the IP of their host, but we implemented a LAN broadcasting system using some basic socket programming.","title":"Week 13"},{"location":"compendium/Networking/#relevant-interviews","text":"","title":"Relevant Interviews"},{"location":"compendium/Networking/#martin-middleton","text":"Technology from Flower to Journey Journey's Peer Networking System","title":"Martin Middleton"},{"location":"compendium/Networking/#amit-patel-rob-shillingsburg","text":"","title":"Amit Patel &amp; Rob Shillingsburg"},{"location":"compendium/Networking/#walt-destler","text":"","title":"Walt Destler"},{"location":"compendium/Networking/#postmortem","text":"Short disclaimer: We never dove deeply into game networking for various reasons, so we can't speak much towards advanced network programming or any beyond-basic features like client-side prediction or world state delta encoding; if you're interested in those things, there are some great papers from games like TRIBES and Doom III that have good information about them. Instead, what we can write about is how one would even start doing network programming for games, especially as someone who hasn't done any network programming before. First, just because you're doing network programming for a game does not mean that you have to use the TRIBES Engine networking model \u2014especially if you're not even making a first-person shooter! We mistakenly believed that we needed to encode the world state and intelligently map out all of our entities as serializable objects when we were starting our networking features, largely due to Multiplayer Game Programming by Glazer and Madhav (which is a great introduction to network programming by the way!). We didn't know what all of that meant, or even what the data was supposed to look like, but we believed that we needed to create a robust, effective, and optimized network model for our engine, so you can imagine what we looked like when we were trying to design an architecture for something we had no idea about! Fortunately, one of our programmers had some higher level networking experience and wisely suggested that we start small: All we need are network messages. We were building on top of yojimbo so we avoided having to program below the session layer , and we were able to implement some rudimentary network messages very quickly. The moment we got messages going across the network, we were ecstatic! This was the cornerstone of anything that we would need for game networking, and we managed to make it happen! We only added one more advanced feature to our network repertoire beyond network messages because we never really needed anything else. Fundamentally, all you need in a networked game is a signal that will tell other computers to run some code, which is what those messages can do. By getting over this bump in the road, we were able to move onto doing networked spawning, networked object transforms, and more, and we didn't waste much of our time trying to over engineer the networking simply because we didn't understand it better. The networking features of a game engine cannot be predicted. Even if you're targeting a specific genre with that engine (like twin-stick shooters or something weird like that), you'll be adding in features that you think games will need and not the ones they actually need. We spent time on things that we thought would be important, such as optimizing our network transforms to take up less bandwidth, and it turned out that we were missing much more important features, like an API for parenting objects across the network. The former wasn't important because in the end, our networking was never able to extend across a LAN setup, whereas the latter was needed for the immediately next game that we made after finishing our network manager API. But we believed that optimizations would be really important because that's stressed a lot by network programmers who are far more experienced than anyone on our team. We talk about this elsewhere, but what's important when developing any game engine system (networking included) is to actually make games when you're doing that . A game engine in isolation would yield networked tech demos, and tech demos test what you already know you have. What's more is that making games also tests the real use cases of the games; as it turns out, a LAN doesn't really need packet optimization, but it does need gameplay quality of life features regarding networking like snapping objects into position. Knowing that requires stepping up and making something real with the system, not just setting up test suites. More things to know: Every new feature that you get working over a network will be an exciting moment, and you should celebrate it with as much of your team as possible . Building your networking system on top of a 3 rd party library will get you most of the way there, especially if you don't have great needs from your networking. Our network message system is flexible and effective, but it requires a lot of duplicated code, which makes it a hassle to set up for new functionalities over the network. Know your end goal before designing your network system architecture. LAN networking should prioritize different things than long-distance networking, like responsiveness over packet reduction.","title":"Postmortem"},{"location":"compendium/Other/","text":"Other Topics \u00b6 Debugging Postmortem \u00b6 Debug drawing was our (and very likely will be your) most used feature in the engine. You can get some utility out of the output log, but seeing information visually in the same space as your actual games and tests makes a world of difference. We didn't anticipate that this would be so heavily used because we figured that our systems would be testable in their own individual ways, but fortunately, one of our programmers decided to do it early on and we've seen great dividends since. Particularly, whenever we're doing any precise or math-oriented feature in the 3D space (i.e. collisions), our debug drawings are usually the only effective form of information. That being said, debug drawing is expensive, invasive, and clunky, so be mindful of how you implement it. Even if you implement it using your rendering engine (which we recommend you do to keep things simple), drawing out a bunch of lines and faces based on individual function calls weighs heavily on the system, so make sure you preprocess your debug code out of your release builds! Similarly, debugging a low-level graphics API can be difficult, especially when you're using a rendering library for your graphics, so try to do your debug drawing features in a sterile project before integrating it all with your own engine's rendering. Visual Studio has a lot of useful features, and knowing how to use them can mean the difference of solving a lot of your bugs. These include the Watch windows (Auto, Local, and general Watch ones), the Call Stack, the Output, the Memory watcher windows, and of course, code and data breakpoints. We won't cover those things here (Google will do them more justice than we can), but if you decide to jump into using Visual Studio or C++ without learning how to use these tools, you will be making a lot of guesswork when trying to fix seemingly strange bugs that in actuality are related to memory alignment, hidden callbacks, or something even worse. We had the fortune of having four programmers who all knew some of what Visual Studio has to offer, so we would often see a good debugging practice from one of the other programmers and adopt it ourselves. But it's very easy to work on a problem the only way you know how for hours without considering that there may be a better way, so you should make an effort to stop and ask yourself, \"is there a better way to find this bug?\" More things to know: Unit testing can be avoided, at great risk. For low level APIs like a math library, you must unit test everything because that will permeate throughout your code. For higher level things, you should do partial unit tests for systems that tend to work with other systems in case a bug regresses it, such as your scene hierarchy. Tech demos will yield similar results, but not as stringently. Be excessive with your assert messages. Asserts are pulled out of release builds entirely, so you won't affect your final performance, and they avoid silly errors that shouldn't happen but do all of the time anyways. Know the difference between your errors and your asserts. For us, asserts are bugs that exist in the engine and are caused by the engine (like allocating less than 0 bytes), whereas exceptions are errors that are caused by a user misusing an engine feature. Visual Studio can be really annoying with how it presents error messages, and when you make a DLL of your engine, it will only get worse. Find a robust method of communicating engine errors to your game developers if you want them to be able to know what goes wrong within your engine.","title":"Other Topics"},{"location":"compendium/Other/#other-topics","text":"","title":"Other Topics"},{"location":"compendium/Other/#debugging-postmortem","text":"Debug drawing was our (and very likely will be your) most used feature in the engine. You can get some utility out of the output log, but seeing information visually in the same space as your actual games and tests makes a world of difference. We didn't anticipate that this would be so heavily used because we figured that our systems would be testable in their own individual ways, but fortunately, one of our programmers decided to do it early on and we've seen great dividends since. Particularly, whenever we're doing any precise or math-oriented feature in the 3D space (i.e. collisions), our debug drawings are usually the only effective form of information. That being said, debug drawing is expensive, invasive, and clunky, so be mindful of how you implement it. Even if you implement it using your rendering engine (which we recommend you do to keep things simple), drawing out a bunch of lines and faces based on individual function calls weighs heavily on the system, so make sure you preprocess your debug code out of your release builds! Similarly, debugging a low-level graphics API can be difficult, especially when you're using a rendering library for your graphics, so try to do your debug drawing features in a sterile project before integrating it all with your own engine's rendering. Visual Studio has a lot of useful features, and knowing how to use them can mean the difference of solving a lot of your bugs. These include the Watch windows (Auto, Local, and general Watch ones), the Call Stack, the Output, the Memory watcher windows, and of course, code and data breakpoints. We won't cover those things here (Google will do them more justice than we can), but if you decide to jump into using Visual Studio or C++ without learning how to use these tools, you will be making a lot of guesswork when trying to fix seemingly strange bugs that in actuality are related to memory alignment, hidden callbacks, or something even worse. We had the fortune of having four programmers who all knew some of what Visual Studio has to offer, so we would often see a good debugging practice from one of the other programmers and adopt it ourselves. But it's very easy to work on a problem the only way you know how for hours without considering that there may be a better way, so you should make an effort to stop and ask yourself, \"is there a better way to find this bug?\" More things to know: Unit testing can be avoided, at great risk. For low level APIs like a math library, you must unit test everything because that will permeate throughout your code. For higher level things, you should do partial unit tests for systems that tend to work with other systems in case a bug regresses it, such as your scene hierarchy. Tech demos will yield similar results, but not as stringently. Be excessive with your assert messages. Asserts are pulled out of release builds entirely, so you won't affect your final performance, and they avoid silly errors that shouldn't happen but do all of the time anyways. Know the difference between your errors and your asserts. For us, asserts are bugs that exist in the engine and are caused by the engine (like allocating less than 0 bytes), whereas exceptions are errors that are caused by a user misusing an engine feature. Visual Studio can be really annoying with how it presents error messages, and when you make a DLL of your engine, it will only get worse. Find a robust method of communicating engine errors to your game developers if you want them to be able to know what goes wrong within your engine.","title":"Debugging Postmortem"},{"location":"compendium/Scene-Loop/","text":"Scene & Engine Loop \u00b6 Timeline \u00b6 Week 0 \u00b6 Module Manager : To control the start-up and shut-down sequences with all of our engine's modules, we created a module manager. We also had a lengthy discussion on its design. Week 3 \u00b6 Engine Loop : We discussed the merits of different simulation timings within a game engine loop, and then determined on a \"best of both worlds\" method for running a variable timestep update (for things like rendering) and a fixed timestep update (for things like networking). Timer : We created a clock class to support multiple clocks in-engine and a stopwatch class to record time, which is definitely useful for our engine loop. Week 5 \u00b6 Module Manager Say Goodbye : We discovered that our module manager was adding more work than it was taking away, and with more systems coming into the mix, it had to go. This also allowed us to refactor some of our engine's update API. Scene Graph: Data-Oriented or Object-Oriented? : Before developing our scene graph, we compared data-oriented designs for it against object-oriented designs . Our decision? A little bit of both! Patch Notes: Explicit StartUp, Update, ShutDown in the Engine Loop : We hit our first \"systemic\" bug with a bad merge shutting down a module twice, and we were able to notice it almost immediately because of our engine loop's design. Week 6 \u00b6 Level, a.k.a. Our Scene Graph : Our scene graph was designed to be Level-Entity-Component, which would allow us to reason about our game's object hierarchy more easily than a data-oriented approach. We wanted loading levels to be as painless as possible for both us and the game developer, so we decided on a C++ level definition system that utilizes some templating reflection magic to register levels to allow loading via user-defined C++ classes. Transform, a.k.a. ...Transform : Our transform hierarchy also began to take shape, which needed to support translation, rotation, and scale in a hierarchical (i.e. parented) fashion. Functionally , we needed our transforms to behave just like all other transform in game engines, but we also needed to optimize our matrix calculations to prevent large, complex scenes from hitting our performance hard. We also decided to give the hierarchy over to the Transform instead of our Entity class, and then we wrapped up our first pass with a flyby camera implementation ! Week 8 \u00b6 Components Inheriting Components : Some of our components, such as our Collider components, inherit from some base class that makes organization and functionality easier, so we needed to be able to get any of such subclass objects with a single GetComponent call. We tried our previous registration pattern and manual template specialization , but the solution came about from using a dummy template parameter to partially specialize the components, which would allow for a very clean API and a nice inheritance-friendly component system! Patch Notes: Exiting the Engine Loop : To escape our engine loop, we hardcoded the Escape key to exit the engine. This upset some of us, which triggered a bigger discussion over the engine's startup and shutdown API. We investigated some options for making EngineLoop more usable for the game developer, and we finally created an Application static class that would purely be an interface for the user. Week 9 \u00b6 More Component Features! : As we built out more of the engine to high-level features, we realized that our component system was still lacking several features. An Awake function was implemented to allow same-frame component initialization, our component hierarchy was optimized by preprocessing it to reduce lookup time, and we added a uniqueness check for components that really are unique butterflies. Week 10 \u00b6 Patch Notes: Initialization Timing with Memory Management : We began using our memory manager for even static variables, which posed a problem because static initialization happens before our system startup! So we made some decisions to reorganize our systems to properly abide by the initialization sequence. Week 11 \u00b6 Level Loading : Loading a level at startup was already a solved problem, but now we needed to load from one level to another. We were prepared to implement this feature, but we found many bugs from assumptions that we were not unloading or reinitializing certain systems upon loading another level. Week 12 \u00b6 Patch Notes: API Design Improvements in Level, Transform, Entity and Component : With our game jam approaching, we revisited our API design for our most basic engine classes. This included our Component , Transform , Entity , and Level classes along with their creation and access methods, and we ended up doing more than one project refactor over the entire process! Week 13 \u00b6 Static Entities Get Stuck : Static entities within our game engine have had a trickle of features be built onto them, and at this point, we added static transforms. Unfortunately, this immediately prevented us from moving any static entities from the origin because that's where they get instantiated! So we redefined \"static\" to mean after the level finishes loading to fix this problem. Revisiting transform in Component The Class : We originally used static transform and entity objects for our component initialization, but this felt very gross, so we casted the const-ness out of our variables in order to get the same functionality (which felt gross, but necessary for the time being). Relevant Interviews \u00b6 Martin Middleton \u00b6 Engines Should Guide Games, Not Direct Them Custom Entity System in Unity Amandine Coget \u00b6 GUI: Immediate vs Retained Modes Postmortem \u00b6 Your transform class will be used everywhere, for better or for worse. When we began our development, we knew that the scene hierarchy of the engine would be important, but we thought of that more as an architectural and design importance across the whole hierarchy. In reality, how you lay out the data is really what impacts the rest of your development. Our team had many debates about where certain attributes should exist\u2014our transforms, our entities, our components, or elsewhere\u2014and those debates spanned many domains, including API design, data-oriented programming, and pure convenience. There's no silver bullet for this decision, so we ended up choosing what was most convenient for our short timeline: Packing the transform class with most of our necessary scene data. Because we decided on this, we were able to move on quickly and implement very scene-dependent features like collisions early on. The longer you debate about a decision like this, the more refactoring you'll have to do elsewhere. Templates are pretty magical, but the sacrifice is real. We knew coming into the project that templates are quite the issue in the realm of game programming, and we wanted to avoid them whenever possible. The reality, however, is that programming without templates tends to take a lot more time and care unless you are programming monolithic systems, which has its own slew of problems. We ended up using templates throughout our engine by the end of development, and we did genuinely benefit from it: Our development was greatly simplified, and it enabled our levels and components to register themselves into our engine painlessly and without some heavy RTTI system. However, also by the end of development, our engine would take a good three-plus minutes to build a single configuration for, and we when we were iterating heavily this would burn us pretty bad. Templates also mask exceptions since the code doesn't have too much to infer from, and in heavily used systems like components, debugging becomes impossible without prior knowledge. The best advice we can give is that templates are capable of wondrous things, but after you've seen what they can do, try to replace them with a non-templated version and see how you fair with that. Some teams are able to keep sane build times because they're not just smart about they do with the code, but also what they put into the code. More things to know: Your scene hierarchy is where most of your matrix math bugs will pop up, and you should be very wary of its robustness until you have at least a few systems that have proven the scene hierarchy to be stable and accurate. The memory layout of your entities and components can boost performance. We originally used pointers for our entity-component architecture, but after switching to a pooled entity system, we saw a 40% increase in performance. Implementing a variable-and-fixed-update engine is not too difficult in theory, but tends to be tricky in practice. Ours has a catch-up mechanism for the fixed update that doesn't prevent frames from being dropped but rather increases the frame drops caused! But this doesn't get to be an issue until you're really pushing the performance of your engine, so don't worry about it at the start. Some component features are not used as much as you might think they will be, and others shouldn't be used as much as you think they should. For instance, we can restrict a component to being unique on an entity in our engine, but that's only specifically necessary for our collision handling system. In all other cases, it slows down performance!","title":"Scene & Engine Loop"},{"location":"compendium/Scene-Loop/#scene-engine-loop","text":"","title":"Scene &amp; Engine Loop"},{"location":"compendium/Scene-Loop/#timeline","text":"","title":"Timeline"},{"location":"compendium/Scene-Loop/#week-0","text":"Module Manager : To control the start-up and shut-down sequences with all of our engine's modules, we created a module manager. We also had a lengthy discussion on its design.","title":"Week 0"},{"location":"compendium/Scene-Loop/#week-3","text":"Engine Loop : We discussed the merits of different simulation timings within a game engine loop, and then determined on a \"best of both worlds\" method for running a variable timestep update (for things like rendering) and a fixed timestep update (for things like networking). Timer : We created a clock class to support multiple clocks in-engine and a stopwatch class to record time, which is definitely useful for our engine loop.","title":"Week 3"},{"location":"compendium/Scene-Loop/#week-5","text":"Module Manager Say Goodbye : We discovered that our module manager was adding more work than it was taking away, and with more systems coming into the mix, it had to go. This also allowed us to refactor some of our engine's update API. Scene Graph: Data-Oriented or Object-Oriented? : Before developing our scene graph, we compared data-oriented designs for it against object-oriented designs . Our decision? A little bit of both! Patch Notes: Explicit StartUp, Update, ShutDown in the Engine Loop : We hit our first \"systemic\" bug with a bad merge shutting down a module twice, and we were able to notice it almost immediately because of our engine loop's design.","title":"Week 5"},{"location":"compendium/Scene-Loop/#week-6","text":"Level, a.k.a. Our Scene Graph : Our scene graph was designed to be Level-Entity-Component, which would allow us to reason about our game's object hierarchy more easily than a data-oriented approach. We wanted loading levels to be as painless as possible for both us and the game developer, so we decided on a C++ level definition system that utilizes some templating reflection magic to register levels to allow loading via user-defined C++ classes. Transform, a.k.a. ...Transform : Our transform hierarchy also began to take shape, which needed to support translation, rotation, and scale in a hierarchical (i.e. parented) fashion. Functionally , we needed our transforms to behave just like all other transform in game engines, but we also needed to optimize our matrix calculations to prevent large, complex scenes from hitting our performance hard. We also decided to give the hierarchy over to the Transform instead of our Entity class, and then we wrapped up our first pass with a flyby camera implementation !","title":"Week 6"},{"location":"compendium/Scene-Loop/#week-8","text":"Components Inheriting Components : Some of our components, such as our Collider components, inherit from some base class that makes organization and functionality easier, so we needed to be able to get any of such subclass objects with a single GetComponent call. We tried our previous registration pattern and manual template specialization , but the solution came about from using a dummy template parameter to partially specialize the components, which would allow for a very clean API and a nice inheritance-friendly component system! Patch Notes: Exiting the Engine Loop : To escape our engine loop, we hardcoded the Escape key to exit the engine. This upset some of us, which triggered a bigger discussion over the engine's startup and shutdown API. We investigated some options for making EngineLoop more usable for the game developer, and we finally created an Application static class that would purely be an interface for the user.","title":"Week 8"},{"location":"compendium/Scene-Loop/#week-9","text":"More Component Features! : As we built out more of the engine to high-level features, we realized that our component system was still lacking several features. An Awake function was implemented to allow same-frame component initialization, our component hierarchy was optimized by preprocessing it to reduce lookup time, and we added a uniqueness check for components that really are unique butterflies.","title":"Week 9"},{"location":"compendium/Scene-Loop/#week-10","text":"Patch Notes: Initialization Timing with Memory Management : We began using our memory manager for even static variables, which posed a problem because static initialization happens before our system startup! So we made some decisions to reorganize our systems to properly abide by the initialization sequence.","title":"Week 10"},{"location":"compendium/Scene-Loop/#week-11","text":"Level Loading : Loading a level at startup was already a solved problem, but now we needed to load from one level to another. We were prepared to implement this feature, but we found many bugs from assumptions that we were not unloading or reinitializing certain systems upon loading another level.","title":"Week 11"},{"location":"compendium/Scene-Loop/#week-12","text":"Patch Notes: API Design Improvements in Level, Transform, Entity and Component : With our game jam approaching, we revisited our API design for our most basic engine classes. This included our Component , Transform , Entity , and Level classes along with their creation and access methods, and we ended up doing more than one project refactor over the entire process!","title":"Week 12"},{"location":"compendium/Scene-Loop/#week-13","text":"Static Entities Get Stuck : Static entities within our game engine have had a trickle of features be built onto them, and at this point, we added static transforms. Unfortunately, this immediately prevented us from moving any static entities from the origin because that's where they get instantiated! So we redefined \"static\" to mean after the level finishes loading to fix this problem. Revisiting transform in Component The Class : We originally used static transform and entity objects for our component initialization, but this felt very gross, so we casted the const-ness out of our variables in order to get the same functionality (which felt gross, but necessary for the time being).","title":"Week 13"},{"location":"compendium/Scene-Loop/#relevant-interviews","text":"","title":"Relevant Interviews"},{"location":"compendium/Scene-Loop/#martin-middleton","text":"Engines Should Guide Games, Not Direct Them Custom Entity System in Unity","title":"Martin Middleton"},{"location":"compendium/Scene-Loop/#amandine-coget","text":"GUI: Immediate vs Retained Modes","title":"Amandine Coget"},{"location":"compendium/Scene-Loop/#postmortem","text":"Your transform class will be used everywhere, for better or for worse. When we began our development, we knew that the scene hierarchy of the engine would be important, but we thought of that more as an architectural and design importance across the whole hierarchy. In reality, how you lay out the data is really what impacts the rest of your development. Our team had many debates about where certain attributes should exist\u2014our transforms, our entities, our components, or elsewhere\u2014and those debates spanned many domains, including API design, data-oriented programming, and pure convenience. There's no silver bullet for this decision, so we ended up choosing what was most convenient for our short timeline: Packing the transform class with most of our necessary scene data. Because we decided on this, we were able to move on quickly and implement very scene-dependent features like collisions early on. The longer you debate about a decision like this, the more refactoring you'll have to do elsewhere. Templates are pretty magical, but the sacrifice is real. We knew coming into the project that templates are quite the issue in the realm of game programming, and we wanted to avoid them whenever possible. The reality, however, is that programming without templates tends to take a lot more time and care unless you are programming monolithic systems, which has its own slew of problems. We ended up using templates throughout our engine by the end of development, and we did genuinely benefit from it: Our development was greatly simplified, and it enabled our levels and components to register themselves into our engine painlessly and without some heavy RTTI system. However, also by the end of development, our engine would take a good three-plus minutes to build a single configuration for, and we when we were iterating heavily this would burn us pretty bad. Templates also mask exceptions since the code doesn't have too much to infer from, and in heavily used systems like components, debugging becomes impossible without prior knowledge. The best advice we can give is that templates are capable of wondrous things, but after you've seen what they can do, try to replace them with a non-templated version and see how you fair with that. Some teams are able to keep sane build times because they're not just smart about they do with the code, but also what they put into the code. More things to know: Your scene hierarchy is where most of your matrix math bugs will pop up, and you should be very wary of its robustness until you have at least a few systems that have proven the scene hierarchy to be stable and accurate. The memory layout of your entities and components can boost performance. We originally used pointers for our entity-component architecture, but after switching to a pooled entity system, we saw a 40% increase in performance. Implementing a variable-and-fixed-update engine is not too difficult in theory, but tends to be tricky in practice. Ours has a catch-up mechanism for the fixed update that doesn't prevent frames from being dropped but rather increases the frame drops caused! But this doesn't get to be an issue until you're really pushing the performance of your engine, so don't worry about it at the start. Some component features are not used as much as you might think they will be, and others shouldn't be used as much as you think they should. For instance, we can restrict a component to being unique on an entity in our engine, but that's only specifically necessary for our collision handling system. In all other cases, it slows down performance!","title":"Postmortem"},{"location":"compendium/Tools/","text":"Tools \u00b6 Timeline \u00b6 Week 1 \u00b6 Profiler : We imported Brofiler , an open-source C++ game profiler, for our profiling needs in our engine, and we forked the repo in preparation of some needed changed. Week 6 \u00b6 Debug Drawing : We implemented a debug drawing system in OpenGL , which required us to dip deeply into our 3D math knowledge early on. Getting it working was frustrating due to dealing with Horde3D at the same time, but it almost immediately paid dividends by revealing some early bugs in our engine . Week 7 \u00b6 Patch Notes: Kind of an Editor : We leveraged our GUI system to make an in-game \"editor\" to see basic transform, hierarchy, and component information. Week 9 \u00b6 In-Game Console : Along with our Logger , config variable, and editor systems, we decided that we wanted a Console that would allow us to change configuration variables at runtime. We did a lot of GUI and backend systems coding to make not just an in-game console, but one with autocomplete! Week 10 \u00b6 Editor Becomes a Component : With the advent of our levels and components, we realized we should probably package our \"editor\" as a nice optional system for the developer to choose to use. Upon looking into this further, we realized that our original editor code was embedded into our core system code! So we pulled everything out into an EditorComponent to keep things convenient and modular. Week 11 \u00b6 Asset Processing Tool : With more games being developed, we found ourselves annoyed with the asset processing pipeline of Horde3D, so we built an asset processing tool to process our COLLADA (.dae) files more efficiently and conveniently. Week 12 \u00b6 Config Editor Tool : As we were developing, we found ourselves tweaking our config files more often, but we wouldn't know what variables to tweak. So we developed a tool to help ourselves and other developers modify our configuration files without having to know a priori the exact properties that we want to modify. Relevant Interviews \u00b6 Elan Ruskin \u00b6 Designer-Driven Tools Intimate Bond of Engine and Tools Effectively Using Profiling Shanee Nishry \u00b6 Editing with a Level Editor Jeff Preshing \u00b6 Low-level Debugging Profiling How Profiling Can Differ Martin Middleton \u00b6 Versioning & Deploying Tools Aras Pranckevi\u010dius \u00b6 Reflecting on Windows Unity Editor and Graphics Abstraction Amandine Coget \u00b6 Thinking about Usability Tommy Refenes \u00b6 Customizing toward Flash Tools for Your Teammates Postmortem \u00b6 If you think a tool will be useful, develop a minimum viable product and get that in front of your users before anything else. Seeing the real-world usage of a tool is the only true way to test if a tool will be effective or not, and that requires getting something made and in the hands of users for a decent amount of time. We were unable to predict which of our own tools would be the most used during our development timeline, but by the end, we were able to clearly tell that our asset processing tool and our engine exporting tool were used more than anything else. The other tools that we made fell somewhere between unused and somewhat used, and as far as the engine is concerned, we probably didn't need to develop the ones that were left unused. Engine development is tied with tool development because tools will facilitate how you build your engine. Tools are what's used to debug the engine. You might call it something else, but the applications and software that you make for the purpose of easing or improving engine development are the same as any tools that you'd make for game development. Recognize the things that you make as part of your engine development that aren't engine systems or features as tools, and design them accordingly. Tools have users who intend to get something specific out of them, so if you think about what that is for a given tool before you develop it, then you can be a lot more efficient about making the tool. More things to know: Build an entity inspector and a scene-graph hierarchy view so that you can see all of the entities in your game, their transformations, and hopefully any other information on them as well (such as components). It doesn't need to be fancy, and it will really help with debugging throughout your engine's development and even into game development. A profiler won't be useful to an engine early on, and it's not usually very hard to integrate. In some initial fraction of an engine's development, the developers will be focusing on correctness over performance, and it won't be difficult to see if someone has caused performance issues with fundamental mistakes. Later down the line, you can opt to use the profiler to see the smaller performance hits accumulate and what needs your attention. An in-game console is nice to have, but isn't very necessary in this day and age. We created one mostly out of interest, and we found that none of us really used it after it was initially developed. Asset processing should be easy and seamless. If possible, create an automatic asset cooker that will prepare any game or engine assets. If it's not possible, then create a tool that is simple to use and doesn't force the user to edit any specific file manually. A lot of assets flow in and out of an engine/game, and reducing friction there will not just save time but will also reduce frustration among developers. Debug drawing will probably be the most used feature by the developers of the engine. Having in-engine visuals is a much stronger indicator that something is wrong than error messages. The debug drawing doesn't need to be very performant either because it can be removed using preprocessing for any release builds.","title":"Tools"},{"location":"compendium/Tools/#tools","text":"","title":"Tools"},{"location":"compendium/Tools/#timeline","text":"","title":"Timeline"},{"location":"compendium/Tools/#week-1","text":"Profiler : We imported Brofiler , an open-source C++ game profiler, for our profiling needs in our engine, and we forked the repo in preparation of some needed changed.","title":"Week 1"},{"location":"compendium/Tools/#week-6","text":"Debug Drawing : We implemented a debug drawing system in OpenGL , which required us to dip deeply into our 3D math knowledge early on. Getting it working was frustrating due to dealing with Horde3D at the same time, but it almost immediately paid dividends by revealing some early bugs in our engine .","title":"Week 6"},{"location":"compendium/Tools/#week-7","text":"Patch Notes: Kind of an Editor : We leveraged our GUI system to make an in-game \"editor\" to see basic transform, hierarchy, and component information.","title":"Week 7"},{"location":"compendium/Tools/#week-9","text":"In-Game Console : Along with our Logger , config variable, and editor systems, we decided that we wanted a Console that would allow us to change configuration variables at runtime. We did a lot of GUI and backend systems coding to make not just an in-game console, but one with autocomplete!","title":"Week 9"},{"location":"compendium/Tools/#week-10","text":"Editor Becomes a Component : With the advent of our levels and components, we realized we should probably package our \"editor\" as a nice optional system for the developer to choose to use. Upon looking into this further, we realized that our original editor code was embedded into our core system code! So we pulled everything out into an EditorComponent to keep things convenient and modular.","title":"Week 10"},{"location":"compendium/Tools/#week-11","text":"Asset Processing Tool : With more games being developed, we found ourselves annoyed with the asset processing pipeline of Horde3D, so we built an asset processing tool to process our COLLADA (.dae) files more efficiently and conveniently.","title":"Week 11"},{"location":"compendium/Tools/#week-12","text":"Config Editor Tool : As we were developing, we found ourselves tweaking our config files more often, but we wouldn't know what variables to tweak. So we developed a tool to help ourselves and other developers modify our configuration files without having to know a priori the exact properties that we want to modify.","title":"Week 12"},{"location":"compendium/Tools/#relevant-interviews","text":"","title":"Relevant Interviews"},{"location":"compendium/Tools/#elan-ruskin","text":"Designer-Driven Tools Intimate Bond of Engine and Tools Effectively Using Profiling","title":"Elan Ruskin"},{"location":"compendium/Tools/#shanee-nishry","text":"Editing with a Level Editor","title":"Shanee Nishry"},{"location":"compendium/Tools/#jeff-preshing","text":"Low-level Debugging Profiling How Profiling Can Differ","title":"Jeff Preshing"},{"location":"compendium/Tools/#martin-middleton","text":"Versioning & Deploying Tools","title":"Martin Middleton"},{"location":"compendium/Tools/#aras-pranckevicius","text":"Reflecting on Windows Unity Editor and Graphics Abstraction","title":"Aras Pranckevi\u010dius"},{"location":"compendium/Tools/#amandine-coget","text":"Thinking about Usability","title":"Amandine Coget"},{"location":"compendium/Tools/#tommy-refenes","text":"Customizing toward Flash Tools for Your Teammates","title":"Tommy Refenes"},{"location":"compendium/Tools/#postmortem","text":"If you think a tool will be useful, develop a minimum viable product and get that in front of your users before anything else. Seeing the real-world usage of a tool is the only true way to test if a tool will be effective or not, and that requires getting something made and in the hands of users for a decent amount of time. We were unable to predict which of our own tools would be the most used during our development timeline, but by the end, we were able to clearly tell that our asset processing tool and our engine exporting tool were used more than anything else. The other tools that we made fell somewhere between unused and somewhat used, and as far as the engine is concerned, we probably didn't need to develop the ones that were left unused. Engine development is tied with tool development because tools will facilitate how you build your engine. Tools are what's used to debug the engine. You might call it something else, but the applications and software that you make for the purpose of easing or improving engine development are the same as any tools that you'd make for game development. Recognize the things that you make as part of your engine development that aren't engine systems or features as tools, and design them accordingly. Tools have users who intend to get something specific out of them, so if you think about what that is for a given tool before you develop it, then you can be a lot more efficient about making the tool. More things to know: Build an entity inspector and a scene-graph hierarchy view so that you can see all of the entities in your game, their transformations, and hopefully any other information on them as well (such as components). It doesn't need to be fancy, and it will really help with debugging throughout your engine's development and even into game development. A profiler won't be useful to an engine early on, and it's not usually very hard to integrate. In some initial fraction of an engine's development, the developers will be focusing on correctness over performance, and it won't be difficult to see if someone has caused performance issues with fundamental mistakes. Later down the line, you can opt to use the profiler to see the smaller performance hits accumulate and what needs your attention. An in-game console is nice to have, but isn't very necessary in this day and age. We created one mostly out of interest, and we found that none of us really used it after it was initially developed. Asset processing should be easy and seamless. If possible, create an automatic asset cooker that will prepare any game or engine assets. If it's not possible, then create a tool that is simple to use and doesn't force the user to edit any specific file manually. A lot of assets flow in and out of an engine/game, and reducing friction there will not just save time but will also reduce frustration among developers. Debug drawing will probably be the most used feature by the developers of the engine. Having in-engine visuals is a much stronger indicator that something is wrong than error messages. The debug drawing doesn't need to be very performant either because it can be removed using preprocessing for any release builds.","title":"Postmortem"},{"location":"engine_docs/audio/","text":"Audio \u00b6 Our audio system is similar to Unity's - we also have an AudioSource component and a AudioClip class that hold actual clips. The audio level is the best place for you to get started. Essential API \u00b6 AudioClip class is responsible for loading in audio files and show be passed to AudioSource component to be played AudioClip::Load(\"filePath\", \"soundName\") : Load audio file as AudioClip , returns AudioClip* . Path is relative to \"Resources\" AudioClip::Find(\"soundName\") : Find already loaded audio file, returns AudioClip* AudioSource component is responsible for playing sound and managing its properties AudioSource::AudioSource(AudioClip* clip) : construct an AudioSource component by passing in an AudioClip* AudioSource::SetAudioClip(AudioClip* clip) : Set the audio clip to be played on an audio source AudioSource::Play() , AudioSource::Pause() , AudioSource::SetVolume(float volume) AudioSource::SetProperty(Property prop, bool value) : Set properties like 3D, loop, and mute AudioLisenter component should be added to your camera or another appropriate entity to enable 3D sound. There should only be 1 AudioListener in the level Code snippets \u00b6 Play a 3D sound:: Entity * cameraEntity = Entity :: Instantiate ( \"Camera\" ); cameraEntity -> AddComponent < CameraComponent > (); cameraEntity -> AddComponent < AudioListener > (); Entity * audio3D = Entity :: Instantiate ( \"3DAudio\" ); audio3D -> transform -> SetWorldPos ( Math :: Vector3 { 0 , 0 , 0 }); // AudioClip::Load loads the audio in the filepath under resource_path // AudioSource constructor parameters: // 0b001 - properties of AudioSource 0b=binary then from left to right: // IS_MUTE, IS_LOOP, IS_3D // This AudioSource is NOT muted, NOT looping, but IS 3D AudioSource * src3D = audio3D -> AddComponent < AudioSource > ( 0 b001 , AudioClip :: Load ( \"Sound/zombie-hit.wav\" )); // AudioPlay component which plays AudioSource on KeyCode press audio3D -> Play (); Play a 2D sound: Entity * audio2D = Entity :: Instantiate ( \"2DAudio\" ); AudioClip * clip = AudioClip :: Load ( \"Sound/sample_sound.mp3\" ); AudioSource * src2D = audio2D -> AddComponent < AudioSource > (); src2D -> SetProperty ( AudioSource :: Property :: IS_3D , false ); src2D -> SetProperty ( AudioSource :: Property :: LOOP , true ); src2D -> clip = clip ; src2D -> SetVolume ( 0.5f ); src2D -> Play ();","title":"Audio"},{"location":"engine_docs/audio/#audio","text":"Our audio system is similar to Unity's - we also have an AudioSource component and a AudioClip class that hold actual clips. The audio level is the best place for you to get started.","title":"Audio"},{"location":"engine_docs/audio/#essential-api","text":"AudioClip class is responsible for loading in audio files and show be passed to AudioSource component to be played AudioClip::Load(\"filePath\", \"soundName\") : Load audio file as AudioClip , returns AudioClip* . Path is relative to \"Resources\" AudioClip::Find(\"soundName\") : Find already loaded audio file, returns AudioClip* AudioSource component is responsible for playing sound and managing its properties AudioSource::AudioSource(AudioClip* clip) : construct an AudioSource component by passing in an AudioClip* AudioSource::SetAudioClip(AudioClip* clip) : Set the audio clip to be played on an audio source AudioSource::Play() , AudioSource::Pause() , AudioSource::SetVolume(float volume) AudioSource::SetProperty(Property prop, bool value) : Set properties like 3D, loop, and mute AudioLisenter component should be added to your camera or another appropriate entity to enable 3D sound. There should only be 1 AudioListener in the level","title":"Essential API"},{"location":"engine_docs/audio/#code-snippets","text":"Play a 3D sound:: Entity * cameraEntity = Entity :: Instantiate ( \"Camera\" ); cameraEntity -> AddComponent < CameraComponent > (); cameraEntity -> AddComponent < AudioListener > (); Entity * audio3D = Entity :: Instantiate ( \"3DAudio\" ); audio3D -> transform -> SetWorldPos ( Math :: Vector3 { 0 , 0 , 0 }); // AudioClip::Load loads the audio in the filepath under resource_path // AudioSource constructor parameters: // 0b001 - properties of AudioSource 0b=binary then from left to right: // IS_MUTE, IS_LOOP, IS_3D // This AudioSource is NOT muted, NOT looping, but IS 3D AudioSource * src3D = audio3D -> AddComponent < AudioSource > ( 0 b001 , AudioClip :: Load ( \"Sound/zombie-hit.wav\" )); // AudioPlay component which plays AudioSource on KeyCode press audio3D -> Play (); Play a 2D sound: Entity * audio2D = Entity :: Instantiate ( \"2DAudio\" ); AudioClip * clip = AudioClip :: Load ( \"Sound/sample_sound.mp3\" ); AudioSource * src2D = audio2D -> AddComponent < AudioSource > (); src2D -> SetProperty ( AudioSource :: Property :: IS_3D , false ); src2D -> SetProperty ( AudioSource :: Property :: LOOP , true ); src2D -> clip = clip ; src2D -> SetVolume ( 0.5f ); src2D -> Play ();","title":"Code snippets"},{"location":"engine_docs/basics/","text":"Basics \u00b6 The Isetta Engine uses an entity-component-system, comparable to Unity's GameObject-MonoBehavior system. For anything math-related, you need to use the Math namespace. For instance, for a 3D vector, you would use Math::Vector3 Assumptions/Conventions \u00b6 Coordinates are right-handed ( Vector3::left = (1, 0, 0) ) Matrices are row-column ( Matrix4 mat[ROW][COLUMN] ) When using the built-in MemoryManager , it does not protect you against yourself (in general the engine does not protect a naive user from shooting themselves in the foot) Execution Order \u00b6","title":"Basics"},{"location":"engine_docs/basics/#basics","text":"The Isetta Engine uses an entity-component-system, comparable to Unity's GameObject-MonoBehavior system. For anything math-related, you need to use the Math namespace. For instance, for a 3D vector, you would use Math::Vector3","title":"Basics"},{"location":"engine_docs/basics/#assumptionsconventions","text":"Coordinates are right-handed ( Vector3::left = (1, 0, 0) ) Matrices are row-column ( Matrix4 mat[ROW][COLUMN] ) When using the built-in MemoryManager , it does not protect you against yourself (in general the engine does not protect a naive user from shooting themselves in the foot)","title":"Assumptions/Conventions"},{"location":"engine_docs/basics/#execution-order","text":"","title":"Execution Order"},{"location":"engine_docs/collisions/","text":"Collisions \u00b6 The collisions level is the best place for you to get started. Collisions are detected by our three collision components: BoxCollider , SphereCollider , and CapsuleCollider . All colliders that are not set as triggers are also prevented from intersecting from one another. To attach any functions to a collider for when other colliders enter, stay, or exit from it, you can use the CollisionHandler component. Collision handlers work as a tree structure, so any colliders that exist directly under an entity with a CollisionHandler component (that is, there are no other entities with CollisionHandler components between it and the collider entity) will fire a function registered to that CollisionHandler . Essential API \u00b6 Collisions::Raycast(ray, rayHit, maxDistance) : fire a ray defined by ray up to the distance of maxDistance and see if it hits anything from the return value, then use rayHit to get info about the collision Entity::AddComponent<CollisionHandler>() : add a collision handler to an entity CollisionHandler::RegisterOnEnter(function<Collider*>) : Registers a given function (lambda or not) to the collision handler to be called when any of the colliders under the handler that are not covered by other handlers are entered by another collider. CollisionHandler::RegisterOnStay(function<Collider*>) CollisionHandler::RegisterOnExit(function<Collider*>) Code Snippets \u00b6 Creating a box collider and attaching a callback to be called when another collider enters it: Entity * entity = Entity :: Instantiate ( \"box-collider\" ); entity -> SetTransform ( Math :: Vector3 { 0 , 1 , 0 }, Math :: Vector3 { 0 , 0 , 0 }); // Create a box collider BoxCollider * bCol = entity -> AddComponent < BoxCollider > (); bCol -> isTrigger = true ; // CollisionHandler has callbacks of Enter/Stay/Exit CollisionHandler * handler = entity -> AddComponent < CollisionHandler > (); // CollisionHandler registering OnEnter callback handler -> RegisterOnEnter ([]( Collider * const col ) { LOG ( \"we have collided with \" + col -> entity -> GetName ()); });","title":"Collisions"},{"location":"engine_docs/collisions/#collisions","text":"The collisions level is the best place for you to get started. Collisions are detected by our three collision components: BoxCollider , SphereCollider , and CapsuleCollider . All colliders that are not set as triggers are also prevented from intersecting from one another. To attach any functions to a collider for when other colliders enter, stay, or exit from it, you can use the CollisionHandler component. Collision handlers work as a tree structure, so any colliders that exist directly under an entity with a CollisionHandler component (that is, there are no other entities with CollisionHandler components between it and the collider entity) will fire a function registered to that CollisionHandler .","title":"Collisions"},{"location":"engine_docs/collisions/#essential-api","text":"Collisions::Raycast(ray, rayHit, maxDistance) : fire a ray defined by ray up to the distance of maxDistance and see if it hits anything from the return value, then use rayHit to get info about the collision Entity::AddComponent<CollisionHandler>() : add a collision handler to an entity CollisionHandler::RegisterOnEnter(function<Collider*>) : Registers a given function (lambda or not) to the collision handler to be called when any of the colliders under the handler that are not covered by other handlers are entered by another collider. CollisionHandler::RegisterOnStay(function<Collider*>) CollisionHandler::RegisterOnExit(function<Collider*>)","title":"Essential API"},{"location":"engine_docs/collisions/#code-snippets","text":"Creating a box collider and attaching a callback to be called when another collider enters it: Entity * entity = Entity :: Instantiate ( \"box-collider\" ); entity -> SetTransform ( Math :: Vector3 { 0 , 1 , 0 }, Math :: Vector3 { 0 , 0 , 0 }); // Create a box collider BoxCollider * bCol = entity -> AddComponent < BoxCollider > (); bCol -> isTrigger = true ; // CollisionHandler has callbacks of Enter/Stay/Exit CollisionHandler * handler = entity -> AddComponent < CollisionHandler > (); // CollisionHandler registering OnEnter callback handler -> RegisterOnEnter ([]( Collider * const col ) { LOG ( \"we have collided with \" + col -> entity -> GetName ()); });","title":"Code Snippets"},{"location":"engine_docs/component/","text":"Components \u00b6 Components in the Isetta Engine are similar to components in other engines, they should be designed for individal tasks and have minimal coupling with other components. Components cannot be standalone/instantiated in a level without being attached to an entity, done with entity->AddComponent<COMPONENT, bool isActive = true>(Component constructor args...) . You can look at any of our example levels for usage entity is an instantiated entity variable as shown above COMPONENT is the class name of your component to add, ie. ExampleComponent bool isActive = true is whether the component starts enabled/disabled and defaults to starting active Component constructor args... are the arguments to pass to the component's constructor (can be empty to use the default constructor -- a component MUST have a default constructor in addition to any other constructors) A component is created with Isetta component macros: DEFINE_COMPONENT(COMPONENT, PARENT, bool unique) and DEFINE_COMPONENT_END(COMPONENT_NAME, PARENT) . Where: - COMPONENT is the name of your new component class, ie. ExampleComponent - PARENT is the name of the parent/base class of component, use Component if you have no base class - bool unique is whether the component must be unique on an entity (whether an entity could have multiple of this component) - The unique variable does NOT mean the component is unique across the level, ie. a singleton To create a component, create a header (.h) and cpp (.cpp) file of the name of your component: ExampleComponent.h #pragma once #include <IsettaEngine.h> using namespace Isetta ; DEFINE_COMPONENT ( ExampleComponent , Component , false ) private : // Private variables of your component public : // A component MUST have a default constructor ExampleComponent () = default ; // Awake is called once, immediately when the component is first created and enabled void Awake () override ; // Start is called once, on the first update frame after the component is created and enabled void Start () override ; // OnEnable is called immediately each time the component becomes active, including after creation void OnEnable () override ; // OnDisable is called immediately each time the component becomes inactive void OnDisable () override ; // Update is called each frame (variable delta time) void Update () override ; // GuiUpdate is called each frame (variable delta time), GUI can only be called in GuiUpdate void GuiUpdate () override ; // LateUpdate is called each frame (variable delta time) void LateUpdate () override ; // FixedUpdate is called on fixed time (constant delta time) void FixedUpdate () override ; // OnDestroy is called once when the component is destroyed void OnDestroy () override ; DEFINE_COMPONENT_END ( ExampleComponent , Component ) The cpp file then contains the definitions for each of these functions. Not all functions need to be defined, if not overriden, the function update will run faster. Example Here are empty template of component header to copy: #pragma once #include <IsettaEngine.h> using namespace Isetta ; DEFINE_COMPONENT ( COMPONENT_NAME , Component , false ) private : public : COMPONENT_NAME () = default ; DEFINE_COMPONENT_END ( COMPONENT_NAME , Component ) Warning Every component must have a default constructor! Warning Components header currently cannot be wrapped in any namespace. Essential API \u00b6 Entity::AddComponent<T> : Add a component to an entity Entity::GetComponent<T> : Get a component from an entity Component::SetActive(bool active) : Toggle active state Component::entity* : Components have a pointer to its owner entity Component::transform* : Components have a pointer to its entity's transform Life time methods see above Predefined Components \u00b6 In our engine, we have lots of predefined components that you can use out of the box! - AI - No Documentation Yet : Component Name Component Description Nav2DAgent Agent that is moved on a Nav2DPlane. Audio - Documentation : Component Name Component Description AudioSource ... AudioListener ... Collisions - Documentation : Component Name Component Description BoxCollider ... CapsuleCollider ... SphereCollider ... CollisionHandler ... General Components - Documentation : Component Name Component Description AxisDrawer ... FlyController ... GridComponent ... NetworkMonitor ... Graphics - Documentation : Component Name Component Description AnimationComponent ... CameraComponent ... LightComponent ... MeshComponent ... ParticleSystemComponent ... Networking - Documentation : Component Name Component Description NetworkDiscovery ... NetworkId ... NetworkTransform ...","title":"Component"},{"location":"engine_docs/component/#components","text":"Components in the Isetta Engine are similar to components in other engines, they should be designed for individal tasks and have minimal coupling with other components. Components cannot be standalone/instantiated in a level without being attached to an entity, done with entity->AddComponent<COMPONENT, bool isActive = true>(Component constructor args...) . You can look at any of our example levels for usage entity is an instantiated entity variable as shown above COMPONENT is the class name of your component to add, ie. ExampleComponent bool isActive = true is whether the component starts enabled/disabled and defaults to starting active Component constructor args... are the arguments to pass to the component's constructor (can be empty to use the default constructor -- a component MUST have a default constructor in addition to any other constructors) A component is created with Isetta component macros: DEFINE_COMPONENT(COMPONENT, PARENT, bool unique) and DEFINE_COMPONENT_END(COMPONENT_NAME, PARENT) . Where: - COMPONENT is the name of your new component class, ie. ExampleComponent - PARENT is the name of the parent/base class of component, use Component if you have no base class - bool unique is whether the component must be unique on an entity (whether an entity could have multiple of this component) - The unique variable does NOT mean the component is unique across the level, ie. a singleton To create a component, create a header (.h) and cpp (.cpp) file of the name of your component: ExampleComponent.h #pragma once #include <IsettaEngine.h> using namespace Isetta ; DEFINE_COMPONENT ( ExampleComponent , Component , false ) private : // Private variables of your component public : // A component MUST have a default constructor ExampleComponent () = default ; // Awake is called once, immediately when the component is first created and enabled void Awake () override ; // Start is called once, on the first update frame after the component is created and enabled void Start () override ; // OnEnable is called immediately each time the component becomes active, including after creation void OnEnable () override ; // OnDisable is called immediately each time the component becomes inactive void OnDisable () override ; // Update is called each frame (variable delta time) void Update () override ; // GuiUpdate is called each frame (variable delta time), GUI can only be called in GuiUpdate void GuiUpdate () override ; // LateUpdate is called each frame (variable delta time) void LateUpdate () override ; // FixedUpdate is called on fixed time (constant delta time) void FixedUpdate () override ; // OnDestroy is called once when the component is destroyed void OnDestroy () override ; DEFINE_COMPONENT_END ( ExampleComponent , Component ) The cpp file then contains the definitions for each of these functions. Not all functions need to be defined, if not overriden, the function update will run faster. Example Here are empty template of component header to copy: #pragma once #include <IsettaEngine.h> using namespace Isetta ; DEFINE_COMPONENT ( COMPONENT_NAME , Component , false ) private : public : COMPONENT_NAME () = default ; DEFINE_COMPONENT_END ( COMPONENT_NAME , Component ) Warning Every component must have a default constructor! Warning Components header currently cannot be wrapped in any namespace.","title":"Components"},{"location":"engine_docs/component/#essential-api","text":"Entity::AddComponent<T> : Add a component to an entity Entity::GetComponent<T> : Get a component from an entity Component::SetActive(bool active) : Toggle active state Component::entity* : Components have a pointer to its owner entity Component::transform* : Components have a pointer to its entity's transform Life time methods see above","title":"Essential API"},{"location":"engine_docs/component/#predefined-components","text":"In our engine, we have lots of predefined components that you can use out of the box! - AI - No Documentation Yet : Component Name Component Description Nav2DAgent Agent that is moved on a Nav2DPlane. Audio - Documentation : Component Name Component Description AudioSource ... AudioListener ... Collisions - Documentation : Component Name Component Description BoxCollider ... CapsuleCollider ... SphereCollider ... CollisionHandler ... General Components - Documentation : Component Name Component Description AxisDrawer ... FlyController ... GridComponent ... NetworkMonitor ... Graphics - Documentation : Component Name Component Description AnimationComponent ... CameraComponent ... LightComponent ... MeshComponent ... ParticleSystemComponent ... Networking - Documentation : Component Name Component Description NetworkDiscovery ... NetworkId ... NetworkTransform ...","title":"Predefined Components"},{"location":"engine_docs/debug_draw/","text":"Debug Drawing \u00b6 The DebugComponent and debug level are the best places for you to get started. Essential API \u00b6 DebugDraw::Point(Math::Vector3 point, Color color = Color::white, float size = 1.0f, float duration = 0.0f, bool depthTest = true) DebugDraw::Line(Math::Vector3 start, Math::Vector3 end, Color color = Color::white, float thickness = 1.0f, float duration = 0.0f, bool depthTest = true) DebugDraw::Ray(Math::Vector3 start, Math::Vector3 direction, Color color = Color::white, float thickness = 1.0f, float duration = 0.0f, bool depthTest = true) DebugDraw::Plane(Math::Matrix4 transformation = Math::Matrix4::identity, Color color = Color::white, float duration = 0.0f, bool depthTest = true) DebugDraw::WirePlane(Math::Matrix4 transformation = Math::Matrix4::identity, Color color = Color::white, float thickness = 1.0f, float duration = 0.0f, bool depthTest = true) DebugDraw::Cube(Math::Matrix4 transformation = Math::Matrix4::identity, Color color = Color::white, float duration = 0.0f, bool depthTest = true) DebugDraw::WireCube(Math::Matrix4 transformation = Math::Matrix4::identity, Color color = Color::white, float thickness = 1.0f, float duration = 0.0f, bool depthTest = true) DebugDraw::Sphere(Math::Vector3 position, float radius, Color color = Color::white, float duration = 0, bool depthTest = true) DebugDraw::WireSphere(Math::Vector3 position, float radius, Color color = Color::white, float thickness = 1.0f, float duration = 0, bool depthTest = true) DebugDraw::WireCapsule(Math::Matrix4 transformation, float radius = 0.5, float height = 2, Color color = Color::white, float thickness = 1.0f, float duration = 0, bool depthTest = true) DebugDraw::Grid(Math::Matrix4 transformation = Math::Matrix4::zero, int lines = 30, Color color = Color::lightGrey, float thickness = 1.0f, float duration = 0) DebugDraw::Axis(Math::Matrix4 transformation = Math::Matrix4::identity, Color xColor = Color::red, Color yColor = Color::green, Color zColor = Color::blue, float thickness = 2.0, float duration = 0, bool depthTest = true) Code Snippets \u00b6 DebugDraw :: Point ( 2 * Math :: Vector3 :: left , Color :: magenta , 20 ); DebugDraw :: Line ( Math :: Vector3 :: zero , 3 * Math :: Vector3 :: one ); // Rotating ray in a circle if ( Input :: IsKeyPressed ( KeyCode :: V )) { static float angle = 0.0f ; angle += 0.4f * EngineLoop :: GetGameClock (). GetDeltaTime (); if ( angle >= 2 * Math :: Util :: PI ) { angle = 0 ; } DebugDraw :: Ray ( Math :: Vector3 :: zero , Math :: Vector3 { Math :: Util :: Cos ( angle ), 0 , Math :: Util :: Sin ( angle )}, Color :: cyan , 2 ); } // Display plane if ( Input :: IsKeyPressed ( KeyCode :: B )) { DebugDraw :: Plane ( Math :: Matrix4 :: identity , Color :: blue , 2 ); } DebugDraw :: WirePlane ( Math :: Matrix4 :: identity ); DebugDraw :: Cube ( Math :: Matrix4 :: Translate ( Math :: Vector3 { 2.8 , 1.1 , 0 }) * Math :: Matrix4 :: Scale ( 2.2 * Math :: Vector3 :: one ), Color :: brown ); DebugDraw :: WireCube ( Math :: Matrix4 :: Translate ( Math :: Vector3 { 0 , 0 , - 2 })); DebugDraw :: WireSphere ( Math :: Vector3 :: up , 1 , Color :: red ); DebugDraw :: WireCapsule ( Math :: Matrix4 :: Translate ( Math :: Vector3 { - 1 , 4 , 1 }), 0.5 , 2 , Color :: blue ); DebugDraw :: AxisSphere ( Math :: Vector3 :: up , 1 ); DebugDraw :: Axis (); DebugDraw :: Grid ();","title":"Debug Draw"},{"location":"engine_docs/debug_draw/#debug-drawing","text":"The DebugComponent and debug level are the best places for you to get started.","title":"Debug Drawing"},{"location":"engine_docs/debug_draw/#essential-api","text":"DebugDraw::Point(Math::Vector3 point, Color color = Color::white, float size = 1.0f, float duration = 0.0f, bool depthTest = true) DebugDraw::Line(Math::Vector3 start, Math::Vector3 end, Color color = Color::white, float thickness = 1.0f, float duration = 0.0f, bool depthTest = true) DebugDraw::Ray(Math::Vector3 start, Math::Vector3 direction, Color color = Color::white, float thickness = 1.0f, float duration = 0.0f, bool depthTest = true) DebugDraw::Plane(Math::Matrix4 transformation = Math::Matrix4::identity, Color color = Color::white, float duration = 0.0f, bool depthTest = true) DebugDraw::WirePlane(Math::Matrix4 transformation = Math::Matrix4::identity, Color color = Color::white, float thickness = 1.0f, float duration = 0.0f, bool depthTest = true) DebugDraw::Cube(Math::Matrix4 transformation = Math::Matrix4::identity, Color color = Color::white, float duration = 0.0f, bool depthTest = true) DebugDraw::WireCube(Math::Matrix4 transformation = Math::Matrix4::identity, Color color = Color::white, float thickness = 1.0f, float duration = 0.0f, bool depthTest = true) DebugDraw::Sphere(Math::Vector3 position, float radius, Color color = Color::white, float duration = 0, bool depthTest = true) DebugDraw::WireSphere(Math::Vector3 position, float radius, Color color = Color::white, float thickness = 1.0f, float duration = 0, bool depthTest = true) DebugDraw::WireCapsule(Math::Matrix4 transformation, float radius = 0.5, float height = 2, Color color = Color::white, float thickness = 1.0f, float duration = 0, bool depthTest = true) DebugDraw::Grid(Math::Matrix4 transformation = Math::Matrix4::zero, int lines = 30, Color color = Color::lightGrey, float thickness = 1.0f, float duration = 0) DebugDraw::Axis(Math::Matrix4 transformation = Math::Matrix4::identity, Color xColor = Color::red, Color yColor = Color::green, Color zColor = Color::blue, float thickness = 2.0, float duration = 0, bool depthTest = true)","title":"Essential API"},{"location":"engine_docs/debug_draw/#code-snippets","text":"DebugDraw :: Point ( 2 * Math :: Vector3 :: left , Color :: magenta , 20 ); DebugDraw :: Line ( Math :: Vector3 :: zero , 3 * Math :: Vector3 :: one ); // Rotating ray in a circle if ( Input :: IsKeyPressed ( KeyCode :: V )) { static float angle = 0.0f ; angle += 0.4f * EngineLoop :: GetGameClock (). GetDeltaTime (); if ( angle >= 2 * Math :: Util :: PI ) { angle = 0 ; } DebugDraw :: Ray ( Math :: Vector3 :: zero , Math :: Vector3 { Math :: Util :: Cos ( angle ), 0 , Math :: Util :: Sin ( angle )}, Color :: cyan , 2 ); } // Display plane if ( Input :: IsKeyPressed ( KeyCode :: B )) { DebugDraw :: Plane ( Math :: Matrix4 :: identity , Color :: blue , 2 ); } DebugDraw :: WirePlane ( Math :: Matrix4 :: identity ); DebugDraw :: Cube ( Math :: Matrix4 :: Translate ( Math :: Vector3 { 2.8 , 1.1 , 0 }) * Math :: Matrix4 :: Scale ( 2.2 * Math :: Vector3 :: one ), Color :: brown ); DebugDraw :: WireCube ( Math :: Matrix4 :: Translate ( Math :: Vector3 { 0 , 0 , - 2 })); DebugDraw :: WireSphere ( Math :: Vector3 :: up , 1 , Color :: red ); DebugDraw :: WireCapsule ( Math :: Matrix4 :: Translate ( Math :: Vector3 { - 1 , 4 , 1 }), 0.5 , 2 , Color :: blue ); DebugDraw :: AxisSphere ( Math :: Vector3 :: up , 1 ); DebugDraw :: Axis (); DebugDraw :: Grid ();","title":"Code Snippets"},{"location":"engine_docs/debug_logging/","text":"Debug Logging \u00b6 Debug logging in the engine is done through Logger.h which has multiple macros to help you log your messages. The Logger works in conjunction with other systems in the engine, so we recommend you use that to std::cout . In case you didn't read this paragraph... Warning DO NOT USE std::cout Our logger will log to Visual Studio's output window, and in-game console window if you add EditorComponent to any entity. Essential API \u00b6 Each macro has multiple variations and will output: [Engine Time][Verbosity][Channel] Filename(Line number) Message Example [0.000][Info][General] gamejam.cpp (64) You are the best jammers! LOG(message) -- defaults to Debug::Channel::General and Debug::Verbosity::Info LOG_INFO(message) -- defaults to Debug::Channel::General and Debug::Verbosity::Info LOG_WARNING(message) -- defaults to Debug::Channel::General and Debug::Verbosity::Warning LOG_ERROR(message) -- defaults to Debug::Channel::General and Debug::Verbosity::ERROR THE ENGINE WILL BREAK WITH THIS MESSAGE Tip To specify a channel, change (message) to (Debug::Channel::CHANNEL, message) Available channels: General, Memory, Networking, Graphics, Horde3D, Collisions, Gameplay, Sound, FileIO, GUI, All Code snippets \u00b6 LOG_INFO ( Debug :: Channel :: Networking , \"Client [%s] with IP [%s] is connected\" , info . machineName . c_str (), info . ip . c_str ()); LOG_WARNING ( Debug :: Channel :: General , \"Entity %s not found!\" , name . data ()); LOG_ERROR ( Debug :: Channel :: Networking , \"Cannot send message from client cause client is not running\" );","title":"Debug Logging"},{"location":"engine_docs/debug_logging/#debug-logging","text":"Debug logging in the engine is done through Logger.h which has multiple macros to help you log your messages. The Logger works in conjunction with other systems in the engine, so we recommend you use that to std::cout . In case you didn't read this paragraph... Warning DO NOT USE std::cout Our logger will log to Visual Studio's output window, and in-game console window if you add EditorComponent to any entity.","title":"Debug Logging"},{"location":"engine_docs/debug_logging/#essential-api","text":"Each macro has multiple variations and will output: [Engine Time][Verbosity][Channel] Filename(Line number) Message Example [0.000][Info][General] gamejam.cpp (64) You are the best jammers! LOG(message) -- defaults to Debug::Channel::General and Debug::Verbosity::Info LOG_INFO(message) -- defaults to Debug::Channel::General and Debug::Verbosity::Info LOG_WARNING(message) -- defaults to Debug::Channel::General and Debug::Verbosity::Warning LOG_ERROR(message) -- defaults to Debug::Channel::General and Debug::Verbosity::ERROR THE ENGINE WILL BREAK WITH THIS MESSAGE Tip To specify a channel, change (message) to (Debug::Channel::CHANNEL, message) Available channels: General, Memory, Networking, Graphics, Horde3D, Collisions, Gameplay, Sound, FileIO, GUI, All","title":"Essential API"},{"location":"engine_docs/debug_logging/#code-snippets","text":"LOG_INFO ( Debug :: Channel :: Networking , \"Client [%s] with IP [%s] is connected\" , info . machineName . c_str (), info . ip . c_str ()); LOG_WARNING ( Debug :: Channel :: General , \"Entity %s not found!\" , name . data ()); LOG_ERROR ( Debug :: Channel :: Networking , \"Cannot send message from client cause client is not running\" );","title":"Code snippets"},{"location":"engine_docs/entity/","text":"Entities/Transforms \u00b6 Entities in the Isetta engine are comparable to GameObjects in Unity and Entities in Unreal. They are the objects that persist in the level/scene that can have a parent entity and can have children entities. Entities hold components and have a transform to locate them in the world. Entities can be add to the level through the macros Entity::Instantiate(std::string name, Entity* parent, bool isStatic) included with #include \"Scene/Entity.h\" , and return an entity pointer. The parent defaults to nullptr which sets the entity's parent to the top level and isStatic defaults to false . Static entities cannot be translated, rotated, or scaled once the level has loaded. Transforms hold the position, rotation, and scale information for an entity. The transformation information has local and world transformation, as well as the local axes of the entity. Entity * entity = Entity :: Instantiate ( \"entityName\" ); Transform * transform = entity -> transform ; entity -> AddComponent < ExampleComponent > (); We also provided API for creating primitive like spheres, capsules and boxes. They are super useful for testing things out quickly, please refer to the PrimitiveLevel for their usage sample. Essential API \u00b6 static Entity::Instantiate : instantiate a new (but empty) entity to the current level Entity::transform* : entities hold a pointer to their transform Transform::SetWorldPos(Math::Vector3 pos) Transform::GetWorldPos() Transform::TranslateWorld(Math::Vector3 delta) ... Transform::GetWorldRot Transform::LookAt(Math::Vector3 target) Transform::GetLocalToWorldMatrix() : returns a Matrix4, can be useful for debug drawing For more, look at the Transform.h file under \"Engine/Scene\" folder Primitive::Create(Primitive::Type, string name, bool withCollider)","title":"Entity/Transform"},{"location":"engine_docs/entity/#entitiestransforms","text":"Entities in the Isetta engine are comparable to GameObjects in Unity and Entities in Unreal. They are the objects that persist in the level/scene that can have a parent entity and can have children entities. Entities hold components and have a transform to locate them in the world. Entities can be add to the level through the macros Entity::Instantiate(std::string name, Entity* parent, bool isStatic) included with #include \"Scene/Entity.h\" , and return an entity pointer. The parent defaults to nullptr which sets the entity's parent to the top level and isStatic defaults to false . Static entities cannot be translated, rotated, or scaled once the level has loaded. Transforms hold the position, rotation, and scale information for an entity. The transformation information has local and world transformation, as well as the local axes of the entity. Entity * entity = Entity :: Instantiate ( \"entityName\" ); Transform * transform = entity -> transform ; entity -> AddComponent < ExampleComponent > (); We also provided API for creating primitive like spheres, capsules and boxes. They are super useful for testing things out quickly, please refer to the PrimitiveLevel for their usage sample.","title":"Entities/Transforms"},{"location":"engine_docs/entity/#essential-api","text":"static Entity::Instantiate : instantiate a new (but empty) entity to the current level Entity::transform* : entities hold a pointer to their transform Transform::SetWorldPos(Math::Vector3 pos) Transform::GetWorldPos() Transform::TranslateWorld(Math::Vector3 delta) ... Transform::GetWorldRot Transform::LookAt(Math::Vector3 target) Transform::GetLocalToWorldMatrix() : returns a Matrix4, can be useful for debug drawing For more, look at the Transform.h file under \"Engine/Scene\" folder Primitive::Create(Primitive::Type, string name, bool withCollider)","title":"Essential API"},{"location":"engine_docs/graphics/","text":"Graphics \u00b6 The best place for you to get started is the mesh anim level . Essential API \u00b6 Entity::AddComponent<MeshComponent>(\"pathToMeshFile\") : the mesh file to be included here has postfix .scene.xml , like \"Zombie/Zombie.scene.xml\" Entity::AddComponent<AnimationComponent>(MeshComponent*) : add an animation component to an entity AnimationComponent::AddAnimation(\"pathToAnimationFile\", int layer, \"startNode\", bool additive) : add an animation to animation component AnimationComponent::TransitToAnimationState(int state, float time) : transit between animations Code Snippets \u00b6 Adding mesh and animation, and transiting between animations: Entity * player = Entity :: Instantiate ( \"Player\" ); MeshComponent * playerMesh = player -> AddComponent < MeshComponent > ( \"Soldier/Soldier.scene.xml\" ); AnimationComponent * playerAnimationComp = player -> AddComponent < AnimationComponent > ( playerMesh ); playerAnimationComp -> AddAnimation ( \"Soldier/Soldier_Idle.anim\" , 0 , \"\" , false ); playerAnimationComp -> AddAnimation ( \"Soldier/Soldier.anim\" , 0 , \"\" , false ); player -> AddComponent < PlayerController > (); playerAnimationComp -> TransitToAnimationState ( 1 , 0.2f ); playerAnimationComp -> TransitToAnimationState ( 0 , 0.2f ); Adding camera to the scene and setting its properties: Entity * cameraEntity = Entity :: Instantiate ( \"Camera\" ); auto * cameraComp = cameraEntity -> AddComponent < CameraComponent > (); cameraEntity -> SetTransform ( Math :: Vector3 { 0 , 5 , 10 }, Math :: Vector3 { - 15 , 0 , 0 }, Math :: Vector3 :: one ); cameraComp -> SetProperty < Property :: FOV > ( CONFIG_VAL ( cameraConfig . fieldOfView )); cameraComp -> SetProperty < Property :: NEAR_PLANE > ( CONFIG_VAL ( cameraConfig . nearClippingPlane )); cameraComp -> SetProperty < Property :: FAR_PLANE > ( CONFIG_VAL ( cameraConfig . farClippingPlane ));","title":"Graphics"},{"location":"engine_docs/graphics/#graphics","text":"The best place for you to get started is the mesh anim level .","title":"Graphics"},{"location":"engine_docs/graphics/#essential-api","text":"Entity::AddComponent<MeshComponent>(\"pathToMeshFile\") : the mesh file to be included here has postfix .scene.xml , like \"Zombie/Zombie.scene.xml\" Entity::AddComponent<AnimationComponent>(MeshComponent*) : add an animation component to an entity AnimationComponent::AddAnimation(\"pathToAnimationFile\", int layer, \"startNode\", bool additive) : add an animation to animation component AnimationComponent::TransitToAnimationState(int state, float time) : transit between animations","title":"Essential API"},{"location":"engine_docs/graphics/#code-snippets","text":"Adding mesh and animation, and transiting between animations: Entity * player = Entity :: Instantiate ( \"Player\" ); MeshComponent * playerMesh = player -> AddComponent < MeshComponent > ( \"Soldier/Soldier.scene.xml\" ); AnimationComponent * playerAnimationComp = player -> AddComponent < AnimationComponent > ( playerMesh ); playerAnimationComp -> AddAnimation ( \"Soldier/Soldier_Idle.anim\" , 0 , \"\" , false ); playerAnimationComp -> AddAnimation ( \"Soldier/Soldier.anim\" , 0 , \"\" , false ); player -> AddComponent < PlayerController > (); playerAnimationComp -> TransitToAnimationState ( 1 , 0.2f ); playerAnimationComp -> TransitToAnimationState ( 0 , 0.2f ); Adding camera to the scene and setting its properties: Entity * cameraEntity = Entity :: Instantiate ( \"Camera\" ); auto * cameraComp = cameraEntity -> AddComponent < CameraComponent > (); cameraEntity -> SetTransform ( Math :: Vector3 { 0 , 5 , 10 }, Math :: Vector3 { - 15 , 0 , 0 }, Math :: Vector3 :: one ); cameraComp -> SetProperty < Property :: FOV > ( CONFIG_VAL ( cameraConfig . fieldOfView )); cameraComp -> SetProperty < Property :: NEAR_PLANE > ( CONFIG_VAL ( cameraConfig . nearClippingPlane )); cameraComp -> SetProperty < Property :: FAR_PLANE > ( CONFIG_VAL ( cameraConfig . farClippingPlane ));","title":"Code Snippets"},{"location":"engine_docs/gui/","text":"GUI \u00b6 For a good overview of GUI see the GUILevel . We ran out of time on this one, so if you need additional help ask Jacob or Yidi! Essential API \u00b6 Loading Font \u00b6 Font::AddFontFromFile(const std::string_view& filename, float fontSize, const std::string_view& fontName) Warning All GUI function calls must be contained with Component::GuiUpdate . Texture \u00b6 Texture::Texture(std::string_view pathToFile, bool load = true) : Load in the texture at the given file path and returns a Texture* GUI::Image(const RectTransform& transform, const class Texture& texture, const ImageStyle& style = {}) : Draw GUI image using the texture loaded before Code Snippets \u00b6 See GUIComponent Example UI \u00b6 Network Monitor Main Menu Main Menu","title":"GUI"},{"location":"engine_docs/gui/#gui","text":"For a good overview of GUI see the GUILevel . We ran out of time on this one, so if you need additional help ask Jacob or Yidi!","title":"GUI"},{"location":"engine_docs/gui/#essential-api","text":"","title":"Essential API"},{"location":"engine_docs/gui/#loading-font","text":"Font::AddFontFromFile(const std::string_view& filename, float fontSize, const std::string_view& fontName) Warning All GUI function calls must be contained with Component::GuiUpdate .","title":"Loading Font"},{"location":"engine_docs/gui/#texture","text":"Texture::Texture(std::string_view pathToFile, bool load = true) : Load in the texture at the given file path and returns a Texture* GUI::Image(const RectTransform& transform, const class Texture& texture, const ImageStyle& style = {}) : Draw GUI image using the texture loaded before","title":"Texture"},{"location":"engine_docs/gui/#code-snippets","text":"See GUIComponent","title":"Code Snippets"},{"location":"engine_docs/gui/#example-ui","text":"Network Monitor Main Menu Main Menu","title":"Example UI"},{"location":"engine_docs/home/","text":"How to use Isetta Engine \u00b6 This page gives you an index of how to do certain game development tasks using Isetta Engine. The best way to get started quickly is to browse through our example levels here . How to set up the development environment? Visit our Isetta-GameTemplate Git repo Basics : what are the assumptions and conventions we made? Levels : what does a level mean and how to define/load levels? Entity/Transform : how to define/create/work with them? Components : how to define/add/work with them? what are they capable of? Execution Order : what's the execution order of Isetta's submodules and different methods in Component? Graphics : how to add meshes and animations, and tweaking lights? Audio : how to read and play audio? Input : how to receive inputs? Collisions : how to work with collisions in Isetta? DebugDraw : how to use debug draw for lines, cubes, axis, etc? Logging : we all know print is the best way to debug, right?! Networking : Wait, you sure you want to make a networked game during this jam??? Important Notes \u00b6 Engine Header You must include #include <IsettaEngine.h> at the first line of all of your .h and .cpp files Memory Management Isetta Engine takes care of memory management for the most part, and you should not try to call new or delete on built in classes like levels and components.","title":"FAQs"},{"location":"engine_docs/home/#how-to-use-isetta-engine","text":"This page gives you an index of how to do certain game development tasks using Isetta Engine. The best way to get started quickly is to browse through our example levels here . How to set up the development environment? Visit our Isetta-GameTemplate Git repo Basics : what are the assumptions and conventions we made? Levels : what does a level mean and how to define/load levels? Entity/Transform : how to define/create/work with them? Components : how to define/add/work with them? what are they capable of? Execution Order : what's the execution order of Isetta's submodules and different methods in Component? Graphics : how to add meshes and animations, and tweaking lights? Audio : how to read and play audio? Input : how to receive inputs? Collisions : how to work with collisions in Isetta? DebugDraw : how to use debug draw for lines, cubes, axis, etc? Logging : we all know print is the best way to debug, right?! Networking : Wait, you sure you want to make a networked game during this jam???","title":"How to use Isetta Engine"},{"location":"engine_docs/home/#important-notes","text":"Engine Header You must include #include <IsettaEngine.h> at the first line of all of your .h and .cpp files Memory Management Isetta Engine takes care of memory management for the most part, and you should not try to call new or delete on built in classes like levels and components.","title":"Important Notes"},{"location":"engine_docs/input/","text":"Input \u00b6 We have both callback based and polling based input API. The InputTestComponent and FlyController are the best place for you to get started Essential API \u00b6 Input::GetMousePosition() Input::RegisterKeyPressCallback(KeyCode, Action<>) Input::RegisterKeyReleaseCallback(KeyCode, Action<>) Input::RegisterMousePressCallback(MouseButtonCode, Action<>) Input::IsKeyPressed(KeyCode) Input::IsMouseButtonPressed(MouseButtonCode) Input::IsGamepadButtonPressed(GamepadButton) Input::GetGamepadAxis(GamepadAxis)","title":"Input"},{"location":"engine_docs/input/#input","text":"We have both callback based and polling based input API. The InputTestComponent and FlyController are the best place for you to get started","title":"Input"},{"location":"engine_docs/input/#essential-api","text":"Input::GetMousePosition() Input::RegisterKeyPressCallback(KeyCode, Action<>) Input::RegisterKeyReleaseCallback(KeyCode, Action<>) Input::RegisterMousePressCallback(MouseButtonCode, Action<>) Input::IsKeyPressed(KeyCode) Input::IsMouseButtonPressed(MouseButtonCode) Input::IsGamepadButtonPressed(GamepadButton) Input::GetGamepadAxis(GamepadAxis)","title":"Essential API"},{"location":"engine_docs/level/","text":"Level \u00b6 Levels in Isetta are comparable to scenes in Unity. Creating Levels \u00b6 To create a level, add the following .h and .cpp file. LEVEL_NAME.h #pragma once #include <IsettaEngine.h> using namespace Isetta ; DEFINE_LEVEL ( LEVEL_NAME ) void Load () override ; void OnUnload () override ; DEFINE_LEVEL_END LEVEL_NAME.cpp #include <IsettaEngine.h> #include \"LEVEL_NAME.h\" using namespace Isetta ; void LEVEL_NAME :: Load () { // Level NEEDS a camera Entity * cameraEntity = Entity :: Instantiate ( \"Camera\" ); cameraEntity -> AddComponent < CameraComponent > (); cameraEntity -> SetTransform ( Math :: Vector3 { 0 , 5 , 10 }, Math :: Vector3 { - 15 , 0 , 0 }, Math :: Vector3 :: one ); } void LEVEL_NAME :: OnUnload () { // Anything you might need to do on the level unloading // Entity's will be destructed/destroyed on actual level unload } To load the file you just added as start up level, go to your user.cfg and set start_level to LEVEL_NAME . Essential API \u00b6 LevelManager::Instance().LoadLevel(\"levelName\") : programmatically load levels LevelManager::Instance().loadedLevel : get information about current level Example Levels \u00b6 Level Inputs Some of the levels have inputs, too. But they display badly here! Go to our Git repo to look like them! Level Name Level Description AILevel Level showing how navigation module works in the engine and how to use the particle system AudioLevel Level with 2D and 3D audio looping and one shot BVHLevel Level testing our dynamic AABB tree CollisionsLevel Level testing our collision intersections CollisionSolverLevel Level testing our collision solving system DebugLevel Level demoing our debug drawing capabilities EditorLevel Level showing the editor components: inspector, heirarchy, and console and level loading menu EmptyLevel Empty level to be used as a starting point for user created levels EventLevel Level demoing our event messaging system with sender and listener components ExampleLevel Level with an animating model and example component GUILevel Level demoing some of our GUI capabilities InputLevel Level demoing some of the input capabilities LevelLoadingLevel Level showing a menu to browse levels and load specific level MeshAnimLevel Level with a mesh that is being animated NetworkLevel Level demoing some of our networking capabilities. The default_server_ip in config should be set to your LAN IP for this level to work. PrimitiveLevel Level displaying all the types of primitive objects SkeletonLevel Level displaying a mesh and entities used to follow the skeleton Halves First demo twin-stick shooter game we created! Only support gamepads KnightGame Game with a knight and a sword, can you take down the most training dummies? Week10MiniGame First demo game with networking. Fool your enemy with your sword young man!","title":"Level"},{"location":"engine_docs/level/#level","text":"Levels in Isetta are comparable to scenes in Unity.","title":"Level"},{"location":"engine_docs/level/#creating-levels","text":"To create a level, add the following .h and .cpp file. LEVEL_NAME.h #pragma once #include <IsettaEngine.h> using namespace Isetta ; DEFINE_LEVEL ( LEVEL_NAME ) void Load () override ; void OnUnload () override ; DEFINE_LEVEL_END LEVEL_NAME.cpp #include <IsettaEngine.h> #include \"LEVEL_NAME.h\" using namespace Isetta ; void LEVEL_NAME :: Load () { // Level NEEDS a camera Entity * cameraEntity = Entity :: Instantiate ( \"Camera\" ); cameraEntity -> AddComponent < CameraComponent > (); cameraEntity -> SetTransform ( Math :: Vector3 { 0 , 5 , 10 }, Math :: Vector3 { - 15 , 0 , 0 }, Math :: Vector3 :: one ); } void LEVEL_NAME :: OnUnload () { // Anything you might need to do on the level unloading // Entity's will be destructed/destroyed on actual level unload } To load the file you just added as start up level, go to your user.cfg and set start_level to LEVEL_NAME .","title":"Creating Levels"},{"location":"engine_docs/level/#essential-api","text":"LevelManager::Instance().LoadLevel(\"levelName\") : programmatically load levels LevelManager::Instance().loadedLevel : get information about current level","title":"Essential API"},{"location":"engine_docs/level/#example-levels","text":"Level Inputs Some of the levels have inputs, too. But they display badly here! Go to our Git repo to look like them! Level Name Level Description AILevel Level showing how navigation module works in the engine and how to use the particle system AudioLevel Level with 2D and 3D audio looping and one shot BVHLevel Level testing our dynamic AABB tree CollisionsLevel Level testing our collision intersections CollisionSolverLevel Level testing our collision solving system DebugLevel Level demoing our debug drawing capabilities EditorLevel Level showing the editor components: inspector, heirarchy, and console and level loading menu EmptyLevel Empty level to be used as a starting point for user created levels EventLevel Level demoing our event messaging system with sender and listener components ExampleLevel Level with an animating model and example component GUILevel Level demoing some of our GUI capabilities InputLevel Level demoing some of the input capabilities LevelLoadingLevel Level showing a menu to browse levels and load specific level MeshAnimLevel Level with a mesh that is being animated NetworkLevel Level demoing some of our networking capabilities. The default_server_ip in config should be set to your LAN IP for this level to work. PrimitiveLevel Level displaying all the types of primitive objects SkeletonLevel Level displaying a mesh and entities used to follow the skeleton Halves First demo twin-stick shooter game we created! Only support gamepads KnightGame Game with a knight and a sword, can you take down the most training dummies? Week10MiniGame First demo game with networking. Fool your enemy with your sword young man!","title":"Example Levels"},{"location":"engine_docs/networking/","text":"Networking \u00b6 Question You still sure you want to use networking in your game??? Networking in Isetta is completely messaging based, there are no Cmd or Rpc or SyncVar s like in Unity. Our example networking level and NetworkTestComponent is the best place for you to get started. A few things to definitely note: - When implementing a network message class, you need to define both the Serialize and the Copy functions. Serialize MUST return true at the end of the function--otherwise your client will disconnect every time you try to send the message and you won't know why! - Each of the serialization functions we use come from yojimbo, and they're the following: - serialize_int: Serializes an integer value and compresses that within the range of min and max - serialize_bits: Serializes the number of bits of a 32-bit value - serialize_bool: Serializes a boolean value using bits - serialize_float: Serializes a float value - serialize_uint32: Serializes a 32-bit unsigned integer value - serialize_uint64: Serializes a 64-bit unsigned integer value by serializing the low and high 32 bits - serialize_double: Serializes a double value by casting it to a 64-bit unsigned integer and serializing that - serialize_bytes: Serializes an array of bytes from a given pointer to data - serialize_string: Serializes a string of a given buffer_size - serialize_object: Serializes an object using a Serialize member function of the object that takes in the stream parameter If you do use networking a lot, the team members in the room are probably the best documentation. Essential API \u00b6 yojimbo::Message : The base class of the message objects NetworkManager::StartHost(\"hostIP\") NetworkManager::StopHost() NetworkManager::StartServer(\"serverIP\") NetworkManager::StopServer() NetworkManager::StartClient(\"clientIP\") NetworkManager::StopClient() NetworkManager::SendMessageFromClient<MyMessage>(Action<MyMessage*> messageInitializer) where T inherits yojimbo::Message NetworkManager::RegisterClientCallback<MyMessage>(Action<yojimbo::Message*>) NetworkManager::SendMessageFromServer<MyMessage>(int clientIndex, Action<MyMessage*> messageInitializer) NetworkManager::RegisterServerCallback<MyMessage>(Action<int clientIndex, yojimbo::Message*>) NetworkManager::.SendMessageFromServerToAll<MyMessage>(MyMessage*) Code Snippets \u00b6 Defining a HandleMessage : // HandleMessage simply sends an integer handle between 0 and 64 across the // network DEFINE_NETWORK_MESSAGE ( HandleMessage ) // IMPORTANT: The Serialize function _must_ be a template around the Stream // type, and it MUST return true at the end of the function. Otherwise the // serialization will be assumed to have failed! template < typename Stream > bool Serialize ( Stream * stream ) { // serialize_int is supplied by yojimbo, along with many other primitive // type serialization functions serialize_int ( stream , handle , 0 , 64 ); return true ; } // The Copy function must be overridden with boilerplate that copies the values // from a given message. This is used for the general SendToAll functions that // the NetworkManager has. void Copy ( const yojimbo :: Message * otherMessage ) override { const HandleMessage * message = reinterpret_cast < const HandleMessage *> ( otherMessage ); handle = message -> handle ; } public : int handle = 0 ; // Obviously we'll just use a handle DEFINE_NETWORK_MESSAGE_END Registering a callback for HandleMessage on the client: exampleClientHandleId = NetworkManager :: Instance (). RegisterClientCallback < HandleMessage > ( []( yojimbo :: Message * message ) { // We'll always have to cast our message into the message we're // anticipating HandleMessage * handleMessage = static_cast < HandleMessage *> ( message ); LOG ( Debug :: Channel :: Networking , \"Server sends handle #%d\" , handleMessage -> handle ); // Depending on the handle, we can do something (we tore out a // couple system here though) if ( handleMessage -> handle == 0 ) { LOG ( Debug :: Channel :: Networking , \"Server says we should play the animation!\" ); } if ( handleMessage -> handle == 1 ) { LOG ( Debug :: Channel :: Networking , \"Server says we should stop the animation!\" ); } });","title":"Networking"},{"location":"engine_docs/networking/#networking","text":"Question You still sure you want to use networking in your game??? Networking in Isetta is completely messaging based, there are no Cmd or Rpc or SyncVar s like in Unity. Our example networking level and NetworkTestComponent is the best place for you to get started. A few things to definitely note: - When implementing a network message class, you need to define both the Serialize and the Copy functions. Serialize MUST return true at the end of the function--otherwise your client will disconnect every time you try to send the message and you won't know why! - Each of the serialization functions we use come from yojimbo, and they're the following: - serialize_int: Serializes an integer value and compresses that within the range of min and max - serialize_bits: Serializes the number of bits of a 32-bit value - serialize_bool: Serializes a boolean value using bits - serialize_float: Serializes a float value - serialize_uint32: Serializes a 32-bit unsigned integer value - serialize_uint64: Serializes a 64-bit unsigned integer value by serializing the low and high 32 bits - serialize_double: Serializes a double value by casting it to a 64-bit unsigned integer and serializing that - serialize_bytes: Serializes an array of bytes from a given pointer to data - serialize_string: Serializes a string of a given buffer_size - serialize_object: Serializes an object using a Serialize member function of the object that takes in the stream parameter If you do use networking a lot, the team members in the room are probably the best documentation.","title":"Networking"},{"location":"engine_docs/networking/#essential-api","text":"yojimbo::Message : The base class of the message objects NetworkManager::StartHost(\"hostIP\") NetworkManager::StopHost() NetworkManager::StartServer(\"serverIP\") NetworkManager::StopServer() NetworkManager::StartClient(\"clientIP\") NetworkManager::StopClient() NetworkManager::SendMessageFromClient<MyMessage>(Action<MyMessage*> messageInitializer) where T inherits yojimbo::Message NetworkManager::RegisterClientCallback<MyMessage>(Action<yojimbo::Message*>) NetworkManager::SendMessageFromServer<MyMessage>(int clientIndex, Action<MyMessage*> messageInitializer) NetworkManager::RegisterServerCallback<MyMessage>(Action<int clientIndex, yojimbo::Message*>) NetworkManager::.SendMessageFromServerToAll<MyMessage>(MyMessage*)","title":"Essential API"},{"location":"engine_docs/networking/#code-snippets","text":"Defining a HandleMessage : // HandleMessage simply sends an integer handle between 0 and 64 across the // network DEFINE_NETWORK_MESSAGE ( HandleMessage ) // IMPORTANT: The Serialize function _must_ be a template around the Stream // type, and it MUST return true at the end of the function. Otherwise the // serialization will be assumed to have failed! template < typename Stream > bool Serialize ( Stream * stream ) { // serialize_int is supplied by yojimbo, along with many other primitive // type serialization functions serialize_int ( stream , handle , 0 , 64 ); return true ; } // The Copy function must be overridden with boilerplate that copies the values // from a given message. This is used for the general SendToAll functions that // the NetworkManager has. void Copy ( const yojimbo :: Message * otherMessage ) override { const HandleMessage * message = reinterpret_cast < const HandleMessage *> ( otherMessage ); handle = message -> handle ; } public : int handle = 0 ; // Obviously we'll just use a handle DEFINE_NETWORK_MESSAGE_END Registering a callback for HandleMessage on the client: exampleClientHandleId = NetworkManager :: Instance (). RegisterClientCallback < HandleMessage > ( []( yojimbo :: Message * message ) { // We'll always have to cast our message into the message we're // anticipating HandleMessage * handleMessage = static_cast < HandleMessage *> ( message ); LOG ( Debug :: Channel :: Networking , \"Server sends handle #%d\" , handleMessage -> handle ); // Depending on the handle, we can do something (we tore out a // couple system here though) if ( handleMessage -> handle == 0 ) { LOG ( Debug :: Channel :: Networking , \"Server says we should play the animation!\" ); } if ( handleMessage -> handle == 1 ) { LOG ( Debug :: Channel :: Networking , \"Server says we should stop the animation!\" ); } });","title":"Code Snippets"},{"location":"engine_postmortem/BuildGames/","text":"When building an engine, there are some obvious ways of testing your code like unit and integration testing. While these traditional software engineering practices help with certain systems, like a math library, it isn't obvious how they transfer to something like an engine which, as Casey Muratori put it, is an amorphous blob of input. Games have a different ecosystem for testing than software. For us, the way we were able to test our code was by a three tiered system: first with tech demos, then with sample games, and ending with a game jam. Tech demos tested if the feature was functional, sample games tested the feature in a gameplay setting with the engine as a whole, and the game jam tested the actual usability of the feature . Lastly, there's one more major reason to build a game: The only way we were able to complete our engine in 15 weeks was because we targeted the engine's features at an existing game that we made. We didn't just target the engine at a genre a genre, or an idea of a game, but an actual, designed game. Specifying the genre of the engine isn't useless because it can help guide as a reference of which features are engine related and which are specific game related. It can be used more in the beginning when designing the architecture, however, ultimately, the game really is the decision factor of what features are needed by the engine. For example, most twin-stick shooters don't need physics but if the game you've designed does, you aren't going to not develop physics because the genre says it's uncommon, that's one of the benefits of having your own engine! You may argue against making a game saying, \"I'm interested in learning engine development, not making a game\", \"I've made games before, I know how they work\", or any other excuse not to build one but don't listen to yourself. Although for us it was useful to have the game designed and implemented in another engine, this doesn't have to be the case for you. Having the game developed in another engine helps to visualize what an engine feature should be capable of, but it can be just as good to have a game design document. The main part of this is to be able to deconstruct a high-level game mechanic into what engine requirements it relies on. This will also help with knowing your priorities and dependencies of your engine features. Having a target game for your engine will stop overscope, feature-creep, and building features that will go unused. With us, we began wrapping the GUI library Dear, ImGUI, which for those of you don't know is a library dedicated to GUI. What we found was that if we didn't restricted ourselves to just abstracting what the game needed we would have lost an additional week to just abstraction of features that weren't actually going to be used! Another use of a target game can be pulled straight from game design, it gives you design pillars to make your decisions. For each decision you make with the engine ask yourself, \"does the game need this?\" If the answer is no, put a TODO there and keep working on features that the game needs. Especially if you are working on a team, you can trust that everyone is furthering the engine each day because no one is going off in their own rabbit-hole of development. A target game also gives you a way to showcase your work, others will be able to contextualize what you've done and you will be able to see what you have left to do. This can act as great motivation to help keep the project done, it is a similar feeling to when you get your first triangle rendered or game mechanic prototype working. Also working incrementally like this will help you set a point when the engine is \"done\". We aren't going to say you will ever feel like the engine is ever truly complete, but when you have the target game working and not completely hacked together you will be able to say it is done. \"Okay so I can just build one game.\" We wish. Before starting this project we dismissed the idea of building multiple games and an engine in 15 weeks, we even dismissed one game originally, but each game will help prove you engine is actually progressing outside the actual engine project. These games don't serve the same purpose of the target game, nor are they the same scale. The target game is what drives the engine decisions, these sample games need to live with those decisions and work around those constraints like a developer would be forced to. We would suggest you try to mimic some pre-existing design with each of these game, and have the game focus around a single mechanic. They don't need to be polished, master-pieces, it's better if they aren't.They're purpose is to use features in a game manner, which is almost certainly different than how they were tested through the tech demos. Another added benefit is if you can convince another developer who didn't develop that feature to use it in a game, this will almost certainly find bugs and API problems that the developer hadn't thought of originally. These games aren't meant to increase the scope of the engine. Don't let them drive new features. However, if you notice multiple games need the same type of system that may be an indication it should be included in your engine. This is also a good time to notice if a certain feature needs to be hacked around or is too specific to be used by multiple games, that it may need to be refactored. This is even the case if the feature is for your target game, because that may mean you have just built game logic into your engine. If we hadn't had this three tiered system of testing, the correctness and usability of the code would have suffered. The individual technical demos served an additional purpose past testing the barebones engine feature, we were able to provide these during our game jam as examples of how to use the engine. Beyond the game jam, these levels act as the living documentation of how to use each system of our engine, and although features are integrated in some capacity, each demo attempts to focus on one thing so a game developer can get a better sense of that from just one level. It is fairly obvious why the tech demos helped with correctness, they were the levels we used to ensure our feature worked, however testing a feature in an isolated setting like these levels isn't what games are about. Games have multiple features working side-by-side, and with this added element, as well as not knowing exactly how the feature will be used, the games found other bugs that the tech demos weren't even testing for or considering. We would recommend that if developing an engine, especially one without software development testing, that you start developing demos and games as soon as you can. You don't have to wait until the engine is export into a dll or exe format to build levels, you should be able to start developing them within the engine (and eventually transfer them into another project). The point at which making games made sense to us was once we had our scene graph hierarchy implemented. All code examples prior to this point are hard to translate into usable examples, as any examples before was mostly hacks in the main engine loop. This is why we'd recommend pushing to get the scene graph hierarchy done much sooner than we had, which was Week 6. The scene graph doesn't rely on as much as you'd believe to start; for example you may associate having a scene graph with models or components but those are needed to start. Remember, you are building a _game _engine whose sole purpose is to help create games. If you don't develop at least one game with your engine you are disconnected from you user.","title":"Build Games with Your Game Engine"},{"location":"engine_postmortem/Crunch/","text":"Not mentioned in the \"What went right\" section was the teams passion for the project and without such enthusiasm the engine and blogs wouldn't have been as successful as it was. However, this is the \"What went wrong\" section, so we should talk how this passion wasn't useful for the project but rather how it was harmful. Developing an engine in 15 weeks requires a certain amount of persistence and drive, which translate into working about 70+ hours each week. This project became a 15 week crunch, which while we don't recommend subjecting yourself or others to this was useful for us to know what crunch feels like so we can recognize it in the future. We didn't treat this project as a marathon, rather a long sprint. We spoke to some of our faculty near the end about how none of us had the motivation to pull from to make the target demo game. Creating the game at the end was no longer fun. For us, we attributed this to burnout; we were too tired to continue to develop. However, that might not be the full truth. The team was excited to develop the engine because it was a new experience where we had the chance to learn and grow as developers. However, the game signified the end of that. The game wasn't about learning, it was a statement that we were done learning with this engine. Where we weren't excited to develop the game, we were still willing to put time into polishing and adding to the engine. So it's not like we were afraid of developing a game because of bugs or broken features; if we were we probably wouldn't have held a game jam where other could see the ugly pieces of engine. We couldn't even predict what features of the engine they would be looking at. Developing engine features, even if small, was a new experience when compared to game development. As much as we say our project was about demystifying engine development, the project was more about learning. Specifically, it was about learning how game engines work, the intricacies of them, and how to be an effective software developer on a team, but learning nonetheless. Learning was what was driving the work and our motivation. This can be simplified to saying, we were excited to problem solve the unknowns of the project, so when the game came to be developed it was a list of \"knowns\", problems that we knew how to solve and had solved before. We had developed games before, games much more complicated than the one we were trying to generate; there wasn't a list of unknowns. Having to develop something that isn't challenging, having just a list of tasks that are \"knowns\", is demotivating. As the 15 weeks neared an end, there was this added pressure in the room, as you would expect for a project running out of time. The team was able to combat the stress through the semester by having team dinners throughout the semester (rather than just one paid for by the school) and having game nights to relax; however near the end there was less time to do this. The pressure could be traced to be coming from two categories: 1) frustration and 2) \"there is still so much work left\". The latter is to be expected from any type of project, especially one trying to do so much with such little time. However, the former just a pressure from frustration isn't a positive sign. The frustration came from not learning. We weren't being given explicit feedback about the engine we were developing from our faculty or others and we were no longer learning engine development. At some point near the end, the project pivoted from its original goal and we had to do something that was no longer learning. And it just became a slog.","title":"Crunch is Bad"},{"location":"engine_postmortem/HostingGameJam/","text":"The game jam we held was a stretch goal of our project, thus had we not held one the product wouldn't be much different. However, what it got us was a memorable experience and the confidence we had truly developed a game engine, it was no longer based on us saying so, there was proof. From the beginning of the semester one of the assumptions we decided on was that our engine wouldn't be foolproof, meaning the developers on the team would be able to use the engine effectively but we couldn't avoid the game developer who would shoot themself in the foot. However, this didn't mean that we wouldn't try to develop a usable engine; we used the project as an exercise/hypothetical situation of \"if we had actual game developers...\". But we had no illusion of others using the Isetta engine to develop games. As the engine progressed, the decisions deviated from what was correct to what might be easier to use or harder to abuse. This was used in conjunction with the target game for decisions. Near the end of engine development, we thought it would be cool to show off the actual engine, something we had struggled to do since the engine was the focus not the games it made. What made sense to us was to hold a game jam, this would force people to look at the engine, if only to use what they needed. It is almost like playtesting the engine. Holding a game jam was a success for two reasons: 1) it confirmed that our engine was actually usable by others not on the team 2) it was exciting for the team. Through the game jam, participants found bugs that we hadn't found because we hadn't used a feature in that specific way, nor did we expect our engine to be bug free. However, the participants were able to work around the bugs, much like you would with a commercial engine that you don't have source code access to. We had been saying for a while that our engine was a twin-stick shooter engine, but that was selling the engine short. The game jammers created an array of games in different genres (racing, artistic, simulation, local multiplayer, puzzle, and shooter), none of which were twin-stick. Saying your game engine is a certain genre doesn't mean the developers will make games for that, they will only be restricted by the feature set.","title":"Hosting a Game Jam"},{"location":"engine_postmortem/ItsNotThatHard/","text":"Rather than thinking of an engine as the monolith that games need to function, it is better said to be a collection of small, achievable subsystems. The reason we find the need to state this is twofold: 1) starting this project each programmer was intimidated by the prospect of developing a game engine and 2) while doing research the internet oozes an elitist attitude about those who've developed an engine. What we pictured to be hard coming into the project was finding what system should be the first designed/implemented, figuring out where to start, and beyond that was figuring out how to develop and integrate each subsystem. The reason for our fear came from trying to learn about engine development in the past, which was to google \"how to develop an engine\" This isn't what you want to google. As we've stated throughout our project, there are relatively few resources that speak of the engine development process and as a whole ( Isetta.io can be added to that list now). Searching engine development just won't get you that far and will leave you with questions. Like any big problem, engine development can be deconstructed into manageable systems. For us, the process revolved around understanding some base engine vocabulary and a high level understanding of what functionality modules have, answering what features make an engine. From there we were able to deconstruct a game into mechanics, mechanics into a list of engine features, and engine features into dependency chart. By having a sense of what your dependencies are you will be able to select a system and start working on one that is near the bottom; don't be afraid that you are missing systems because you probably are, you are probably are also including systems that aren't needed. Your diagram, much like our was, will be wrong and you will have to iterate on it as you develop. What we want you to know, and wish someone told us, was that you may feel you architecture is wrong or you aren't doing things right but there isn't a right answer, just compromises. This applies beyond just planning and architecture but to all decisions you make on your engine. Researching engine development can still prove to be useful as you will get a sense of the vocabulary typically used with engines. Most of this vocabulary revolves around the programming and design patterns used in engines, as well as the different paradigms, like data-oriented vs. object-oriented. While it is certainly useful to understand this terminology, trying to implement the latest craze in engine development can be intimidating. It is typically the latest thing because there are nuances to it and the professionals are still working to understand it better. For us, sticking with something that was tried and true, as well as what we were more familiar and comfortable with, proved to be a better idea for our first engine. Something one of faculty members has said is, you have a limited amount of innovation \"points\" on a project; don't spend time trying to innovate on every aspect of a project, that will lead to overscoping. While game engines aren't a searchable subject, the subsystems of the architecture are easily searchable. Systems like graphics, physics, file I/O, memory, etc. aren't just game engine topics, they are software topics which have tutorials, examples, and well-documented knowledge on. As one of our faculty members put it, intelligent people have already captured the tech details of game engines and made them accessible for novices. Solving subsystems isn't a difficult process, it may take time to learn, however the implementation is something that can be iterated on. If your implementation isn't right (in either correctness, performance, memory) it can be fixed. The engine won't implode from a bad implementation. The subsystems are the puzzle pieces and the integration of the engine is like the picture of the completed puzzle. It takes some work to find a certain piece of the puzzle, but is doable; however without the picture it is difficult to assemble the puzzle at all.","title":"It's Not That Hard"},{"location":"engine_postmortem/LeveragingProfessionals/","text":"The team recognized the fact we were novices without the answers, as well as that no matter how hard we tried we just couldn't make all the \"right\" decisions, decisions that are made at a AAA scale. We incorporated the advice of industry professionals as a counterbalance to our inexperience, and although not directly on our team to guide us we could use their experience to help influence ours. We used their advice at each stage of our project: pre-production, production, and post-production; but not in the same way each time. Our pre-production phase was us figuring out how we could make this project work. We weren't yet into the technical details, but more asking questions revolving around how a software development project at our scale could work and what they, the professionals, would be interested in seeing from a project like ours. We asked because they will be the ones hiring us, might as well use that to help. By time we started development we had spoken with about 6 industry professionals along with our entire faculty (~20 professors). We had an idea of what we wanted out of the project and how we best thought we could accomplish that, but the feedback is what molded the final direction. Most of the advice we took at this stage was based on how we felt about it, which might not have been the best selection criteria. If we were excited about the advice, such as developing a game, we would take it, but if the advice was to make multiple games, we chose to ignore it at the time because it seemed unfeasible and intimidating. Had we taken all feedback the project wouldn't have happened because everyone has their own two cents. In addition, some of the advice was counter to what we wanted to do, such as removing networking from architecture to the extreme of not developing an engine at all. Talking with professionals was also different from talking with faculty, where faculty think of educational value and the student growth. The advice from the professionals leaned toward warnings and pitfalls they had experienced from their projects. In production, we were then able to ask technical questions to professionals, although usually not specifically about our project. The project was no longer a hypothetical situation of, \"we are going to do...\", but rather something that we had learned from. Having some experience to pull from to ask questions was valuable, otherwise it would have been difficult to contextualize what we were trying to say. The added benefit of talking to professionals during production, is that you don't know what you don't know. As a team we would often create solutions which worked best for our engine and made sense to the team but didn't have reference to how others, particularly the industry, does it. Talking to professionals gave us this glimpse which was enough to confirm we were headed in the right direction or we needed to refactor/redirect. We had our own terminology for some implementation and features, and in conversations with professionals we would be introduced to the actual engine developer vocabulary, which was searchable and had information on unlike our team's internal definition. What may also happen during these conversations is that the professional may ask about technical details of your project out of curiosity then discuss how they've done it in past, this will help you reflect and rethink your systems. Professional response in post-production isn't necessarily useful but rather motivational.","title":"Leveraging Professionals for Input"},{"location":"engine_postmortem/NoviceTeam/","text":"Part where the team thought we could provide value to demystifying engine development was by being a team of inexperience engine developers. We thought this would be valuable because we wouldn't have any preconceived notion or assumption in development and could then document what we assumed for others. There was another added benefit of us being novices, no one on the team could feed us answers. A significant part of the engine development process was from making decisions about the engine, which took place in team discussions when we would gather around a computer or the whiteboard. In these discussions, someone would present the problem and then typically their original naive solution and why it wouldn't work; from there the team would throw around ideas and prod for more details. At a minimum these meetings provided everyone context on a feature that was being developed in the engine and forced some problem solving. However, if someone on the team already had engine development experience before the meetings would probably end before they even started because that person could just say, this is the answer because I did it this way before. This highlights a challenge of working with more experienced people; while that experience helps the product ship and your skills, it can also be harmful for learning. In the beginning of the project, we had a pseudo expert in the room in the shape of a book, \"Game Engine Architecture\". Everyone on the team had read the book in some capacity, which gave us this vocabulary to use in our discussions. However, a pitfall we almost fell into, especially in the beginning, was following \"our expert\" too closely. Some of our discussions and decisions started to boil down to, \"what does the book do\", and while this was helpful to continue development, it wasn't really good for our growth. Don't rely on or imitate your expert's experience too closely, particularly when just starting off learning. With professional engine development, the goal is the product and learning along the way is a side effect. But for our project, the goal wasn't to have a perfect, bug-free engine but to learn engine development and having a functioning engine at the end was the side effect. By eliminating the expert from the room, we made mistakes which impacted the engine but learned more from those mistakes than had we had someone tell us how to do it.","title":"Having a Team of Novices"},{"location":"engine_postmortem/QuestionsForProfessionals/","text":"Although we leveraged the use of professionals, the questions we asked professionals could have been different. We believe we asked good questions, backed by some of our interviewees saying so, however the questions weren't necessarily the ones we wanted to ask. The questions we generated revolved around our perceived idea of what the interviewee could answer and would be comfortable answering, based on their history and work experience. We were asking questions about what we thought the interviewee knew, which wasn't necessarily what we wanted to know. While we wrote some questions in the beginning along the lines of what we wanted to know, those didn't seem to land so we just avoided them for the remainder of the semester. The burning questions we were unable to ask were about specific technical implementation, such as TODO. As the semester progressed so did our engine and our understanding of engine development, so our questions became more advanced, in-depth, and assuming of our audience and interviewees of prior knowledge. While having more technical questions isn't a negative, the assumption of knowledge is, specifically for our target audience of novices. Another result of personally understanding game engines better was we had fewer questions to ask, making generating questions harder. So we began asking the interviewees for information about what they were currently working on and interested in. However, it was still difficult. This is something we came to accept, interviewing is hard. We aren't studying to be interviewers either. Something we wish we had done, although not sure how we would have, would be to ask for feedback specifically about what we developing. The reason we feel we didn't deviated from asking the questions we did was because of how positively our interviewees responded to our questions. Our questions were generated from meticulously researching that person, almost stalking. The problem with trying to go to this other question format was also how to ask about feedback if they don't know much about our project, which was to be expected. We couldn't expect our developers to take time to look at what we were doing, as they were already donating at least an hour to interview with us. It may be that we need to have another outlet for this feedback but that wasn't possible with our scope. Part of the original plan was also to have the content of the interviews match our development schedule but that didn't happen. We were never guaranteed to talk about what we had recently developed in an interview, because it wasn't guaranteed the interviewee had experience in that subject or enjoyed talking about the topic. This is for some of the same reason as above.","title":"Questions for the Professionals"},{"location":"engine_postmortem/RubberduckDeveloping/","text":"Being forced to explain your decisions to someone else will be where you get the most value and learning from, not the actual development. If you were to say this to the team a the beginning our response would have been, \"What!? No, we are developers and we learn from doing!\" Which, to an extent, we still agree with however our documentation forced us to understand. Having to write blogs about our process was a blessing in disguise. From just developing a feature, you won't fully understand what you've done. You will probably have difficulty implementing it again without referencing your code or articles; nor will you remember the downsides of your implementation as compared to the other solutions you could have chosen. You will forget about the answer to these if you just keep continuing to develop the engine, but forcing yourself to explain what you implemented and why you chose this solution to your past-self, or someone in similar shoes as you, will help you remember your decisions (and if you don't remember you can revisit them). Although writing for another person, the actual audience of your explanations can just be yourself or your team. Testing documentation is an involved process, one that you won't have time for while developing an engine, but this doesn't mean you can't write as if it were going to be given to someone else. It is similar to API design of an engine, even if you will be the only one using it you want it to be understood by more than just yourself. You won't be able to retain all the decisions you've made when you've finished developing your engine, you documentation will be there to remind you of certain quirks to a feature and assumptions you were making. Pairing an explanation with diagrams of the problem and videos of before/after will take more time, however you typically will learn something that you didn't discover during development. Each of us on the team had sometime during the semester where we would be explaining what we had done only to realize there was an optimization that could be done, another solution, or even a flaw in our assumptions! While writing about collision solving box collisions, we realized we had made an assumption that all boxes were uniform, this certainly won't be true! We hadn't noticed a problem in testing because we were focusing on getting the different shapes to solve and weren't as concerned with different sizes. You could argue that it takes time away from learning the next system or growing the engine, but it doesn't. It forces you to slow down and actually digest what you've just done and typically reconsider all the decisions you made along the way. Although this takes more time, it will help you have a better thought through engine, especially if you are developing by yourself, and stop you from having to investigate mysterious bugs because you will know the specific drawbacks of your implementation. The next thing isn't going anywhere, you will only rackup more technical debt by going as fast as you can, especially if there is an assumption in your implementation that you know now but will forget tomorrow. We feel we must also state there is something magical between the extremes of documentation, With one extreme being where you only develop and don't reflect or explain, this case seems to be easier to fall into but as stated above not as useful for learning. The other extreme is much less common where you only write documentation about engine development without any development, the two pitfalls we see from this are 1) this sounds miserable from a developer perspective and 2) you would be writing from a theoretical stance and the ideal scenario isn't always how things pan out, there are strange bugs that you don't see from just thinking. Pairing application with reflection brings about something unique; do then explain.","title":"Rubberduck Developing: Forced Explanation"},{"location":"engine_postmortem/SoftwareDevelopment/","text":"Engine development at its core is a software application, which requires software engineers to develop. The team has had a solid focus on the game development side of engine development, however we are all junior developers and still learning best practices of basic software development. Although we successfully developed a game engine, we struggled with some of the basic software development practices that engineers in industry have locked down: testing, commenting, formating, and version control. Testing an engine can manifest in different forms, with tech demos and games, however at the core should also have the typical testing framework used in software development: unit and integration tests. We did unit tests... If we said that to a professional, then showed them our testing, we would be laughed off the stage. We definitely didn't have full-coverage, it would even be bold of us to say of us to say we had partial coverage. For us there simply wasn't time to develop a strong testing framework, however that isn't to say there wasn't the need. Our main foray into testing was unit testing our math classes, however we didn't follow test-driven development, writing the tests before the actual implementation. We didn't have a strict policy or a process at all of determining what should be tested. This lead us to not writing tests for some of the methods that actually had errors in them, mainly because these were the harder methods and classes to test. The reason we were eventually able to fix the errors in those math classes was from github users poking through our code and opening a issue or because we were fighting a bug that could only be caused by a math problem. These bugs were the worst to face, because we had the fake confidence that our math was tested. What we learned was that testing was almost like a full-time job for the project. But that isn't completely accurate to say either, as it is more akin to a way of developing, one developer can't be solely responsible for testing. The team needs to have the mantra of testing, the code needs to be developed with the thought of testing, it is a way of developing not just a part of it. For us, we were too enamoured by developing features that we weren't going to be slowed down by testing. We often went weeks without running the few tests that we did have! This even deterred us from creating tests, as our test framework had become out of date and required large amount of work to get working again, just for tests. We were asked whether the project would have benefit from a stricter testing phase to developing, with each code iteration requiring some testing; and the answer is no. While our engine could have used more testing, the projects mission was to learn and we wouldn't have been able to learn as much by being more methodical. Testing would have required us to remove features which would have produced a stable engine as a product, however we have no care for a stable engine (and admit the Isetta Engine isn't). What the Isetta Engine allowed us to do was see the entire picture of engine development as a whole, what it takes to build one, and the engine development process. If we were hoping to ship the engine or have users really use it for a project (not just a game jam), we should test the codebase. This also ensure the engine doesn't regress. However, if you want to learn engine development to learn, full test coverage just isn't necessary. Commenting, every developers favorite part of development... At best, we can say we tried to comment but really it was something that we were just so unenthusiastic about that we all let each other slip by. What we didn't expect when we were trying to comment was how much effort and time it actually took to write them. This is something we thought may lose an hour of development a week to not multiple. However the week we started commenting was the week we lost an entire week to comments. No features or systems were developed during that week. And we weren't even commenting our cpp files, only the header files public variables and functions! Additionally, what we hadn't factored into this was how much iteration and refactoring goes into an engine. The comments we wrote that week quickly became obsolete, requiring them to be rewritten, that's just time wasted for us! If we continued down the path of commenting the engine, the engine simply wouldn't be complete. We saw the benefit to comments, for the hypothetical game developer of the engine and the others who might be interested in learning from the engine code, but we had to admit it wasn't feasible for us. We attempted commenting one other time, prior to our game jam, which taught us game developers don't care about the functions that you have commented only the ones that aren't. What we mean by this is that as soon as that they find something uncommented, they no longer trust the engine to provide any information to them. On top of that, we found that our game jammers were much more interested in sample code over any comment. The way in which we used comments effectively was in the form of TODO s. When a feature needed to be developed, but not necessarily for our target game we would place a TODO comment there. We hope that these comments can help encourage curious engineers into trying to develop a feature with the Isetta engine. However, even with these TODO s, there are far more than we ever intended to have and most lack a good task description to be useful for someone who might not be familiar with the system already. We almost used TODO s as a replacement for failing unit tests, which is probably not how they should be used. Something that we had received advice on prior to starting development was to establish a formatting guideline, there wouldn't be enough time during development to fight over silly formatting practices. This advice was so right, and luckily we chose a linter which would automatically format our code. However, if it ended there we wouldn't be speaking about it here. Although we had chosen a linter, there was nothing (no one) to force us into this styling. Frequently all members of the team would push unlinted code or have a weird inconsistent styling with the rest of the engine because it was convenient at the time. This also corresponds heavily with our interview with Casey Muratori about a programmer's native language; we each had our own way of writing code and trying to write it in this rigid, formatted way was difficult for each of us. The only thing we can recommend to others is have a style that your team can agree on and when you find exceptions to that styling be consistent about it; but mainly don't waste time over formatting arguments, especially when time is constrainted. Our struggle with version control can be traced more specifically to Git. Each version control has its own unique, best practices and it took us awhile to establish a practice using Git that worked for the team. And we still feel it was inadequate. We had all used Git previously and were familiar with it enough that we initially weren't concerned with using it. However, it has possibly caused the most headache and time waste when compared to any engine system. We were using the process of Git-flow, however had ulterior motives for our branches. The branches didn't only dictate a task but rather an entire system of the engine, so that those interested could follow along with a branch if they were interested in a specific feature. This caused us to have an exorbitant amount of branches, with older branches rapidly going out of date. This was only compounded by each developer on the team using Git slightly differently, with some people rebasing, others only merging, etc. which caused a mess of commit messages. In addition to our personal Git problems, we were possibly creating problems for others because we hadn't developed any type of smoke testing prior to pushing to our develop/staging branches. This inevitably caused regression for the engine and frustration for the developer who was stuck with broken code they didn't create. We also experience some weird and unnecessary merge conflicts which we believe came about because of refactoring. The refactors of the engine went beyond changing a single class name, it went all the way to refactoring the name of the engine! This was a massive headache for us to merge our code into because most of that file directory no longer existed by that name. It took us longer than we care to admit to halt any refactors until all development had been \"checked-in\", which we then did these refactors late in the night/early in the morning when no one else was developing. No one was here to keep us in check with our software development practices. Our faculty advisers could only advise, and no one on the team knew enough to course correct us. Our skills with these practices increased through the semester, however this could have been somewhere we could have used an experienced developer the most.","title":"Basic Software Development Practices"},{"location":"interviews/","text":"Interviewing the Professionals \u00b6 Here you will find interviews with industry professionals about game engine development. The topics of conversation run the gamut from low-level systems like memory management to abstraction to high level gameplay. Each interview was transcribed and edited to help with digestability and readability, and they also became more searchable as a result. Within the compendium , the interviews are grouped by subject\u2014however, most of them have overlap with most topics, so you may end up reading them all! If you are more of a physical person you can purchase a published version of the interviews in book format, here . The price is $3, which is basically to cover printing cost.","title":"Introduction"},{"location":"interviews/#interviewing-the-professionals","text":"Here you will find interviews with industry professionals about game engine development. The topics of conversation run the gamut from low-level systems like memory management to abstraction to high level gameplay. Each interview was transcribed and edited to help with digestability and readability, and they also became more searchable as a result. Within the compendium , the interviews are grouped by subject\u2014however, most of them have overlap with most topics, so you may end up reading them all! If you are more of a physical person you can purchase a published version of the interviews in book format, here . The price is $3, which is basically to cover printing cost.","title":"Interviewing the Professionals"},{"location":"interviews/AdamSerdar-interview/","text":"The Definition and Beginning of a Game Engine \u00b6 Adam Serdar is a Senior Game Engineer at Schell Games where he works on projects that need server technology, graphic effects, or frame rate optimization. Most recently, Serdar was part of the team that created HoloLAB Champions, a virtual reality lab practice game in which he was deeply involved with its integral game systems, including how virtual objects could realistically simulate solids and liquids and how they are handled. When he's not coding at Schell Games, where he has been employed for twelve years, he is working on costume and robotics development projects as well as learning, practicing, and teaching Kung Fu. (The following is the edited transcription of a conversation we had with Adam Serdar.) A Game Engine and its Needs \u00b6 A game engine is, in its broadest sense, a set of tools that enables game creation. Depending on your platform, this could be as simple as having a scripting language which makes it easier for developers to interact and develop. Oftentimes an engine is a collection of tools, an update loop, and a scripting language. The Unity game engine accesses the update loop through the MonoBehaviour 1 system, as well as tool development with ScriptableObjects 2 . An engine can also be a simple system which has an update loop with rendering, giving the developer the ability to display bitmaps on screen. The simple engine can be expanded to have an animation system or other systems that the specific game being developed requires. That engine will grow and become more comprehensive as more developers spend time with it. Most engines are going to have some sort of graphics, some sort of at least minimal physics, some sort of audio\u2014mainly because those are going to be your most common systems that every game is going to want. Graphics are going to be one of the most common things that your game engine will do. Graphics of an engine can be simplified to the type of asset that needs to be imported because the graphics engine only needs to support these types of files. A simpler 2D game may only need to be able to import bitmaps, but if you're developing a 3D game models with animation might need to be supported. However, I know of at least three games off-hand that are completely audio-based. They wouldn't have any need for graphics. They're just showing a blank screen. They are more focused on audio bouncing off physical representations of the environment, kind of like a sonar-type game. If you're building an engine that's all about simulating that kind of system, the engine doesn't need to support graphics. Scripting languages are extremely nice to have so that programming is simpler and less brain-taxing; however, it's not required and you can usually just write code in the exact same language as the engine. Engine vs. Framework: What's the Difference? \u00b6 If you think of the line between an engine and a framework as layers, the lowest level is certainly the engine. As you work more with that engine, you might find yourself developing a framework, and the question of whether that's part of the engine or not depends on the number of users the framework has relative to the engine. The framework developed at Schell Games, which is built on top of Unity, is exclusive to Schell Games and the employees who take it home. That being said, sometimes a framework can get folded into the engine. For our case at Schell Games, that would require our framework to be assimilated by Unity. Sometimes features in frameworks are built prior to being included in the engine, and sometimes the specific implementation of the framework is better suited for our development. A framework is additive: It's a layer between the final game code, which is very specific to a game, and the engine. The line between the two is fuzzy; however, when there is game-specific code shared between multiple projects it belongs in a framework-type system. And although an engine is constantly being updated, it is relatively constant and self-contained. The Engine Dictates the Game \u00b6 As you're building a project, from a design perspective, you look at the nails and hammers you have and you say, \"this, this, and this\u2014those are great, we just need to add this extra nail.\" And sometimes this extra nail is system-level code. Or it's game-specific code. Having access to the entirety of your engine code does allow you to take a feature you need and scope out what's required of the team. And in picking your game, you get a pretty good idea of the features, so you know at least nominally what features you want in your engine. In theory, your system should be fine. With our projects, we often have a little bit of that decision-making for which engine we are going to use. Some of that comes from the client; some of that comes from our internal experience. And we really have to weigh the fact that if we don't use Unity, or we don't use some other familiar engine, there's a certain amount of cost with ramping up to that other system. We'd either have to tack that on as a cost or take that as earning more experience with this new system. In the Isetta Engine's case, the team has a twin-stick shooter game that they want to make, so that tells them which features to include in the engine. They themselves are gonna have to decide whether they have enough time to implement all of those features and make it as close as possible to this first game. This is not the way of things usually. In fact, I'd go so far as to say most of the time, you already have some level of engine already built, so it's usually a little closer to taking a design proposal and determining how to use our systems to make that. For example, a game by EA ( Madden , for instance), they typically just use the features already existing in their engine while upgrading them. If they need to move the franchise to a brand new platform, then they would have to upgrade the engine for that. Adding New Engine Features \u00b6 Updating an existing engine is somewhat iterative. If we're looking at Madden specifically, the very basic rules are always going to be the same, but there are always going to be features that the team would like to add. Offhand, what I'm thinking is the first iteration on the Playstation 2. The team needed to render players, and the players also had to be customizable enough to be recognizable. Different sizes, widths, girths\u2014lots of different qualities. And the different players need to interact reasonably well. In the second iteration, the team considers a more advanced problem. What if three or four players all collide into the center? Initially, the implementation may require players to collide in a sequence, like the first two, then the next two, etc. Is that good enough? The team may decide that they want some animation-blended system 3 where all of the players come into a giant huddle to make the pile-ups even more realistic. Also, as you're moving from Gen 1 PlayStation 4 to Gen N PlayStation 4, there's a lot of performance gains that can be made. Maybe a core 4 on the PlayStation 4 was hardly being used before, so the team decides to make it the \"audience core.\" Instead of little stick figures on flats, the audience is now full of 3D instanced people who are animated! There are bigger leaps, too. For example, moving a system from PlayStation 3 to PlayStation 4, or responding to years of stagnation and infrequent updates. Where that's driven from is an interesting question. Oftentimes, it's both the engine guys wanting to do something cool that they couldn't before and the designers or scripters wanting to fix the weird things they've had to deal with. Iterative vs Waterfall for Engine Design \u00b6 For the design and development of a project, it is better to have an iterative approach to that of a waterfall schedule 5 . Start simple then layer in more features, with the caveat of removing or adjusting particular sections when they don't make sense. I recommend an iterative approach to projects, unless time becomes an issue. Oftentimes, with shorter projects, prototypes turn into the final product quicker than you'd expect, or at least large chunks of code are copied regardless of the quality. Ideally, you can do iteration, but practically it just may not happen. Neglected Systems \u00b6 From my perspective, usually graphics is the first thing people are worrying about, and poor audio is always last. That tends to be my experience in a development space of working with N number of features. We'll get to audio when we get to audio, but the shiny and pretty stuff tends to come first. Is anything neglected? I would say no. Even audio has a bunch of people that are very enthusiastic about the systems that they care about, and even Unity is spending time creating brand new\u2014well, more likely importing new systems to make their audio better. Usually, if it's neglected from a Unity standpoint, it means Unity is at least marginally aware of it and is probably looking for a solution to integrate cleanly. Some of their newest systems are mostly graphics, like the node-based shader system 6 , the Scriptable Render Pipeline 7 , that sort of thing. They're going through a very radical shift over what you can and can't do with the graphics pipeline, which I'm personally excited about. Industry Standards for Engine Systems \u00b6 There's no huge standard for engine systems other than the APIs that other systems are using. Usually, if you want to make your own custom audio system, then you're going to have to spend a lot of time building up threads, make sure they play through the systems appropriately, how they get loaded, and all that. Or you can spend time learning the APIs for OpenAL 8 or whatever it is, and then they kind of have a way they're expected to be used. It's kind of about what you expect to write yourself and what you expect to use as an external library. DirectX and OpenGL will have very specific calls that you're basically required to do in a proper order or it's just not gonna do what you want it to do! It's more about making sure your system flows with the interface of the systems that you're using externally. It's been a while since I've looked at those low-level APIs, and (when I was in the position) I definitely didn't want to write most of it myself. While I wanted low-level access to OpenGL or DirectX so I could do fancy graphics stuff, I was less worried about physics or audio or whatever because I wouldn't have to rewrite or force it. Much like building most games, building a comprehensive engine these days is teamwork. And oftentimes, that's finding a thing you're interested in, and working on it 'til it's awesome. And that's probably true for game development as well. If you've got some guy that hates UI and you push him onto UI, and you're surprised when it doesn't work out so well\u2014whose fault is that? Adam's Engine-Building Experience \u00b6 At the time of building my first engine, I was in Panda3D 9 and Unity, and I wanted to know more about that rendering pipeline and what was required. And so, the engine that I built was very simple but very dedicated to, having a mesh, shader, and fancy particles moving around just on the GPU with nothing else knowing about it. It was a bit more focused rather than a generalist system. But I also wasn't expecting to actually make a game with this without significant investment in it. You can spend a lot of time building an engine and then building a game, and then realizing as soon as you finish the game, it's now five-year-old technology. It may be cool, but it couldn't keep up to date with teams of programmers and artists and all these guys working together to build... for instance, in a horror game, where you want very interesting graphical effects. If you make your own engine, you've got very precise control over what's being rendered! Or, a team of people could have made the same thing in Unity, for instance, and had it out the door years before you're finished with the engine. So it's really a trade-off thing from a professional point of view, and that's why I chose to do a more focused deep-dive of asking what I'd need if I was to get access to the core render loop 10 of Unity. And that's kind of what my stupid little engine explored. Integrating Libraries into an Engine \u00b6 Once you have a whiteboard plan, definitely set up a version control system. In determining which libraries could be useful it is better to consider the feature set your project needs to support. Obviously, if you're not doing a lot of string manipulation, don't look for a library that does that. When using a library for the first time, there is some time lost to setup and figuring how to properly configure your IDE to work with the library. A simple decision you may have early on is determining whether you are supporting OpenGL or DirectX. Does being close to open source matter to your project? This decision will affect discussions with all libraries you maybe thinking of including. Another benefit of open source libraries is you can modify the code if you aren't happy with how the library is operating; you have control. Another factor to consider in deciding on libraries is looking for a particular set of specifications/requirements for it to satisfy your project's needs. If you are going to have physics, you'll probably want to use Nvidia's PhysX 11 system to have all the computation on the GPU. The audio will usually run on its own thread. I would lay out each system the engine will include. How Existing Engines Limit Game Development \u00b6 The way I like to talk about Unity is it's very good at making 95% of the game, and that last 5% is going to be like pulling teeth. Performance is an issue, though they're getting better about that. VR is currently kind of \"hacked\" into Unity, and it's almost good enough, and that's where I think the scriptable render pipeline is going to be good enough\u2014or at least pretty good. On the other hand, my understanding is that Unreal from an engine perspective is very difficult to modify. It's a huge codebase at this point and recompiling that beast is an undertaking. Unreal has a lot of good things going for it, including graphics fidelity. You'll hear the insult: \"Oh, that looks like a Unity game.\" You don't hear the same insult of \"Oh, that looks like an Unreal game,\" because Unreal looks pretty awesome! If you need to do tweaking to that, it then gets much more difficult to do. That's my understanding, though I have not used it in years. The Relationships of Engines in the Industry \u00b6 I think you're starting to see a lot of cross-pollination between Unreal and Unity and other systems. Unity was always very good at making something quick\u2014real quick\u2014and making it playable and fun. If they want it to look great, that means spending more time; that's the last 5% sometimes. Whereas Unreal tends to have all of those pretty features already active, and as long as you know the gameplay scripting and the Blueprints 12 and whatnot, you're going to have a pretty good time. If you need to constantly tweak what it's doing, though, you're going to have a different kind of time. The funny thing is we are looking into possibly taking at least some of the rendering tech of Unreal and putting it into Unity. Then people tell us, \"this isn't a Unity game, it looks like Unreal!\" And we'll say, \"yes, that's exactly what we're going for!\" We'll see if that really pans out, though. There's a couple interesting rendering systems that they've got that the scriptable render pipeline might make it very feasible to automate the process. But, who knows, that's future-seeing. Interview conducted May 15, 2018. MonoBehaviour is the base class within Unity that all components which attach to GameObjects must derive from, it has methods for start, update, and destroy. \u21a9 ScriptableObjects are scripts which cannot be attached to GameObjects but still store (serialize) user data. \u21a9 An animation-blended system can be a graph of multiple animations and transitions from an animation to another, i.e., a idle animation to a walking animation, and the blend system is how the animations are \"mixed\" together. It extrapolates from the starting animation to the ending animation. \u21a9 A core refers to a CPU in a multi-core processor, it is one of the processing units in the single computing component that read and execute machine instructions. \u21a9 A waterfall schedule is a linear schedule where each subsequent item is dependent on the previous components being completed, it is less iterative and flexible because the flow is usually mono-directional. \u21a9 Node-based means the interface is visual with components, \"boxes\", that are connected to each other with outputs connected to inputs. A shader is a program that alters the graphical look of an object. A node-based shader system means a shader is edited through nodes. \u21a9 The Scriptable Render Pipeline is a system in Unity that allows the game developer to configure and control the graphics and rendering process via high-level scripting. \u21a9 OpenAL is an audio library used for games, although it contains the word open it actually isn't open-sourced. Its open-source counterpart is OpenALSoft. \u21a9 Panda3D is a game engine, a framework for 3D rendering and game development for Python and C++ programs. It was originally developed by Disney and expanded by past ETC projects. \u21a9 Core render loop is the loop where the rendering function is called. The way the rendering occurs/is called varies from engine to engine, but is usually performed at the end of the main game loop. \u21a9 NVIDIA PhysX is a proprietary real-time physics engine SDK created by NVIDIA. It is used in most commercial game engines such as Unity, Unreal, and Lumberyard. \u21a9 Unreal's Blueprint Visual Scripting System is the node-based scripting in the Unreal Engine used for gameplay scripting. \u21a9","title":"Interview"},{"location":"interviews/AdamSerdar-interview/#the-definition-and-beginning-of-a-game-engine","text":"Adam Serdar is a Senior Game Engineer at Schell Games where he works on projects that need server technology, graphic effects, or frame rate optimization. Most recently, Serdar was part of the team that created HoloLAB Champions, a virtual reality lab practice game in which he was deeply involved with its integral game systems, including how virtual objects could realistically simulate solids and liquids and how they are handled. When he's not coding at Schell Games, where he has been employed for twelve years, he is working on costume and robotics development projects as well as learning, practicing, and teaching Kung Fu. (The following is the edited transcription of a conversation we had with Adam Serdar.)","title":"The Definition and Beginning of a Game Engine"},{"location":"interviews/AdamSerdar-interview/#a-game-engine-and-its-needs","text":"A game engine is, in its broadest sense, a set of tools that enables game creation. Depending on your platform, this could be as simple as having a scripting language which makes it easier for developers to interact and develop. Oftentimes an engine is a collection of tools, an update loop, and a scripting language. The Unity game engine accesses the update loop through the MonoBehaviour 1 system, as well as tool development with ScriptableObjects 2 . An engine can also be a simple system which has an update loop with rendering, giving the developer the ability to display bitmaps on screen. The simple engine can be expanded to have an animation system or other systems that the specific game being developed requires. That engine will grow and become more comprehensive as more developers spend time with it. Most engines are going to have some sort of graphics, some sort of at least minimal physics, some sort of audio\u2014mainly because those are going to be your most common systems that every game is going to want. Graphics are going to be one of the most common things that your game engine will do. Graphics of an engine can be simplified to the type of asset that needs to be imported because the graphics engine only needs to support these types of files. A simpler 2D game may only need to be able to import bitmaps, but if you're developing a 3D game models with animation might need to be supported. However, I know of at least three games off-hand that are completely audio-based. They wouldn't have any need for graphics. They're just showing a blank screen. They are more focused on audio bouncing off physical representations of the environment, kind of like a sonar-type game. If you're building an engine that's all about simulating that kind of system, the engine doesn't need to support graphics. Scripting languages are extremely nice to have so that programming is simpler and less brain-taxing; however, it's not required and you can usually just write code in the exact same language as the engine.","title":"A Game Engine and its Needs"},{"location":"interviews/AdamSerdar-interview/#engine-vs-framework-whats-the-difference","text":"If you think of the line between an engine and a framework as layers, the lowest level is certainly the engine. As you work more with that engine, you might find yourself developing a framework, and the question of whether that's part of the engine or not depends on the number of users the framework has relative to the engine. The framework developed at Schell Games, which is built on top of Unity, is exclusive to Schell Games and the employees who take it home. That being said, sometimes a framework can get folded into the engine. For our case at Schell Games, that would require our framework to be assimilated by Unity. Sometimes features in frameworks are built prior to being included in the engine, and sometimes the specific implementation of the framework is better suited for our development. A framework is additive: It's a layer between the final game code, which is very specific to a game, and the engine. The line between the two is fuzzy; however, when there is game-specific code shared between multiple projects it belongs in a framework-type system. And although an engine is constantly being updated, it is relatively constant and self-contained.","title":"Engine vs. Framework: What's the Difference?"},{"location":"interviews/AdamSerdar-interview/#the-engine-dictates-the-game","text":"As you're building a project, from a design perspective, you look at the nails and hammers you have and you say, \"this, this, and this\u2014those are great, we just need to add this extra nail.\" And sometimes this extra nail is system-level code. Or it's game-specific code. Having access to the entirety of your engine code does allow you to take a feature you need and scope out what's required of the team. And in picking your game, you get a pretty good idea of the features, so you know at least nominally what features you want in your engine. In theory, your system should be fine. With our projects, we often have a little bit of that decision-making for which engine we are going to use. Some of that comes from the client; some of that comes from our internal experience. And we really have to weigh the fact that if we don't use Unity, or we don't use some other familiar engine, there's a certain amount of cost with ramping up to that other system. We'd either have to tack that on as a cost or take that as earning more experience with this new system. In the Isetta Engine's case, the team has a twin-stick shooter game that they want to make, so that tells them which features to include in the engine. They themselves are gonna have to decide whether they have enough time to implement all of those features and make it as close as possible to this first game. This is not the way of things usually. In fact, I'd go so far as to say most of the time, you already have some level of engine already built, so it's usually a little closer to taking a design proposal and determining how to use our systems to make that. For example, a game by EA ( Madden , for instance), they typically just use the features already existing in their engine while upgrading them. If they need to move the franchise to a brand new platform, then they would have to upgrade the engine for that.","title":"The Engine Dictates the Game"},{"location":"interviews/AdamSerdar-interview/#adding-new-engine-features","text":"Updating an existing engine is somewhat iterative. If we're looking at Madden specifically, the very basic rules are always going to be the same, but there are always going to be features that the team would like to add. Offhand, what I'm thinking is the first iteration on the Playstation 2. The team needed to render players, and the players also had to be customizable enough to be recognizable. Different sizes, widths, girths\u2014lots of different qualities. And the different players need to interact reasonably well. In the second iteration, the team considers a more advanced problem. What if three or four players all collide into the center? Initially, the implementation may require players to collide in a sequence, like the first two, then the next two, etc. Is that good enough? The team may decide that they want some animation-blended system 3 where all of the players come into a giant huddle to make the pile-ups even more realistic. Also, as you're moving from Gen 1 PlayStation 4 to Gen N PlayStation 4, there's a lot of performance gains that can be made. Maybe a core 4 on the PlayStation 4 was hardly being used before, so the team decides to make it the \"audience core.\" Instead of little stick figures on flats, the audience is now full of 3D instanced people who are animated! There are bigger leaps, too. For example, moving a system from PlayStation 3 to PlayStation 4, or responding to years of stagnation and infrequent updates. Where that's driven from is an interesting question. Oftentimes, it's both the engine guys wanting to do something cool that they couldn't before and the designers or scripters wanting to fix the weird things they've had to deal with.","title":"Adding New Engine Features"},{"location":"interviews/AdamSerdar-interview/#iterative-vs-waterfall-for-engine-design","text":"For the design and development of a project, it is better to have an iterative approach to that of a waterfall schedule 5 . Start simple then layer in more features, with the caveat of removing or adjusting particular sections when they don't make sense. I recommend an iterative approach to projects, unless time becomes an issue. Oftentimes, with shorter projects, prototypes turn into the final product quicker than you'd expect, or at least large chunks of code are copied regardless of the quality. Ideally, you can do iteration, but practically it just may not happen.","title":"Iterative vs Waterfall for Engine Design"},{"location":"interviews/AdamSerdar-interview/#neglected-systems","text":"From my perspective, usually graphics is the first thing people are worrying about, and poor audio is always last. That tends to be my experience in a development space of working with N number of features. We'll get to audio when we get to audio, but the shiny and pretty stuff tends to come first. Is anything neglected? I would say no. Even audio has a bunch of people that are very enthusiastic about the systems that they care about, and even Unity is spending time creating brand new\u2014well, more likely importing new systems to make their audio better. Usually, if it's neglected from a Unity standpoint, it means Unity is at least marginally aware of it and is probably looking for a solution to integrate cleanly. Some of their newest systems are mostly graphics, like the node-based shader system 6 , the Scriptable Render Pipeline 7 , that sort of thing. They're going through a very radical shift over what you can and can't do with the graphics pipeline, which I'm personally excited about.","title":"Neglected Systems"},{"location":"interviews/AdamSerdar-interview/#industry-standards-for-engine-systems","text":"There's no huge standard for engine systems other than the APIs that other systems are using. Usually, if you want to make your own custom audio system, then you're going to have to spend a lot of time building up threads, make sure they play through the systems appropriately, how they get loaded, and all that. Or you can spend time learning the APIs for OpenAL 8 or whatever it is, and then they kind of have a way they're expected to be used. It's kind of about what you expect to write yourself and what you expect to use as an external library. DirectX and OpenGL will have very specific calls that you're basically required to do in a proper order or it's just not gonna do what you want it to do! It's more about making sure your system flows with the interface of the systems that you're using externally. It's been a while since I've looked at those low-level APIs, and (when I was in the position) I definitely didn't want to write most of it myself. While I wanted low-level access to OpenGL or DirectX so I could do fancy graphics stuff, I was less worried about physics or audio or whatever because I wouldn't have to rewrite or force it. Much like building most games, building a comprehensive engine these days is teamwork. And oftentimes, that's finding a thing you're interested in, and working on it 'til it's awesome. And that's probably true for game development as well. If you've got some guy that hates UI and you push him onto UI, and you're surprised when it doesn't work out so well\u2014whose fault is that?","title":"Industry Standards for Engine Systems"},{"location":"interviews/AdamSerdar-interview/#adams-engine-building-experience","text":"At the time of building my first engine, I was in Panda3D 9 and Unity, and I wanted to know more about that rendering pipeline and what was required. And so, the engine that I built was very simple but very dedicated to, having a mesh, shader, and fancy particles moving around just on the GPU with nothing else knowing about it. It was a bit more focused rather than a generalist system. But I also wasn't expecting to actually make a game with this without significant investment in it. You can spend a lot of time building an engine and then building a game, and then realizing as soon as you finish the game, it's now five-year-old technology. It may be cool, but it couldn't keep up to date with teams of programmers and artists and all these guys working together to build... for instance, in a horror game, where you want very interesting graphical effects. If you make your own engine, you've got very precise control over what's being rendered! Or, a team of people could have made the same thing in Unity, for instance, and had it out the door years before you're finished with the engine. So it's really a trade-off thing from a professional point of view, and that's why I chose to do a more focused deep-dive of asking what I'd need if I was to get access to the core render loop 10 of Unity. And that's kind of what my stupid little engine explored.","title":"Adam's Engine-Building Experience"},{"location":"interviews/AdamSerdar-interview/#integrating-libraries-into-an-engine","text":"Once you have a whiteboard plan, definitely set up a version control system. In determining which libraries could be useful it is better to consider the feature set your project needs to support. Obviously, if you're not doing a lot of string manipulation, don't look for a library that does that. When using a library for the first time, there is some time lost to setup and figuring how to properly configure your IDE to work with the library. A simple decision you may have early on is determining whether you are supporting OpenGL or DirectX. Does being close to open source matter to your project? This decision will affect discussions with all libraries you maybe thinking of including. Another benefit of open source libraries is you can modify the code if you aren't happy with how the library is operating; you have control. Another factor to consider in deciding on libraries is looking for a particular set of specifications/requirements for it to satisfy your project's needs. If you are going to have physics, you'll probably want to use Nvidia's PhysX 11 system to have all the computation on the GPU. The audio will usually run on its own thread. I would lay out each system the engine will include.","title":"Integrating Libraries into an Engine"},{"location":"interviews/AdamSerdar-interview/#how-existing-engines-limit-game-development","text":"The way I like to talk about Unity is it's very good at making 95% of the game, and that last 5% is going to be like pulling teeth. Performance is an issue, though they're getting better about that. VR is currently kind of \"hacked\" into Unity, and it's almost good enough, and that's where I think the scriptable render pipeline is going to be good enough\u2014or at least pretty good. On the other hand, my understanding is that Unreal from an engine perspective is very difficult to modify. It's a huge codebase at this point and recompiling that beast is an undertaking. Unreal has a lot of good things going for it, including graphics fidelity. You'll hear the insult: \"Oh, that looks like a Unity game.\" You don't hear the same insult of \"Oh, that looks like an Unreal game,\" because Unreal looks pretty awesome! If you need to do tweaking to that, it then gets much more difficult to do. That's my understanding, though I have not used it in years.","title":"How Existing Engines Limit Game Development"},{"location":"interviews/AdamSerdar-interview/#the-relationships-of-engines-in-the-industry","text":"I think you're starting to see a lot of cross-pollination between Unreal and Unity and other systems. Unity was always very good at making something quick\u2014real quick\u2014and making it playable and fun. If they want it to look great, that means spending more time; that's the last 5% sometimes. Whereas Unreal tends to have all of those pretty features already active, and as long as you know the gameplay scripting and the Blueprints 12 and whatnot, you're going to have a pretty good time. If you need to constantly tweak what it's doing, though, you're going to have a different kind of time. The funny thing is we are looking into possibly taking at least some of the rendering tech of Unreal and putting it into Unity. Then people tell us, \"this isn't a Unity game, it looks like Unreal!\" And we'll say, \"yes, that's exactly what we're going for!\" We'll see if that really pans out, though. There's a couple interesting rendering systems that they've got that the scriptable render pipeline might make it very feasible to automate the process. But, who knows, that's future-seeing. Interview conducted May 15, 2018. MonoBehaviour is the base class within Unity that all components which attach to GameObjects must derive from, it has methods for start, update, and destroy. \u21a9 ScriptableObjects are scripts which cannot be attached to GameObjects but still store (serialize) user data. \u21a9 An animation-blended system can be a graph of multiple animations and transitions from an animation to another, i.e., a idle animation to a walking animation, and the blend system is how the animations are \"mixed\" together. It extrapolates from the starting animation to the ending animation. \u21a9 A core refers to a CPU in a multi-core processor, it is one of the processing units in the single computing component that read and execute machine instructions. \u21a9 A waterfall schedule is a linear schedule where each subsequent item is dependent on the previous components being completed, it is less iterative and flexible because the flow is usually mono-directional. \u21a9 Node-based means the interface is visual with components, \"boxes\", that are connected to each other with outputs connected to inputs. A shader is a program that alters the graphical look of an object. A node-based shader system means a shader is edited through nodes. \u21a9 The Scriptable Render Pipeline is a system in Unity that allows the game developer to configure and control the graphics and rendering process via high-level scripting. \u21a9 OpenAL is an audio library used for games, although it contains the word open it actually isn't open-sourced. Its open-source counterpart is OpenALSoft. \u21a9 Panda3D is a game engine, a framework for 3D rendering and game development for Python and C++ programs. It was originally developed by Disney and expanded by past ETC projects. \u21a9 Core render loop is the loop where the rendering function is called. The way the rendering occurs/is called varies from engine to engine, but is usually performed at the end of the main game loop. \u21a9 NVIDIA PhysX is a proprietary real-time physics engine SDK created by NVIDIA. It is used in most commercial game engines such as Unity, Unreal, and Lumberyard. \u21a9 Unreal's Blueprint Visual Scripting System is the node-based scripting in the Unreal Engine used for gameplay scripting. \u21a9","title":"The Relationships of Engines in the Industry"},{"location":"interviews/AdamSerdar-video/","text":"Shaders in Panda3D \u00b6","title":"Video Story"},{"location":"interviews/AdamSerdar-video/#shaders-in-panda3d","text":"","title":"Shaders in Panda3D"},{"location":"interviews/AliceChing-advice/","text":"Alice is currently working as a lead developer on Wattam at Funomena, where she helps her teammates stretch the capabilities of Unity. She has experienced large and small company environments, working at companies like Visceral Games, Sanzaru, and Google. Some of her past projects include Sly Cooper: Thieves in Time at Sanzaru Games and Godfather II at Visceral Games. Fostering Usability \u00b6 Advice ( not verbatim): Engines will differ depending on the genre, but more can be shared than it seems. Visceral Games had two forks 1 from the Godfather Engine, one for Dead Space (a survival horror game), and one for Godfather II (an open-world game). Each team modified their fork based on their needs. Other technologies can typically still be shared across different game genres, like tools and optimizations. Determining which technologies to merge and share across projects becomes an important subject, and much of the responsibility for this is on the technical leads. Code reviews are time-consuming, but important for many different parts of a project. Some studios like EA use code reviews before every submission to ensure the quality of the code is up to snuff. At Funomena, the Wattam team is using them as a way to sanity check 2 our code. Code reviews most significantly help with proper commenting, readability, reduction of bad habits, and clearing up misunderstandings in complex functions and systems. Breaking all of your changes into tiny submissions helps make the code review process easier for the team. It's okay not to know everything, and it will actually be common on a game engine team. At large companies, you will typically only communicate with your immediate peers, so it's hard to know about the problems that other teams are facing. If you are only familiar with certain pieces of the engine that you've been working on, that prevents you from reworking and over-optimizing your code. Working on a game engine team, you also learn to let go of refactoring any \"bad\" code unless it's legitimately broken. You learn to let go of refactoring \"bad\" code unless it is actually broken. Bring QA in, especially if you're working on more than 3 systems If you have data-driven pipelines, debugging and ensuring correct patches is even more difficult because you can't tell from the code that something else in the engine depends on particular parameters. Having developers test parts of the game that they don't work on can help you gain perspective on the bigger picture and wrap your head around such a complex system. A project fork is a when developers take a copy of source code from one software package and develop on it independent from the original. \u21a9 A sanity check is a basic test to quickly evaluate whether a result can possibly be true or correct. \u21a9","title":"Alice Ching"},{"location":"interviews/AliceChing-advice/#fostering-usability","text":"Advice ( not verbatim): Engines will differ depending on the genre, but more can be shared than it seems. Visceral Games had two forks 1 from the Godfather Engine, one for Dead Space (a survival horror game), and one for Godfather II (an open-world game). Each team modified their fork based on their needs. Other technologies can typically still be shared across different game genres, like tools and optimizations. Determining which technologies to merge and share across projects becomes an important subject, and much of the responsibility for this is on the technical leads. Code reviews are time-consuming, but important for many different parts of a project. Some studios like EA use code reviews before every submission to ensure the quality of the code is up to snuff. At Funomena, the Wattam team is using them as a way to sanity check 2 our code. Code reviews most significantly help with proper commenting, readability, reduction of bad habits, and clearing up misunderstandings in complex functions and systems. Breaking all of your changes into tiny submissions helps make the code review process easier for the team. It's okay not to know everything, and it will actually be common on a game engine team. At large companies, you will typically only communicate with your immediate peers, so it's hard to know about the problems that other teams are facing. If you are only familiar with certain pieces of the engine that you've been working on, that prevents you from reworking and over-optimizing your code. Working on a game engine team, you also learn to let go of refactoring any \"bad\" code unless it's legitimately broken. You learn to let go of refactoring \"bad\" code unless it is actually broken. Bring QA in, especially if you're working on more than 3 systems If you have data-driven pipelines, debugging and ensuring correct patches is even more difficult because you can't tell from the code that something else in the engine depends on particular parameters. Having developers test parts of the game that they don't work on can help you gain perspective on the bigger picture and wrap your head around such a complex system. A project fork is a when developers take a copy of source code from one software package and develop on it independent from the original. \u21a9 A sanity check is a basic test to quickly evaluate whether a result can possibly be true or correct. \u21a9","title":"Fostering Usability"},{"location":"interviews/AmandineCoget-interview/","text":"Engine Programming is All Plumbing \u00b6 Amandine Coget is a French game engine plumber living in Stockholm, freelancing after having worked on the Frostbite and Bitsquid engines. Other interests include crafts, politics, and the politics of crafts. (The following is the edited transcription of a conversation we had with Amandine Coget.) Introduction \u00b6 You can probably guess from the name that I was born and raised in France. I also studied there; the degrees I took were programming oriented but not game-specific. It was very different from a Bachelor's in Computer Science in that half of my studies were computer related (what French calls \"Informatique\") but the other half was mathematics, economics, communication, and accounting. That has been extremely useful in my work as a consultant. Ironically, my most useful class has been accounting. Career-wise, I spent three years at DICE working on Battlefield 4 and the Frostbite Engine. After that I spent six months at BitSquid 1 , and have been freelancing since then. As for my hobbies, there's weaving which I'm trying to take beyond hobby these days, there's making my own yarn, and there's a bit of glassblowing and ceramics. The Modes and Pipeline of an Engine \u00b6 When I was starting out in engine development, the most challenging aspect for me was how much of it is actually related to the pipeline. With small projects, you generally have the data directly in files, like images and such, and then you've got the game executable which just loads everything. As soon as you've got big projects, though, you will have data that gets processed by a pipeline and then used by the game. So typically you will have a game executable that can run in several modes: it can run as a tool, or it can run as a game. That was initially tricky to wrap my head around: that you had one executable running in different modes and that the code you wrote needed to work in every one of those modes. That's something you don't encounter on small projects, because all you need to do there is something as simple as loading the PNGs. For example, if you're working on mobile, you're going to want compressed file formats, and you can do that manually by running a standalone tool, or you can have an integrated game pipeline that just processes all the data into compressed formats efficiently. That's the part that is rarely taught because, again, pipelines are only relevant for large-scale projects. I would argue the pipeline is always part of the engine because it is something that's so tightly connected to your runtime\u2014the same team will usually be working on it. The pipeline could be one standard executable that's compiled with different parameters, or you could pass different command line parameters to your game, but the basic idea is that you don't just have a \"single\" exe file. You've got those different modes and configurations. It also ties into multi-platform development because when you have so many paths, it takes a mindset shift. Onboarding for Programmers \u00b6 In my experience of onboarding, a specific point of frustration is the lack of up-to-date, complete documentation. This is especially the case for programmers. We usually have resources for content creators because there's a lot more churn with content creators. Artists, arguably designers, and scripters are the people at a company that change a lot between and during projects. I mean, you know by now that the game industry feeds on blood. As such, programmer onboarding is rarely as developed even though it requires just as much explanation. Programmers new to a codebase need documentation highlighting the company's established process, and time to find their way around the code. I remember when a new co-worker joined my team on Battlefield 4 and was just starting on her first feature. She asked something, so I directed her to someone I thought knew the answer, who directed her to someone else\u2014I think there were like four or five people in the chain\u2014and eventually, the question circled back around to me. You've got this problem of knowledge tracking and documentation; figuring out where the knowledge lives. In the end, knowledge management has been the hardest part of onboarding for me. While other industries have entire teams working on compiling knowledge, the games industry typically doesn't. I think that along with the high rate of turnover where we lose people and therefore knowledge, not having proper processes in place or rarely having tech writers will hurt us. To remedy this issue, it helps to follow good, readable code practices. Comments and documentation can help, but then they have to be kept up to date. As for new programmers being dropped into this scenario, the big thing is resisting the urge to say \"this is crap\" early on. The urge to criticize will always be there, and the criticism is often valid. Three months later, though, you get the context and the history, and you'll understand why things are done that way. In fact, when working on the Customization screen for Battlefield 4 , which was my big system rewrite, it was a mess but the deeper I went and the more I re-implemented, the more I came to understand why the previous decisions were made. Give up on the idea of writing perfect code\u2014it will not happen, I'm sorry to tell you. And here is another thing you may not want to hear, but in my experience, after the onboarding, a solid 98% of an engine programmer's job is plumbing and legacy management. Even if you do write new code, a lot of it will be interfacing with the old. I think only experience can teach you how to best modify old systems, because as you build up experience, you build up what I like to call the list of bullshit factoids. Just small points about how your actions can backfire, or certain quirks an API has. As you build up this list of factoids, you build an instinct. While I have six years of experience under my belt now, I'm well aware the people who have been doing this for 15 or 20 years will have a much more refined instinct. However, they might also be bogged down by old factoids that aren't relevant anymore. That's why it's good to have a team that's a blend of ages, experience levels, and backgrounds, because you can have different instincts that can work together. Thinking about Usability \u00b6 Going from working on UI programming to graphics to core systems, the biggest thing I've noticed is that it's become impossible for me to ignore usability. In a way, UI is all the way at the end of the slope, because you will get feature requests from the gameplay team and your work comes at the end of the process. You're the last one to come in when shit rolls downhill, so you will run into every corner that was cut every step of the way. A part of that work is fetching data from the gameplay code, so if corners have been cut in gameplay code, your UI won't work. When the gameplay team comes in with last-minute changes, meaning the UI has to be changed, it starts getting messy really quickly. This reinforces the focus on users, talking to your users, and thinking about usability in general. It's not as common for programmers on a standalone engine team to talk with designers. But I've kept the a habit, even for engine features, to talk with my users a lot. When I was on the rendering team, for instance, I needed to understand if I was on the right track with what was wanted of a feature. Since I was in the DICE offices, I just grabbed an artist, sat them down in front of my computer, and asked them to try to and use what I'd built. This plan really worked because they immediately tried to do something I hadn't thought about! Because of that experience, I now regularly ask others if my tools are confusing to use: even if it looked great on paper, what's the interface like to the user? Your user can be a programmer as well, so you can apply this same UX mindset when designing APIs. I spend a lot of my time \"fighting for the users\". I absolutely think a large-ish engine team should always have a dedicated UX designer on hand because it really is a skill. As programmers, we have the nasty tendency to think that we can just figure it out and come up with a decent enough interface on our own. But there are people who study usability for years and know what they're doing. GUI: Immediate vs Retained Modes \u00b6 When it comes to what form of GUI to use, as always it depends. I haven't followed how Casey 's take on IMGUI 2 (IM) has evolved over the last few years. For context, while I was working on Battlefield 4 , what we were doing was shifting the entire UI from using Scaleform 3 , which is heavily retained 4 , over to something that was more immediate mode and in C++. We rewrote every last bit of UI in the game basically and what we discovered more and more is where IM worked really well and where it broke down. In my opinion, IM is great for simple things, so for debug interfaces, small tools, or demos, it's a no-brainer. As soon as you need some degree of persistence, like with data-driven systems or allowing designers and artists to customize UI, you will need some kind of extra layer. And that can be something that gets baked down by the pipeline into something that's more immediate, or it can be something that actually exists at runtime. But at scale, I find IM code just gets really messy when you're fetching data from all over the place. This is especially the case if you're interfacing with a visual scripting system like Frostbite 5 has, which we also call \"noodles\". You need some kind of persistent entity just so you can drag one of those noodles into it, and while that gives a lot of power to artists and designers, it gives a lot of headaches to programmers. Noodles are code; they need to be debugged like code, and it turns out that artists are good at making horrendously complicated things really fast. For a project with the scale of the Isetta Engine, where you have only three months, IM is the right choice. For a large AAA project, though, I wouldn't recommend it as the only approach. If you can pair IM and retained modes cleanly when designing your system, then it is absolutely a good idea to have both. Battlefield 4 did a little bit of that because the HUD 6 was very immediate mode-ish and so it had to be fast. My main work on Battlefield 4 was the customization screen, which is tons of small widgets controlled by a ton of data. Working on that screen taught me the biggest challenge with UI is fetching all that data. In that case, having persistent entities that just fetch the stuff once and hold on to it can help set up a nice architecture, but it depends on your particular project. Rendering UI can also be challenging because you're doing layered 2D rendering with a lot of transparency, which modern renderers don't like. And with UI there will often be more string manipulation than you would like, so you need to optimize that. A Brief History of the Frostbite Engine \u00b6 While Frostbite is known today for accommodating many game genres, it wasn't designed to do so early on. It had some growing pains. Each new game in Frostbite showed some shortcomings or some things that were missing, and so the engine evolved organically. That shows in the codebase, but I don't want to harp on Frostbite specifically, what can you do when you've got two million lines of code supporting that many games and features? For a bit of history, Frostbite was originally the Battlefield engine, made by DICE in Stockholm. Medal of Honor: Warfighter, which we like to forget about, was made on Frostbite as well. Because that game was also a FPS, it was very close to Battlefield . The next game to use the engine was Army of Two: The Devil's Cartel , which we also like to forget about, which brought third-person co-op. The Frostbite team needed to figure out how to support that gameplay style and what updates needed to be made to the engine as a result. Need for Speed: The Run was the first racing game to use the engine. It brought a lot to the engine tech-wise, because it showed where things cracked. For instance the Need for Speed team made a bunch of things for The Run , like a road making tool and handling a car's speed. The vehicles in Battlefield are done in a certain way, but cars in Need for Speed move differently, which affects how to stream in new assets. So each game brought new problems. Dragon Age: Inquisition was a massive challenge, because you're going from an engine designed for FPS titles to a super-massive RPG with conversation systems and more. What I know is that the entire serialization system and the entire saving subsystem had to be rewritten, for example. That's terrifying to mess with, as you can imagine. Part of that challenge fell to the Frostbite team itself, which at that point was divided between Stockholm and Vancouver, and a lot of it was on the BioWare engine people, who did an amazing job just taking this immense codebase and making tools that they could use to make their game. Compartmentalizing Your Knowledge \u00b6 In a big AAA game engine, you can't see where things could have been done better just by reading the code because it has over two million lines. It's so much that you can't wrap your head around the entire thing. So you will discover what you're missing as you try to make things, and sometimes you won't have these realizations until the last second, and you discover that the key connection is missing and that brings us back to plumbing. Once you're six months into production, you have to make it work. Again, that's AAA-specific\u2014too big to fail. You can't know in advance, so you do as much pre-production, as much research, and as much talking with other teams as you can, but in the end you will just go in and see what breaks. A skill I've taken from my time in AAA to my time as a consultant is not needing to understand the entire codebase. When you're working with this huge, sprawling thing that breaks Intellisense 7 , you learn to navigate the code without understanding all of it. You learn to find the part you need to work on and only figure that out, and you accept that you will never understand the whole thing. Focusing on the smaller stuff has been a really precious skill as a consultant because I can just go in your codebase and find my way around in a week or two at most. Versioning an Engine \u00b6 Whether you decide to do versioning inside of your engine or write a tool to change old data really depends on your data and on your users. Can you afford to break retro compatibility 8 ? How annoyed do you get at an API that deprecates something you're relying on? Something like Unity, for instance, is heavily bogged down by having to keep old projects running. Can they afford to break old projects, though? Probably not. If you're in AAA and you've got a dedicated team that has to take the new engine version all at the same time and will have months of work put into this upgrade, you can afford to break old stuff (just don't do it without telling people, that's not very nice). Can the conversion be done automatically? Sometimes yes, sometimes no\u2014it depends. Will Perforce's automatic merging be able to handle changes? Possibly, possibly not. I had the case of a for loop being merged with a while loop so the counter was still being incremented at the end of it, and we only had half the UI rendering. When you are merging tens of thousands of files, you are going to miss that. This is why you have senior engineers who can guide you on good choices to make, thanks to their experience and instinct. You will have tech directors who call the shots telling you what they're going for, keeping all the trade-offs in mind. Producers also help with this by keeping you on schedule, because in the end you have to ship. Parallelism and Data-Oriented Design \u00b6 While working on parallelizing Stingray's data compilation, the biggest challenge for me personally was one very high-profile SDK not being thread-safe 9 , meaning that you can only call the \"compile asset\" function in a single thread; otherwise, the engine breaks down. That's the problem when you're parallelizing: if you do it from the start, then you can make sure that everything you're writing is parallelization friendly, but if you're working with existing code and doing plumbing you're going to find all the ways in which it cracks and breaks. Does that happen because it's using global variables in such a way that it's not thread-safe at all? Is it some other complex C++ nonsense that makes it thread-unsafe? Parallelization itself isn't hard per se \u2014though it's tricky to do well\u2014it's mostly a challenge if you're using existing code that's not thread-safe, parallelization ready, and so on. Data-oriented design can really help with that problem, because when you've got tight data, it's a lot easier to split up the work. But if you're not doing that, how do you handle things like std::iterator s 10 ? Exposing Data to the Developers \u00b6 Knowing how to manage memory allocation between subsystems is something that comes from testing, and exposing your engine's settings to data so the users can change it. Depending on the game's profile, there will be a lighter or heavier load on different subsystems. So it has to be configured by the user. An example would be if you've got a game that's very effects heavy, your particle system would probably need more memory than it would in a 2D puzzle game that doesn't have as many particle effects. If you've got a game that's only 2D, your UI system will probably take a heavier load than the 3D system that you're not using at all. When it comes to developers abusing that exposed data, that's where the problem of whether your user is an expert or a neophyte 11 comes in again. Is it something that you can put in documentation, or something that should be impossible to abuse, and cause a crash if used wrong? Is it something that should just be used with caution? Is it a warning, is it an error, etc. When you're dealing with games in particular, you will generally have users who are focused on shipping and getting the product out the door the way they want it. That's why I am a strong advocate for engine source access as well, because when you're doing this last push three-six months before finishing your game, you don't care about keeping it clean anymore; you just want to get the thing done so the game can be as good as you want it and hopefully release on time. Going back to what it means to abuse an engine, should we assume that as engine developers, we always know best or that in the end it's our user, the game developer, who should get to make those calls? I still don't have the answer to this. It's All Plumbing \u00b6 For a project like the Isetta Engine, aiming at showing what game engine development is like, it's very good to put the engine together from existing code. Because that's also true in the professional world: you very rarely get to make features from scratch. It's all plumbing. Interview conducted September 27. 2018. BitSquid , more modernly known as Autodesk Stingray , is a discontinued game engine from Stockholm, Sweden. End of sale was announced for January 7, 2018, and afterward it became a plugin for Autodesk 3DS Max known as 3DS Max Interactive. \u21a9 IMGUI stands for immediate mode GUI which is a code-driven GUI system where on each rendering frame the application needs to issue the draw commands of the GUI (the GUI is not stored for multiple frames) \u21a9 Scaleform is a vector graphics rendering engine used to display Adobe Flash-based user interfaces and HUDs for video games. \u21a9 Retained GUI also known as canvas/scene graph, is where GUI is registered once and is displayed, \"retained\", on screen until it removes itself from rendering. \u21a9 Frostbite is EA's proprietary game engine used across most of their studios. \u21a9 HUD stands for Heads-Up-Display. It usually refers to overlay on the screen that presents important information to the player \u21a9 Intellisense is an intelligent code completion feature in Microsoft Visual Studio that is capable of detailing information about the code and objects that the programmer is working with while coding. \u21a9 Retro compatibility, also known as backward compatibility, is when a system is setup such that it works with legacy code/input. \u21a9 Thread-safe code only manipulates shared data structures in a manner that ensures that all threads behave properly and fulfill their design specifications without unintended interaction. \u21a9 std::iterator is a C++ type that can be used to iterate through collections of elements based on that collection. \u21a9 A neophyte is a novice to some field, skill, or practice. \u21a9","title":"Interview"},{"location":"interviews/AmandineCoget-interview/#engine-programming-is-all-plumbing","text":"Amandine Coget is a French game engine plumber living in Stockholm, freelancing after having worked on the Frostbite and Bitsquid engines. Other interests include crafts, politics, and the politics of crafts. (The following is the edited transcription of a conversation we had with Amandine Coget.)","title":"Engine Programming is All Plumbing"},{"location":"interviews/AmandineCoget-interview/#introduction","text":"You can probably guess from the name that I was born and raised in France. I also studied there; the degrees I took were programming oriented but not game-specific. It was very different from a Bachelor's in Computer Science in that half of my studies were computer related (what French calls \"Informatique\") but the other half was mathematics, economics, communication, and accounting. That has been extremely useful in my work as a consultant. Ironically, my most useful class has been accounting. Career-wise, I spent three years at DICE working on Battlefield 4 and the Frostbite Engine. After that I spent six months at BitSquid 1 , and have been freelancing since then. As for my hobbies, there's weaving which I'm trying to take beyond hobby these days, there's making my own yarn, and there's a bit of glassblowing and ceramics.","title":"Introduction"},{"location":"interviews/AmandineCoget-interview/#the-modes-and-pipeline-of-an-engine","text":"When I was starting out in engine development, the most challenging aspect for me was how much of it is actually related to the pipeline. With small projects, you generally have the data directly in files, like images and such, and then you've got the game executable which just loads everything. As soon as you've got big projects, though, you will have data that gets processed by a pipeline and then used by the game. So typically you will have a game executable that can run in several modes: it can run as a tool, or it can run as a game. That was initially tricky to wrap my head around: that you had one executable running in different modes and that the code you wrote needed to work in every one of those modes. That's something you don't encounter on small projects, because all you need to do there is something as simple as loading the PNGs. For example, if you're working on mobile, you're going to want compressed file formats, and you can do that manually by running a standalone tool, or you can have an integrated game pipeline that just processes all the data into compressed formats efficiently. That's the part that is rarely taught because, again, pipelines are only relevant for large-scale projects. I would argue the pipeline is always part of the engine because it is something that's so tightly connected to your runtime\u2014the same team will usually be working on it. The pipeline could be one standard executable that's compiled with different parameters, or you could pass different command line parameters to your game, but the basic idea is that you don't just have a \"single\" exe file. You've got those different modes and configurations. It also ties into multi-platform development because when you have so many paths, it takes a mindset shift.","title":"The Modes and Pipeline of an Engine"},{"location":"interviews/AmandineCoget-interview/#onboarding-for-programmers","text":"In my experience of onboarding, a specific point of frustration is the lack of up-to-date, complete documentation. This is especially the case for programmers. We usually have resources for content creators because there's a lot more churn with content creators. Artists, arguably designers, and scripters are the people at a company that change a lot between and during projects. I mean, you know by now that the game industry feeds on blood. As such, programmer onboarding is rarely as developed even though it requires just as much explanation. Programmers new to a codebase need documentation highlighting the company's established process, and time to find their way around the code. I remember when a new co-worker joined my team on Battlefield 4 and was just starting on her first feature. She asked something, so I directed her to someone I thought knew the answer, who directed her to someone else\u2014I think there were like four or five people in the chain\u2014and eventually, the question circled back around to me. You've got this problem of knowledge tracking and documentation; figuring out where the knowledge lives. In the end, knowledge management has been the hardest part of onboarding for me. While other industries have entire teams working on compiling knowledge, the games industry typically doesn't. I think that along with the high rate of turnover where we lose people and therefore knowledge, not having proper processes in place or rarely having tech writers will hurt us. To remedy this issue, it helps to follow good, readable code practices. Comments and documentation can help, but then they have to be kept up to date. As for new programmers being dropped into this scenario, the big thing is resisting the urge to say \"this is crap\" early on. The urge to criticize will always be there, and the criticism is often valid. Three months later, though, you get the context and the history, and you'll understand why things are done that way. In fact, when working on the Customization screen for Battlefield 4 , which was my big system rewrite, it was a mess but the deeper I went and the more I re-implemented, the more I came to understand why the previous decisions were made. Give up on the idea of writing perfect code\u2014it will not happen, I'm sorry to tell you. And here is another thing you may not want to hear, but in my experience, after the onboarding, a solid 98% of an engine programmer's job is plumbing and legacy management. Even if you do write new code, a lot of it will be interfacing with the old. I think only experience can teach you how to best modify old systems, because as you build up experience, you build up what I like to call the list of bullshit factoids. Just small points about how your actions can backfire, or certain quirks an API has. As you build up this list of factoids, you build an instinct. While I have six years of experience under my belt now, I'm well aware the people who have been doing this for 15 or 20 years will have a much more refined instinct. However, they might also be bogged down by old factoids that aren't relevant anymore. That's why it's good to have a team that's a blend of ages, experience levels, and backgrounds, because you can have different instincts that can work together.","title":"Onboarding for Programmers"},{"location":"interviews/AmandineCoget-interview/#thinking-about-usability","text":"Going from working on UI programming to graphics to core systems, the biggest thing I've noticed is that it's become impossible for me to ignore usability. In a way, UI is all the way at the end of the slope, because you will get feature requests from the gameplay team and your work comes at the end of the process. You're the last one to come in when shit rolls downhill, so you will run into every corner that was cut every step of the way. A part of that work is fetching data from the gameplay code, so if corners have been cut in gameplay code, your UI won't work. When the gameplay team comes in with last-minute changes, meaning the UI has to be changed, it starts getting messy really quickly. This reinforces the focus on users, talking to your users, and thinking about usability in general. It's not as common for programmers on a standalone engine team to talk with designers. But I've kept the a habit, even for engine features, to talk with my users a lot. When I was on the rendering team, for instance, I needed to understand if I was on the right track with what was wanted of a feature. Since I was in the DICE offices, I just grabbed an artist, sat them down in front of my computer, and asked them to try to and use what I'd built. This plan really worked because they immediately tried to do something I hadn't thought about! Because of that experience, I now regularly ask others if my tools are confusing to use: even if it looked great on paper, what's the interface like to the user? Your user can be a programmer as well, so you can apply this same UX mindset when designing APIs. I spend a lot of my time \"fighting for the users\". I absolutely think a large-ish engine team should always have a dedicated UX designer on hand because it really is a skill. As programmers, we have the nasty tendency to think that we can just figure it out and come up with a decent enough interface on our own. But there are people who study usability for years and know what they're doing.","title":"Thinking about Usability"},{"location":"interviews/AmandineCoget-interview/#gui-immediate-vs-retained-modes","text":"When it comes to what form of GUI to use, as always it depends. I haven't followed how Casey 's take on IMGUI 2 (IM) has evolved over the last few years. For context, while I was working on Battlefield 4 , what we were doing was shifting the entire UI from using Scaleform 3 , which is heavily retained 4 , over to something that was more immediate mode and in C++. We rewrote every last bit of UI in the game basically and what we discovered more and more is where IM worked really well and where it broke down. In my opinion, IM is great for simple things, so for debug interfaces, small tools, or demos, it's a no-brainer. As soon as you need some degree of persistence, like with data-driven systems or allowing designers and artists to customize UI, you will need some kind of extra layer. And that can be something that gets baked down by the pipeline into something that's more immediate, or it can be something that actually exists at runtime. But at scale, I find IM code just gets really messy when you're fetching data from all over the place. This is especially the case if you're interfacing with a visual scripting system like Frostbite 5 has, which we also call \"noodles\". You need some kind of persistent entity just so you can drag one of those noodles into it, and while that gives a lot of power to artists and designers, it gives a lot of headaches to programmers. Noodles are code; they need to be debugged like code, and it turns out that artists are good at making horrendously complicated things really fast. For a project with the scale of the Isetta Engine, where you have only three months, IM is the right choice. For a large AAA project, though, I wouldn't recommend it as the only approach. If you can pair IM and retained modes cleanly when designing your system, then it is absolutely a good idea to have both. Battlefield 4 did a little bit of that because the HUD 6 was very immediate mode-ish and so it had to be fast. My main work on Battlefield 4 was the customization screen, which is tons of small widgets controlled by a ton of data. Working on that screen taught me the biggest challenge with UI is fetching all that data. In that case, having persistent entities that just fetch the stuff once and hold on to it can help set up a nice architecture, but it depends on your particular project. Rendering UI can also be challenging because you're doing layered 2D rendering with a lot of transparency, which modern renderers don't like. And with UI there will often be more string manipulation than you would like, so you need to optimize that.","title":"GUI: Immediate vs Retained Modes"},{"location":"interviews/AmandineCoget-interview/#a-brief-history-of-the-frostbite-engine","text":"While Frostbite is known today for accommodating many game genres, it wasn't designed to do so early on. It had some growing pains. Each new game in Frostbite showed some shortcomings or some things that were missing, and so the engine evolved organically. That shows in the codebase, but I don't want to harp on Frostbite specifically, what can you do when you've got two million lines of code supporting that many games and features? For a bit of history, Frostbite was originally the Battlefield engine, made by DICE in Stockholm. Medal of Honor: Warfighter, which we like to forget about, was made on Frostbite as well. Because that game was also a FPS, it was very close to Battlefield . The next game to use the engine was Army of Two: The Devil's Cartel , which we also like to forget about, which brought third-person co-op. The Frostbite team needed to figure out how to support that gameplay style and what updates needed to be made to the engine as a result. Need for Speed: The Run was the first racing game to use the engine. It brought a lot to the engine tech-wise, because it showed where things cracked. For instance the Need for Speed team made a bunch of things for The Run , like a road making tool and handling a car's speed. The vehicles in Battlefield are done in a certain way, but cars in Need for Speed move differently, which affects how to stream in new assets. So each game brought new problems. Dragon Age: Inquisition was a massive challenge, because you're going from an engine designed for FPS titles to a super-massive RPG with conversation systems and more. What I know is that the entire serialization system and the entire saving subsystem had to be rewritten, for example. That's terrifying to mess with, as you can imagine. Part of that challenge fell to the Frostbite team itself, which at that point was divided between Stockholm and Vancouver, and a lot of it was on the BioWare engine people, who did an amazing job just taking this immense codebase and making tools that they could use to make their game.","title":"A Brief History of the Frostbite Engine"},{"location":"interviews/AmandineCoget-interview/#compartmentalizing-your-knowledge","text":"In a big AAA game engine, you can't see where things could have been done better just by reading the code because it has over two million lines. It's so much that you can't wrap your head around the entire thing. So you will discover what you're missing as you try to make things, and sometimes you won't have these realizations until the last second, and you discover that the key connection is missing and that brings us back to plumbing. Once you're six months into production, you have to make it work. Again, that's AAA-specific\u2014too big to fail. You can't know in advance, so you do as much pre-production, as much research, and as much talking with other teams as you can, but in the end you will just go in and see what breaks. A skill I've taken from my time in AAA to my time as a consultant is not needing to understand the entire codebase. When you're working with this huge, sprawling thing that breaks Intellisense 7 , you learn to navigate the code without understanding all of it. You learn to find the part you need to work on and only figure that out, and you accept that you will never understand the whole thing. Focusing on the smaller stuff has been a really precious skill as a consultant because I can just go in your codebase and find my way around in a week or two at most.","title":"Compartmentalizing Your Knowledge"},{"location":"interviews/AmandineCoget-interview/#versioning-an-engine","text":"Whether you decide to do versioning inside of your engine or write a tool to change old data really depends on your data and on your users. Can you afford to break retro compatibility 8 ? How annoyed do you get at an API that deprecates something you're relying on? Something like Unity, for instance, is heavily bogged down by having to keep old projects running. Can they afford to break old projects, though? Probably not. If you're in AAA and you've got a dedicated team that has to take the new engine version all at the same time and will have months of work put into this upgrade, you can afford to break old stuff (just don't do it without telling people, that's not very nice). Can the conversion be done automatically? Sometimes yes, sometimes no\u2014it depends. Will Perforce's automatic merging be able to handle changes? Possibly, possibly not. I had the case of a for loop being merged with a while loop so the counter was still being incremented at the end of it, and we only had half the UI rendering. When you are merging tens of thousands of files, you are going to miss that. This is why you have senior engineers who can guide you on good choices to make, thanks to their experience and instinct. You will have tech directors who call the shots telling you what they're going for, keeping all the trade-offs in mind. Producers also help with this by keeping you on schedule, because in the end you have to ship.","title":"Versioning an Engine"},{"location":"interviews/AmandineCoget-interview/#parallelism-and-data-oriented-design","text":"While working on parallelizing Stingray's data compilation, the biggest challenge for me personally was one very high-profile SDK not being thread-safe 9 , meaning that you can only call the \"compile asset\" function in a single thread; otherwise, the engine breaks down. That's the problem when you're parallelizing: if you do it from the start, then you can make sure that everything you're writing is parallelization friendly, but if you're working with existing code and doing plumbing you're going to find all the ways in which it cracks and breaks. Does that happen because it's using global variables in such a way that it's not thread-safe at all? Is it some other complex C++ nonsense that makes it thread-unsafe? Parallelization itself isn't hard per se \u2014though it's tricky to do well\u2014it's mostly a challenge if you're using existing code that's not thread-safe, parallelization ready, and so on. Data-oriented design can really help with that problem, because when you've got tight data, it's a lot easier to split up the work. But if you're not doing that, how do you handle things like std::iterator s 10 ?","title":"Parallelism and Data-Oriented Design"},{"location":"interviews/AmandineCoget-interview/#exposing-data-to-the-developers","text":"Knowing how to manage memory allocation between subsystems is something that comes from testing, and exposing your engine's settings to data so the users can change it. Depending on the game's profile, there will be a lighter or heavier load on different subsystems. So it has to be configured by the user. An example would be if you've got a game that's very effects heavy, your particle system would probably need more memory than it would in a 2D puzzle game that doesn't have as many particle effects. If you've got a game that's only 2D, your UI system will probably take a heavier load than the 3D system that you're not using at all. When it comes to developers abusing that exposed data, that's where the problem of whether your user is an expert or a neophyte 11 comes in again. Is it something that you can put in documentation, or something that should be impossible to abuse, and cause a crash if used wrong? Is it something that should just be used with caution? Is it a warning, is it an error, etc. When you're dealing with games in particular, you will generally have users who are focused on shipping and getting the product out the door the way they want it. That's why I am a strong advocate for engine source access as well, because when you're doing this last push three-six months before finishing your game, you don't care about keeping it clean anymore; you just want to get the thing done so the game can be as good as you want it and hopefully release on time. Going back to what it means to abuse an engine, should we assume that as engine developers, we always know best or that in the end it's our user, the game developer, who should get to make those calls? I still don't have the answer to this.","title":"Exposing Data to the Developers"},{"location":"interviews/AmandineCoget-interview/#its-all-plumbing","text":"For a project like the Isetta Engine, aiming at showing what game engine development is like, it's very good to put the engine together from existing code. Because that's also true in the professional world: you very rarely get to make features from scratch. It's all plumbing. Interview conducted September 27. 2018. BitSquid , more modernly known as Autodesk Stingray , is a discontinued game engine from Stockholm, Sweden. End of sale was announced for January 7, 2018, and afterward it became a plugin for Autodesk 3DS Max known as 3DS Max Interactive. \u21a9 IMGUI stands for immediate mode GUI which is a code-driven GUI system where on each rendering frame the application needs to issue the draw commands of the GUI (the GUI is not stored for multiple frames) \u21a9 Scaleform is a vector graphics rendering engine used to display Adobe Flash-based user interfaces and HUDs for video games. \u21a9 Retained GUI also known as canvas/scene graph, is where GUI is registered once and is displayed, \"retained\", on screen until it removes itself from rendering. \u21a9 Frostbite is EA's proprietary game engine used across most of their studios. \u21a9 HUD stands for Heads-Up-Display. It usually refers to overlay on the screen that presents important information to the player \u21a9 Intellisense is an intelligent code completion feature in Microsoft Visual Studio that is capable of detailing information about the code and objects that the programmer is working with while coding. \u21a9 Retro compatibility, also known as backward compatibility, is when a system is setup such that it works with legacy code/input. \u21a9 Thread-safe code only manipulates shared data structures in a manner that ensures that all threads behave properly and fulfill their design specifications without unintended interaction. \u21a9 std::iterator is a C++ type that can be used to iterate through collections of elements based on that collection. \u21a9 A neophyte is a novice to some field, skill, or practice. \u21a9","title":"It's All Plumbing"},{"location":"interviews/AmandineCoget-video/","text":"Realizing Your Team's Vision \u00b6","title":"Video Story"},{"location":"interviews/AmandineCoget-video/#realizing-your-teams-vision","text":"","title":"Realizing Your Team's Vision"},{"location":"interviews/AmitRob-advice/","text":"Amit is maintaining a super helpful website called Red Blob Games , where he explains math and computer science topics in an accessible and playful way. Rob Shillingsburg earned his Ph.D. from Princeton University and was an early employee of Google. He later co-founded Wild Shadow Studios, maker of 2012 IGF finalist Realm of the Mad God . We approached Amit because he has extensive experience in producing excellent blog posts on game dev related topics (again, they are really great!). We were able to meet up during GDC 2018 and Rob joined the meeting, too. Advice ( not verbatim): \u00b6 On getting started Get something on the screen as soon as possible. Make something as simple as a blank screen, or a triangle. Then you can start off from there. It's a great way to keep you motivated. Also, always have something that runs. Try to make a \"toy game\" in 2 weeks, as you will figure out what problems you might encounter when making the engine. It's a great way for you to know your \"unknown unknowns.\" Keep everything as simple as possible. List out your features as a tree and do Breadth-First implementation on them. It's hard to plan everything upfront. If you do, keep your plans adaptive. On networking Different games need different netcode. For example, the RTS genre has very different requirements on networking than the FPS genre. General Most game engines come from actual games. People only make them when they find that a lot of things are common among different games and make them by pulling those parts out and reuse them. So, maybe consider making multiple games throughout your semester.","title":"Amit Patel & Rob Shillingsburg"},{"location":"interviews/AmitRob-advice/#advice-not-verbatim","text":"On getting started Get something on the screen as soon as possible. Make something as simple as a blank screen, or a triangle. Then you can start off from there. It's a great way to keep you motivated. Also, always have something that runs. Try to make a \"toy game\" in 2 weeks, as you will figure out what problems you might encounter when making the engine. It's a great way for you to know your \"unknown unknowns.\" Keep everything as simple as possible. List out your features as a tree and do Breadth-First implementation on them. It's hard to plan everything upfront. If you do, keep your plans adaptive. On networking Different games need different netcode. For example, the RTS genre has very different requirements on networking than the FPS genre. General Most game engines come from actual games. People only make them when they find that a lot of things are common among different games and make them by pulling those parts out and reuse them. So, maybe consider making multiple games throughout your semester.","title":"Advice (not verbatim):"},{"location":"interviews/ArasPranckevicius-interview/","text":"Growing Pains in Engine Development \u00b6 Aras Pranckevi\u010dius is a Lithuanian programmer who has been working on the Unity game engine since 2006. Before that, he worked on some demoscene demos and small games that you have never heard about. (The following is the edited transcription of a conversation we had with Aras Pranckevi\u010dius.) Experts Are Human Too \u00b6 When I first started programming, I was mostly just toying around with computer graphics. What made learning engine programming difficult back then was that the Internet was still in its infancy. Even Google didn't exist yet. My school had limited resources as well, so my only real option was books, which were all written in Russian at the time. The hardest thing for me to learn specifically was how to program on a team. When other people are thrown into the mix, all of a sudden everything changes. You no longer do everything by yourself and you have to somehow collaborate. As a complete introvert, communicating with people was one of the biggest challenges I faced. Learning how to work in a team is a super-valuable skill that the universities I've seen don't talk about a lot, or maybe not at all. As you work on your shared code base and your engine ideas that you have, teamwork is extremely useful. It is hard in some places, but at the end of the day there's only so much one programmer can do. Also, in many of my blog posts , like my recent path tracer 1 series of blogs , I write about topics where I still do not completely understand them\u2014even as a professional with roughly 15 years of experience. People have told me that admitting I have no idea what I'm doing is a refreshing thing to see, because there's this expectation that industry veterans understand everything. Since we're all human, that's obviously not the case! Dingus: An Engine to Forget \u00b6 Back when I made things for the demoscene 2 , I worked on a game engine called Dingus with a few others. I don't think the engine had any special architecture or technology; it was just a bunch of code that we found useful. Back in around 2002 or 2003, the only engines to come with actual tools were RenderWare 3 and Unreal Engine 2. Engines at the time didn't come out of the box with any decent tools. An engine was basically just a bunch of code, and there was nothing in there for artists. So our \"engine\" was just a bunch of C++ code that we used in our demos, and the only tools we had were mesh exporters from 3ds Max 4 . These days, you just export any FBX 5 or glTF 6 file and there are ready-to-make libraries to read that, which was not the case back then. To be fair, I don't think I had any particularly clever insights when writing Dingus that would help me in the future. What it helped me with in my career, though, is that it was the main reason why I got hired at Unity! The Unity founders told me the reason why I got hired was because I was writing some some messages to a mailing list about physics engines. In particular, it was about a physics engine called the Open Dynamics Engine 7 , which Unity used before it moved on to PhysX. The Unity founders were reading this mailing list, and they saw my messages. I guess they thought I was not I was not totally stupid, because I ended up getting the job! It also helped immensely that I had my own website with a blog and tech demos. That said, I don't think the actual C++ code I was writing for Dingus back then was useful in the end. If you are looking to get hired today as an engine programmer, I think making content like blog posts and videos about your work will be more useful than your actual code. Reflecting on Windows Unity Editor and Graphics Abstraction \u00b6 Although we had to shuffle a lot of code around to make the Unity editor for Windows, there weren't many decisions that I made that I regret. Unity started as Mac-only software in 2004 or 2005. Back then, actually, Macs were not the hip thing they've become in the last 10 years; this was before iPhone existed. At the time, almost nobody had a Mac. Despite this, for some reason, Unity started on a Mac and remained that way for a long time. When the team realized the majority of game developers are not on Mac, we knew we needed to make a Windows version. This was a huge undertaking because Unity's editor tools were written with a lot of Mac-only assumptions. It was essentially Cocoa 8 for the UI and the various UNIX 9 assumptions about files. The most painful aspect of porting to Windows was the asset pipeline in particular. On a Mac, for example, there's no such thing as an application having exclusive access to a file. If some process is reading a file, you can virtually delete it, and then when that process goes away, the file actually gets deleted. On Windows, if a file is being used, another process cannot just go and delete it. Stuff like that was probably the most annoying to get through, as well as the differences between how Cocoa and Windows UI. There were definitely some decisions I made back then that I regret. Since Unity started on a Mac, the engine had been OpenGL 11 only. Neither Metal 12 nor any other alternatives existed at the time, so OpenGL was our only option. From there, we started to add Direct3D 10 9 (DX9) support, and so we made a little abstraction layer for the graphics API. Since we were doing this in around 2006, shaders already existed but more complex elements\u2014like the concept of compute shaders 13 \u2014didn't exist at all. Our abstraction layer for the graphics looked like a very DX9-style API, which we later modified when adding Direct3D 11 (DX11) and PlayStation 3 compatibility. It stayed in this sort of legacy DX9/DX11-style API for a very long time because didn't do enough internal refactoring. Right now, a bunch of people at Unity are doing that, but, for example, getting DX12 working with this DX11-style abstraction was very painful. There probably isn't a good way to abstract out a single system. You don't know what or how to abstract until you have two different ways that you need to do some particular systems. Or, in terms of graphics, until you get two or three graphics API's that you need to abstract. For example, our abstraction for DX9 at the time was okay, but we neglected to keep modernizing. Currently we are doing that, but we were several years too late that made catching up a very painful process. Modifying engine architecture on software as big as Unity is painful. Some aspects of it are insanely hard to change. I saw a tweet many months ago that said: Library design is this: You have made a mistake. It is too late to fix it. There is production code depending on the mistake working exactly the way the mistake works. You will never be able to fix it. You will never be able to fix anything. You wrote this code nine seconds ago. It's not exactly true that you will never be able to change it, but some of the decisions that we made in the engine are near-impossible to change without adding a parallel system that, for some amount of time, lives right next to the old one, that it has to replace. Hopefully the new system is better than the old one and people move toward the new one, and then maybe, eventually, you can remove the old one. As we speak, I think some of the components in Unity that are used for the in-game UI system are from three UI-system generations in the past. Right now, we have in-game UI, and then before that we had something like IMGUI-based 14 in-game UI, and before that there was the GUI text component from Unity 1.0. I think we are removing those right now, so it only took about 12 years ! API Responsibility \u00b6 How much the API protects the developer depends on who your target audience is. At Unity, we have exactly this problem; a large percentage of Unity users are not very experienced developers, so they need something that's easy and robust. At the same time, we want to have content that pushes the limits of hardware to serve people who actually know what they're doing. For the experienced programmers, the API and the system shouldn't get in their way. Admittedly, we are not always great at this. The best approach I've seen overall is where you have two levels of API's. One would be a low-level API that is super efficient, super explicit, and doesn't protect you from anything. An example of this would be Vulkan 15 , or DX12, both of which you have to be an expert to use. In addition to the low-level API, you should have another API that's easy to use, even if it's not 100% efficient. For the other 6 billion of us, it gets the job done. We're trying to bring this low-level API and high-level API split into our systems at Unity, but we're not quite there yet. If you're making your first engine as a learning project, you don't need to worry about this, but it could be valuable for your next engine. There are some ways to handle adding new features to an engine without breaking production code, one of which is having a package manager 16 . We just shipped the Unity with package support a year ago, so not everyone in the Unity ecosystem has moved to that yet. We'll see how that goes. Conceptually, package management is different from engine versioning in that you could upgrade parts of the engine that you care about. For example, if there's a new feature in the physics system that you really really want, but you want to stay on the current rendering engine, you could just upgrade the physics system. In a perfect world, physics would be a separate package without dependencies to anything and you could just mix and match. In that case, you could upgrade physics and keep everything else the same\u2014whether that will actually work in practice, we'll see. Right now, there's a whole bunch of the Unity engine that is not put into packages yet. To some extent, we want to get everything packaged. Getting modules to talk to each other, however, is not a trivial problem. I think the only way to design interfaces that's actually stood the test of time was not to design them in terms of function calls or classes, but to design them in terms of data formats. If you get a TGA image file, which has been around for 30 years in its simplest form, it's just specifications of how the data is layered. And then, you basically don't care how the TGA writer/reader is implemented. I think one way of making systems communicate is to define the data protocol between them. They could communicate through a shared memory space or a socket 17 , and then you don't care whether the class changes that is writing the data or if they're even the same language. However, there's a bunch of current functionality that we don't plan to package. That's especially true for the systems that we are about to replace. For example, we made the Scriptable Render Pipeline (SRP), where users can write their own graphics pipeline in a high-level C# API. We don't intend to put our legacy pipelines into separate packages anytime soon, because we expect that people will move on to the new pipelines anyway. Our approach with the SRP was to make the actual API where you express your rendering pipeline high-level enough that you never operate on a single object. Instead, you operate on sets of objects. For example, there could be an API call that does culling. It doesn't return a list of visible objects; it returns some handle to the whole set of visible objects that you can only do partial queries on. With that, you could search for everything that's visible or everything that's opaque, but you don't have to iterate over each object and do decisions on each of those. Conceptually, the API works on sets of objects, sets of lights, etc. We also weren't exactly making a switch to a less efficient or tightly-pathed system. As our previous non-malleable rendering pipeline grew over ten or twelve years, it gained so many hidden decisions and branches to handle various feature interactions that it was no longer super tight and efficient. It was now being used to handle various corner cases that only happen in rare cases. From Graphics to Plumbing \u00b6 Recently in my career at Unity, I changed roles to be a dev tools engineer. Switching from graphics to build tools didn't impact my perception of game development very much, and I think that's because I was already dabbling in non-graphics work during Unity's early days. Even during my time in graphics, once in a while I would be doing something else, so I already had a good overview of various systems outside of graphics. What I didn't fully understand was the differences in machine configurations when people build code. You would expect everyone who uses Windows gets the Windows version of software. It turns out, though, there are about ten people who are on Windows but run everything from a Cygwin 18 shell, and the Cygwin shell pretends it's Linux. Some people check out their source code into a folder that is over 100 characters long, and Windows, to this day, still has a maximum path length of 260 characters. Someone else might have a Windows that's localized in French, so all their error messages are in French, which means your tools cannot parse the error messages and expect something understandable. Conceptually you might understand these various exceptions, but you don't realize how much of a hassle all that is until you have to deal with that . My advice would be to get your team onto a setup that is as uniform as possible. Not having to worry about tech differences makes things so much easier. I watched part of the Isetta team's talk with Casey Muratori , and in one part he brought up the question of who \"build engineers\" are. Actually, that's what I am! I guess Casey comes from a different setup, because at Unity we have five hundred engineers writing code on the same codebase. Now, you could argue whether that's a good thing or a bad thing, or whether you should have five hundred engineers in the first place. With that many engineers at work, I think having at least two or three build engineers actually helps; whatever you can do to make your programmers' lives more productive or less annoying is a useful thing. Part of that work has been profiling our code, which can be very useful. However, if you add profiling capture that no one will ever look or do something about it, it's kind of pointless, right? What I have noticed, and especially in the in the build area, is that you have to make profile information really visible. For example, right now in Unity's build system, what we do is that as each C++ file is being compiled, it prints the time it took right to next to the file name. The numbers are nicely aligned, too, so that if it takes two digits of seconds, it's easy to spot. Just adding that was a super easy thing to do, and that makes people who wouldn't ordinarily pay attention to build times take notice and address the problems they find. Modularity is the Future, Maybe? \u00b6 I don't know what the future holds for game engines. Looking back to when we were just getting started with Unity, I remember some people thought it was a stupid idea because nothing we could make would be able to compete with Unreal. That is still true to an extent in the AAA game space, where aside from in-house engines, Unreal Engine 4 is the strongest option today. Knowing this, we tried for a long time not to compete with Unreal Engine; we positioned ourselves as an indie web/mobile engine (for better or worse). I think if we were to compete with Unreal from Day 1, we wouldn't have survived as a company. That said, I see the future of game engines being more modular. That's what we are currently trying to do with Unity itself in terms of packages and modules, but the risk is going overboard with modularity. For everything that's good about JavaScript npm 19 , they sometimes go too crazy; they have a single line of code becoming its own module. As Unity and Unreal are currently these big, monolithic pieces of tools and functionality, they aren't inherently versatile enough for different game genres. While they both have some malleability, you still have to deal with gigabytes of extra stuff. I think something more modular will happen in the future \u2013 I can hope that will be Unity, but we'll see. Interview conducted on October 8, 2018. Path tracing is a realistic lighting algorithm that simulates light bouncing around a scene. It uses the Monte Carlo method to give a faithful rendition of the global illumination of the scene. \u21a9 The demoscene is an international computer art subculture focused on producing demos, which are self-contained, sometimes extremely small, audio-visual computer programs. \u21a9 RenderWare is a game engine by Criterion Software that launched in 1993 and continued to regularly support games through 2010. It was known for providing an off-the-shelf solution to the difficulties of PS2 graphics programming. \u21a9 Autodesk 3ds Max , formerly 3D Studio and 3D Studio Max, is a professional 3D computer graphics program for making 3D animations, models, games, and images. \u21a9 FBX is a proprietary file format owned by Autodesk that is mostly commonly used for 3D model and animation data within the games industry. \u21a9 glTF (or GL Transmission Format) is a royalty-free file format for 3D scenes and models using the JSON standard. \u21a9 Open Dynamics Engine , or ODE , is a free and open source physics engine written in C/C++ that can do both rigid body dynamics simulation and collision detection. \u21a9 Cocoa is Apple's native object-oriented API for macOS. \u21a9 UNIX is a family of multitasking, multiuser operating systems that derive from the original AT&T Unix, originally developed at Ken Thompson, Dennis Ritchie, and others at Bell Labs. Its main comparable is Microsoft's DOS, which is mono-task and mono-user. \u21a9 Direct3D is a graphics API within Microsoft DirectX used for creating windows and rendering, and serves similar purposes as OpenGL. \u21a9 OpenGL , short for Open Graphics Library, a cross-language, cross-platform application programming interface (API) for rendering 2D and 3D vector graphics. The API is typically used to interact with a graphics processing unit (GPU), to achieve hardware-accelerated rendering. It's the underlying rendering library for many modern game engines. \u21a9 Metal is a low-level, low-overhead hardware-accelerated 3D graphic and compute shader API developed by Apple and debuted in iOS 8. It combines functions similar to OpenGL and OpenCL under one API. \u21a9 A compute shader is a shader stage in the graphics rendering pipeline that is used entirely for computing arbitrary information. It is typically used for tasks unrelated to rendering. \u21a9 IMGUI stands for immediate mode GUI which is a code-driven GUI system where on each rendering frame the application needs to issue the draw commands of the GUI (the GUI is not stored for multiple frames) \u21a9 Vulkan is a low-overhead, cross-platform 3D graphics and compute API that targets high-performance real-time 3D graphics, such as video games. \u21a9 A package manager is a system which handles the installing, updating, configuring, and removing of a collection of software libraries. \u21a9 A socket is an internal endpoint for sending or receiving data within a node on a computer network. \u21a9 Cygwin is a Unix-like environment and command-line interface for Microsoft Windows. It provides native integration of Windows-based applications and resources with those of a Unix-like environment. \u21a9 npm is a package manager for the JavaScript programming language. It's the default package manager for the JavaScript runtime environment Node.js. \u21a9","title":"Interview"},{"location":"interviews/ArasPranckevicius-interview/#growing-pains-in-engine-development","text":"Aras Pranckevi\u010dius is a Lithuanian programmer who has been working on the Unity game engine since 2006. Before that, he worked on some demoscene demos and small games that you have never heard about. (The following is the edited transcription of a conversation we had with Aras Pranckevi\u010dius.)","title":"Growing Pains in Engine Development"},{"location":"interviews/ArasPranckevicius-interview/#experts-are-human-too","text":"When I first started programming, I was mostly just toying around with computer graphics. What made learning engine programming difficult back then was that the Internet was still in its infancy. Even Google didn't exist yet. My school had limited resources as well, so my only real option was books, which were all written in Russian at the time. The hardest thing for me to learn specifically was how to program on a team. When other people are thrown into the mix, all of a sudden everything changes. You no longer do everything by yourself and you have to somehow collaborate. As a complete introvert, communicating with people was one of the biggest challenges I faced. Learning how to work in a team is a super-valuable skill that the universities I've seen don't talk about a lot, or maybe not at all. As you work on your shared code base and your engine ideas that you have, teamwork is extremely useful. It is hard in some places, but at the end of the day there's only so much one programmer can do. Also, in many of my blog posts , like my recent path tracer 1 series of blogs , I write about topics where I still do not completely understand them\u2014even as a professional with roughly 15 years of experience. People have told me that admitting I have no idea what I'm doing is a refreshing thing to see, because there's this expectation that industry veterans understand everything. Since we're all human, that's obviously not the case!","title":"Experts Are Human Too"},{"location":"interviews/ArasPranckevicius-interview/#dingus-an-engine-to-forget","text":"Back when I made things for the demoscene 2 , I worked on a game engine called Dingus with a few others. I don't think the engine had any special architecture or technology; it was just a bunch of code that we found useful. Back in around 2002 or 2003, the only engines to come with actual tools were RenderWare 3 and Unreal Engine 2. Engines at the time didn't come out of the box with any decent tools. An engine was basically just a bunch of code, and there was nothing in there for artists. So our \"engine\" was just a bunch of C++ code that we used in our demos, and the only tools we had were mesh exporters from 3ds Max 4 . These days, you just export any FBX 5 or glTF 6 file and there are ready-to-make libraries to read that, which was not the case back then. To be fair, I don't think I had any particularly clever insights when writing Dingus that would help me in the future. What it helped me with in my career, though, is that it was the main reason why I got hired at Unity! The Unity founders told me the reason why I got hired was because I was writing some some messages to a mailing list about physics engines. In particular, it was about a physics engine called the Open Dynamics Engine 7 , which Unity used before it moved on to PhysX. The Unity founders were reading this mailing list, and they saw my messages. I guess they thought I was not I was not totally stupid, because I ended up getting the job! It also helped immensely that I had my own website with a blog and tech demos. That said, I don't think the actual C++ code I was writing for Dingus back then was useful in the end. If you are looking to get hired today as an engine programmer, I think making content like blog posts and videos about your work will be more useful than your actual code.","title":"Dingus: An Engine to Forget"},{"location":"interviews/ArasPranckevicius-interview/#reflecting-on-windows-unity-editor-and-graphics-abstraction","text":"Although we had to shuffle a lot of code around to make the Unity editor for Windows, there weren't many decisions that I made that I regret. Unity started as Mac-only software in 2004 or 2005. Back then, actually, Macs were not the hip thing they've become in the last 10 years; this was before iPhone existed. At the time, almost nobody had a Mac. Despite this, for some reason, Unity started on a Mac and remained that way for a long time. When the team realized the majority of game developers are not on Mac, we knew we needed to make a Windows version. This was a huge undertaking because Unity's editor tools were written with a lot of Mac-only assumptions. It was essentially Cocoa 8 for the UI and the various UNIX 9 assumptions about files. The most painful aspect of porting to Windows was the asset pipeline in particular. On a Mac, for example, there's no such thing as an application having exclusive access to a file. If some process is reading a file, you can virtually delete it, and then when that process goes away, the file actually gets deleted. On Windows, if a file is being used, another process cannot just go and delete it. Stuff like that was probably the most annoying to get through, as well as the differences between how Cocoa and Windows UI. There were definitely some decisions I made back then that I regret. Since Unity started on a Mac, the engine had been OpenGL 11 only. Neither Metal 12 nor any other alternatives existed at the time, so OpenGL was our only option. From there, we started to add Direct3D 10 9 (DX9) support, and so we made a little abstraction layer for the graphics API. Since we were doing this in around 2006, shaders already existed but more complex elements\u2014like the concept of compute shaders 13 \u2014didn't exist at all. Our abstraction layer for the graphics looked like a very DX9-style API, which we later modified when adding Direct3D 11 (DX11) and PlayStation 3 compatibility. It stayed in this sort of legacy DX9/DX11-style API for a very long time because didn't do enough internal refactoring. Right now, a bunch of people at Unity are doing that, but, for example, getting DX12 working with this DX11-style abstraction was very painful. There probably isn't a good way to abstract out a single system. You don't know what or how to abstract until you have two different ways that you need to do some particular systems. Or, in terms of graphics, until you get two or three graphics API's that you need to abstract. For example, our abstraction for DX9 at the time was okay, but we neglected to keep modernizing. Currently we are doing that, but we were several years too late that made catching up a very painful process. Modifying engine architecture on software as big as Unity is painful. Some aspects of it are insanely hard to change. I saw a tweet many months ago that said: Library design is this: You have made a mistake. It is too late to fix it. There is production code depending on the mistake working exactly the way the mistake works. You will never be able to fix it. You will never be able to fix anything. You wrote this code nine seconds ago. It's not exactly true that you will never be able to change it, but some of the decisions that we made in the engine are near-impossible to change without adding a parallel system that, for some amount of time, lives right next to the old one, that it has to replace. Hopefully the new system is better than the old one and people move toward the new one, and then maybe, eventually, you can remove the old one. As we speak, I think some of the components in Unity that are used for the in-game UI system are from three UI-system generations in the past. Right now, we have in-game UI, and then before that we had something like IMGUI-based 14 in-game UI, and before that there was the GUI text component from Unity 1.0. I think we are removing those right now, so it only took about 12 years !","title":"Reflecting on Windows Unity Editor and Graphics Abstraction"},{"location":"interviews/ArasPranckevicius-interview/#api-responsibility","text":"How much the API protects the developer depends on who your target audience is. At Unity, we have exactly this problem; a large percentage of Unity users are not very experienced developers, so they need something that's easy and robust. At the same time, we want to have content that pushes the limits of hardware to serve people who actually know what they're doing. For the experienced programmers, the API and the system shouldn't get in their way. Admittedly, we are not always great at this. The best approach I've seen overall is where you have two levels of API's. One would be a low-level API that is super efficient, super explicit, and doesn't protect you from anything. An example of this would be Vulkan 15 , or DX12, both of which you have to be an expert to use. In addition to the low-level API, you should have another API that's easy to use, even if it's not 100% efficient. For the other 6 billion of us, it gets the job done. We're trying to bring this low-level API and high-level API split into our systems at Unity, but we're not quite there yet. If you're making your first engine as a learning project, you don't need to worry about this, but it could be valuable for your next engine. There are some ways to handle adding new features to an engine without breaking production code, one of which is having a package manager 16 . We just shipped the Unity with package support a year ago, so not everyone in the Unity ecosystem has moved to that yet. We'll see how that goes. Conceptually, package management is different from engine versioning in that you could upgrade parts of the engine that you care about. For example, if there's a new feature in the physics system that you really really want, but you want to stay on the current rendering engine, you could just upgrade the physics system. In a perfect world, physics would be a separate package without dependencies to anything and you could just mix and match. In that case, you could upgrade physics and keep everything else the same\u2014whether that will actually work in practice, we'll see. Right now, there's a whole bunch of the Unity engine that is not put into packages yet. To some extent, we want to get everything packaged. Getting modules to talk to each other, however, is not a trivial problem. I think the only way to design interfaces that's actually stood the test of time was not to design them in terms of function calls or classes, but to design them in terms of data formats. If you get a TGA image file, which has been around for 30 years in its simplest form, it's just specifications of how the data is layered. And then, you basically don't care how the TGA writer/reader is implemented. I think one way of making systems communicate is to define the data protocol between them. They could communicate through a shared memory space or a socket 17 , and then you don't care whether the class changes that is writing the data or if they're even the same language. However, there's a bunch of current functionality that we don't plan to package. That's especially true for the systems that we are about to replace. For example, we made the Scriptable Render Pipeline (SRP), where users can write their own graphics pipeline in a high-level C# API. We don't intend to put our legacy pipelines into separate packages anytime soon, because we expect that people will move on to the new pipelines anyway. Our approach with the SRP was to make the actual API where you express your rendering pipeline high-level enough that you never operate on a single object. Instead, you operate on sets of objects. For example, there could be an API call that does culling. It doesn't return a list of visible objects; it returns some handle to the whole set of visible objects that you can only do partial queries on. With that, you could search for everything that's visible or everything that's opaque, but you don't have to iterate over each object and do decisions on each of those. Conceptually, the API works on sets of objects, sets of lights, etc. We also weren't exactly making a switch to a less efficient or tightly-pathed system. As our previous non-malleable rendering pipeline grew over ten or twelve years, it gained so many hidden decisions and branches to handle various feature interactions that it was no longer super tight and efficient. It was now being used to handle various corner cases that only happen in rare cases.","title":"API Responsibility"},{"location":"interviews/ArasPranckevicius-interview/#from-graphics-to-plumbing","text":"Recently in my career at Unity, I changed roles to be a dev tools engineer. Switching from graphics to build tools didn't impact my perception of game development very much, and I think that's because I was already dabbling in non-graphics work during Unity's early days. Even during my time in graphics, once in a while I would be doing something else, so I already had a good overview of various systems outside of graphics. What I didn't fully understand was the differences in machine configurations when people build code. You would expect everyone who uses Windows gets the Windows version of software. It turns out, though, there are about ten people who are on Windows but run everything from a Cygwin 18 shell, and the Cygwin shell pretends it's Linux. Some people check out their source code into a folder that is over 100 characters long, and Windows, to this day, still has a maximum path length of 260 characters. Someone else might have a Windows that's localized in French, so all their error messages are in French, which means your tools cannot parse the error messages and expect something understandable. Conceptually you might understand these various exceptions, but you don't realize how much of a hassle all that is until you have to deal with that . My advice would be to get your team onto a setup that is as uniform as possible. Not having to worry about tech differences makes things so much easier. I watched part of the Isetta team's talk with Casey Muratori , and in one part he brought up the question of who \"build engineers\" are. Actually, that's what I am! I guess Casey comes from a different setup, because at Unity we have five hundred engineers writing code on the same codebase. Now, you could argue whether that's a good thing or a bad thing, or whether you should have five hundred engineers in the first place. With that many engineers at work, I think having at least two or three build engineers actually helps; whatever you can do to make your programmers' lives more productive or less annoying is a useful thing. Part of that work has been profiling our code, which can be very useful. However, if you add profiling capture that no one will ever look or do something about it, it's kind of pointless, right? What I have noticed, and especially in the in the build area, is that you have to make profile information really visible. For example, right now in Unity's build system, what we do is that as each C++ file is being compiled, it prints the time it took right to next to the file name. The numbers are nicely aligned, too, so that if it takes two digits of seconds, it's easy to spot. Just adding that was a super easy thing to do, and that makes people who wouldn't ordinarily pay attention to build times take notice and address the problems they find.","title":"From Graphics to Plumbing"},{"location":"interviews/ArasPranckevicius-interview/#modularity-is-the-future-maybe","text":"I don't know what the future holds for game engines. Looking back to when we were just getting started with Unity, I remember some people thought it was a stupid idea because nothing we could make would be able to compete with Unreal. That is still true to an extent in the AAA game space, where aside from in-house engines, Unreal Engine 4 is the strongest option today. Knowing this, we tried for a long time not to compete with Unreal Engine; we positioned ourselves as an indie web/mobile engine (for better or worse). I think if we were to compete with Unreal from Day 1, we wouldn't have survived as a company. That said, I see the future of game engines being more modular. That's what we are currently trying to do with Unity itself in terms of packages and modules, but the risk is going overboard with modularity. For everything that's good about JavaScript npm 19 , they sometimes go too crazy; they have a single line of code becoming its own module. As Unity and Unreal are currently these big, monolithic pieces of tools and functionality, they aren't inherently versatile enough for different game genres. While they both have some malleability, you still have to deal with gigabytes of extra stuff. I think something more modular will happen in the future \u2013 I can hope that will be Unity, but we'll see. Interview conducted on October 8, 2018. Path tracing is a realistic lighting algorithm that simulates light bouncing around a scene. It uses the Monte Carlo method to give a faithful rendition of the global illumination of the scene. \u21a9 The demoscene is an international computer art subculture focused on producing demos, which are self-contained, sometimes extremely small, audio-visual computer programs. \u21a9 RenderWare is a game engine by Criterion Software that launched in 1993 and continued to regularly support games through 2010. It was known for providing an off-the-shelf solution to the difficulties of PS2 graphics programming. \u21a9 Autodesk 3ds Max , formerly 3D Studio and 3D Studio Max, is a professional 3D computer graphics program for making 3D animations, models, games, and images. \u21a9 FBX is a proprietary file format owned by Autodesk that is mostly commonly used for 3D model and animation data within the games industry. \u21a9 glTF (or GL Transmission Format) is a royalty-free file format for 3D scenes and models using the JSON standard. \u21a9 Open Dynamics Engine , or ODE , is a free and open source physics engine written in C/C++ that can do both rigid body dynamics simulation and collision detection. \u21a9 Cocoa is Apple's native object-oriented API for macOS. \u21a9 UNIX is a family of multitasking, multiuser operating systems that derive from the original AT&T Unix, originally developed at Ken Thompson, Dennis Ritchie, and others at Bell Labs. Its main comparable is Microsoft's DOS, which is mono-task and mono-user. \u21a9 Direct3D is a graphics API within Microsoft DirectX used for creating windows and rendering, and serves similar purposes as OpenGL. \u21a9 OpenGL , short for Open Graphics Library, a cross-language, cross-platform application programming interface (API) for rendering 2D and 3D vector graphics. The API is typically used to interact with a graphics processing unit (GPU), to achieve hardware-accelerated rendering. It's the underlying rendering library for many modern game engines. \u21a9 Metal is a low-level, low-overhead hardware-accelerated 3D graphic and compute shader API developed by Apple and debuted in iOS 8. It combines functions similar to OpenGL and OpenCL under one API. \u21a9 A compute shader is a shader stage in the graphics rendering pipeline that is used entirely for computing arbitrary information. It is typically used for tasks unrelated to rendering. \u21a9 IMGUI stands for immediate mode GUI which is a code-driven GUI system where on each rendering frame the application needs to issue the draw commands of the GUI (the GUI is not stored for multiple frames) \u21a9 Vulkan is a low-overhead, cross-platform 3D graphics and compute API that targets high-performance real-time 3D graphics, such as video games. \u21a9 A package manager is a system which handles the installing, updating, configuring, and removing of a collection of software libraries. \u21a9 A socket is an internal endpoint for sending or receiving data within a node on a computer network. \u21a9 Cygwin is a Unix-like environment and command-line interface for Microsoft Windows. It provides native integration of Windows-based applications and resources with those of a Unix-like environment. \u21a9 npm is a package manager for the JavaScript programming language. It's the default package manager for the JavaScript runtime environment Node.js. \u21a9","title":"Modularity is the Future, Maybe?"},{"location":"interviews/ArasPranckevicius-video/","text":"Software for Millions of Users \u00b6","title":"Video Story"},{"location":"interviews/ArasPranckevicius-video/#software-for-millions-of-users","text":"","title":"Software for Millions of Users"},{"location":"interviews/CaseyMuratori-hmh-video/","text":"Interview with Handmade Hero \u00b6 See the transcribed interview here.","title":"Video Interview"},{"location":"interviews/CaseyMuratori-hmh-video/#interview-with-handmade-hero","text":"See the transcribed interview here.","title":"Interview with Handmade Hero"},{"location":"interviews/CaseyMuratori-interview/","text":"Why Game Engine Development is Worth Learning \u00b6 Casey Muratori is the lead programmer on 1935 , an upcoming interactive story engine project, and the host of Handmade Hero , an instructional series for game engine programmers. His past projects include The Witness , the Bink 2 video codec , and the Granny Character Animation System . (The following is the edited transcription of a conversation we had with Casey Muratori .) The Shortage of Engine Programmers \u00b6 I won't pretend I have the data to support that there is a real shortage of engine programmers, but if today you told me I needed to staff up a modest ten-person engine team, I would have almost no idea where to get eight of those people. And the two that I maybe do know where to get, I'd have to hire from some other team. So I'm strictly talking about this from personal experience in the game industry. It is an anecdote, not a statistic, when I say I just don't find that people know where to hire engine programmers. I know plenty of people who have hired people who they really don't think are even that good at engine programming, but who were definitely the best they could get. Similarly, I know plenty of people who would hire another engine programmer right now if they could find one, but there's no one who meets their cutoff. Some of that is - especially on smaller teams -because you can't afford to hire lots of junior people. Maybe there's some raw talent out there and it just needs to be developed. Data-wise, I don't want to make claims about this that I can't substantiate, but real engine programming experts are very hard to find, and I can't offer you a reason for that. I just assume the reason is that even something as simple as using Unity kind of requires you to understand 3D math at a pretty decent level if you're not gonna ship one of these games with tons of obvious bugs. Being a game engine programmer is just a hard profession. It demands that you know a lot of things, so it's not for the faint of heart. It requires a lot of expertise, a lot of time spent learning even to just be a competent engine-side programmer. You need a lot of skill to modify Unreal Engine in a small way, or even to competently program a motion controller. These are just skills that people don't seem to have in abundance. Even today when you can just go get an engine so you don't need to build it from scratch, we still haven't gotten to the point where we have enough people with expertise. We don't have enough expertise versus how much we could be using to make games better. I suspect if you talked to other people in the game industry, they would have similar opinions about how hard it is to find good engine programmers. I don't want to put words in their mouth, but it's not the kind of thing that you can just go find someone to do. I think we're getting better at pipelines for finding artists thanks to social media and online tutorials about what goes into making a game asset. Perhaps also because of school curricula that was not around in the old days. What I have not seen change is the number of engine programmers. The last three engine programmer hires I know about - or was in any way involved with - all came from Handmade Hero. If it wasn't for Handmade Hero, who knows if they even would have done it? That was the reason for Handmade Hero: because I believe this way of life is worth preserving. It's talking specifically about a practice and a discipline and a mental model. I never said I want to show you how I make a game. I said \"this way of life\" specifically. I feel like making a game is covered well enough elsewhere. The Confusion with Game Engines \u00b6 Problem 1: An Amorphous System \u00b6 There are two fundamental problems in game engine development that I think set it apart from a lot of other types of development. The first one is that it is not a stateable problem in any way. Half of everything that programming does in the past or today looks a lot like stdin /stdout 1 . You can phrase it as \"here are the set of inputs, and here are the set of outputs that come out of it.\" So most of the things that programmers are used to thinking about are \"I'm working on deep learning 2 . Here's a set of input images and a set of output tuned neural net parameters. How do I make the best translation between these two things?\" Or, \"I'm a natural language processing person, here's all the corpus 3 I want in and here are the noun tags I want out, or the sentence tags I want.\" I think one of the biggest challenges for an early game engine programmer is making the leap from input-output thinking to this amorphous system. It's very confusing how that happens at first, because even though you're not necessarily aware of it, everything you've ever done prior to that looks a lot more like this input-output phrasing. One of the really good things that you can do at first is to try to figure out the \"core loop\" of a game engine; the golden differentiator of a simulation. In a flight simulator or a game, they look like this real-time loop where I have a set of entities (each with their own states) and I go through a simulation to change those states. That's a certain process. I then go through a way of presenting those things, and then I return to the beginning. Just getting yourself comfortable with that and the fact that everything you build has to build out of that is just the first big step. It's not input-output anymore. It doesn't look like a web program; it doesn't look like the neural network; it doesn't look like a parser; doesn't look like a server; doesn't look at any of these things. It looks like that\u2014a \"core loop\". Problem 2: The Complexity Explosion \u00b6 The other thing is the complexity explosion. So again, it's not input to output where things are very clean. You now have all of these systems that are all happening at the same time. They all interact, they all overlay on top of each other. The physics and the rendering and the blob, etc. All this stuff comes together. You really need to be able to do discoverable architecture in order to do these things correctly. Most people are just not familiar with that process\u2014I called it Compression-Oriented Programming in the past. I would define that as the technique of starting with the simplest way possible. I think this is Jonathan Blow 's term that he uses often: Write whatever is obvious to you. Don't even think about the rest of the game engine. Focus on the one thing you need to do at the time. Think about it more like the standard in, standard out (stdin, stdout) that we're all used to. How do I it to just get the input and the output; to do the thing I need to do in isolation? No code design, no cleanliness, no nothing. You need to be able to clear everything from your head and just do that, and then pull that out into the architecture. One of the biggest mistakes I think even experienced game programmers will make is they go the other direction. They try to start with the architecture and drill down to the thing they want to do. Never in my entire life have I seen a good result from that. It almost always has to be \"refactored\" at the end. That's because when you start from a conception of how something will plug together, you don't see all the details. You don't know all the things that you're gonna have to do when you go to solve the problem, so you always forget some of the details. On the other hand, if you just implement the real thing first and then pull it up into the architecture, you almost always end up with better results. I like to think about those problems. The \"Boundary Value Problem\" Architecture Method \u00b6 There's this thing called boundary value problems 4 in mathematics, and there are different ways of solving them. There's the shooting method: I start here, I see where I end up. There's also the method of solving backward, where I look from the endpoint and try to see what I could do differently. Building up a skill set of how you work on code that allows you to work from either side of the problem ends up being really valuable. Like I said, you usually want to start with an isolated solution and loft it up into the architecture. But then, when you get to a certain level of that lofting you want to think from the other side down. You want to determine what a good API for this should look like, what are some things you maybe didn't think about while implementing it. And things that you should maybe change, that wouldn't break the algorithm, but that would let it work better with this integration. You almost want a ping-pong development process where you start at the solution, work backward a little, go to the architecture side, work forwards a little, and come back. The best programmers at doing this sort of thing are able to make it fit really nicely at the end, so both sides can be happy. I'd like to give a shout out to Allen Webster 5 for that, he works at RAD Game Tools now. I've talked about those two things separately before, and he pointed out that I need to connect them. Competence, Coding Style, and Working in Teams \u00b6 Casey's Definition for a Competent Programmer \u00b6 Part of developing a good programming style has very little to do with things that people normally focus on, which are minor things like \"Did I overload operator= 6 to prevent a copy\". They focus on all of these rules that they never even tested. They ignore the most important thing which, in my opinion, is how easy is it to read your own code later and know exactly what it does. This is also why I tend to not comment code until it's \"done done,\" because I find that the comments end up being out of date and counterproductive. Since the comments are describing the thing that was before the latest one, it's actually worse than having a no-comment. So that's I think what keeps Handmade Hero so flexible and easy for me: To not have to worry when I come back to it on a weekend to start doing a stream. I know part of being a competent programmer is writing code in a way that doesn't require me to keep it all in my head. I can easily go look at the function names and I just know what they do. This is doubly easy if I'm the programmer, because I know what assumptions I make and if I just always make those same assumptions then I know I don't have to investigate those things further. I know I'm not gonna call \"new\" in the middle of a thing because I never do that. If you develop a programming style that you find is effective, simply leaning on that style in your own code will allow you to keep it flexible and easy to remember. I think those are the two aspects. One of them is about becoming a good programmer, which is making sure that your style actually pays real benefits and not hypothetical unproven things. The other one is keeping the code small and straightforward. I think people overstate the difficulty of Handmade Hero; it's not that hard when the code is that small. If I tried doing that on the Unreal Engine codebase, I would not be coming back to it and know exactly where I left off. I'd be completely lost, and have to step through it in the debugger for six hours before I really knew. Everyone's Own Coding Language \u00b6 In a similar way, working with different peoples' coding styles is probably one of the biggest problems in programming today. If you imagine that programming is a two-part process, there are two things that are going on at any time. One is coming up with a \"language\", and the other is speaking in that \"language\". In the industry, we talk about having a programming language, but we really don't have one that we use. What we have when we talk about a programming language is the building blocks for the actual language that we will use. It should probably be called a \"programming alphabet\" or \"programming phonemes\" if we're honest. Because what happens in a game engine is that first you take the language that you have, like C++, and you build your own \"language\" on top of it. This will be a sort of functional language of core things that everyone will use. It'll be used to talk about things like how the memory is managed and how to implement the render pipeline and how do we pass things back and forth, what is the job control 7 story, and so on. Those specifics form a secondary language. It's a very recursive process. You could think of it as making lots of languages on top of each other. Some programming languages and projects are so poorly thought through that they end up with languages on top of languages on top of languages. If you think of each programmer as having their own ideal language they would like to see, that's the substrate that's right on top of the regular language that they all had to conform to. They're all fluent in that, presumably their own one. So when you bring two programmers together you essentially have a problem where they need to write a book together but one speaks French and the other speaks Spanish. They have to figure out how to come together to write the book, which would have to be in a Latin style typography but not actually in either of their languages. So I think that the jury is still out. I might even say that the appellees and the plaintiffs and defendants are still out on what the right way to do that is. There's certainly procrustean 8 approaches where companies will demand everyone have this many tabs and every class looks like this and there's a file for every block. It's like everyone must conform. There are other places that use a more laissez-faire approach, and they hope that everyone will figure out what to do at the boundaries. I think that we don't know the answers to what's right about that and I would be absolutely lying if I said I thought I had a good solution to it. Fluency, Efficiency, and Cooperation \u00b6 There is no question in my mind that there is a loss of speed, efficacy and quality of code that comes from shifting from your native language to another one. Shifting from your native programming style into somebody else's costs you. You have to balance that cost against the fact that if we have more competent programmers on a project, assuming each of them is capable of writing something useful for the project, we can get more done. While working on The Witness , I took a pretty massive productivity hit working in that codebase compared to my own. There's nothing you can do about that. I did build some of my own language in there. I put some of my own tools in there over time so that I can be effective, but it was on an on-demand basis. There's an entire article I wrote about a time when I had a bug, which only came up because I made a wrong assumption about the math library. And that's the reason that I and Jon Blow are both able to write certain types of code quickly - because we both make assumptions about what our math libraries do. If we couldn't make those, we'd literally have to read the code for cross-product 9 or something, every time, to figure out whether it was right-handed or left-handed. The process would be much slower and there'd be way more bugs. It's really hard to overstate just how important that is. I think a lot of people don't necessarily realize it, because when you program your first engine, you're usually not a very mature programmer. If you've never written an engine alone after you've had a lot of experience and are more self-confident in your skills, you may not realize how fast you actually are when everything is done the way you expect, because you've only ever done it at Valve or something where everything works maybe a little bit differently than how you would want it to. And so it's a very important thing to be aware of. On that front, I think working with an existing engine would be really good for engine programmers to start with. In an engine that's made a bunch of decisions, whether they're right or wrong, you have to live with them. Because that's probably going to be your job when you're first starting out. For example, the Unreal Engine does different things wrong but you can't change those because they're baked in the architecture. So it would be important for new engine programmers to learn how you make these improvements, or how you make this one part better. Handmade Hero as a Learning Tool \u00b6 The goal of Handmade Hero was to stream it, because I wanted to have that complete record. I thought there were things people would learn by watching an engine programmer just do what they do, rather than just telling them the result. I think it's been really good at that because I've had many people actually tell me that that was a big breakthrough for them. They would say, \"Seeing what you actually do totally makes me feel more comfortable about doing this.\" So that, I think, was the main thing I was trying to do with Handmade Hero. That's what I hope people get out of it. Everything else I have no idea, because the goal wasn't to teach people, say, 3D math. I just do that because I wanted people to see an explanation of those things as we used them. I don't know if it's a good way to learn it and I don't know what other things would be valuable. If I was gonna make a course for people to learn 3D math for engines, it wouldn't look like Handmade Hero. So there's definitely a lot of room for more learning materials, regardless of whether or not such resources are available right now. So I don't think that Handmade Hero is the best we can do for learning specific, individual topics. I can say that for sure, because if I sat down to make something like that myself, it wouldn't look like Handmade Hero. I don't think you ever want to use any one programmer's ideas or code as your sole resource. The reason for that is because everyone's brain works a little differently. That means the most efficient way to think through a problem is not the same for everybody. Unless your brain literally works exactly the same way that mine does (and there may be some people out there whose do), then the chances that my streams are telling you literally all the things you might want to know about programming is zero. There are going to be very valuable approaches and things that I might not be aware of that you will find effective. So I think it's important for a programmer's brain to be able to distinguish between those styles in a concrete way. That's the way you can find the primary styles that work for you. As I got better at programming, I found myself actually thinking, \"How do I figure out what elements of programming style are good and bad for me?\" I don't care what people write in these books. I've realized that they don't really know what they're talking about a lot of the time. I want to come up with ways I can evaluate this stuff for real, and I want to see how it works when I do it. I felt like I became a much better programmer when I realized there was a really big divide between the prevailing theories and actual programming practice. I think it's important for programmers to try to cross that divide for themselves. They're not going to cross it by watching a series like mine and assuming that everything I do is right. Even if everything I did was right for them, they still wouldn't have developed the critical faculties necessary to know why. I also suspect that for most programmers, not everything I do in the exact way I do it will be the way that they should do it, because they will go on to find different techniques that are better for them. It's the Process, not the Product \u00b6 If I had to start from square one on Handmade Hero , obviously there are things I would do differently, but that's because the point of Handmade Hero and for recording the process was that I wanted people to see me go down a few avenues before picking the one I wanted. I wanted them to see how an amorphous, unstructured piece of code resolves into a structured one. For example, one thing we did over the past three weeks was we made the renderer into a standalone DLL 10 . Now anyone can use it to make a renderer that does all the things that Handmade Hero's does. So the depth peeling 11 , sprites, and the camera code... all that stuff is in a separate library now. So if you wanted to make a game that has an instant 3D-hybrid sprite block renderer, you just have one. Those are exactly what I wanted people to see in Handmade Hero. So if I lost the whole thing, I almost would say I would start a different project. If I was going into Handmade Hero with the knowledge of how everything worked out, it would totally ruin the entire point of the series. Watching me type in code I already know how to do isn't worthless, but that's what a blog or GitHub repo is for. On Handmade Hero, it was critical that I not have a solution in mind when I started. Reusing Past Subsystems \u00b6 Utilizing code from past projects is not something I'm an expert on. I'm only 42 years old! Maybe when I'm 82 I'll be able to tell you if I had a piece of code that was worth reusing. There are a couple of things I can say along those lines. For the very first time, on 1935 12 , which is the main project I'm working on now, I decided that I never want to write a platform layer or standard library again. I'm expecting honestly this project is probably a five-year project, which is a long time for one programmer. It wouldn't be that much for a team if you think about it in programmer years, but for one person it's a lot. So on this project, I felt like I was ready to do that for the first time. Whether it succeeds or not, I'm not sure. But what I will say is I took an approach that was slightly different from approaches I had taken before. I documented the whole development process. I have a notes file, and every time there is a question about what I'm going to do in the codebase, I document exactly why I thought there was a question. I discuss the solutions that I've tried. Finally, I explain why I ended up selecting the solution I did out of the ones that I tried. So there is literally, for the first time on any product that I've ever done, a 100% complete documentation of why every last thing works the way it does. Before this project, I never sat down and committed to thinking all these things through, because a lot of times to do a platform layer you just do it. Justifying Brought-In Systems \u00b6 Even for myself \u2014who advocates a very limber style of programming where I don't think you do a lot of upfront design \u2014even I want to spend more time recording and justifying decisions. I don't do them from the top down like a UML diagram 13 disaster situation or anything like that, but I do feel that a higher level of rigor is necessary. What I try to do in this situation is sort of play the devil's advocate; as if I was more than one programmer. I try to come at it from different angles. Can I justify it from a speed perspective? An ease-of-use perspective? A compatibility perspective? So we'll see if it works. Ask me in ten years! Builds: Keep 'em Simple \u00b6 In our current weird programming culture, there's a term called \"build engineer\" 14 . All I can say about that is the way to approach a build is to realize that computers today are so fast that almost any game engine you care to make, especially on a team as small as yours, can be built with a batch file in like ten seconds. Anything you do more complicated than that is a waste of your time. People throw in integrated continuous build servers and Cmake 15 and Ninja 16 and distributed builds and Python, but none of it is necessary. All you need to do is just make a thing that says \"compiler, here are the files, this is the executable I want, build.\" That's it! On Handmade Hero, we showed how to do this on the first day. You don't need anything more complicated than that and I would encourage you to start there. Eventually, somewhere down the line, there are some arguments to be made for doing some of the slightly more complicated things, like a continuous integration server when you have multiple platforms. This is just so that not everyone has to have every devkit to make sure they don't break the build. Otherwise, a single-line build is the build you want. It shouldn't be more complicated than that. Interview conducted September 8, 2018. stdin and stdout are the functions that handle program input/output in the standard library of the C programming language \u21a9 Deep learning is a machine learning technique that teaches computers to do what comes naturally to humans: learn by example. Deep learning is a key technology behind driverless cars, enabling them to recognize a stop sign or to distinguish a pedestrian from a lamppost. \u21a9 In linguistics, a corpus or text corpus is a large and structured set of texts. \u21a9 A boundary value problem in mathematics is a problem whose solution that satisfies boundary conditions that act as constraints for the solution. More can be learned on its Wikipedia page . \u21a9 Allen is an entrepreneur and engineer interested in working on the tools that drive digital creation. He is currently working on \"4coder\", a programming environment targeted at the problems of real-world high-end C/C++ problems, under the Handmade Network. \u21a9 Overloading an operator replaces the functionality of that operator for a given class. For example, you could overload the assignment operator + on a list object to instead add the given other object to said list object. \u21a9 Job control is the control of multiple tasks on a computer system that may be \"in-flight\" at the same time. It requires proper allocation of resources and locked access to prevent deadlocks and failures. \u21a9 Definition: enforcing uniformity or conformity without regard to natural variation or individuality \u21a9 Cross-product is the 3D math operation where the input is two vectors and the output is one vector that's perpendicular to both input vectors. However, the direction of the output vector depends on whether the space is defined as left handed or right handed. \u21a9 A DLL is a dynamic-linked library which is Microsoft shared library concept which can be transported around easier than a project and contains information about the compiled project. \u21a9 Depth peeling is a method of order-independent transparency when rendering 3D geometry. It determines what should be drawn on top by rendering multiple passes of a scene and comparing depths. \u21a9 1935 is an upcoming game from the MollyRocket team, which includes Casey. \u21a9 UML is the Unified Modeling Language, whose purpose is visually representing a software system with its actors and roles so that a programmer can better understand and design said system. Sometimes, UML diagrams can end up as a \"disaster situation\". \u21a9 A build engineer is in charge of the infrastructure that builds a software application, as well as testing and troubleshooting code for before the software's release. \u21a9 CMake is a cross-platform, open-source application for managing the build process of software in a compiler-independent way. \u21a9 Ninja is a small build system that is designed to run builds as fast as possible. \u21a9","title":"Interview"},{"location":"interviews/CaseyMuratori-interview/#why-game-engine-development-is-worth-learning","text":"Casey Muratori is the lead programmer on 1935 , an upcoming interactive story engine project, and the host of Handmade Hero , an instructional series for game engine programmers. His past projects include The Witness , the Bink 2 video codec , and the Granny Character Animation System . (The following is the edited transcription of a conversation we had with Casey Muratori .)","title":"Why Game Engine Development is Worth Learning"},{"location":"interviews/CaseyMuratori-interview/#the-shortage-of-engine-programmers","text":"I won't pretend I have the data to support that there is a real shortage of engine programmers, but if today you told me I needed to staff up a modest ten-person engine team, I would have almost no idea where to get eight of those people. And the two that I maybe do know where to get, I'd have to hire from some other team. So I'm strictly talking about this from personal experience in the game industry. It is an anecdote, not a statistic, when I say I just don't find that people know where to hire engine programmers. I know plenty of people who have hired people who they really don't think are even that good at engine programming, but who were definitely the best they could get. Similarly, I know plenty of people who would hire another engine programmer right now if they could find one, but there's no one who meets their cutoff. Some of that is - especially on smaller teams -because you can't afford to hire lots of junior people. Maybe there's some raw talent out there and it just needs to be developed. Data-wise, I don't want to make claims about this that I can't substantiate, but real engine programming experts are very hard to find, and I can't offer you a reason for that. I just assume the reason is that even something as simple as using Unity kind of requires you to understand 3D math at a pretty decent level if you're not gonna ship one of these games with tons of obvious bugs. Being a game engine programmer is just a hard profession. It demands that you know a lot of things, so it's not for the faint of heart. It requires a lot of expertise, a lot of time spent learning even to just be a competent engine-side programmer. You need a lot of skill to modify Unreal Engine in a small way, or even to competently program a motion controller. These are just skills that people don't seem to have in abundance. Even today when you can just go get an engine so you don't need to build it from scratch, we still haven't gotten to the point where we have enough people with expertise. We don't have enough expertise versus how much we could be using to make games better. I suspect if you talked to other people in the game industry, they would have similar opinions about how hard it is to find good engine programmers. I don't want to put words in their mouth, but it's not the kind of thing that you can just go find someone to do. I think we're getting better at pipelines for finding artists thanks to social media and online tutorials about what goes into making a game asset. Perhaps also because of school curricula that was not around in the old days. What I have not seen change is the number of engine programmers. The last three engine programmer hires I know about - or was in any way involved with - all came from Handmade Hero. If it wasn't for Handmade Hero, who knows if they even would have done it? That was the reason for Handmade Hero: because I believe this way of life is worth preserving. It's talking specifically about a practice and a discipline and a mental model. I never said I want to show you how I make a game. I said \"this way of life\" specifically. I feel like making a game is covered well enough elsewhere.","title":"The Shortage of Engine Programmers"},{"location":"interviews/CaseyMuratori-interview/#the-confusion-with-game-engines","text":"","title":"The Confusion with Game Engines"},{"location":"interviews/CaseyMuratori-interview/#problem-1-an-amorphous-system","text":"There are two fundamental problems in game engine development that I think set it apart from a lot of other types of development. The first one is that it is not a stateable problem in any way. Half of everything that programming does in the past or today looks a lot like stdin /stdout 1 . You can phrase it as \"here are the set of inputs, and here are the set of outputs that come out of it.\" So most of the things that programmers are used to thinking about are \"I'm working on deep learning 2 . Here's a set of input images and a set of output tuned neural net parameters. How do I make the best translation between these two things?\" Or, \"I'm a natural language processing person, here's all the corpus 3 I want in and here are the noun tags I want out, or the sentence tags I want.\" I think one of the biggest challenges for an early game engine programmer is making the leap from input-output thinking to this amorphous system. It's very confusing how that happens at first, because even though you're not necessarily aware of it, everything you've ever done prior to that looks a lot more like this input-output phrasing. One of the really good things that you can do at first is to try to figure out the \"core loop\" of a game engine; the golden differentiator of a simulation. In a flight simulator or a game, they look like this real-time loop where I have a set of entities (each with their own states) and I go through a simulation to change those states. That's a certain process. I then go through a way of presenting those things, and then I return to the beginning. Just getting yourself comfortable with that and the fact that everything you build has to build out of that is just the first big step. It's not input-output anymore. It doesn't look like a web program; it doesn't look like the neural network; it doesn't look like a parser; doesn't look like a server; doesn't look at any of these things. It looks like that\u2014a \"core loop\".","title":"Problem 1: An Amorphous System"},{"location":"interviews/CaseyMuratori-interview/#problem-2-the-complexity-explosion","text":"The other thing is the complexity explosion. So again, it's not input to output where things are very clean. You now have all of these systems that are all happening at the same time. They all interact, they all overlay on top of each other. The physics and the rendering and the blob, etc. All this stuff comes together. You really need to be able to do discoverable architecture in order to do these things correctly. Most people are just not familiar with that process\u2014I called it Compression-Oriented Programming in the past. I would define that as the technique of starting with the simplest way possible. I think this is Jonathan Blow 's term that he uses often: Write whatever is obvious to you. Don't even think about the rest of the game engine. Focus on the one thing you need to do at the time. Think about it more like the standard in, standard out (stdin, stdout) that we're all used to. How do I it to just get the input and the output; to do the thing I need to do in isolation? No code design, no cleanliness, no nothing. You need to be able to clear everything from your head and just do that, and then pull that out into the architecture. One of the biggest mistakes I think even experienced game programmers will make is they go the other direction. They try to start with the architecture and drill down to the thing they want to do. Never in my entire life have I seen a good result from that. It almost always has to be \"refactored\" at the end. That's because when you start from a conception of how something will plug together, you don't see all the details. You don't know all the things that you're gonna have to do when you go to solve the problem, so you always forget some of the details. On the other hand, if you just implement the real thing first and then pull it up into the architecture, you almost always end up with better results. I like to think about those problems.","title":"Problem 2: The Complexity Explosion"},{"location":"interviews/CaseyMuratori-interview/#the-boundary-value-problem-architecture-method","text":"There's this thing called boundary value problems 4 in mathematics, and there are different ways of solving them. There's the shooting method: I start here, I see where I end up. There's also the method of solving backward, where I look from the endpoint and try to see what I could do differently. Building up a skill set of how you work on code that allows you to work from either side of the problem ends up being really valuable. Like I said, you usually want to start with an isolated solution and loft it up into the architecture. But then, when you get to a certain level of that lofting you want to think from the other side down. You want to determine what a good API for this should look like, what are some things you maybe didn't think about while implementing it. And things that you should maybe change, that wouldn't break the algorithm, but that would let it work better with this integration. You almost want a ping-pong development process where you start at the solution, work backward a little, go to the architecture side, work forwards a little, and come back. The best programmers at doing this sort of thing are able to make it fit really nicely at the end, so both sides can be happy. I'd like to give a shout out to Allen Webster 5 for that, he works at RAD Game Tools now. I've talked about those two things separately before, and he pointed out that I need to connect them.","title":"The \"Boundary Value Problem\" Architecture Method"},{"location":"interviews/CaseyMuratori-interview/#competence-coding-style-and-working-in-teams","text":"","title":"Competence, Coding Style, and Working in Teams"},{"location":"interviews/CaseyMuratori-interview/#caseys-definition-for-a-competent-programmer","text":"Part of developing a good programming style has very little to do with things that people normally focus on, which are minor things like \"Did I overload operator= 6 to prevent a copy\". They focus on all of these rules that they never even tested. They ignore the most important thing which, in my opinion, is how easy is it to read your own code later and know exactly what it does. This is also why I tend to not comment code until it's \"done done,\" because I find that the comments end up being out of date and counterproductive. Since the comments are describing the thing that was before the latest one, it's actually worse than having a no-comment. So that's I think what keeps Handmade Hero so flexible and easy for me: To not have to worry when I come back to it on a weekend to start doing a stream. I know part of being a competent programmer is writing code in a way that doesn't require me to keep it all in my head. I can easily go look at the function names and I just know what they do. This is doubly easy if I'm the programmer, because I know what assumptions I make and if I just always make those same assumptions then I know I don't have to investigate those things further. I know I'm not gonna call \"new\" in the middle of a thing because I never do that. If you develop a programming style that you find is effective, simply leaning on that style in your own code will allow you to keep it flexible and easy to remember. I think those are the two aspects. One of them is about becoming a good programmer, which is making sure that your style actually pays real benefits and not hypothetical unproven things. The other one is keeping the code small and straightforward. I think people overstate the difficulty of Handmade Hero; it's not that hard when the code is that small. If I tried doing that on the Unreal Engine codebase, I would not be coming back to it and know exactly where I left off. I'd be completely lost, and have to step through it in the debugger for six hours before I really knew.","title":"Casey's Definition for a Competent Programmer"},{"location":"interviews/CaseyMuratori-interview/#everyones-own-coding-language","text":"In a similar way, working with different peoples' coding styles is probably one of the biggest problems in programming today. If you imagine that programming is a two-part process, there are two things that are going on at any time. One is coming up with a \"language\", and the other is speaking in that \"language\". In the industry, we talk about having a programming language, but we really don't have one that we use. What we have when we talk about a programming language is the building blocks for the actual language that we will use. It should probably be called a \"programming alphabet\" or \"programming phonemes\" if we're honest. Because what happens in a game engine is that first you take the language that you have, like C++, and you build your own \"language\" on top of it. This will be a sort of functional language of core things that everyone will use. It'll be used to talk about things like how the memory is managed and how to implement the render pipeline and how do we pass things back and forth, what is the job control 7 story, and so on. Those specifics form a secondary language. It's a very recursive process. You could think of it as making lots of languages on top of each other. Some programming languages and projects are so poorly thought through that they end up with languages on top of languages on top of languages. If you think of each programmer as having their own ideal language they would like to see, that's the substrate that's right on top of the regular language that they all had to conform to. They're all fluent in that, presumably their own one. So when you bring two programmers together you essentially have a problem where they need to write a book together but one speaks French and the other speaks Spanish. They have to figure out how to come together to write the book, which would have to be in a Latin style typography but not actually in either of their languages. So I think that the jury is still out. I might even say that the appellees and the plaintiffs and defendants are still out on what the right way to do that is. There's certainly procrustean 8 approaches where companies will demand everyone have this many tabs and every class looks like this and there's a file for every block. It's like everyone must conform. There are other places that use a more laissez-faire approach, and they hope that everyone will figure out what to do at the boundaries. I think that we don't know the answers to what's right about that and I would be absolutely lying if I said I thought I had a good solution to it.","title":"Everyone's Own Coding Language"},{"location":"interviews/CaseyMuratori-interview/#fluency-efficiency-and-cooperation","text":"There is no question in my mind that there is a loss of speed, efficacy and quality of code that comes from shifting from your native language to another one. Shifting from your native programming style into somebody else's costs you. You have to balance that cost against the fact that if we have more competent programmers on a project, assuming each of them is capable of writing something useful for the project, we can get more done. While working on The Witness , I took a pretty massive productivity hit working in that codebase compared to my own. There's nothing you can do about that. I did build some of my own language in there. I put some of my own tools in there over time so that I can be effective, but it was on an on-demand basis. There's an entire article I wrote about a time when I had a bug, which only came up because I made a wrong assumption about the math library. And that's the reason that I and Jon Blow are both able to write certain types of code quickly - because we both make assumptions about what our math libraries do. If we couldn't make those, we'd literally have to read the code for cross-product 9 or something, every time, to figure out whether it was right-handed or left-handed. The process would be much slower and there'd be way more bugs. It's really hard to overstate just how important that is. I think a lot of people don't necessarily realize it, because when you program your first engine, you're usually not a very mature programmer. If you've never written an engine alone after you've had a lot of experience and are more self-confident in your skills, you may not realize how fast you actually are when everything is done the way you expect, because you've only ever done it at Valve or something where everything works maybe a little bit differently than how you would want it to. And so it's a very important thing to be aware of. On that front, I think working with an existing engine would be really good for engine programmers to start with. In an engine that's made a bunch of decisions, whether they're right or wrong, you have to live with them. Because that's probably going to be your job when you're first starting out. For example, the Unreal Engine does different things wrong but you can't change those because they're baked in the architecture. So it would be important for new engine programmers to learn how you make these improvements, or how you make this one part better.","title":"Fluency, Efficiency, and Cooperation"},{"location":"interviews/CaseyMuratori-interview/#handmade-hero-as-a-learning-tool","text":"The goal of Handmade Hero was to stream it, because I wanted to have that complete record. I thought there were things people would learn by watching an engine programmer just do what they do, rather than just telling them the result. I think it's been really good at that because I've had many people actually tell me that that was a big breakthrough for them. They would say, \"Seeing what you actually do totally makes me feel more comfortable about doing this.\" So that, I think, was the main thing I was trying to do with Handmade Hero. That's what I hope people get out of it. Everything else I have no idea, because the goal wasn't to teach people, say, 3D math. I just do that because I wanted people to see an explanation of those things as we used them. I don't know if it's a good way to learn it and I don't know what other things would be valuable. If I was gonna make a course for people to learn 3D math for engines, it wouldn't look like Handmade Hero. So there's definitely a lot of room for more learning materials, regardless of whether or not such resources are available right now. So I don't think that Handmade Hero is the best we can do for learning specific, individual topics. I can say that for sure, because if I sat down to make something like that myself, it wouldn't look like Handmade Hero. I don't think you ever want to use any one programmer's ideas or code as your sole resource. The reason for that is because everyone's brain works a little differently. That means the most efficient way to think through a problem is not the same for everybody. Unless your brain literally works exactly the same way that mine does (and there may be some people out there whose do), then the chances that my streams are telling you literally all the things you might want to know about programming is zero. There are going to be very valuable approaches and things that I might not be aware of that you will find effective. So I think it's important for a programmer's brain to be able to distinguish between those styles in a concrete way. That's the way you can find the primary styles that work for you. As I got better at programming, I found myself actually thinking, \"How do I figure out what elements of programming style are good and bad for me?\" I don't care what people write in these books. I've realized that they don't really know what they're talking about a lot of the time. I want to come up with ways I can evaluate this stuff for real, and I want to see how it works when I do it. I felt like I became a much better programmer when I realized there was a really big divide between the prevailing theories and actual programming practice. I think it's important for programmers to try to cross that divide for themselves. They're not going to cross it by watching a series like mine and assuming that everything I do is right. Even if everything I did was right for them, they still wouldn't have developed the critical faculties necessary to know why. I also suspect that for most programmers, not everything I do in the exact way I do it will be the way that they should do it, because they will go on to find different techniques that are better for them.","title":"Handmade Hero as a Learning Tool"},{"location":"interviews/CaseyMuratori-interview/#its-the-process-not-the-product","text":"If I had to start from square one on Handmade Hero , obviously there are things I would do differently, but that's because the point of Handmade Hero and for recording the process was that I wanted people to see me go down a few avenues before picking the one I wanted. I wanted them to see how an amorphous, unstructured piece of code resolves into a structured one. For example, one thing we did over the past three weeks was we made the renderer into a standalone DLL 10 . Now anyone can use it to make a renderer that does all the things that Handmade Hero's does. So the depth peeling 11 , sprites, and the camera code... all that stuff is in a separate library now. So if you wanted to make a game that has an instant 3D-hybrid sprite block renderer, you just have one. Those are exactly what I wanted people to see in Handmade Hero. So if I lost the whole thing, I almost would say I would start a different project. If I was going into Handmade Hero with the knowledge of how everything worked out, it would totally ruin the entire point of the series. Watching me type in code I already know how to do isn't worthless, but that's what a blog or GitHub repo is for. On Handmade Hero, it was critical that I not have a solution in mind when I started.","title":"It's the Process, not the Product"},{"location":"interviews/CaseyMuratori-interview/#reusing-past-subsystems","text":"Utilizing code from past projects is not something I'm an expert on. I'm only 42 years old! Maybe when I'm 82 I'll be able to tell you if I had a piece of code that was worth reusing. There are a couple of things I can say along those lines. For the very first time, on 1935 12 , which is the main project I'm working on now, I decided that I never want to write a platform layer or standard library again. I'm expecting honestly this project is probably a five-year project, which is a long time for one programmer. It wouldn't be that much for a team if you think about it in programmer years, but for one person it's a lot. So on this project, I felt like I was ready to do that for the first time. Whether it succeeds or not, I'm not sure. But what I will say is I took an approach that was slightly different from approaches I had taken before. I documented the whole development process. I have a notes file, and every time there is a question about what I'm going to do in the codebase, I document exactly why I thought there was a question. I discuss the solutions that I've tried. Finally, I explain why I ended up selecting the solution I did out of the ones that I tried. So there is literally, for the first time on any product that I've ever done, a 100% complete documentation of why every last thing works the way it does. Before this project, I never sat down and committed to thinking all these things through, because a lot of times to do a platform layer you just do it.","title":"Reusing Past Subsystems"},{"location":"interviews/CaseyMuratori-interview/#justifying-brought-in-systems","text":"Even for myself \u2014who advocates a very limber style of programming where I don't think you do a lot of upfront design \u2014even I want to spend more time recording and justifying decisions. I don't do them from the top down like a UML diagram 13 disaster situation or anything like that, but I do feel that a higher level of rigor is necessary. What I try to do in this situation is sort of play the devil's advocate; as if I was more than one programmer. I try to come at it from different angles. Can I justify it from a speed perspective? An ease-of-use perspective? A compatibility perspective? So we'll see if it works. Ask me in ten years!","title":"Justifying Brought-In Systems"},{"location":"interviews/CaseyMuratori-interview/#builds-keep-em-simple","text":"In our current weird programming culture, there's a term called \"build engineer\" 14 . All I can say about that is the way to approach a build is to realize that computers today are so fast that almost any game engine you care to make, especially on a team as small as yours, can be built with a batch file in like ten seconds. Anything you do more complicated than that is a waste of your time. People throw in integrated continuous build servers and Cmake 15 and Ninja 16 and distributed builds and Python, but none of it is necessary. All you need to do is just make a thing that says \"compiler, here are the files, this is the executable I want, build.\" That's it! On Handmade Hero, we showed how to do this on the first day. You don't need anything more complicated than that and I would encourage you to start there. Eventually, somewhere down the line, there are some arguments to be made for doing some of the slightly more complicated things, like a continuous integration server when you have multiple platforms. This is just so that not everyone has to have every devkit to make sure they don't break the build. Otherwise, a single-line build is the build you want. It shouldn't be more complicated than that. Interview conducted September 8, 2018. stdin and stdout are the functions that handle program input/output in the standard library of the C programming language \u21a9 Deep learning is a machine learning technique that teaches computers to do what comes naturally to humans: learn by example. Deep learning is a key technology behind driverless cars, enabling them to recognize a stop sign or to distinguish a pedestrian from a lamppost. \u21a9 In linguistics, a corpus or text corpus is a large and structured set of texts. \u21a9 A boundary value problem in mathematics is a problem whose solution that satisfies boundary conditions that act as constraints for the solution. More can be learned on its Wikipedia page . \u21a9 Allen is an entrepreneur and engineer interested in working on the tools that drive digital creation. He is currently working on \"4coder\", a programming environment targeted at the problems of real-world high-end C/C++ problems, under the Handmade Network. \u21a9 Overloading an operator replaces the functionality of that operator for a given class. For example, you could overload the assignment operator + on a list object to instead add the given other object to said list object. \u21a9 Job control is the control of multiple tasks on a computer system that may be \"in-flight\" at the same time. It requires proper allocation of resources and locked access to prevent deadlocks and failures. \u21a9 Definition: enforcing uniformity or conformity without regard to natural variation or individuality \u21a9 Cross-product is the 3D math operation where the input is two vectors and the output is one vector that's perpendicular to both input vectors. However, the direction of the output vector depends on whether the space is defined as left handed or right handed. \u21a9 A DLL is a dynamic-linked library which is Microsoft shared library concept which can be transported around easier than a project and contains information about the compiled project. \u21a9 Depth peeling is a method of order-independent transparency when rendering 3D geometry. It determines what should be drawn on top by rendering multiple passes of a scene and comparing depths. \u21a9 1935 is an upcoming game from the MollyRocket team, which includes Casey. \u21a9 UML is the Unified Modeling Language, whose purpose is visually representing a software system with its actors and roles so that a programmer can better understand and design said system. Sometimes, UML diagrams can end up as a \"disaster situation\". \u21a9 A build engineer is in charge of the infrastructure that builds a software application, as well as testing and troubleshooting code for before the software's release. \u21a9 CMake is a cross-platform, open-source application for managing the build process of software in a compiler-independent way. \u21a9 Ninja is a small build system that is designed to run builds as fast as possible. \u21a9","title":"Builds: Keep 'em Simple"},{"location":"interviews/CaseyMuratori-video/","text":"Recognizing Your Passion \u00b6","title":"Video Story"},{"location":"interviews/CaseyMuratori-video/#recognizing-your-passion","text":"","title":"Recognizing Your Passion"},{"location":"interviews/ElanRuskin-interview/","text":"Wisdom from Working at AAA Studios for 15 Years \u00b6 Elan Ruskin is a senior engine programmer at Insomniac Games, where he has worked on critically-acclaimed titles including Marvel's Spider-Man and Ratchet & Clank. Prior to his time at Insomniac, Elan worked at Valve, Naughty Dog, and Maxis on many of their flagship titles as a gameplay and engine programmer. When he's not programming, Elan enjoys theater, music, and Star Trek. (The following is the edited transcription of a conversation we had with Elan Ruskin.) Designer-Driven Tools \u00b6 The real advantage of data-driven systems is that it's designer-driven; you're decreasing the iteration time for the designers. It puts the ability to make and see changes into the hands of the person actually making the content, and away from the programmer-compiler loop which requires programmers to develop and compile before any change can take place. The problems with data-driven design are 1) the code has to be a little more complex to support this flexibility and 2) you're loading bulkier content. You can get around that, though, if you use a builder 1 to pack down the content into something that code can load more efficiently. What surprised me about data-driven systems is that it ended up being much less of a problem than I expected. It turns out that you load things relatively infrequently, and if you do end up loading them frequently, you can bake them down. So as much as it would make the me of 20 years ago sad to hear me say it, having things in an open-text format that gets parsed turns out just not to be a performance issue. I'm not saying performance doesn't matter! But that turned out to not be the issue. The other problem with data-driven design is that people will do weird, unexpected, and strange things with data that you didn't anticipate and will possibly break your systems. That's because the connection between changing the data and something in the game breaking is not quite as obvious as it is with code, where you can set a breakpoint and see exactly what happened. The importance of good error reporting, diagnostics, and designing the authoring tools in a way that prevents people from getting themselves into trouble was not clear to me when I began engine programming. With tools, the things that you can't anticipate are usually the problem, because the designers and artists are always trying to solve their own problems. They're not out to break the tools or fumble around oafishly; they have specific needs, like \"I need this tree to have another tree on top of it so I can turn one of them off and the other one on, because it's winter and we need the leaves to be gone.\" In this scenario, they might not know that if you have two things in the same place at the same time, it causes a problem. That's something they would have no way to know about until they did it. So really, the way to deal with that is to find a way to prevent people from making the mistake, which would then make the causal connection obvious. Ideally, we should make it impossible to do bad things, but again, that comes back to anticipating things. It's also not a reliable strategy to just go over and yell at people for having done the content wrong or give them a gigantic document that explains how to use your tools. You can't expect people to hold that much content in their heads at once. When developing tools for writers, an assumption a lot of people make is that writers are not technical and therefore need easier tools, which is completely untrue. Writers can learn computers as well as anyone else! What I learned from my time at Valve is writers need flexibility; any given line of dialogue goes through many iterations to get it right. The thing to be cognizant of is that writers are part of a whole pipeline of content that has to get made. The writer's text appears in the game while it's still being prototyped, but even at that time we have to cast the voice actor and record them. Then, we put the voiced lines into the game only to realize the dialogue is clunky. So we have to change the line, go back to the booth, and repeat the process. This puts the writers in the middle of a pipeline that has audio waggling at the other end. The advantage of building a complete suite of tools is you can integrate the whole process of tracking where lines of dialogue are located in the game, as well as who's been cast to play it, and whether or not it's been recorded and localized yet. In Campo Santo's talk on Firewatch , they discussed how they integrated the dialogue system with the \"recording-tracking\" system. Taking that approach saved them a lot of agita. Intimate Bond of Engine and Tools \u00b6 At Insomniac, the engine team and the tools team are the same team. That works well for us because the team is not especially big, and because the engine and the tools are intimately bound. The engine is loading the assets, the builders are cooking the assets into a binary 2 , and the tools are feeding the things to the builders. These are not separate operations; it's all the same lump of data. As teams grow, you'll need to specialize the labor because they are different skills to an extent. I personally don't think there's that much value in separating the engine runtime from the tools in terms of being different teams, unless your studio is gigantic. In that case, you have to for organizational purposes. Along the same lines, it's almost a necessity to version the tools and the engines together. Part of this is the obvious reason that if you change the runtime format and the tools need to export in your format, the engine needs to be able to read it. Again, they're operating on the same data. What's more, anytime you need a new capability the engine, the tools have to support it, so they really move in lockstep. One of our attempts at improving our tools ecosystem was to use web tools. For the entire rundown of why Insomniac went with web tools and why we stopped using them, you should see Andreas Frederiksson 's GDC talk . The reason for moving towards web tools was that we thought it would be much more flexible to make a UI in the web, and also that it would be easier to hire people who have a web UI experience than people who have C++ UI experience. That just turned out not to be the case. We ended up hiring the people that we would hire anyway, and then teaching them JavaScript. What's more, the scaling issues of web tools are enormous. The web is good at doing 100 or 200 of something; it's not so good at doing 30,000 of something. So just performance and memory were gigantic issues, in addition to all the other issues like Chrome continually breaking underneath their feeds, JavaScript is just bad! On the usability side, we made our new web tools work almost exactly like the old tools, only with less bugs. We tried to keep the interface consistent. The problems that we ran into were that the new tools didn't have all of the features of the old tools to begin with, because we couldn't rebuild everything at once all in one piece. As a result, the team would have to learn how to work around the missing pieces. Because we kept the the workflow the same, it was fine, plus everything got faster and less buggy. We have also made several attempts at making a tool to track feature regression. We wrote a tool that loads every level of the game, takes a snapshot of the memory, and then unloads it and repeats the process. From there it would just create this whole spreadsheet report that nobody ever looked at, because it was always days out of date. We had another tool that was meant to interrogate a level; it would look at all of its dependencies and the dependencies' dependencies, and recursively try to see how much memory everything would consume. The problem of that is it's an estimate, because you often don't know how much runtime memory something is going to consume until you've loaded it due to dependencies. People will write code that causes dependent allocations that aren't even in the asset. But trying to detect memory consumption outside of runtime is a bad approach. If you're building an engine now, try really, really hard so you can look at an asset on-disk and know how much memory it's going to take up. Doing that saves you so much agitation. Failing that, write a regression tester, go into the level, and see the footprints. Keeping regression testing working\u2014keeping the whole machinery of it working\u2014is a full-time job. At least in a AAA-size team, somebody has to keep that pipeline going. We just didn't have the manpower to maintain that machinery, and that's why it fell down. Allocate, Allocate, Reallocate \u00b6 Memory allocation is always a problem. I don't just mean that the actual use of memory is a problem\u2014the whole apparatus for allocating memory is only becoming more difficult as costs get bigger and we have more memory to allocate. A few companies that have had fixed memory pools 3 have gone back to using dynamic memory pools 4 , just because of how much stuff was coming in and out of memory. It was a nightmare to fix it all. I think people are trying to find a way to get back to more static allocations 5 , or at least block allocations fixed in size. When I say that, I mean the size of the individual allocations being fixed as opposed to the whole pool being fixed ahead of time. It's just hard, and that's why no one is doing it. Maybe someone much smarter than me is doing it! At Insomniac, the way we handle memory allocation is we have one allocator 6 for assets, a different allocator for render memory texture and graphics, a different one for physics, and one more for gameplay components. Each pool has different characteristics, because we're trying to allocate things of different size with different lifetimes. We have different pool allocators for different purposes. Insomniac's allocators are actually not pooled, because the assets are all different sizes. That said, we do use pooled allocators for stuff like physical objects; for example, pedestrians in Spider-Man are coming out of the pool. So in that situation, they are used for things that are all of the same type. Balancing memory usage is an ongoing, iterative process. As you're making your level and you realize you need a few more textures over here, you may have to take some geometric complexity out somewhere else. You might need to put in some more actors, so the textures have to go. It's this continual give-and-take of budgeting. The upshot is that you really need good budget reporting, even more so than being able to decide what your budget is ahead of time. When people put in content, they need to know the weight of the content and they need to know the \"pie chart\" of where all the memory is going. Otherwise, you have no way to make trade-offs. Effectively Using Profiling \u00b6 The ability to sum together scopes across an entire frame is important. That's because you can have a profile scope that says how long to tick this asset's physics component, so if you're doing a hierarchy then it's gonna appear a thousand times. It's helpful to aggregate that whole thing together and be able to plot the aggregated number. A convenient way of exporting a report from a run of the game is as a spreadsheet that you can then import into Excel. Because then, when I'm making a change, I can run the game before the change and then after the change as control and experimental groups. I can also do a Student T-test 7 between them; doing statistics becomes more important. Otherwise when you make a change, you don't know if you actually fixed anything. Also, presenting the data from the the profiling part of the profiler is not that hard; it's presenting the data to the people who can act on it that's hard. For a discussion of how I tracked crashes, go see my GDC talk on Forensic Debugging, where I babble about this concept for about an hour. The short of it is, you start with the call stack, and from there you can just start pulling the thread backwards. Most crashes are due to bad data, so anything you can do to validate the data as it's coming in\u2014before it blows you up\u2014will save you an enormous amount of effort down the line. Concerning general performance, there's one common mistake that I need to bring up. People who use the function tolower : Stop it. Just stop it ! I've been seeing this used to compare case-insensitive strings a lot, but there is absolutely no need to convert uppercase character to lowercase ones. If you've got a string in your engine, just turn it into a hash to start with. I saw an online multiplayer game spend seven percent of its time just turning uppercase characters into lowercase ones; think about how much money it costs to run that on a dedicated server! It's madness! Synchronization of Time \u00b6 Clocks and timers are inherent to video games. For example, look at the update loop and how many frames the game renders every second. Whether to synchronize the multitude of clocks in the engine warrants depends on the game. Albert Einstein conclusively demonstrated that it is impossible for observers and different reference frames to agree on a common clock. Valve uses a multiplicity of clocks, and essentially when you're making a networked game you always have to. That's because you've got your client's clock, the other client's clock, and you get the server clock. Occasionally you want to decouple the simulation from the animation, so you'll also have the animation clock, which takes faster or ticks less constantly. All of these clocks are working at different rates. I don't have a good and complete answer for time synchronization because it depends on the kind of game that you're making, it depends on the way that your animation system works, and it depends on your circumstances. For a game like Spider-Man , we actually had to have\u2014for gameplay purposes\u2014multiple clocks, because in a superhero game time slows down and speeds up in dramatic moments. If Spidey smashes through the ceiling, we want to put things in slow-motion, or pause the game when bringing up the weapon wheel. Stuff like that changes how you approach clocks. Engines Can Change, Little by Little \u00b6 Adding streaming late in the Insomniac engine is a funny story. Insomniac had made an engine for Resistance , and that was used later on for Fuse. When we designed the engine, we went into it with the mentality that our games were primarily in linear indoor environments. As such, we decided the only thing the engine wouldn't support would be open world streaming 8 . So of course, we immediately landed the contract for Sunset Overdrive 9 ! The point that I would learn from this is that even if you think you haven't designed for something from the ground up, you can get there; you just get there a little bit at a time. The whole open world mechanism in Sunset Overdrive came from the fact that we had airlocks 10 in Fuse to control progress. So essentially, we just said \"Well, what if we have airlocks in nine directions but don't actually have airlocks?\" Then you just use that same mechanism a piece at a time. It's often not necessary to go back and rebuild something from the beginning. There's always a step you can take in the right direction. The downside is you end up flooded with technical debt when you do that, but you also end up not having to start over. Keeping the Team in Sync \u00b6 When you're working on larger engine teams and want to publicize bugs, there's bug tracking software you can use. Either JIRA or DevTrack are good options; this is a well-solved problem in tech. On the other hand, publicizing features and changes remains a huge issue. The bigger your team gets, the more of a problem that is. Simply saying \"we need to communicate better\" is a gesture, not an answer. We do the following things at Insomniac; some of them work and some of them don't. When we make a big usability change and we write a new tool, we will do the presentation on that tool to the people who need to use it. That is somewhat useful in that it's a training scenario, but it tends to go in one ear and out the other. There are also people who you don't realize rely on that tool who won't be part of the discussion. We also write up documentation, but hardly anyone reads it. One helpful thing we do occasionally is record video walkthroughs of how to use the tool. The problem of the video is that it's slow and annoying to watch, but the advantage is that you can follow along with while you're doing it. Think of it like you're cooking and following a recipe. As for communicating inside the team, Insomniac has a team of about 25 people, so it's a little bit easier. Our former core team director Mike Acton had us do something to get us in the right mindset for every weekly department meeting; someone on the team would do a changelist review. They would look through the change history and comment on interesting changes on a changelist of another member on the team. The reason that we do this is so that people get in the habit of looking at the change lists that are going into the repository. My advice about having those domain experts came from an era where people were going from PC development to console development more than they are now, and that knowledge hadn't really percolated out. You had studios that had really good PC engines and workflows who were just new to consoles. It is ideal if everybody in the team knows how to get a console devkit 11 set up and working, but it's also not realistic because it's such specific knowledge. In theory, you could write documentation and tutorials and workflows, and that's helpful as far as they go. However, people are always going to have questions that are not answered by the tutorials. As such, it still helps to have somebody who is an expert in the topic. That doesn't need to be their whole job; it just needs to be the person who's really good at doing that thing. Onboarding engineers is a really hard problem no matter where you are. This is actually where some of the advantage of having a domain expert on how to use the devkit comes from, because then you can ask the person for lessons. At Insomniac, we try to address this by assigning a new person to fix something that's broken in an obvious and easily fixed way. By starting with that sort of problem, you start to pull on the thread of all the pieces that you need to work at the company. The bigger your company is, the more training you can afford. With Great Power Comes Great Responsibility \u00b6 Technology is always changing. The amount of power available to consoles and PCs has grown precipitously. PC's seem to have blown up recently, but consoles just keep getting more and more CPU power and memory. People underestimate the importance of RAM because they've been told it affects the amount of art you can get on the screen. Obviously, mobile is a big change\u2014mobile and tablet games are going to be more and more significant over time. AAA isn't so much in that space yet, but a lot of other people are. In terms of revenue, mobile is a huge chunk of the market now, and of course the ubiquitous availability of networking has totally revolutionized the industry. That's because you can always count on the network being filled with people at any given time. This not only applies for multiplayer games, but also for downloadable content, achievements, and all the other good stuff that comes with being connected. Engines are going to face a few challenges ahead. One is that consoles are getting bigger and have more memory, and teams are getting bigger as well. That's a deal; the tools have to cope with it and the engine has to deal with loading more heterogeneous assets at once. Mobile, obviously, is gonna be a bigger and bigger thing. The tricky thing with mobile is that tablets are actually catching up now in terms of computing power, but they're not catching up in terms of electrical power. Electricity conservation has become a surprising concern, as well as the different UI that you have with tablets. I think people will start to rely on networks more and more, but I think actually the penetration of networking is pretty slow. This is especially true in large countries like the United States, so it will be a while before people are building stuff around the cloud. The age of cloud rendering is not here yet. VR could be a big deal, or it could not\u2014the jury is still out for that. A builder is a tool used to process assets from their editable forms (files editable by external software) into a more compact, unreadable file to be used by the engine for a game. The file format is typically proprietary and specific to the engine, and engine metadata is stored within the file. \u21a9 Binary files are files stored in binary format, a format that is easily computer-readable but not really human readable. These are more compact in size than human readable files. \u21a9 Fixed memory pools is a data structure for dynamic allocation of fixed block-size memory chunks. \u21a9 Dynamic memory pools are pools in which memory sizes are determined during runtime, and are changing per allocation rather than being fixed for all. \u21a9 Static allocations is the allocation of memory at compile time, therefore faster than dynamic because the computer doesn't need to switch into kernel mode to grab more memory. \u21a9 An allocator is a data structure that encapsulates memory management and doles out memory on request. There are different types of allocators based on the needs (amount and lifetime) of the memory. \u21a9 The Student T-Test is a statistic test to determine whether a sample set passes hypothesis, the chance the samples are the same or different. For more information see Elan's GDC talk . \u21a9 Open world streaming refers to the the process of \"streaming\" (loading) the world/map (sections of the world) into memory while the player moves throughout the world. While the player moves the game decides which section of the map should be loaded into memory, the engine needs to manage memory and framerate during these times of loading. \u21a9 Sunset Overdrive is a game developed by Insomniac Games for the Xbox One. It is a fast-paced, open world action-adventure, third-person shooter. \u21a9 An airlock in games refer to an area in which loading of the next chunk/section of the map is being performed. Within this zone the next and previous sections can both be in memory, or with limited memory only the next is being loaded into memory. In the first case the player can enter both the previous and next zones (once loading completes), however in the latter the player can neither advance or backtrack their game's progress while the loading happens. \u21a9 A game development kit (devkit) is hardware different from the commercially available version of the hardware, specialized for development. It will have a way of booting with a development version of the game, and modern developer kits have debugging features for the developers. \u21a9","title":"Interview"},{"location":"interviews/ElanRuskin-interview/#wisdom-from-working-at-aaa-studios-for-15-years","text":"Elan Ruskin is a senior engine programmer at Insomniac Games, where he has worked on critically-acclaimed titles including Marvel's Spider-Man and Ratchet & Clank. Prior to his time at Insomniac, Elan worked at Valve, Naughty Dog, and Maxis on many of their flagship titles as a gameplay and engine programmer. When he's not programming, Elan enjoys theater, music, and Star Trek. (The following is the edited transcription of a conversation we had with Elan Ruskin.)","title":"Wisdom from Working at AAA Studios for 15 Years"},{"location":"interviews/ElanRuskin-interview/#designer-driven-tools","text":"The real advantage of data-driven systems is that it's designer-driven; you're decreasing the iteration time for the designers. It puts the ability to make and see changes into the hands of the person actually making the content, and away from the programmer-compiler loop which requires programmers to develop and compile before any change can take place. The problems with data-driven design are 1) the code has to be a little more complex to support this flexibility and 2) you're loading bulkier content. You can get around that, though, if you use a builder 1 to pack down the content into something that code can load more efficiently. What surprised me about data-driven systems is that it ended up being much less of a problem than I expected. It turns out that you load things relatively infrequently, and if you do end up loading them frequently, you can bake them down. So as much as it would make the me of 20 years ago sad to hear me say it, having things in an open-text format that gets parsed turns out just not to be a performance issue. I'm not saying performance doesn't matter! But that turned out to not be the issue. The other problem with data-driven design is that people will do weird, unexpected, and strange things with data that you didn't anticipate and will possibly break your systems. That's because the connection between changing the data and something in the game breaking is not quite as obvious as it is with code, where you can set a breakpoint and see exactly what happened. The importance of good error reporting, diagnostics, and designing the authoring tools in a way that prevents people from getting themselves into trouble was not clear to me when I began engine programming. With tools, the things that you can't anticipate are usually the problem, because the designers and artists are always trying to solve their own problems. They're not out to break the tools or fumble around oafishly; they have specific needs, like \"I need this tree to have another tree on top of it so I can turn one of them off and the other one on, because it's winter and we need the leaves to be gone.\" In this scenario, they might not know that if you have two things in the same place at the same time, it causes a problem. That's something they would have no way to know about until they did it. So really, the way to deal with that is to find a way to prevent people from making the mistake, which would then make the causal connection obvious. Ideally, we should make it impossible to do bad things, but again, that comes back to anticipating things. It's also not a reliable strategy to just go over and yell at people for having done the content wrong or give them a gigantic document that explains how to use your tools. You can't expect people to hold that much content in their heads at once. When developing tools for writers, an assumption a lot of people make is that writers are not technical and therefore need easier tools, which is completely untrue. Writers can learn computers as well as anyone else! What I learned from my time at Valve is writers need flexibility; any given line of dialogue goes through many iterations to get it right. The thing to be cognizant of is that writers are part of a whole pipeline of content that has to get made. The writer's text appears in the game while it's still being prototyped, but even at that time we have to cast the voice actor and record them. Then, we put the voiced lines into the game only to realize the dialogue is clunky. So we have to change the line, go back to the booth, and repeat the process. This puts the writers in the middle of a pipeline that has audio waggling at the other end. The advantage of building a complete suite of tools is you can integrate the whole process of tracking where lines of dialogue are located in the game, as well as who's been cast to play it, and whether or not it's been recorded and localized yet. In Campo Santo's talk on Firewatch , they discussed how they integrated the dialogue system with the \"recording-tracking\" system. Taking that approach saved them a lot of agita.","title":"Designer-Driven Tools"},{"location":"interviews/ElanRuskin-interview/#intimate-bond-of-engine-and-tools","text":"At Insomniac, the engine team and the tools team are the same team. That works well for us because the team is not especially big, and because the engine and the tools are intimately bound. The engine is loading the assets, the builders are cooking the assets into a binary 2 , and the tools are feeding the things to the builders. These are not separate operations; it's all the same lump of data. As teams grow, you'll need to specialize the labor because they are different skills to an extent. I personally don't think there's that much value in separating the engine runtime from the tools in terms of being different teams, unless your studio is gigantic. In that case, you have to for organizational purposes. Along the same lines, it's almost a necessity to version the tools and the engines together. Part of this is the obvious reason that if you change the runtime format and the tools need to export in your format, the engine needs to be able to read it. Again, they're operating on the same data. What's more, anytime you need a new capability the engine, the tools have to support it, so they really move in lockstep. One of our attempts at improving our tools ecosystem was to use web tools. For the entire rundown of why Insomniac went with web tools and why we stopped using them, you should see Andreas Frederiksson 's GDC talk . The reason for moving towards web tools was that we thought it would be much more flexible to make a UI in the web, and also that it would be easier to hire people who have a web UI experience than people who have C++ UI experience. That just turned out not to be the case. We ended up hiring the people that we would hire anyway, and then teaching them JavaScript. What's more, the scaling issues of web tools are enormous. The web is good at doing 100 or 200 of something; it's not so good at doing 30,000 of something. So just performance and memory were gigantic issues, in addition to all the other issues like Chrome continually breaking underneath their feeds, JavaScript is just bad! On the usability side, we made our new web tools work almost exactly like the old tools, only with less bugs. We tried to keep the interface consistent. The problems that we ran into were that the new tools didn't have all of the features of the old tools to begin with, because we couldn't rebuild everything at once all in one piece. As a result, the team would have to learn how to work around the missing pieces. Because we kept the the workflow the same, it was fine, plus everything got faster and less buggy. We have also made several attempts at making a tool to track feature regression. We wrote a tool that loads every level of the game, takes a snapshot of the memory, and then unloads it and repeats the process. From there it would just create this whole spreadsheet report that nobody ever looked at, because it was always days out of date. We had another tool that was meant to interrogate a level; it would look at all of its dependencies and the dependencies' dependencies, and recursively try to see how much memory everything would consume. The problem of that is it's an estimate, because you often don't know how much runtime memory something is going to consume until you've loaded it due to dependencies. People will write code that causes dependent allocations that aren't even in the asset. But trying to detect memory consumption outside of runtime is a bad approach. If you're building an engine now, try really, really hard so you can look at an asset on-disk and know how much memory it's going to take up. Doing that saves you so much agitation. Failing that, write a regression tester, go into the level, and see the footprints. Keeping regression testing working\u2014keeping the whole machinery of it working\u2014is a full-time job. At least in a AAA-size team, somebody has to keep that pipeline going. We just didn't have the manpower to maintain that machinery, and that's why it fell down.","title":"Intimate Bond of Engine and Tools"},{"location":"interviews/ElanRuskin-interview/#allocate-allocate-reallocate","text":"Memory allocation is always a problem. I don't just mean that the actual use of memory is a problem\u2014the whole apparatus for allocating memory is only becoming more difficult as costs get bigger and we have more memory to allocate. A few companies that have had fixed memory pools 3 have gone back to using dynamic memory pools 4 , just because of how much stuff was coming in and out of memory. It was a nightmare to fix it all. I think people are trying to find a way to get back to more static allocations 5 , or at least block allocations fixed in size. When I say that, I mean the size of the individual allocations being fixed as opposed to the whole pool being fixed ahead of time. It's just hard, and that's why no one is doing it. Maybe someone much smarter than me is doing it! At Insomniac, the way we handle memory allocation is we have one allocator 6 for assets, a different allocator for render memory texture and graphics, a different one for physics, and one more for gameplay components. Each pool has different characteristics, because we're trying to allocate things of different size with different lifetimes. We have different pool allocators for different purposes. Insomniac's allocators are actually not pooled, because the assets are all different sizes. That said, we do use pooled allocators for stuff like physical objects; for example, pedestrians in Spider-Man are coming out of the pool. So in that situation, they are used for things that are all of the same type. Balancing memory usage is an ongoing, iterative process. As you're making your level and you realize you need a few more textures over here, you may have to take some geometric complexity out somewhere else. You might need to put in some more actors, so the textures have to go. It's this continual give-and-take of budgeting. The upshot is that you really need good budget reporting, even more so than being able to decide what your budget is ahead of time. When people put in content, they need to know the weight of the content and they need to know the \"pie chart\" of where all the memory is going. Otherwise, you have no way to make trade-offs.","title":"Allocate, Allocate, Reallocate"},{"location":"interviews/ElanRuskin-interview/#effectively-using-profiling","text":"The ability to sum together scopes across an entire frame is important. That's because you can have a profile scope that says how long to tick this asset's physics component, so if you're doing a hierarchy then it's gonna appear a thousand times. It's helpful to aggregate that whole thing together and be able to plot the aggregated number. A convenient way of exporting a report from a run of the game is as a spreadsheet that you can then import into Excel. Because then, when I'm making a change, I can run the game before the change and then after the change as control and experimental groups. I can also do a Student T-test 7 between them; doing statistics becomes more important. Otherwise when you make a change, you don't know if you actually fixed anything. Also, presenting the data from the the profiling part of the profiler is not that hard; it's presenting the data to the people who can act on it that's hard. For a discussion of how I tracked crashes, go see my GDC talk on Forensic Debugging, where I babble about this concept for about an hour. The short of it is, you start with the call stack, and from there you can just start pulling the thread backwards. Most crashes are due to bad data, so anything you can do to validate the data as it's coming in\u2014before it blows you up\u2014will save you an enormous amount of effort down the line. Concerning general performance, there's one common mistake that I need to bring up. People who use the function tolower : Stop it. Just stop it ! I've been seeing this used to compare case-insensitive strings a lot, but there is absolutely no need to convert uppercase character to lowercase ones. If you've got a string in your engine, just turn it into a hash to start with. I saw an online multiplayer game spend seven percent of its time just turning uppercase characters into lowercase ones; think about how much money it costs to run that on a dedicated server! It's madness!","title":"Effectively Using Profiling"},{"location":"interviews/ElanRuskin-interview/#synchronization-of-time","text":"Clocks and timers are inherent to video games. For example, look at the update loop and how many frames the game renders every second. Whether to synchronize the multitude of clocks in the engine warrants depends on the game. Albert Einstein conclusively demonstrated that it is impossible for observers and different reference frames to agree on a common clock. Valve uses a multiplicity of clocks, and essentially when you're making a networked game you always have to. That's because you've got your client's clock, the other client's clock, and you get the server clock. Occasionally you want to decouple the simulation from the animation, so you'll also have the animation clock, which takes faster or ticks less constantly. All of these clocks are working at different rates. I don't have a good and complete answer for time synchronization because it depends on the kind of game that you're making, it depends on the way that your animation system works, and it depends on your circumstances. For a game like Spider-Man , we actually had to have\u2014for gameplay purposes\u2014multiple clocks, because in a superhero game time slows down and speeds up in dramatic moments. If Spidey smashes through the ceiling, we want to put things in slow-motion, or pause the game when bringing up the weapon wheel. Stuff like that changes how you approach clocks.","title":"Synchronization of Time"},{"location":"interviews/ElanRuskin-interview/#engines-can-change-little-by-little","text":"Adding streaming late in the Insomniac engine is a funny story. Insomniac had made an engine for Resistance , and that was used later on for Fuse. When we designed the engine, we went into it with the mentality that our games were primarily in linear indoor environments. As such, we decided the only thing the engine wouldn't support would be open world streaming 8 . So of course, we immediately landed the contract for Sunset Overdrive 9 ! The point that I would learn from this is that even if you think you haven't designed for something from the ground up, you can get there; you just get there a little bit at a time. The whole open world mechanism in Sunset Overdrive came from the fact that we had airlocks 10 in Fuse to control progress. So essentially, we just said \"Well, what if we have airlocks in nine directions but don't actually have airlocks?\" Then you just use that same mechanism a piece at a time. It's often not necessary to go back and rebuild something from the beginning. There's always a step you can take in the right direction. The downside is you end up flooded with technical debt when you do that, but you also end up not having to start over.","title":"Engines Can Change, Little by Little"},{"location":"interviews/ElanRuskin-interview/#keeping-the-team-in-sync","text":"When you're working on larger engine teams and want to publicize bugs, there's bug tracking software you can use. Either JIRA or DevTrack are good options; this is a well-solved problem in tech. On the other hand, publicizing features and changes remains a huge issue. The bigger your team gets, the more of a problem that is. Simply saying \"we need to communicate better\" is a gesture, not an answer. We do the following things at Insomniac; some of them work and some of them don't. When we make a big usability change and we write a new tool, we will do the presentation on that tool to the people who need to use it. That is somewhat useful in that it's a training scenario, but it tends to go in one ear and out the other. There are also people who you don't realize rely on that tool who won't be part of the discussion. We also write up documentation, but hardly anyone reads it. One helpful thing we do occasionally is record video walkthroughs of how to use the tool. The problem of the video is that it's slow and annoying to watch, but the advantage is that you can follow along with while you're doing it. Think of it like you're cooking and following a recipe. As for communicating inside the team, Insomniac has a team of about 25 people, so it's a little bit easier. Our former core team director Mike Acton had us do something to get us in the right mindset for every weekly department meeting; someone on the team would do a changelist review. They would look through the change history and comment on interesting changes on a changelist of another member on the team. The reason that we do this is so that people get in the habit of looking at the change lists that are going into the repository. My advice about having those domain experts came from an era where people were going from PC development to console development more than they are now, and that knowledge hadn't really percolated out. You had studios that had really good PC engines and workflows who were just new to consoles. It is ideal if everybody in the team knows how to get a console devkit 11 set up and working, but it's also not realistic because it's such specific knowledge. In theory, you could write documentation and tutorials and workflows, and that's helpful as far as they go. However, people are always going to have questions that are not answered by the tutorials. As such, it still helps to have somebody who is an expert in the topic. That doesn't need to be their whole job; it just needs to be the person who's really good at doing that thing. Onboarding engineers is a really hard problem no matter where you are. This is actually where some of the advantage of having a domain expert on how to use the devkit comes from, because then you can ask the person for lessons. At Insomniac, we try to address this by assigning a new person to fix something that's broken in an obvious and easily fixed way. By starting with that sort of problem, you start to pull on the thread of all the pieces that you need to work at the company. The bigger your company is, the more training you can afford.","title":"Keeping the Team in Sync"},{"location":"interviews/ElanRuskin-interview/#with-great-power-comes-great-responsibility","text":"Technology is always changing. The amount of power available to consoles and PCs has grown precipitously. PC's seem to have blown up recently, but consoles just keep getting more and more CPU power and memory. People underestimate the importance of RAM because they've been told it affects the amount of art you can get on the screen. Obviously, mobile is a big change\u2014mobile and tablet games are going to be more and more significant over time. AAA isn't so much in that space yet, but a lot of other people are. In terms of revenue, mobile is a huge chunk of the market now, and of course the ubiquitous availability of networking has totally revolutionized the industry. That's because you can always count on the network being filled with people at any given time. This not only applies for multiplayer games, but also for downloadable content, achievements, and all the other good stuff that comes with being connected. Engines are going to face a few challenges ahead. One is that consoles are getting bigger and have more memory, and teams are getting bigger as well. That's a deal; the tools have to cope with it and the engine has to deal with loading more heterogeneous assets at once. Mobile, obviously, is gonna be a bigger and bigger thing. The tricky thing with mobile is that tablets are actually catching up now in terms of computing power, but they're not catching up in terms of electrical power. Electricity conservation has become a surprising concern, as well as the different UI that you have with tablets. I think people will start to rely on networks more and more, but I think actually the penetration of networking is pretty slow. This is especially true in large countries like the United States, so it will be a while before people are building stuff around the cloud. The age of cloud rendering is not here yet. VR could be a big deal, or it could not\u2014the jury is still out for that. A builder is a tool used to process assets from their editable forms (files editable by external software) into a more compact, unreadable file to be used by the engine for a game. The file format is typically proprietary and specific to the engine, and engine metadata is stored within the file. \u21a9 Binary files are files stored in binary format, a format that is easily computer-readable but not really human readable. These are more compact in size than human readable files. \u21a9 Fixed memory pools is a data structure for dynamic allocation of fixed block-size memory chunks. \u21a9 Dynamic memory pools are pools in which memory sizes are determined during runtime, and are changing per allocation rather than being fixed for all. \u21a9 Static allocations is the allocation of memory at compile time, therefore faster than dynamic because the computer doesn't need to switch into kernel mode to grab more memory. \u21a9 An allocator is a data structure that encapsulates memory management and doles out memory on request. There are different types of allocators based on the needs (amount and lifetime) of the memory. \u21a9 The Student T-Test is a statistic test to determine whether a sample set passes hypothesis, the chance the samples are the same or different. For more information see Elan's GDC talk . \u21a9 Open world streaming refers to the the process of \"streaming\" (loading) the world/map (sections of the world) into memory while the player moves throughout the world. While the player moves the game decides which section of the map should be loaded into memory, the engine needs to manage memory and framerate during these times of loading. \u21a9 Sunset Overdrive is a game developed by Insomniac Games for the Xbox One. It is a fast-paced, open world action-adventure, third-person shooter. \u21a9 An airlock in games refer to an area in which loading of the next chunk/section of the map is being performed. Within this zone the next and previous sections can both be in memory, or with limited memory only the next is being loaded into memory. In the first case the player can enter both the previous and next zones (once loading completes), however in the latter the player can neither advance or backtrack their game's progress while the loading happens. \u21a9 A game development kit (devkit) is hardware different from the commercially available version of the hardware, specialized for development. It will have a way of booting with a development version of the game, and modern developer kits have debugging features for the developers. \u21a9","title":"With Great Power Comes Great Responsibility"},{"location":"interviews/ElanRuskin-video/","text":"Switching to Solid State Drives \u00b6","title":"Video Story"},{"location":"interviews/ElanRuskin-video/#switching-to-solid-state-drives","text":"","title":"Switching to Solid State Drives"},{"location":"interviews/JasonGregory-advice/","text":"Jason is a USC lecturer on game and game engine development and author of the ubiquitous _Game Engine Architecture book. He is also a lead programmer at Naughty Dog, where he has worked on the_ Uncharted series and The Last of Us,_ and he is currently working on_ The Last of Us Part II. We approached Jason because of his experience with not only game engine development but also in teaching game engine development at the University of Southern California. His years spent in the industry also provides a good standpoint to discuss the iterations game engine development has gone through. We did a Skype audio call with him. Advice ( not verbatim): \u00b6 Engine Design Almost all engine subsystems have influence from other disciplines like artists and designers. Jason sees his job as as providing tools for other disciplines, such as artists and designers, to express themselves. Understanding their needs, what their workflow is like, and how he can improve upon it. A lot of the job is working with the design team to figure out the designs for a complex multiplayer game. Day to day, it's often deciding which way to go when solving a problem: Solution A or Solution B. Oftentimes the artists and designers will come over and bring up a concern or difficulty, and the design of the software will be altered. High-level vs Low-level Any piece of software is built on layers, and the lower you get in the stack the more important the hardware is. The upper layers of the Naughty Dog engine would work on any console because the concepts are still the same. The differences happen on things like single-threading to multi-threading conversion, interfacing with GPU or other specialized services, etc. How we approach optimization can really depend what level you're working on. The Naughty Dog team has a rule against prematurely optimizing or over-optimizing; developers working on high-level gameplay systems are encouraged to just slap something together and get it on the screen. Then they iterate with the designers or artists a ton until the system works really well. When you do have to solve a low-level problem, it's very interesting and sometimes involves assembly language programming. You may want to write a blogpost about solving one of these problems, but the truth is that you just do good, structured C++ most of the time. Structuring Your First Engine It's not bad to integrate pre-existing things into a game engine in order to build it, because the act of making a game engine is about getting many distinct systems to work together. Focus on a very simple core. Keep it as simple as you can! It's more important that you get something that's bare minimum but will function. Something that shows, \"it works!\" Then build on this core, adding more features and iterating on functionality until you have a fully-fleshed out system. Maybe use a pre-existing game design and build the engine around that. You should even develop a demo that works in your engine. Be prepared that by the end of it, you're gonna have something that's not close at all to anything big! One thing you could do is to take an older game engine like Quake and replace subsystems on it. Choosing Subsystems (Networking vs Physics) Choose whatever your team is more excited about! It's all about what you find more interesting! In terms of absolute need, it depends on the game you're making. There's probably less available learning resources on networking compared to physics. You could, in theory, use PhysX or ODE. Multiplayer and physics don't usually mesh well anyway, so keeping physics would mean removing the networked multiplayer.","title":"Jason Gregory"},{"location":"interviews/JasonGregory-advice/#advice-not-verbatim","text":"Engine Design Almost all engine subsystems have influence from other disciplines like artists and designers. Jason sees his job as as providing tools for other disciplines, such as artists and designers, to express themselves. Understanding their needs, what their workflow is like, and how he can improve upon it. A lot of the job is working with the design team to figure out the designs for a complex multiplayer game. Day to day, it's often deciding which way to go when solving a problem: Solution A or Solution B. Oftentimes the artists and designers will come over and bring up a concern or difficulty, and the design of the software will be altered. High-level vs Low-level Any piece of software is built on layers, and the lower you get in the stack the more important the hardware is. The upper layers of the Naughty Dog engine would work on any console because the concepts are still the same. The differences happen on things like single-threading to multi-threading conversion, interfacing with GPU or other specialized services, etc. How we approach optimization can really depend what level you're working on. The Naughty Dog team has a rule against prematurely optimizing or over-optimizing; developers working on high-level gameplay systems are encouraged to just slap something together and get it on the screen. Then they iterate with the designers or artists a ton until the system works really well. When you do have to solve a low-level problem, it's very interesting and sometimes involves assembly language programming. You may want to write a blogpost about solving one of these problems, but the truth is that you just do good, structured C++ most of the time. Structuring Your First Engine It's not bad to integrate pre-existing things into a game engine in order to build it, because the act of making a game engine is about getting many distinct systems to work together. Focus on a very simple core. Keep it as simple as you can! It's more important that you get something that's bare minimum but will function. Something that shows, \"it works!\" Then build on this core, adding more features and iterating on functionality until you have a fully-fleshed out system. Maybe use a pre-existing game design and build the engine around that. You should even develop a demo that works in your engine. Be prepared that by the end of it, you're gonna have something that's not close at all to anything big! One thing you could do is to take an older game engine like Quake and replace subsystems on it. Choosing Subsystems (Networking vs Physics) Choose whatever your team is more excited about! It's all about what you find more interesting! In terms of absolute need, it depends on the game you're making. There's probably less available learning resources on networking compared to physics. You could, in theory, use PhysX or ODE. Multiplayer and physics don't usually mesh well anyway, so keeping physics would mean removing the networked multiplayer.","title":"Advice (not verbatim):"},{"location":"interviews/JeetShroff-FlorianStrauss-interview/","text":"Upcoming Interview with Jeet Shroff and Florian Strauss \u00b6 We had an interview with Jeet Shroff and Florian Strauss of Sony Santa Monica Studios, where we discussed the merits of game engines at the scale of AAA studios. Sign up on our mailing list to know when we publish it online! Subscribe to our mailing list Get notifications about the upcoming blogs and interviews!","title":"Upcoming Interview with Jeet Shroff and Florian Strauss"},{"location":"interviews/JeetShroff-FlorianStrauss-interview/#upcoming-interview-with-jeet-shroff-and-florian-strauss","text":"We had an interview with Jeet Shroff and Florian Strauss of Sony Santa Monica Studios, where we discussed the merits of game engines at the scale of AAA studios. Sign up on our mailing list to know when we publish it online!","title":"Upcoming Interview with Jeet Shroff and Florian Strauss"},{"location":"interviews/JeffPreshing-interview/","text":"An Engine Developer's Toolbox \u00b6 Jeff Preshing is a programmer with close to 20 years of experience working for various game companies and non-game companies. He recently wrote a C++ game engine from scratch and is using it to make a dope cartoon-action game for mobile. You should follow his blog at preshing.com . He likes rock climbing and Vindaloo curry. (The following is the edited transcription of a conversation we had with Jeff Preshing.) The Arc80 Engine Architecture \u00b6 Back when I was starting my career, I considered myself a good programmer, but I would look at existing games and game engines and just be unable to even fathom how everything all fit together. That's the most confusing part, and to illustrate, I've provided my own engine's architecture here. This can give you an architectural overview of my own game engine; it's something that I would have liked to see early on in my career. You can find other overviews like that online, but having a particular view of the modules, the dependencies between them, and how an engine goes from high level to low level is a good start. That was by far the most confusing thing: Getting the big picture of it all. Engine Design Principles \u00b6 I have my own set of personal design principles behind how I write and maintain my own engine. They probably won't apply to anybody else, but that's what I've chosen and I think it's an interesting way of looking at things. To the extent that I can, I avoid building custom tools. I don't want to spend time building them. That's a big part of real AAA game development at a studio\u2014because you've got hundreds of people on the team, the investment that you make in user-friendly tools pays off. That said, it's a big investment, and for me personally I don't want to spend time on custom tools. To get around that, I leverage Blender 1 ; I do all my level editing and character design in Blender. Another one of my personal design principles is that I always want to focus on the application running on the device. All my effort is targeted at keeping that lean and mean and efficient, so rather than focus on tools, I focus on the game application itself. The other principle that I follow to an extreme is maintaining reusable modules, because everyone likes modularity. The way I make sure that my modules are reusable is by building small applications that compile and link with individual components, and I can't do that unless the components are decoupled from each other. When I say components, every small box on my architecture diagram is a component. Each one has its own directory of header files, and when each box is compiled, it's a library and it has access to the headers of specific other modules that it's allowed to depend on. Because of that, I'm really aggressive in terms of modularity. What I found at game studios is that engines tend to be monolithic code bases, so I'm deliberately making an effort to go the other way. I would almost say that what I've got is more of an SDK 2 than an engine\u2014I just happen to be making a game out of this SDK. Creating Your First Engine \u00b6 For my own engine, I write everything. I've written my own containers, string class and file system class, which includes the physics and the audio engine. I use almost nothing from the standard library, except for type traits 3 . If you're a novice game engine programmer and are setting out to do your first project, you don't want to do that . For everything that I've written, there are really good open-source equivalents. So I don't know if it's a good or bad idea, but one way to build your own initial game engine is to look at it as an integration exercise: Go get GLM 4 , download Bullet 5 , download some renderer like Horde3D, and so on. You've got libraries for a lot of things, like libraries that import 3D models and everything, so you can stitch together an engine out of those. I think that's a good way of going about it, since it will expose you to the interfaces to each component. That's the best starting point, and then depending on how your needs evolve, maybe you'll end up opening those black boxes and modifying things under the hood. For a beginner, you can integrate available code rather than doing everything from scratch, because that's taken me a long time. But that's just my style. Skills of Engine Development \u00b6 These were skills I didn't expect to need in the beginning of my career, but I ended up developing them during my career. The reality is that every engine programmer ends up exposed to them, so don't be surprised to find yourself getting better at these skills by necessity. Systems Integration \u00b6 I call the first one Systems Integration . It's actually a big category, as it has to do with integrating third-party libraries\u2014taking separate codebases and making them build and link together to run as one thing. Another systems integration exercise would be porting to new platforms, as well as doing big three-way merges (you've branched a library, integrated it at some point, then maybe you've made changes both to your code and inside the library itself). That happens a lot in the industry; months later, there's a new version of a library you want to integrate, which is the latest and greatest of that dependency 6 . That can end up being a huge task that can take weeks or months to accomplish. Between all of those things, Systems Integration is a valuable category of skills to have. However, a lot of C++ programmers want to avoid it because they hate it. Many C++ programmers hate integrating libraries; they want everything to be a header-only library, just to make their lives easier. If you learn how it works, though, you're doing yourself a big service. Getting more familiar with it and tools like CMake 7 to help you generate build pipelines instead of tweaking Visual Studio properties is just a good skill to have. It's especially useful when you're going cross-platform. Low-level Debugging \u00b6 Low-level Debugging is another great skill to have, especially when you are tracking down crashes in the final optimized build. Especially at the end of the project, you'll have testers hammering on the optimized build, so there's bound to be crashes and issues that don't happen in a debug build. The ability to diagnose and fix those bugs is useful because you want to ship a stable game! How you develop that skill is a different story, but at some point you or someone on your team will have to go into the disassembly 8 window or the memory view and figure things out at that level. It's kind of considered a black art, which is a bit of a shame because it doesn't really have to be. I started developing my low-level debugging skills when I had no other choice but to figure it out. In 2005, we needed to ship a project but it kept crashing. I knew it was possible to solve the problem because, at an earlier job, I had a teammate who would constantly go look at crash dumps and disassembly and memory views. I would always think, \"What the heck is this guy doing?\" I asked him how he did it, but he never explained it to me; he just said \"I like computers.\" So I knew that was a skill that was possible to have. I don't remember exactly how I picked it up, but at some point that became my bread and butter at Ubisoft. My teammates there would always come to me to if they had a crash and I would help them out. Eventually it got to the point where Ubisoft started giving an in-house course on this subject. In my experience, it's something that's passed on from one person to the other, and yet no one really seems to want to get down to that level. Everyone's more interested in what sort of high-level programming paradigms can help us be more productive, but the most productive thing is to ship the game, and to ship the game you have to fix these issues at a low level. Profiling \u00b6 The next skill on the list is Profiling . Everyone agrees that it's important\u2014you want to profile before you decide where and how you should optimize. How you use profilers is kind of similar to low-level debugging in that it's a skill passed on from one person to another on a project. I would point to a talk I gave on profiling at a student conference . I think it's a pretty good introduction to the different types of profilers and examples of how you would use them in practice. Concurrency \u00b6 The fourth skill worth developing is Concurrency , or multi-threading, which is probably no surprise considering everyone talks about it. Everyone knows it's part of life as a game developer. Iterative Development \u00b6 Number five, Iterative Development , is the ability to look at feature goals and envision how it'll work at a level. If you can break that into bite-sized tasks and decide what's your rollout plan (which tasks you'll do in what order) is a good skill. Managers love it because they want these enumerated tasks, which helps them create a schedule. Development Journal \u00b6 I also think an engine developer should keep a Journal . When you're working on a task or debugging a problem, you might open NotePad or a text editor and start taking notes, you could copy and paste call stacks 9 , and maybe you take notes when you're researching things on Google and Stack Overflow or even while brainstorming. I especially make a lot of notes when I'm brainstorming\u2014looking at all my options and alternatives and trying to figure out what's the best one. So when I talk about keeping a journal, I'm not talking about keeping a diary of what I did each day. What I mean is having a place to store your notes so you can go back to them in the future. The way I personally do this is I have a folder full of text files, and it's just organized by date. I don't even try to organize by category. I just have one text file per month and at the start of the day, if I have something to note, I put a heading for the date and I jot things down. I can't work without doing that personally, because I'm constantly trying to remember what my observations were on a subject in the past. It's great to be able to search that directory of my notes and refresh my memory that way. I think it's a useful skill, but maybe every person has their own way of working in that respect. Reflection and Serialization \u00b6 The next two things on my list of skills are Reflection and Serialization . In the industry, every game engine has their own approach for this. You don't even need to have an approach at all, but just be aware that there are varying approaches to reflecting data structures. There's different reasons to have reflection, ranging from serialization to managing shader parameters to networking. Being able to think in those terms is a particularly useful skill for game development because C++ has nothing for runtime reflection. Well, there is RTTI 10 , but RTTI doesn't provide any information about data members, so it can't help you implement things like serialization. Professionalism \u00b6 The final thing is a soft skill: being Professional . You'll have to do most of your work on a team, so obviously it's a matter of treating your teammates with respect. This was a mistake I made at the beginning, when I really wanted to advance in my career and I really wanted to shine as a good contributor to the project. A better way of thinking, though, is to help make sure that the game ships. Five to ten years from now, no one will remember who shined and who didn't, but if you have a shipped project on your resume, that's like currency; employers love to see that. So you need to stay professional and not get attached to your way of doing things. Letting other people tackle the problem their way, even if you don't think it's the best way, will minimize friction on the team, and that helps the game ship. Console Development Experiences \u00b6 The first time I did a seek-free loading 11 system, it was only for the Xbox console on the game Rainbow Six: Lockdown . Since we were loading from DVDs, seeks 12 were expensive, so you wanted to avoid them. My job was looking at the problem of how the game is opening all these files and reading from different places. All I did was implement a mode where the game runs, and as it starts up it's logging which files it's reading from and in what positions. It then takes that data and makes a linear version of the same data with no seeks. I also added another mode of running the game that uses the seek-free files, so I was able to just focus on the strategy of how and when the data is accessed. That also included fallbacks, so if the game loaded a little differently the next time, it wouldn't totally break. At that time, though, I didn't have to worry so much about platform differences. The second time that I did it I already had experience with the strategy, but I had to do it in a cross-platform way: for PC, PS3, and Xbox 360. That meant it was just a matter of reimplementing the same thing, but using cross-platform primitives. The engine already had some cross-platform wrappers for low-level things like opening files and such, but not everything. I remember on the PS3, there was a limit to how many files you could have open, and that limit was very small. At some point, I had to make a wrapper layer between the engine and the hardware that let the game pretend it was opening a lot of files, but if it wasn't actually reading from those open file handles\u2014I was closing the files and it would be opening them on next access. The reason I had to do a lot of PS3 optimization was that the Rainbow Six: Vegas engine was actually a branch of Unreal Engine 3. That project actually branched very early, before Unreal 3 was even officialized, so we didn't have a lot of the work that Epic Games did that we could just integrate. That was actually one of my favorite projects because when Rainbow Six: Vegas shipped on PlayStation 3, it didn't perform very well: It was pixely, it was blurry, and it had a low framerate. But on Rainbow Six: Vegas 2, we optimized things and made it look good. The PS3 version almost became indistinguishable from the Xbox 360 version. We didn't even use the SPU's 13 a whole lot, as I recall. I remember SPU's were used by audio mixing, but I don't recall that we used them for the engine itself. Most of it was basics, like making sure you had the best frame buffer 14 pixel format for the GPU because the PS3 needed a specific set of conditions maintained for Z-buffering 15 to work in the optimal way. It was hierarchical Z, and it was very easy to break that, so using the tools that Sony gave us to identify where it was broken and fixing those, that was a huge boost. Avoiding redundant state changes being sent to the GPU was also a huge win. So a lot of it was basics, not crazy concurrent SPU stuff. I saw a lot more SPU stuff on later projects, but I didn't gain that much experience developing for SPU's myself. I did a lot of debugging on them for sure, but not so much on the development side. How Profiling Can Differ \u00b6 Profiling on Child of Light was completely different from profiling on Assassin's Creed: Unity . Child of Light was a much simpler game engine by far; when you put the controller down and let the character sit there in an environment, activity on the CPU and GPU was pretty consistent from one frame to the next. With Assassin's Creed: Unity , on the other hand, it was a totally different thing because the assassin was often in a crowd, and the crowd is walking all over the place and bumping into each other. That movement causes a lot of fluctuation in CPU activity from one frame to the next, and that made it harder to profile. It kind of bummed me out a little bit, but it seemed like most of the Assassin's Creed team's approach to optimization was just moving stuff around from one core to the next. They have this job system and you capture a profile and you see which jobs ended up on which cores, and if there were holes in the schedule, they would try to move jobs around. There was not as much direct optimization going on. To be fair, it was such a mature engine that it was hard to find those optimizations. The Game Engine Marketplace \u00b6 If you ask me, \"What's the next big thing in game engines?\" I would say the Arc80 Engine, obviously! Just kidding. I mean, I would love it if I could use the Arc80 Engine for the rest of my career. But I don't believe in the \"next big thing\" in terms of a predetermined path that the industry is inevitably walking along. Right now, there are people who think that real-time ray tracing will be the next big thing in game engines, and there are people who think that cloud gaming is the future. I don't know about either of those things. I do know that history is littered with inventions that people thought were going to become huge, but weren't, because market forces just didn't support the idea. Game engines are just another marketplace. The ideas that succeed in this market will be the ones that meet a demand, either by enabling more interesting games for consumers, by making developers more productive, or both, or in a different way altogether. The only way something will become \"the next big thing\" is if an idea is so good, so compelling that a big segment of the industry adopts it. But by necessity, that will always be hard to predict, because if an idea was both compelling and obvious, people would already have adopted it! The point is that future of game engines has not yet been written. As engine programmers, we're the ones who get to write it. Interview conducted October 24, 2018. Blender is an open-source 3D computer graphics software used in creating 3D models, animations, and interactive applications. \u21a9 A software developer kit (SDK) is a set of programs used in developing another program. \u21a9 Type traits define a compile-time templated-base interface to query or modify the properties of types, for more see type support . \u21a9 OpenGL Mathematics (GLM) is a mathematics library based off of OpenGL specifications, that contains definitions for typical math constructs used in graphics. \u21a9 Bullet Physics is a real-time physics simulation library, a physics engine, which simulate collisions for soft and rigid body dynamics. \u21a9 Dependencies are links that are required between programs, such that one program is reliant on another. \u21a9 CMake is a cross-platform, open-source application for managing the build process of software in a compiler-independent way. \u21a9 Disassembly is the assembly language, translated from machine code, of a program; it is the compiler's version of the program. \u21a9 The call stack is a stack data structure that stores the information about the active routines of a computer. It can be walked up/down into the callee functions and function definitions, respectively. \u21a9 Run-time type information/identification (RTTI) is specific to C++ in that information about an object's type is available at runtime. \u21a9 Seek-free loading is a system which is able to read a file \"free\" (without) \"seeking\", (searching) a file, for data/position within a file. Seeking causes disk activity which is generally slower than CPU performance. \u21a9 A seek is a programming concept related to file reading, where a program has a file pointer associated with a position and a seek moves the pointer to a specific position within that file. \u21a9 A Synergistic Processing Unit (SPU) is the processing unit associated with the Cell architecture of the PS3. The PS3 had seven as part of its hardware, only six of which were usable by game developers. \u21a9 Frame buffers are a portion of RAM containing a bitmap of the display, containing the data for that given frame on the video display. \u21a9 The z-buffer also known as the depth buffer, contains information regarding the distance from the camera, the depth. Z-buffering can also refer to the technique in which pixels are culled from the frame, not rendered, because another pixel's depth is closer to the camera, therefore the pixel in the background is being covered. \u21a9","title":"Interview"},{"location":"interviews/JeffPreshing-interview/#an-engine-developers-toolbox","text":"Jeff Preshing is a programmer with close to 20 years of experience working for various game companies and non-game companies. He recently wrote a C++ game engine from scratch and is using it to make a dope cartoon-action game for mobile. You should follow his blog at preshing.com . He likes rock climbing and Vindaloo curry. (The following is the edited transcription of a conversation we had with Jeff Preshing.)","title":"An Engine Developer's Toolbox"},{"location":"interviews/JeffPreshing-interview/#the-arc80-engine-architecture","text":"Back when I was starting my career, I considered myself a good programmer, but I would look at existing games and game engines and just be unable to even fathom how everything all fit together. That's the most confusing part, and to illustrate, I've provided my own engine's architecture here. This can give you an architectural overview of my own game engine; it's something that I would have liked to see early on in my career. You can find other overviews like that online, but having a particular view of the modules, the dependencies between them, and how an engine goes from high level to low level is a good start. That was by far the most confusing thing: Getting the big picture of it all.","title":"The Arc80 Engine Architecture"},{"location":"interviews/JeffPreshing-interview/#engine-design-principles","text":"I have my own set of personal design principles behind how I write and maintain my own engine. They probably won't apply to anybody else, but that's what I've chosen and I think it's an interesting way of looking at things. To the extent that I can, I avoid building custom tools. I don't want to spend time building them. That's a big part of real AAA game development at a studio\u2014because you've got hundreds of people on the team, the investment that you make in user-friendly tools pays off. That said, it's a big investment, and for me personally I don't want to spend time on custom tools. To get around that, I leverage Blender 1 ; I do all my level editing and character design in Blender. Another one of my personal design principles is that I always want to focus on the application running on the device. All my effort is targeted at keeping that lean and mean and efficient, so rather than focus on tools, I focus on the game application itself. The other principle that I follow to an extreme is maintaining reusable modules, because everyone likes modularity. The way I make sure that my modules are reusable is by building small applications that compile and link with individual components, and I can't do that unless the components are decoupled from each other. When I say components, every small box on my architecture diagram is a component. Each one has its own directory of header files, and when each box is compiled, it's a library and it has access to the headers of specific other modules that it's allowed to depend on. Because of that, I'm really aggressive in terms of modularity. What I found at game studios is that engines tend to be monolithic code bases, so I'm deliberately making an effort to go the other way. I would almost say that what I've got is more of an SDK 2 than an engine\u2014I just happen to be making a game out of this SDK.","title":"Engine Design Principles"},{"location":"interviews/JeffPreshing-interview/#creating-your-first-engine","text":"For my own engine, I write everything. I've written my own containers, string class and file system class, which includes the physics and the audio engine. I use almost nothing from the standard library, except for type traits 3 . If you're a novice game engine programmer and are setting out to do your first project, you don't want to do that . For everything that I've written, there are really good open-source equivalents. So I don't know if it's a good or bad idea, but one way to build your own initial game engine is to look at it as an integration exercise: Go get GLM 4 , download Bullet 5 , download some renderer like Horde3D, and so on. You've got libraries for a lot of things, like libraries that import 3D models and everything, so you can stitch together an engine out of those. I think that's a good way of going about it, since it will expose you to the interfaces to each component. That's the best starting point, and then depending on how your needs evolve, maybe you'll end up opening those black boxes and modifying things under the hood. For a beginner, you can integrate available code rather than doing everything from scratch, because that's taken me a long time. But that's just my style.","title":"Creating Your First Engine"},{"location":"interviews/JeffPreshing-interview/#skills-of-engine-development","text":"These were skills I didn't expect to need in the beginning of my career, but I ended up developing them during my career. The reality is that every engine programmer ends up exposed to them, so don't be surprised to find yourself getting better at these skills by necessity.","title":"Skills of Engine Development"},{"location":"interviews/JeffPreshing-interview/#systems-integration","text":"I call the first one Systems Integration . It's actually a big category, as it has to do with integrating third-party libraries\u2014taking separate codebases and making them build and link together to run as one thing. Another systems integration exercise would be porting to new platforms, as well as doing big three-way merges (you've branched a library, integrated it at some point, then maybe you've made changes both to your code and inside the library itself). That happens a lot in the industry; months later, there's a new version of a library you want to integrate, which is the latest and greatest of that dependency 6 . That can end up being a huge task that can take weeks or months to accomplish. Between all of those things, Systems Integration is a valuable category of skills to have. However, a lot of C++ programmers want to avoid it because they hate it. Many C++ programmers hate integrating libraries; they want everything to be a header-only library, just to make their lives easier. If you learn how it works, though, you're doing yourself a big service. Getting more familiar with it and tools like CMake 7 to help you generate build pipelines instead of tweaking Visual Studio properties is just a good skill to have. It's especially useful when you're going cross-platform.","title":"Systems Integration"},{"location":"interviews/JeffPreshing-interview/#low-level-debugging","text":"Low-level Debugging is another great skill to have, especially when you are tracking down crashes in the final optimized build. Especially at the end of the project, you'll have testers hammering on the optimized build, so there's bound to be crashes and issues that don't happen in a debug build. The ability to diagnose and fix those bugs is useful because you want to ship a stable game! How you develop that skill is a different story, but at some point you or someone on your team will have to go into the disassembly 8 window or the memory view and figure things out at that level. It's kind of considered a black art, which is a bit of a shame because it doesn't really have to be. I started developing my low-level debugging skills when I had no other choice but to figure it out. In 2005, we needed to ship a project but it kept crashing. I knew it was possible to solve the problem because, at an earlier job, I had a teammate who would constantly go look at crash dumps and disassembly and memory views. I would always think, \"What the heck is this guy doing?\" I asked him how he did it, but he never explained it to me; he just said \"I like computers.\" So I knew that was a skill that was possible to have. I don't remember exactly how I picked it up, but at some point that became my bread and butter at Ubisoft. My teammates there would always come to me to if they had a crash and I would help them out. Eventually it got to the point where Ubisoft started giving an in-house course on this subject. In my experience, it's something that's passed on from one person to the other, and yet no one really seems to want to get down to that level. Everyone's more interested in what sort of high-level programming paradigms can help us be more productive, but the most productive thing is to ship the game, and to ship the game you have to fix these issues at a low level.","title":"Low-level Debugging"},{"location":"interviews/JeffPreshing-interview/#profiling","text":"The next skill on the list is Profiling . Everyone agrees that it's important\u2014you want to profile before you decide where and how you should optimize. How you use profilers is kind of similar to low-level debugging in that it's a skill passed on from one person to another on a project. I would point to a talk I gave on profiling at a student conference . I think it's a pretty good introduction to the different types of profilers and examples of how you would use them in practice.","title":"Profiling"},{"location":"interviews/JeffPreshing-interview/#concurrency","text":"The fourth skill worth developing is Concurrency , or multi-threading, which is probably no surprise considering everyone talks about it. Everyone knows it's part of life as a game developer.","title":"Concurrency"},{"location":"interviews/JeffPreshing-interview/#iterative-development","text":"Number five, Iterative Development , is the ability to look at feature goals and envision how it'll work at a level. If you can break that into bite-sized tasks and decide what's your rollout plan (which tasks you'll do in what order) is a good skill. Managers love it because they want these enumerated tasks, which helps them create a schedule.","title":"Iterative Development"},{"location":"interviews/JeffPreshing-interview/#development-journal","text":"I also think an engine developer should keep a Journal . When you're working on a task or debugging a problem, you might open NotePad or a text editor and start taking notes, you could copy and paste call stacks 9 , and maybe you take notes when you're researching things on Google and Stack Overflow or even while brainstorming. I especially make a lot of notes when I'm brainstorming\u2014looking at all my options and alternatives and trying to figure out what's the best one. So when I talk about keeping a journal, I'm not talking about keeping a diary of what I did each day. What I mean is having a place to store your notes so you can go back to them in the future. The way I personally do this is I have a folder full of text files, and it's just organized by date. I don't even try to organize by category. I just have one text file per month and at the start of the day, if I have something to note, I put a heading for the date and I jot things down. I can't work without doing that personally, because I'm constantly trying to remember what my observations were on a subject in the past. It's great to be able to search that directory of my notes and refresh my memory that way. I think it's a useful skill, but maybe every person has their own way of working in that respect.","title":"Development Journal"},{"location":"interviews/JeffPreshing-interview/#reflection-and-serialization","text":"The next two things on my list of skills are Reflection and Serialization . In the industry, every game engine has their own approach for this. You don't even need to have an approach at all, but just be aware that there are varying approaches to reflecting data structures. There's different reasons to have reflection, ranging from serialization to managing shader parameters to networking. Being able to think in those terms is a particularly useful skill for game development because C++ has nothing for runtime reflection. Well, there is RTTI 10 , but RTTI doesn't provide any information about data members, so it can't help you implement things like serialization.","title":"Reflection and Serialization"},{"location":"interviews/JeffPreshing-interview/#professionalism","text":"The final thing is a soft skill: being Professional . You'll have to do most of your work on a team, so obviously it's a matter of treating your teammates with respect. This was a mistake I made at the beginning, when I really wanted to advance in my career and I really wanted to shine as a good contributor to the project. A better way of thinking, though, is to help make sure that the game ships. Five to ten years from now, no one will remember who shined and who didn't, but if you have a shipped project on your resume, that's like currency; employers love to see that. So you need to stay professional and not get attached to your way of doing things. Letting other people tackle the problem their way, even if you don't think it's the best way, will minimize friction on the team, and that helps the game ship.","title":"Professionalism"},{"location":"interviews/JeffPreshing-interview/#console-development-experiences","text":"The first time I did a seek-free loading 11 system, it was only for the Xbox console on the game Rainbow Six: Lockdown . Since we were loading from DVDs, seeks 12 were expensive, so you wanted to avoid them. My job was looking at the problem of how the game is opening all these files and reading from different places. All I did was implement a mode where the game runs, and as it starts up it's logging which files it's reading from and in what positions. It then takes that data and makes a linear version of the same data with no seeks. I also added another mode of running the game that uses the seek-free files, so I was able to just focus on the strategy of how and when the data is accessed. That also included fallbacks, so if the game loaded a little differently the next time, it wouldn't totally break. At that time, though, I didn't have to worry so much about platform differences. The second time that I did it I already had experience with the strategy, but I had to do it in a cross-platform way: for PC, PS3, and Xbox 360. That meant it was just a matter of reimplementing the same thing, but using cross-platform primitives. The engine already had some cross-platform wrappers for low-level things like opening files and such, but not everything. I remember on the PS3, there was a limit to how many files you could have open, and that limit was very small. At some point, I had to make a wrapper layer between the engine and the hardware that let the game pretend it was opening a lot of files, but if it wasn't actually reading from those open file handles\u2014I was closing the files and it would be opening them on next access. The reason I had to do a lot of PS3 optimization was that the Rainbow Six: Vegas engine was actually a branch of Unreal Engine 3. That project actually branched very early, before Unreal 3 was even officialized, so we didn't have a lot of the work that Epic Games did that we could just integrate. That was actually one of my favorite projects because when Rainbow Six: Vegas shipped on PlayStation 3, it didn't perform very well: It was pixely, it was blurry, and it had a low framerate. But on Rainbow Six: Vegas 2, we optimized things and made it look good. The PS3 version almost became indistinguishable from the Xbox 360 version. We didn't even use the SPU's 13 a whole lot, as I recall. I remember SPU's were used by audio mixing, but I don't recall that we used them for the engine itself. Most of it was basics, like making sure you had the best frame buffer 14 pixel format for the GPU because the PS3 needed a specific set of conditions maintained for Z-buffering 15 to work in the optimal way. It was hierarchical Z, and it was very easy to break that, so using the tools that Sony gave us to identify where it was broken and fixing those, that was a huge boost. Avoiding redundant state changes being sent to the GPU was also a huge win. So a lot of it was basics, not crazy concurrent SPU stuff. I saw a lot more SPU stuff on later projects, but I didn't gain that much experience developing for SPU's myself. I did a lot of debugging on them for sure, but not so much on the development side.","title":"Console Development Experiences"},{"location":"interviews/JeffPreshing-interview/#how-profiling-can-differ","text":"Profiling on Child of Light was completely different from profiling on Assassin's Creed: Unity . Child of Light was a much simpler game engine by far; when you put the controller down and let the character sit there in an environment, activity on the CPU and GPU was pretty consistent from one frame to the next. With Assassin's Creed: Unity , on the other hand, it was a totally different thing because the assassin was often in a crowd, and the crowd is walking all over the place and bumping into each other. That movement causes a lot of fluctuation in CPU activity from one frame to the next, and that made it harder to profile. It kind of bummed me out a little bit, but it seemed like most of the Assassin's Creed team's approach to optimization was just moving stuff around from one core to the next. They have this job system and you capture a profile and you see which jobs ended up on which cores, and if there were holes in the schedule, they would try to move jobs around. There was not as much direct optimization going on. To be fair, it was such a mature engine that it was hard to find those optimizations.","title":"How Profiling Can Differ"},{"location":"interviews/JeffPreshing-interview/#the-game-engine-marketplace","text":"If you ask me, \"What's the next big thing in game engines?\" I would say the Arc80 Engine, obviously! Just kidding. I mean, I would love it if I could use the Arc80 Engine for the rest of my career. But I don't believe in the \"next big thing\" in terms of a predetermined path that the industry is inevitably walking along. Right now, there are people who think that real-time ray tracing will be the next big thing in game engines, and there are people who think that cloud gaming is the future. I don't know about either of those things. I do know that history is littered with inventions that people thought were going to become huge, but weren't, because market forces just didn't support the idea. Game engines are just another marketplace. The ideas that succeed in this market will be the ones that meet a demand, either by enabling more interesting games for consumers, by making developers more productive, or both, or in a different way altogether. The only way something will become \"the next big thing\" is if an idea is so good, so compelling that a big segment of the industry adopts it. But by necessity, that will always be hard to predict, because if an idea was both compelling and obvious, people would already have adopted it! The point is that future of game engines has not yet been written. As engine programmers, we're the ones who get to write it. Interview conducted October 24, 2018. Blender is an open-source 3D computer graphics software used in creating 3D models, animations, and interactive applications. \u21a9 A software developer kit (SDK) is a set of programs used in developing another program. \u21a9 Type traits define a compile-time templated-base interface to query or modify the properties of types, for more see type support . \u21a9 OpenGL Mathematics (GLM) is a mathematics library based off of OpenGL specifications, that contains definitions for typical math constructs used in graphics. \u21a9 Bullet Physics is a real-time physics simulation library, a physics engine, which simulate collisions for soft and rigid body dynamics. \u21a9 Dependencies are links that are required between programs, such that one program is reliant on another. \u21a9 CMake is a cross-platform, open-source application for managing the build process of software in a compiler-independent way. \u21a9 Disassembly is the assembly language, translated from machine code, of a program; it is the compiler's version of the program. \u21a9 The call stack is a stack data structure that stores the information about the active routines of a computer. It can be walked up/down into the callee functions and function definitions, respectively. \u21a9 Run-time type information/identification (RTTI) is specific to C++ in that information about an object's type is available at runtime. \u21a9 Seek-free loading is a system which is able to read a file \"free\" (without) \"seeking\", (searching) a file, for data/position within a file. Seeking causes disk activity which is generally slower than CPU performance. \u21a9 A seek is a programming concept related to file reading, where a program has a file pointer associated with a position and a seek moves the pointer to a specific position within that file. \u21a9 A Synergistic Processing Unit (SPU) is the processing unit associated with the Cell architecture of the PS3. The PS3 had seven as part of its hardware, only six of which were usable by game developers. \u21a9 Frame buffers are a portion of RAM containing a bitmap of the display, containing the data for that given frame on the video display. \u21a9 The z-buffer also known as the depth buffer, contains information regarding the distance from the camera, the depth. Z-buffering can also refer to the technique in which pixels are culled from the frame, not rendered, because another pixel's depth is closer to the camera, therefore the pixel in the background is being covered. \u21a9","title":"The Game Engine Marketplace"},{"location":"interviews/JeffPreshing-video/","text":"Celebrating your Successes \u00b6","title":"Video Story"},{"location":"interviews/JeffPreshing-video/#celebrating-your-successes","text":"","title":"Celebrating your Successes"},{"location":"interviews/MartinMiddleton-interview/","text":"Thinking about the Data \u00b6 Martin Middleton is the CTO at Funomena, an independent game studio in San Francisco he co-founded in 2012 with Robin Hunicke. Funomena has put out award-winning titles on a variety of platforms, including Luna, Woorld, and the upcoming Wattam. Previously Martin was an engine programmer at thatgamecompany, where he worked on Flow, Flower, and Journey. (The following is the edited transcription of a conversation we had with Martin Middleton.) Pipeline of Code Optimization \u00b6 When I started engine programming, the most challenging aspect for me was probably developing core performance. Back in school while I was learning, the best practices for performance were heavily object-oriented with a lot of abstraction. My most useful classes were Electrical Engineering, which is especially beneficial for developing on consoles like the PS3, which has really limited resources and requires you have a deep understanding of what the actual hardware is doing. An example of this would be thinking about your memory usage so when you assign a variable, where is that value actually coming from? Is it in main memory, is it a local cache, is it already in the register on the CPU... There's an order of magnitude of speed difference between all those different layers, and it can be really easy to ignore that because most programming languages don't really make that explicit. Overcoming this challenge means thinking more about it for yourself. Whenever you're writing code, you have to internally plan out when you'll be loading up certain values or making sure that the data sticks around in local memory long enough for you to use it. You might also be thinking about what else the processor can be doing while it's waiting that memory to be loaded. Console platform-based design is very low-level stuff; there aren't any console architectures similar to the PS3 anymore. However, everything is multithreaded these days, so that's useful knowledge I learned from PS3 and SPU 1 programming that has served me no matter what type of engineering I'm doing. Figuring out how to shuffle data between different parts of the hardware so that you're splitting up this computation, \"Do I do this on the CPU, do I do it on the GPU, how do I transfer the data between a CPU and the GPU, and what am I doing while the data is transferring...\" I think all that stuff is useful, whatever the hardware platform is. In a certain way, the PS3 was ahead of its time because all of the technology was going in that direction anyway. So it's just sort of the early proving technology, where people learn how to do things that way. Unity's new component systems are structured really similarly to how PS3 engines were structured; we were focused on batching and pipelining things into really small chunks of code that just reads through. It was all about structuring everything to process buffers of data as easily as possible. I don't think it's a coincidence that Unity hired a bunch of senior engineers from Naughty Dog and Insomniac\u2026 At thatgamecompany , I was doing a lot of SPU intrinsic programming, which is a subset of C++. You would use functions that utilize assembly commands, which would tell the processor what exactly to do. However, the problem with this is that you're focusing on one specific problem, and it makes your code brittle. If you need to change that code later on, you have to undo a lot of what you've already done. But there is sort of a halfway point; if you can get into a mindset where you're always thinking about memory usage, that's something that you will benefit from across the board. Writing code in that style makes it very straightforward, so writing up optimizable code is something that always pays off. Usually that involves being really explicit about when you're loading or writing data. That's not really abstracting things away too much so there's sort of no \"magic.\" If something happens automatically, or \"magically,\" it's usually very suspicious. It's also more about knowing what exactly is being allocated, and then in what order code it's being updated, not whether it is object or data-oriented. John Carmack has this recommendation that you step through an entire frame per game and step into every single function so you can experience like the entirety of everything that happens in a frame. That takes a really long time, so designing your engine in a way that makes that possible is a good methodology. Engines Should Guide Games, Not Direct Them \u00b6 Most engines are built to guide you towards a specific type of game or a specific type of implementation. This is one of the reasons why at thatgamecompany we used PhyreEngine 2 as a framework, we had access to the source code and we were especially deliberate about which features we decided to implement. We didn't want the existing engine to influence how our game progressed. If you're regularly fighting your engine, then you start to question the point in using an engine. You might as well use something a bit lighter, like a framework. So there's always a trade-off when it comes to how much of this engine you can use versus what really needs to be sidetracked. With Unity, one example would be the update system; the engine doesn't give you explicit control over the update loop. You have a script execution order, but the MonoBehaviour system can be really heavy, especially if you have a lot of objects. Often you end up writing your own entity in a really lightweight entity-system 3 and writing your own explicit update system so you have full control over that. Custom Entity System in Unity \u00b6 For our entity system in Unity, we were trying to solve two separate problems. One was when we'd have a whole bunch of objects that are represented by particles or not tied to a mesh. Having 100 game objects is really expensive, so instead we'd just turn that into a really lightweight class and separate that from the game object hierarchy, which makes it very specific to a system. The other problem is controlling the updates. A lot of game objects don't necessarily need to update every frame, and I think Unity has optimized this a lot, but when we first started using Unity, there was just a really big overhead to having even an empty MonoBehaviour 4 with nothing implemented. Being able to explicitly have an update loop and dictate which objects are active and which functions they own is good for optimizing, as opposed to having to send messages in every single object to see if it has a handler or not. For Wattam , we're using both our own entity system and Unity's GameObject system. The way it works is, by default, we will start with something derived from MonoBehaviour , and if it turns out that we need a lot of those \"residents\" in an array at a time, then we'll decide they don't all need to have their own transform in the scene graph 5 and I can just give them a \"simple transform\", which uses a Vector3 for the position and the Quaternion for rotation. I think that's actually my biggest gripe with Unity, that in order to just store a Transform you have to hook it into the scene graph, which just makes everything really slow and heavy. So I think having a lot of objects where you can write out their position and rotation without needing to be plugged into an update loop is really good. Thoughts on Unity and PhyreEngine \u00b6 While Unity is free for non-professionals, I haven't found it to be as accessible as PhyreEngine. Phyre was available for anyone who was a PlayStation developer; you could just download it from their dev forum. Sony also made Phyre's source code available so you can modify it, whereas Unity is much more of a black box. For professional development, Unity is actually our second-highest cost in software, coming only after Maya. Unity has a somewhat pricey monthly subscription fee as well. On the other hand, Unity is much more widely used while the Phyre team was very small within Sony. They were very limited resources-wise, whereas Unity is an enormous organization with lots of engineers. That being said, Unity had not prioritized console development at all for a long time; they just didn't see that as their audience. Developing a console title with Unity was kind of a struggle, and still is in some ways. They're just now starting to support console development more, I think because the platform holders themselves are investing resources because they know that a lot of people use Unity. Technology from Flower to Journey \u00b6 A lot of the Journey engine was the Flower engine. We started developing Journey with the Flower engine pretty early on because we really wanted to focus on an engine we could iterate on and develop and use for multiple projects. The structure of it was meant to support multithreading really well, and having systems for gathering up data and sending them off to the SPU's and synchronizing and reporting back when those SPU's were finished with the data. Animation support needed to be built for Journey , because the Flower engine initially didn't have much, since most of the animation in Flower was procedural. Journey 's sand system also evolved from the grass in Flower ; I kind of started off by using the structure of the grass system. In that game, the grass was our test for SPU usage, so any time we had extra SPU resources we would just give it to the grass system so it could render a little bit more grass, or push out the LOD 6 a bit more. The other main system we had to add for Journey was networking. Going from a non-networked game to a networked game is a pretty big shift, because all of a sudden, all your important objects and game events have to be serialized 7 and addressable with IDs. At that point, you can't just store a list of pointers anymore, because those pointers aren't going to work across machines. You have to think of a higher level way of referring to them. Switching those references from pointers to an index/ID-based reference system was something that was more challenging than expected. Another big challenge was figuring out how many players to support beforehand. Journey was peer-to-peer 8 , so one of the clients acted as the server. Initially, we wanted to support four people at a time, but that wound up being too complicated and out of scope for us to implement. That changes the design a lot, since scenarios meant for four people needed to work for two people. Figuring out the specifics of multiplayer is good to do early on in the development process. There's also the whole aspect of synchronization between machines. We had two different update paths: One was for content that relies on the other machine to know about, and then the other for things, like particle effects, that can be done locally. If you start off with that in mind, then it gets a lot easier later on. Otherwise, it can be hard to keep track of what's being synchronized and what isn't. If you start to try to synchronize something after the fact, then you have all these side effects that you weren't expecting, which often leads to many other things you have to synchronize as well. It depends on the authority 9 and on how accurate you need to be. For example, in a competitive multiplayer game where you are targeting something, which player's machine decides if the shot actually connects? Then waiting for the other machine to agree with you can sometimes take too long, so you have to start reacting to what you think happened, and then be able to back out of that if it turns out that both machines don't agree. Journey 's Peer Networking System \u00b6 For Journey 's networking, the peer-matching system was based on the lobby system that Sony provided. The way it worked was levels were split up into grids, so depending on which grid cell you were in, you'd join a room that was sort of like a hash 10 with the grid cell and their specific game state. Once you join that room, everyone in the room gets a message that you joined and they send you their data. It compares the two players' pings and what game flags have changed, and from there it determines whether the two people are compatible. One interesting aspect to this is that we implemented a maximum room size\u2014otherwise the room could be flooded by tons of people spamming each other with messages. If a room becomes too big, then you create a new room. That means the number of rooms grows as the audience grows. But the game's online play goes through periods of more and less activity, so then you have the issue where there are a whole lot of rooms but there are only a couple people in each room, and they're all stranded from each other. To solve this, I ran a room defragmentation 11 system where if a player is in one room for too long, they would leave and try to join a busier room. Matchmaking was one of my biggest fears in the development process. We had been talking to other developers of online games at the time, and they said when your game first launches, there's a big spike in players but that often tails off over time as players move on from your game. You have to design for two online environments: The popular and unpopular. On Journey , we planned for both early on, but the fact that people were still playing six months after launch was nice to see. We weren't expecting that because we had heard how quickly online player bases drop off. Even years after, I was still able to show somebody Journey and managed to connect with people in-game, which really makes me happy! Versioning & Deploying Tools \u00b6 At thatgamecompany, we initially developed web tools for two reasons. One was the restart game issue\u2014we wanted our developers to be able to live update values, but developing the editor into the actual engine itself would have been way too hard. So we knew we needed some kind of remote editor. Our second reason was the fact that UI for web is immensely easier, especially at the time. I think web tools can be a pretty good system, but the main issue is that once you get to a certain level of complexity, it becomes much harder. I don't see a really advanced complex animation tool like Maya, for instance, being deployed on a web platform anytime soon. With executable/desktop tools there were some deployment issues where I'd add a new feature, and it would take everyone on the team a while to be using that change. For that change to propagate, it would get deployed on people's machines on the PS3 side. When you're developing any type of networking, early on you learn that you need to implement a version-number protocol 12 that doesn't really change so that it's backward-compatible. If you have this version, in a worst-case scenario you can just ignore any messages that are coming from an older version. On the website with our tools on it, I like supporting several older versions at a time so that my teammates can download the new PS3 code and update it on their time. Implications of Prototyping \u00b6 At Funomena, we are still figuring out how to handle prototyping phases. The platonic \"ideal\" is that you handle prototyping separately, and then everything is locked down and you implement it. Unfortunately, it never really works that way, because after you've invested a lot of work into a system, it's tough to start from scratch all over again\u2014especially since it's around that time when people are really itching to see a more polished demo. For certain prototypes, I definitely encourage doing that in entirely separate projects so you're working in a different codebase. Maybe you branch it so that you can clean it up and try merging it back in. If you take the path of least resistance, that code will eventually make its way back in, so don't be too sloppy with it. It can be challenging to make the separation between your different feature implementations. With Journey, for instance, a lot of systems were tightly interconnected with each other. That made it hard to design a specific thing in isolation because it was depending on all these other things as well. In that situation, I think what you do is you get more comfortable at dealing with the technical bit and foreseeing what problems could arise when combining two things. The trick is to be really explicit about your assumptions in the code, so down the line you'll know to change it if those assumptions are no longer true. The Spectrum of Engine Development \u00b6 I think if you're learning about engines, one way to do it is to start with what specifically you want to learn (i.e. how application context 16 works in Windows), and with other things (i.e. graphics rendering), you can find a lightweight framework to use. Then, over time, you can replace it bit by bit with something you make yourself. In this way, you can make something fully your own. I have also done a lot of audio programming on games that I've worked on, but I never wrote an actual audio renderer 17 . I always relied on either SCREAM, which was Sony's audio library, or Wwise, the industry standard. At some point, I think it would be fun to write my own audio code, but it's already written and more importantly most of the sound designers that we work with are comfortable with Wwise, and so it doesn't make sense to mess with that. Unity is far on one end of the spectrum, and then at the other end is the Casey Muratori \"write everything yourself\" method in Handmade Hero . There's a lot of in-between as well; you don't have to go so heavy-handed as an engine. There's SDL 13 , or graphics wrappers where you just write forward to the wrapper and it handles the underlying OpenGL 14 or DirectX 15 . There are also lighter frameworks that bring in bits and pieces like meshoptimizer to build a library\u2014things you wouldn't have the time to make otherwise. I think that's a really good way to learn. Interview conducted October 3, 2018. A Synergistic Processing Unit ( SPU ) is the processing unit associated with the Cell architecture of the PS3. The PS3 had seven as part of its hardware, only six of which were usable by game developers. \u21a9 PhyreEngine is Sony's game engine that is freely available for PlayStation developers. The engine is compatible with the PlayStation platforms of the last decade. \u21a9 A pure entity-system is similar to a flat hierarchy, where the entities hold data and functions to be called by the system manager. \u21a9 MonoBehaviour is the base class within Unity that all components which attach to GameObject must derive from, it has methods for start, update, and destroy, among a ton of others. \u21a9 The scene graph of a game engine holds the entities and components (including transforms and parent hierarchies) of a level, also known as a scene. \u21a9 Level of Detail ( LOD ) is the process of simplifying a model/mesh by removing vertices and detail. This typically occurs when the model is far enough such that the details are relatively insignificant compared to others in the viewport. \u21a9 Serialization is the process of data being converted into a byte stream for easier storage and transfer, think of it as similar to a save and load system. \u21a9 Peer-to-peer networking is where every machine to one another, which requires more bandwidth per client and more complex data authority handling but avoids needing a dedicated server. Peer-to-peer is generally harder to implement than client-server. \u21a9 Authority with regards to networking is when a certain machine, typically the server, has the control (final decision) on the state of a variable/script/entity. \u21a9 A hash is a structure that maps keys to values through a formula defined to convert structures into an index, typically the formula is constructed to avoid collisions between similar objects. The hash of the same object will always return the same value. \u21a9 Defragmentation is the process of reducing fragmentation (well that's a dumb definition), where fragmentation is where memory is used inefficiently with lots of gaps in between used memory chunks. In this context, defragmentation is used in the sense of keeping the rooms fully utilized, not wasting space on a fairly empty room. \u21a9 Each layer of the network stack has a duty, but they can follow those duties in different ways, and the different implementations of a layer is labeled a protocol . \u21a9 Simple DirectMedia Layer ( SDL) is a hardware abstraction layer for audio, input, and graphics across multiple platforms. \\ \u21a9 OpenGL , short for Open Graphics Library, a cross-language, cross-platform application programming interface (API) for rendering 2D and 3D vector graphics. The API is typically used to interact with a graphics processing unit (GPU), to achieve hardware-accelerated rendering. It's the underlying rendering library for many modern game engines. \u21a9 Microsoft DirectX is a collection of application programming interfaces (APIs) for handling tasks related to multimedia, especially game programming, on Microsoft platforms, like Windows and Xbox. It is most known for Direct3D which is the graphics API used for creating windows and rendering, and serves similar purposes as OpenGL. \u21a9 Application context is the context, the set of data required to interrupt and continue a task, of an application. \u21a9 An audio renderer is a system which plays/outputs spatialized sound, sound that is positioned in the world. \u21a9","title":"Martin Middleton"},{"location":"interviews/MartinMiddleton-interview/#thinking-about-the-data","text":"Martin Middleton is the CTO at Funomena, an independent game studio in San Francisco he co-founded in 2012 with Robin Hunicke. Funomena has put out award-winning titles on a variety of platforms, including Luna, Woorld, and the upcoming Wattam. Previously Martin was an engine programmer at thatgamecompany, where he worked on Flow, Flower, and Journey. (The following is the edited transcription of a conversation we had with Martin Middleton.)","title":"Thinking about the Data"},{"location":"interviews/MartinMiddleton-interview/#pipeline-of-code-optimization","text":"When I started engine programming, the most challenging aspect for me was probably developing core performance. Back in school while I was learning, the best practices for performance were heavily object-oriented with a lot of abstraction. My most useful classes were Electrical Engineering, which is especially beneficial for developing on consoles like the PS3, which has really limited resources and requires you have a deep understanding of what the actual hardware is doing. An example of this would be thinking about your memory usage so when you assign a variable, where is that value actually coming from? Is it in main memory, is it a local cache, is it already in the register on the CPU... There's an order of magnitude of speed difference between all those different layers, and it can be really easy to ignore that because most programming languages don't really make that explicit. Overcoming this challenge means thinking more about it for yourself. Whenever you're writing code, you have to internally plan out when you'll be loading up certain values or making sure that the data sticks around in local memory long enough for you to use it. You might also be thinking about what else the processor can be doing while it's waiting that memory to be loaded. Console platform-based design is very low-level stuff; there aren't any console architectures similar to the PS3 anymore. However, everything is multithreaded these days, so that's useful knowledge I learned from PS3 and SPU 1 programming that has served me no matter what type of engineering I'm doing. Figuring out how to shuffle data between different parts of the hardware so that you're splitting up this computation, \"Do I do this on the CPU, do I do it on the GPU, how do I transfer the data between a CPU and the GPU, and what am I doing while the data is transferring...\" I think all that stuff is useful, whatever the hardware platform is. In a certain way, the PS3 was ahead of its time because all of the technology was going in that direction anyway. So it's just sort of the early proving technology, where people learn how to do things that way. Unity's new component systems are structured really similarly to how PS3 engines were structured; we were focused on batching and pipelining things into really small chunks of code that just reads through. It was all about structuring everything to process buffers of data as easily as possible. I don't think it's a coincidence that Unity hired a bunch of senior engineers from Naughty Dog and Insomniac\u2026 At thatgamecompany , I was doing a lot of SPU intrinsic programming, which is a subset of C++. You would use functions that utilize assembly commands, which would tell the processor what exactly to do. However, the problem with this is that you're focusing on one specific problem, and it makes your code brittle. If you need to change that code later on, you have to undo a lot of what you've already done. But there is sort of a halfway point; if you can get into a mindset where you're always thinking about memory usage, that's something that you will benefit from across the board. Writing code in that style makes it very straightforward, so writing up optimizable code is something that always pays off. Usually that involves being really explicit about when you're loading or writing data. That's not really abstracting things away too much so there's sort of no \"magic.\" If something happens automatically, or \"magically,\" it's usually very suspicious. It's also more about knowing what exactly is being allocated, and then in what order code it's being updated, not whether it is object or data-oriented. John Carmack has this recommendation that you step through an entire frame per game and step into every single function so you can experience like the entirety of everything that happens in a frame. That takes a really long time, so designing your engine in a way that makes that possible is a good methodology.","title":"Pipeline of Code Optimization"},{"location":"interviews/MartinMiddleton-interview/#engines-should-guide-games-not-direct-them","text":"Most engines are built to guide you towards a specific type of game or a specific type of implementation. This is one of the reasons why at thatgamecompany we used PhyreEngine 2 as a framework, we had access to the source code and we were especially deliberate about which features we decided to implement. We didn't want the existing engine to influence how our game progressed. If you're regularly fighting your engine, then you start to question the point in using an engine. You might as well use something a bit lighter, like a framework. So there's always a trade-off when it comes to how much of this engine you can use versus what really needs to be sidetracked. With Unity, one example would be the update system; the engine doesn't give you explicit control over the update loop. You have a script execution order, but the MonoBehaviour system can be really heavy, especially if you have a lot of objects. Often you end up writing your own entity in a really lightweight entity-system 3 and writing your own explicit update system so you have full control over that.","title":"Engines Should Guide Games, Not Direct Them"},{"location":"interviews/MartinMiddleton-interview/#custom-entity-system-in-unity","text":"For our entity system in Unity, we were trying to solve two separate problems. One was when we'd have a whole bunch of objects that are represented by particles or not tied to a mesh. Having 100 game objects is really expensive, so instead we'd just turn that into a really lightweight class and separate that from the game object hierarchy, which makes it very specific to a system. The other problem is controlling the updates. A lot of game objects don't necessarily need to update every frame, and I think Unity has optimized this a lot, but when we first started using Unity, there was just a really big overhead to having even an empty MonoBehaviour 4 with nothing implemented. Being able to explicitly have an update loop and dictate which objects are active and which functions they own is good for optimizing, as opposed to having to send messages in every single object to see if it has a handler or not. For Wattam , we're using both our own entity system and Unity's GameObject system. The way it works is, by default, we will start with something derived from MonoBehaviour , and if it turns out that we need a lot of those \"residents\" in an array at a time, then we'll decide they don't all need to have their own transform in the scene graph 5 and I can just give them a \"simple transform\", which uses a Vector3 for the position and the Quaternion for rotation. I think that's actually my biggest gripe with Unity, that in order to just store a Transform you have to hook it into the scene graph, which just makes everything really slow and heavy. So I think having a lot of objects where you can write out their position and rotation without needing to be plugged into an update loop is really good.","title":"Custom Entity System in Unity"},{"location":"interviews/MartinMiddleton-interview/#thoughts-on-unity-and-phyreengine","text":"While Unity is free for non-professionals, I haven't found it to be as accessible as PhyreEngine. Phyre was available for anyone who was a PlayStation developer; you could just download it from their dev forum. Sony also made Phyre's source code available so you can modify it, whereas Unity is much more of a black box. For professional development, Unity is actually our second-highest cost in software, coming only after Maya. Unity has a somewhat pricey monthly subscription fee as well. On the other hand, Unity is much more widely used while the Phyre team was very small within Sony. They were very limited resources-wise, whereas Unity is an enormous organization with lots of engineers. That being said, Unity had not prioritized console development at all for a long time; they just didn't see that as their audience. Developing a console title with Unity was kind of a struggle, and still is in some ways. They're just now starting to support console development more, I think because the platform holders themselves are investing resources because they know that a lot of people use Unity.","title":"Thoughts on Unity and PhyreEngine"},{"location":"interviews/MartinMiddleton-interview/#technology-from-flower-to-journey","text":"A lot of the Journey engine was the Flower engine. We started developing Journey with the Flower engine pretty early on because we really wanted to focus on an engine we could iterate on and develop and use for multiple projects. The structure of it was meant to support multithreading really well, and having systems for gathering up data and sending them off to the SPU's and synchronizing and reporting back when those SPU's were finished with the data. Animation support needed to be built for Journey , because the Flower engine initially didn't have much, since most of the animation in Flower was procedural. Journey 's sand system also evolved from the grass in Flower ; I kind of started off by using the structure of the grass system. In that game, the grass was our test for SPU usage, so any time we had extra SPU resources we would just give it to the grass system so it could render a little bit more grass, or push out the LOD 6 a bit more. The other main system we had to add for Journey was networking. Going from a non-networked game to a networked game is a pretty big shift, because all of a sudden, all your important objects and game events have to be serialized 7 and addressable with IDs. At that point, you can't just store a list of pointers anymore, because those pointers aren't going to work across machines. You have to think of a higher level way of referring to them. Switching those references from pointers to an index/ID-based reference system was something that was more challenging than expected. Another big challenge was figuring out how many players to support beforehand. Journey was peer-to-peer 8 , so one of the clients acted as the server. Initially, we wanted to support four people at a time, but that wound up being too complicated and out of scope for us to implement. That changes the design a lot, since scenarios meant for four people needed to work for two people. Figuring out the specifics of multiplayer is good to do early on in the development process. There's also the whole aspect of synchronization between machines. We had two different update paths: One was for content that relies on the other machine to know about, and then the other for things, like particle effects, that can be done locally. If you start off with that in mind, then it gets a lot easier later on. Otherwise, it can be hard to keep track of what's being synchronized and what isn't. If you start to try to synchronize something after the fact, then you have all these side effects that you weren't expecting, which often leads to many other things you have to synchronize as well. It depends on the authority 9 and on how accurate you need to be. For example, in a competitive multiplayer game where you are targeting something, which player's machine decides if the shot actually connects? Then waiting for the other machine to agree with you can sometimes take too long, so you have to start reacting to what you think happened, and then be able to back out of that if it turns out that both machines don't agree.","title":"Technology from Flower to Journey"},{"location":"interviews/MartinMiddleton-interview/#journeys-peer-networking-system","text":"For Journey 's networking, the peer-matching system was based on the lobby system that Sony provided. The way it worked was levels were split up into grids, so depending on which grid cell you were in, you'd join a room that was sort of like a hash 10 with the grid cell and their specific game state. Once you join that room, everyone in the room gets a message that you joined and they send you their data. It compares the two players' pings and what game flags have changed, and from there it determines whether the two people are compatible. One interesting aspect to this is that we implemented a maximum room size\u2014otherwise the room could be flooded by tons of people spamming each other with messages. If a room becomes too big, then you create a new room. That means the number of rooms grows as the audience grows. But the game's online play goes through periods of more and less activity, so then you have the issue where there are a whole lot of rooms but there are only a couple people in each room, and they're all stranded from each other. To solve this, I ran a room defragmentation 11 system where if a player is in one room for too long, they would leave and try to join a busier room. Matchmaking was one of my biggest fears in the development process. We had been talking to other developers of online games at the time, and they said when your game first launches, there's a big spike in players but that often tails off over time as players move on from your game. You have to design for two online environments: The popular and unpopular. On Journey , we planned for both early on, but the fact that people were still playing six months after launch was nice to see. We weren't expecting that because we had heard how quickly online player bases drop off. Even years after, I was still able to show somebody Journey and managed to connect with people in-game, which really makes me happy!","title":"Journey's Peer Networking System"},{"location":"interviews/MartinMiddleton-interview/#versioning-deploying-tools","text":"At thatgamecompany, we initially developed web tools for two reasons. One was the restart game issue\u2014we wanted our developers to be able to live update values, but developing the editor into the actual engine itself would have been way too hard. So we knew we needed some kind of remote editor. Our second reason was the fact that UI for web is immensely easier, especially at the time. I think web tools can be a pretty good system, but the main issue is that once you get to a certain level of complexity, it becomes much harder. I don't see a really advanced complex animation tool like Maya, for instance, being deployed on a web platform anytime soon. With executable/desktop tools there were some deployment issues where I'd add a new feature, and it would take everyone on the team a while to be using that change. For that change to propagate, it would get deployed on people's machines on the PS3 side. When you're developing any type of networking, early on you learn that you need to implement a version-number protocol 12 that doesn't really change so that it's backward-compatible. If you have this version, in a worst-case scenario you can just ignore any messages that are coming from an older version. On the website with our tools on it, I like supporting several older versions at a time so that my teammates can download the new PS3 code and update it on their time.","title":"Versioning &amp; Deploying Tools"},{"location":"interviews/MartinMiddleton-interview/#implications-of-prototyping","text":"At Funomena, we are still figuring out how to handle prototyping phases. The platonic \"ideal\" is that you handle prototyping separately, and then everything is locked down and you implement it. Unfortunately, it never really works that way, because after you've invested a lot of work into a system, it's tough to start from scratch all over again\u2014especially since it's around that time when people are really itching to see a more polished demo. For certain prototypes, I definitely encourage doing that in entirely separate projects so you're working in a different codebase. Maybe you branch it so that you can clean it up and try merging it back in. If you take the path of least resistance, that code will eventually make its way back in, so don't be too sloppy with it. It can be challenging to make the separation between your different feature implementations. With Journey, for instance, a lot of systems were tightly interconnected with each other. That made it hard to design a specific thing in isolation because it was depending on all these other things as well. In that situation, I think what you do is you get more comfortable at dealing with the technical bit and foreseeing what problems could arise when combining two things. The trick is to be really explicit about your assumptions in the code, so down the line you'll know to change it if those assumptions are no longer true.","title":"Implications of Prototyping"},{"location":"interviews/MartinMiddleton-interview/#the-spectrum-of-engine-development","text":"I think if you're learning about engines, one way to do it is to start with what specifically you want to learn (i.e. how application context 16 works in Windows), and with other things (i.e. graphics rendering), you can find a lightweight framework to use. Then, over time, you can replace it bit by bit with something you make yourself. In this way, you can make something fully your own. I have also done a lot of audio programming on games that I've worked on, but I never wrote an actual audio renderer 17 . I always relied on either SCREAM, which was Sony's audio library, or Wwise, the industry standard. At some point, I think it would be fun to write my own audio code, but it's already written and more importantly most of the sound designers that we work with are comfortable with Wwise, and so it doesn't make sense to mess with that. Unity is far on one end of the spectrum, and then at the other end is the Casey Muratori \"write everything yourself\" method in Handmade Hero . There's a lot of in-between as well; you don't have to go so heavy-handed as an engine. There's SDL 13 , or graphics wrappers where you just write forward to the wrapper and it handles the underlying OpenGL 14 or DirectX 15 . There are also lighter frameworks that bring in bits and pieces like meshoptimizer to build a library\u2014things you wouldn't have the time to make otherwise. I think that's a really good way to learn. Interview conducted October 3, 2018. A Synergistic Processing Unit ( SPU ) is the processing unit associated with the Cell architecture of the PS3. The PS3 had seven as part of its hardware, only six of which were usable by game developers. \u21a9 PhyreEngine is Sony's game engine that is freely available for PlayStation developers. The engine is compatible with the PlayStation platforms of the last decade. \u21a9 A pure entity-system is similar to a flat hierarchy, where the entities hold data and functions to be called by the system manager. \u21a9 MonoBehaviour is the base class within Unity that all components which attach to GameObject must derive from, it has methods for start, update, and destroy, among a ton of others. \u21a9 The scene graph of a game engine holds the entities and components (including transforms and parent hierarchies) of a level, also known as a scene. \u21a9 Level of Detail ( LOD ) is the process of simplifying a model/mesh by removing vertices and detail. This typically occurs when the model is far enough such that the details are relatively insignificant compared to others in the viewport. \u21a9 Serialization is the process of data being converted into a byte stream for easier storage and transfer, think of it as similar to a save and load system. \u21a9 Peer-to-peer networking is where every machine to one another, which requires more bandwidth per client and more complex data authority handling but avoids needing a dedicated server. Peer-to-peer is generally harder to implement than client-server. \u21a9 Authority with regards to networking is when a certain machine, typically the server, has the control (final decision) on the state of a variable/script/entity. \u21a9 A hash is a structure that maps keys to values through a formula defined to convert structures into an index, typically the formula is constructed to avoid collisions between similar objects. The hash of the same object will always return the same value. \u21a9 Defragmentation is the process of reducing fragmentation (well that's a dumb definition), where fragmentation is where memory is used inefficiently with lots of gaps in between used memory chunks. In this context, defragmentation is used in the sense of keeping the rooms fully utilized, not wasting space on a fairly empty room. \u21a9 Each layer of the network stack has a duty, but they can follow those duties in different ways, and the different implementations of a layer is labeled a protocol . \u21a9 Simple DirectMedia Layer ( SDL) is a hardware abstraction layer for audio, input, and graphics across multiple platforms. \\ \u21a9 OpenGL , short for Open Graphics Library, a cross-language, cross-platform application programming interface (API) for rendering 2D and 3D vector graphics. The API is typically used to interact with a graphics processing unit (GPU), to achieve hardware-accelerated rendering. It's the underlying rendering library for many modern game engines. \u21a9 Microsoft DirectX is a collection of application programming interfaces (APIs) for handling tasks related to multimedia, especially game programming, on Microsoft platforms, like Windows and Xbox. It is most known for Direct3D which is the graphics API used for creating windows and rendering, and serves similar purposes as OpenGL. \u21a9 Application context is the context, the set of data required to interrupt and continue a task, of an application. \u21a9 An audio renderer is a system which plays/outputs spatialized sound, sound that is positioned in the world. \u21a9","title":"The Spectrum of Engine Development"},{"location":"interviews/OliverFranzke-advice/","text":"Oliver is a senior programmer at Double Fine Productions, where he has worked on many of their remastered classics like Grim Fandango Remastered and Day of the Tentacle Remastered. He lead the engineering helm of the new IP Broken Age as the lead programmer, where he extended all across its engine's development. We approached Oliver because of his presence as an engineer in the indie side of the industry, where he has spoken frequently about his experience and work as not only a programmer but also a leader. Seeing him in the documentary Double Fine Adventure! gave us a good understanding of some of his work prior to having our conversation. We did a Skype video call with him. Advice ( not verbatim): \u00b6 High-level vs Low-level Both are very important. The ratio between the two probably varies project to project, especially when you work from scratch. With Broken Age , the programmers needed to consider the high level concepts first before we got to the low level implementations. In games, things like UML don't make much sense. Low-level is still important because, at the end of the day, you need to get all of your work implemented on the hardware. For instance, the PS3. You need to be careful about not micro-optimizing all the way down because it's not always important. Multi-platform Development Double Fine Productions didn't use their previous engines; they went with an open source engine called Moai--it was already ported to Android and iOS. The first task was taking all of the amateur pieces and turning them professional, and trying to get older platforms like the Vita running. Because of the Kickstarter, the developers knew from day one that this was geared towards varying platforms, and in previous projects, they had faced that can of worms later in the project. Sometimes after you ship a game, you realize it would be good for other platforms and you need to rework your assets. Art Support vs. Programming Support Oliver prioritizes content creation issues; fortunately, at Double Fine, everyone was in the same room, so he could overhear some issue and immediately jump in on the conversation to solve the technical issues. Structuring Your First Engine \"I don't believe in generic game engines anymore;\" usually, there's a focus for the engine like 1 st or 3 rd person shooters. So pick something to aim at. Unless it's something that you want to make a commercial project, don't focus on abstractions. It'll make it cumbersome to get to the data you really need. Read a lot! Look for books on game engine architecture. Choosing Subsystems (Networking vs Physics) Adding a network to any game or game technology will multiply the complexity of said technology and edge cases that you'll find in games. Maybe you can design the technology with networking in mind, but leave the implementation for later. If you're doing physics AND networking, do absolutely the bare minimum for everything else! Just doing physics and networking is a super complicated problem; networking and physics live on opposite sides of the spectrum, with one being prediction-based and the other being time/physically-based. Technical Skills Technical skills to know for engine development: Multithreading/concurrency. Make parallel programming your second nature with every problem you think about--how you structure your data, when the problem is done- everything. Also, really get your linear algebra down because you have to do it at all times. Use hobby projects to get these technical skills down by focusing on the context of these particular skills.","title":"Oliver Franzke"},{"location":"interviews/OliverFranzke-advice/#advice-not-verbatim","text":"High-level vs Low-level Both are very important. The ratio between the two probably varies project to project, especially when you work from scratch. With Broken Age , the programmers needed to consider the high level concepts first before we got to the low level implementations. In games, things like UML don't make much sense. Low-level is still important because, at the end of the day, you need to get all of your work implemented on the hardware. For instance, the PS3. You need to be careful about not micro-optimizing all the way down because it's not always important. Multi-platform Development Double Fine Productions didn't use their previous engines; they went with an open source engine called Moai--it was already ported to Android and iOS. The first task was taking all of the amateur pieces and turning them professional, and trying to get older platforms like the Vita running. Because of the Kickstarter, the developers knew from day one that this was geared towards varying platforms, and in previous projects, they had faced that can of worms later in the project. Sometimes after you ship a game, you realize it would be good for other platforms and you need to rework your assets. Art Support vs. Programming Support Oliver prioritizes content creation issues; fortunately, at Double Fine, everyone was in the same room, so he could overhear some issue and immediately jump in on the conversation to solve the technical issues. Structuring Your First Engine \"I don't believe in generic game engines anymore;\" usually, there's a focus for the engine like 1 st or 3 rd person shooters. So pick something to aim at. Unless it's something that you want to make a commercial project, don't focus on abstractions. It'll make it cumbersome to get to the data you really need. Read a lot! Look for books on game engine architecture. Choosing Subsystems (Networking vs Physics) Adding a network to any game or game technology will multiply the complexity of said technology and edge cases that you'll find in games. Maybe you can design the technology with networking in mind, but leave the implementation for later. If you're doing physics AND networking, do absolutely the bare minimum for everything else! Just doing physics and networking is a super complicated problem; networking and physics live on opposite sides of the spectrum, with one being prediction-based and the other being time/physically-based. Technical Skills Technical skills to know for engine development: Multithreading/concurrency. Make parallel programming your second nature with every problem you think about--how you structure your data, when the problem is done- everything. Also, really get your linear algebra down because you have to do it at all times. Use hobby projects to get these technical skills down by focusing on the context of these particular skills.","title":"Advice (not verbatim):"},{"location":"interviews/RaymondGraham-interview/","text":"A Lost Art \u00b6 Raymond Graham has extensive experience working at the bleeding edge of technology. He has over 19 years experience developing 3D interactive entertainment products for various platforms (Xbox One, PS4, iOS, Xbox 360, PS3 and many others). Ray has worked in technical management, leadership and individual contributor positions at several leading Gaming and Entertainment companies, including Ubisoft, 2K Marin, Electronic Arts and Visual Concepts. (The following is the edited transcription of a conversation we had with Raymond Graham.) Who is Raymond Graham? \u00b6 I'm a graphics programmer currently working at Unity, but I've been all over the place. I was born in Jamaica, grew up in Toronto,Ontario, and went to school at the University of Waterloo. Out of school, I worked at NuFX in Chicago on some NBA games, then proceeded to work at Visual Concepts, EA, 2K Marin, and Ubisoft, working on games like NBA2K, The Godfather , BioShock 2 , and Splinter Cell: Blacklist as a graphics and tech lead. I spent some time at Apple working on mobile GPUs, then ended up going to Unity so that I could still help game developers even if I'm not working on games. I've been involved with Gameheads Oakland, a nonprofit group that teaches kids from high school to early college, who have little to no game development background how to make video games. I'm also part of /dev/color, an organization of black software engineers across all disciplines, but it's kind of weird because I'm one of only two video game engineers in that group in San Francisco; everybody else works at tech companies! It's kind of like a mentorship group, where everyone is trying to help each other achieve their professional goals. It's a great way of meeting more people in software engineering that are like me, and right now there are about 300+ members across San Francisco, New York, and most recently Seattle and Atlanta. The Console Evolution and Engine Implications \u00b6 For me, the most confusing part of programming on game engines was understanding how everything fits together. I wondered how a game engine even worked in the first place. The first game I did engine development for was NBA Street, where I was responsible for all the graphics work as well as loading assets on disk. I did a really terrible job of it at the time. The game shipped just fine, but I think I could have done a much better job if I had learned about things like disk I/O 1 and how long it takes something to read off disk and into memory. If I were to go back today and do it again, I think I could do it way better. There weren't any graphics or engine tricks we employed with early basketball games; we just worked a lot. It turned out to be harder than you'd think, because one of the main objectives was working within the memory and performance budgets. Every year the games need to introduce new features, which makes it a challenge to find enough memory to keep pace and fit everything in. What's more, you only have in a year to do all that. That's one of the reasons why I stopped working on basketball games; I wanted more time to try more cool things and research more things and make even better features. There were some really impressive things we did on the basketball games, but they were only important for sports games. Moving on in my career allowed me to explore what else was out there and have more learning experiences. Developing for consoles in the late 90's compared to consoles today was very similar, but also different in a lot of ways. That's because the Nintendo 64 and PlayStation were completely different pieces of hardware with different specs and texture requirements, which meant the whole pipeline of how you actually built your data was different. The Nintendo 64 had a completely different graphics pipeline, but we still tried to abstract as much as we could. For instance, the engine was designed with the gameplay stuff in one layer and then the core stuff in another layer that talks to the hardware so that core layer was kind of the same on all platforms. Nowadays, with Xbox One, PS4, and PC all essentially having the same architecture, I would say the process is a little bit easier. I think the console makers want to make things easier on the developers that are making games, and so they want their platform to be as easy to program for as possible. With the way hardware is now, though, I don't think we ever will go back to the PS3s' style. The thing about SPU 2 is that it works really differently from every other platform around, and so being able to to do it in a way that's cross-platform and that gives you enough time to actually really get the most out of that platform is definitely a challenge. It's a pity. It makes me real sad, because that's likely the way it's going to stay. It's disappointing because when you look at PS3 exclusive games, like the ones Naughty Dog made; they milked the most they could out of the console. It's an incredible platform to get the most power out of, but it's just too specialized. I loved working with the PS3 cell architecture! A little bit of background: When I worked on NBA Street , PS2 had the VU architecture with VU0 and VU1 chips 3 , and with that if you wanted to get the most out of your graphics platform, you would have VU1 basically doing the draw calls 4 , batching 5 up the polygons to send them over to the graphics chip. At the time, that was all a form of assembly language where you would just have to figure it out. I actually had the four black manuals on my desk that I would pull out to determine which bit goes where, and I found that really fun! PS3 and SPU is very similar to that, except its programming language is either C or C++. My brain naturally understood how it's supposed to work: You DMA 6 in and you double buffer 7 , and you work on the data as more data is being DMA'd in. From there you stream it out, switch to another buffer, call for the next DMA. Because of this, you can work on batches of data in a streaming parallelized fashion. Porting between Non-Compatible Architectures \u00b6 Porting BioShock to PS3 was hard, because one of the main requirements we were given was to keep all the data and level loading flow the same. The problem with that is that while Xbox 360 and PC have unified memory architecture 8 , the PS3 does not. PS3 has video memory and it has main memory, and you can't use the video memory for general purpose stuff because it was too slow to access it directly. With BioShock , we had a game made for unified memory architecture, and we were trying to get it to run on PS3; most of the system memory was graphics related but it couldn't all fit in the 256 megabytes of the PS3's video RAM. So there was this constant struggle of figuring out how to get the memory to fit. The last thing that we could have done (but would have been too much work) was cutting the levels up and adding loading screens. Because there was no streaming at that point, it was still \"load level\" and that was it. We didn't want to do that because it would change how the player experiences the game, and it would take all sorts of technical work to do that. In the end, we handled the issue by enabling the virtual memory 9 ; PS3 had the ability to use its hard drive as a backing for virtual memory, so we used that to fit the stuff that spilled over into virtual memory. It wasn't the greatest solution, and there's definitely some noticeable lag in the final product, but it was the only way that game would have shipped. After that, we were able to take what we learned when working on BioShock 2 and were able to budget for the PS3's memory restrictions and do it right. Even so, BioShock 2 shipped with the same PS3 virtual memory system. We had good intentions, but sometimes you just have to do whatever it takes to get the game done. That was a really hard problem; even at Ubisoft we faced the same issue. On PS3 you have these two different memory pools and then on Xbox 360 you have one, so managing memory in such a different way was a real challenge. On the graphics side of BioShock 2 , our improvements were more about making the engine ready to do better visuals on PS3 and Xbox 360 at the same time. We also added a few graphics features here and there to improve the game's look. For example, we added motion blur and implemented Unreal's material editor 10 so that the artists could actually have a proper material editor to make shaders. Previously the artists would have to bother programmers to implement every little one-off shader. All in all, though, we didn't do too many new engine things for BioShock 2 , since the art style was the same as the original. It was more about finding little places to improve, and also making sure the PS3 version was rock-solid this time. Once that was done we felt confident shipping it. The Winding Road of an Engine Developer \u00b6 I decided to go work at Apple because I had spent about 15 years in the game industry. After years at video game studios, I didn't really like how the games industry was ballooning. When I started on NBA Street, I was on a team of only six programmers. The NBA 2K team was maybe 20 programmers, and then by the time I was on Splinter Cell: Blacklist, I would be in meetings with over 100 other programmers. As video games got more complex, teams got bigger. Today, it's not uncommon to see a 800-1,000 person team. I was tired of working on those big teams, and I just wanted to do something smaller. Much like Apple, Unity definitely has a very rigorous testing process, because we make software for millions of people and that makes testing your code essential. I think I brought back some knowledge I had of the actual workings of how the chips on those devices work, which helps us to figure out what the fastest path is. That's huge when figuring out the best path to deliver graphics to phones. Being at Apple helped a lot in understanding how mobile devices in general work, so that broadened my skill set for sure. PC development is also definitely changing with the times. I think we're starting to see more people embracing low-level graphics APIs like Metal 11 and Vulkan 12 . Pretty much all the console devs are telling them, \"Welcome to the party, we've been doing this for years!\" On PS2 and on PS3, we were working with very specialized low-level API's that gave you access to the hardware. Finally now that people in the desktop space want Vulkan and Metal, we can tell them why that's important. Also, we're trying to use ECS 13 systems and data-oriented design at Unity, which I think is also something that we had to do on the console side to get performance. You had to have your data laid out efficiently to save memory and get performance. Now you're starting to see that be more of a focus in general code, which I think is a good thing. A lot of the other features we make at Unity are driven by the artists; if there's something they need to be able to do but don't have a solution, we create a solution out of necessity. As a graphics programmer, your number-one client is the art team. We're just making sure that they have all the tools they need to actually use the system. Working with artists and developers as part of the Spotlight team is kind of a mix of things. Most of our work is either implementing features for teams and then those features get rolled back into the engine itself, or we implement a feature for a team that just makes their game look cool. From there we can write a blog post about the cool feature that we made. Sometimes we come in near the end of development or in the middle, so it's kind of hard to change processes of how the team's working because, at that point, they're just interested in getting the game shipped. For that reason, we find it better to work with teams at the beginning of development so we can run them through how to make a Unity game really efficient, and from there they should be good to go. We usually work with a couple teams from the beginning stages on long engagements that will probably take a year or more. At the same time, we also will help small teams with one-off things that take only a month or two. With these different teams, we provide a variety of assistance. Advice to Kickstart a Career \u00b6 The changes in development from early 3D to now are hard to describe. I think I saw the advent of the programmable shader pipeline 15 , which I think completely changed everything. Then we saw the advent of compute shaders 16 , which has made getting into 3D graphics way harder. Back in the day it was much easier, because all your work just consisted of polygons and lights, and that was it. Now there's all these features and techniques that people are using for specific things, and all these different render paths 14 ; it's gotten a lot more complex these days. Keeping up with all of these new additions and bringing them into my day-to-day work has been my biggest challenge. I've been thinking a lot about how to help make things accessible for new graphics programmers. Early on, I think every programmer I knew had made a ray tracer 17 , and now that's all the rage again\u2014everyone's making ray tracers. I think that's a really good starting point, because it's just understanding the fundamentals of how light transport, reflections, refractions, and other essentials work. That's what helps graphics programmers build a solid foundation, and then they can build on that with more advanced skills. On top of that, definitely read every paper that's coming out and the latest things people are doing in the field. Quite frankly, there's just too much stuff to know. I think every good graphics programmer out there has to be able to communicate with their artists. As a graphics engineer, you're responsible for getting them the tools they need to make sure they fit within performance budgets and memory budgets. You need to be willing to take criticism and understand their goals. One of the main pointers I can give is that when people just come up to you and ask for a new feature, oftentimes younger programmers will go off and immediately get to work on that feature. When they bring it back, though, they've made something that is only kind of like what the artist asked for. When the artist sees it, they ask for something different that will meet their goals, and start piling more stuff onto the programmer. So before you do anything else, it's good to have an understanding of what the problem your artists are trying to solve is. From there, you can get the requirements of what is needed to solve that problem, and work with them on how to present the feature to them. A lot of times programmers will make a feature and then put some \"programmer UI\" on it and say it's done, but it's completely unusable thanks to that UI. So figure out how to make it usable for people who are not you\u2014that's also another key thing. Further down your career when you might be managing, balancing that with development is a common problem, one that I still have to this day. I try to manage it by keeping the team small. When I was at EA, the team was made up of three to four other graphics engineers, so I was still able to do my usual work while managing the team. I don't think I was a terribly good manager at that time, but I was still able to keep a 50/50 balance. I think all my management jobs have been like that, where I try to keep the teams small and still be able to work while managing. However, if I know there's a task that's going to take months of my time or requires me to sit down and really concentrate on something, I'm not gonna have time. I have to pass something like that on to somebody else who can focus 100% on that. One task like that specifically was all of the vision modes in Splinter Cell: Blacklist , like infrared. While it's a cool task, it also requires working really closely with artists, and then the design of it is gonna change constantly. Because I didn't have the time, I handed it off to another guy on the team so I don't even have to think or worry about it. A Lost Art \u00b6 Engine programming is a lost art. It's important there are students and developers out there trying something like making a game engine, because it's just not being taught anymore.Then thinking about it from the perspective of wanting to make a game, there's the question of if one should spend two to three years making an engine for the game, or if they should just use Unity or Unreal and call it a day. While there are some real monetary advantages to not writing your own engine, there's also different advantages to writing your own engine. You have to balance the pros and cons of the engine development process. There's some people online on Twitter who will scream that you have to write your own engine and you have to know it yourself, but I don't think that's the solution for everybody. At the same time, I think understanding how engines work and the low-level stuff is incredibly important for all programmers. I think we'll get to a point where only a few people know how to make engines really well; we might already be at that point.** These days, I think hiring a graphics or engine programmer is close to impossible. ** It's too hard to find people that know this stuff. Interview conducted October 15, 2018. Disk I/O includes read or write operations involving a physical disk. In general, to load an asset from the disk, the system will need to read it from the hard disk, write it into the memory (and possibly cache), which takes a lot of time. \u21a9 A Synergistic Processing Unit (SPU) is the processing unit associated with the Cell architecture of the PS3. The PS3 had seven as part of its hardware, only six of which were usable by game developers. \u21a9 Vector unit architecture (VU) is the architecture for the Emotion Engine that was used in the Playstation 2 console. The two processing units were focused for 3D math and predecessor for the vertex shader pipelines. \u21a9 A draw call is a command from CPU to GPU that contains all the information encapsulated by CPU about textures, states, shaders, rendering objects, buffers, etc. \u21a9 Encapsulating a draw call is expensive, and the GPU can render fairly fast, so batching draw calls up is a good technique to speed up. \u21a9 Direct memory access (DMA) is a technique of computer systems that allows certain hardware subsystems to access main system memory without taking up the CPU cycles. \u21a9 Double buffer is the use of two buffers to hold data. By switching the buffers, the reader can see the complete version of data instead of a partially written one. \u21a9 Unified memory architecture use a portion of a computer's RAM rather than dedicated graphics memory. It is a single memory address space accessible from any processor in a system. \u21a9 Virtual memory is a memory management technique that abstracts uniformed memory space from different kind of storage device. \u21a9 The Unreal Material Editor is a node-based graph interface that enables you to create shaders. For more see the Unreal Documentation . \u21a9 Metal is a low-level, low-overhead hardware-accelerated 3D graphic and compute shader application programming interface (API) developed by Apple Inc. \u21a9 Vulkan is a low-overhead, cross-platform 3D graphics and compute API targeting high-performance realtime 3D graphics applications such as video games and interactive media across all platforms. \u21a9 Entity-Component-System (ECS) is an architectural pattern that follows composition over inheritance principle and is mostly used in games. \u21a9 Render paths are programs to affect the shading/rendering of lighting and shadow fidelity, along with other graphic details, with different performance characteristics. \u21a9 Programmable shader pipeline allows the developer to customize some phases in the render pipeline (mostly the vertex processing phase and the fragment shader phase). It was introduced by OpenGL 3.2 in 2009. \u21a9 A Compute Shader is a shader stage that is used entirely for computing arbitrary information. While it can do rendering, it is generally used for tasks not directly related to drawing triangles and pixels. \u21a9 In computer graphics,** ray tracing** is a rendering technique for generating an image by tracing the path of light as pixels in an image plane and simulating the effects of its encounters with virtual objects. \u21a9","title":"Interview"},{"location":"interviews/RaymondGraham-interview/#a-lost-art","text":"Raymond Graham has extensive experience working at the bleeding edge of technology. He has over 19 years experience developing 3D interactive entertainment products for various platforms (Xbox One, PS4, iOS, Xbox 360, PS3 and many others). Ray has worked in technical management, leadership and individual contributor positions at several leading Gaming and Entertainment companies, including Ubisoft, 2K Marin, Electronic Arts and Visual Concepts. (The following is the edited transcription of a conversation we had with Raymond Graham.)","title":"A Lost Art"},{"location":"interviews/RaymondGraham-interview/#who-is-raymond-graham","text":"I'm a graphics programmer currently working at Unity, but I've been all over the place. I was born in Jamaica, grew up in Toronto,Ontario, and went to school at the University of Waterloo. Out of school, I worked at NuFX in Chicago on some NBA games, then proceeded to work at Visual Concepts, EA, 2K Marin, and Ubisoft, working on games like NBA2K, The Godfather , BioShock 2 , and Splinter Cell: Blacklist as a graphics and tech lead. I spent some time at Apple working on mobile GPUs, then ended up going to Unity so that I could still help game developers even if I'm not working on games. I've been involved with Gameheads Oakland, a nonprofit group that teaches kids from high school to early college, who have little to no game development background how to make video games. I'm also part of /dev/color, an organization of black software engineers across all disciplines, but it's kind of weird because I'm one of only two video game engineers in that group in San Francisco; everybody else works at tech companies! It's kind of like a mentorship group, where everyone is trying to help each other achieve their professional goals. It's a great way of meeting more people in software engineering that are like me, and right now there are about 300+ members across San Francisco, New York, and most recently Seattle and Atlanta.","title":"Who is Raymond Graham?"},{"location":"interviews/RaymondGraham-interview/#the-console-evolution-and-engine-implications","text":"For me, the most confusing part of programming on game engines was understanding how everything fits together. I wondered how a game engine even worked in the first place. The first game I did engine development for was NBA Street, where I was responsible for all the graphics work as well as loading assets on disk. I did a really terrible job of it at the time. The game shipped just fine, but I think I could have done a much better job if I had learned about things like disk I/O 1 and how long it takes something to read off disk and into memory. If I were to go back today and do it again, I think I could do it way better. There weren't any graphics or engine tricks we employed with early basketball games; we just worked a lot. It turned out to be harder than you'd think, because one of the main objectives was working within the memory and performance budgets. Every year the games need to introduce new features, which makes it a challenge to find enough memory to keep pace and fit everything in. What's more, you only have in a year to do all that. That's one of the reasons why I stopped working on basketball games; I wanted more time to try more cool things and research more things and make even better features. There were some really impressive things we did on the basketball games, but they were only important for sports games. Moving on in my career allowed me to explore what else was out there and have more learning experiences. Developing for consoles in the late 90's compared to consoles today was very similar, but also different in a lot of ways. That's because the Nintendo 64 and PlayStation were completely different pieces of hardware with different specs and texture requirements, which meant the whole pipeline of how you actually built your data was different. The Nintendo 64 had a completely different graphics pipeline, but we still tried to abstract as much as we could. For instance, the engine was designed with the gameplay stuff in one layer and then the core stuff in another layer that talks to the hardware so that core layer was kind of the same on all platforms. Nowadays, with Xbox One, PS4, and PC all essentially having the same architecture, I would say the process is a little bit easier. I think the console makers want to make things easier on the developers that are making games, and so they want their platform to be as easy to program for as possible. With the way hardware is now, though, I don't think we ever will go back to the PS3s' style. The thing about SPU 2 is that it works really differently from every other platform around, and so being able to to do it in a way that's cross-platform and that gives you enough time to actually really get the most out of that platform is definitely a challenge. It's a pity. It makes me real sad, because that's likely the way it's going to stay. It's disappointing because when you look at PS3 exclusive games, like the ones Naughty Dog made; they milked the most they could out of the console. It's an incredible platform to get the most power out of, but it's just too specialized. I loved working with the PS3 cell architecture! A little bit of background: When I worked on NBA Street , PS2 had the VU architecture with VU0 and VU1 chips 3 , and with that if you wanted to get the most out of your graphics platform, you would have VU1 basically doing the draw calls 4 , batching 5 up the polygons to send them over to the graphics chip. At the time, that was all a form of assembly language where you would just have to figure it out. I actually had the four black manuals on my desk that I would pull out to determine which bit goes where, and I found that really fun! PS3 and SPU is very similar to that, except its programming language is either C or C++. My brain naturally understood how it's supposed to work: You DMA 6 in and you double buffer 7 , and you work on the data as more data is being DMA'd in. From there you stream it out, switch to another buffer, call for the next DMA. Because of this, you can work on batches of data in a streaming parallelized fashion.","title":"The Console Evolution and Engine Implications"},{"location":"interviews/RaymondGraham-interview/#porting-between-non-compatible-architectures","text":"Porting BioShock to PS3 was hard, because one of the main requirements we were given was to keep all the data and level loading flow the same. The problem with that is that while Xbox 360 and PC have unified memory architecture 8 , the PS3 does not. PS3 has video memory and it has main memory, and you can't use the video memory for general purpose stuff because it was too slow to access it directly. With BioShock , we had a game made for unified memory architecture, and we were trying to get it to run on PS3; most of the system memory was graphics related but it couldn't all fit in the 256 megabytes of the PS3's video RAM. So there was this constant struggle of figuring out how to get the memory to fit. The last thing that we could have done (but would have been too much work) was cutting the levels up and adding loading screens. Because there was no streaming at that point, it was still \"load level\" and that was it. We didn't want to do that because it would change how the player experiences the game, and it would take all sorts of technical work to do that. In the end, we handled the issue by enabling the virtual memory 9 ; PS3 had the ability to use its hard drive as a backing for virtual memory, so we used that to fit the stuff that spilled over into virtual memory. It wasn't the greatest solution, and there's definitely some noticeable lag in the final product, but it was the only way that game would have shipped. After that, we were able to take what we learned when working on BioShock 2 and were able to budget for the PS3's memory restrictions and do it right. Even so, BioShock 2 shipped with the same PS3 virtual memory system. We had good intentions, but sometimes you just have to do whatever it takes to get the game done. That was a really hard problem; even at Ubisoft we faced the same issue. On PS3 you have these two different memory pools and then on Xbox 360 you have one, so managing memory in such a different way was a real challenge. On the graphics side of BioShock 2 , our improvements were more about making the engine ready to do better visuals on PS3 and Xbox 360 at the same time. We also added a few graphics features here and there to improve the game's look. For example, we added motion blur and implemented Unreal's material editor 10 so that the artists could actually have a proper material editor to make shaders. Previously the artists would have to bother programmers to implement every little one-off shader. All in all, though, we didn't do too many new engine things for BioShock 2 , since the art style was the same as the original. It was more about finding little places to improve, and also making sure the PS3 version was rock-solid this time. Once that was done we felt confident shipping it.","title":"Porting between Non-Compatible Architectures"},{"location":"interviews/RaymondGraham-interview/#the-winding-road-of-an-engine-developer","text":"I decided to go work at Apple because I had spent about 15 years in the game industry. After years at video game studios, I didn't really like how the games industry was ballooning. When I started on NBA Street, I was on a team of only six programmers. The NBA 2K team was maybe 20 programmers, and then by the time I was on Splinter Cell: Blacklist, I would be in meetings with over 100 other programmers. As video games got more complex, teams got bigger. Today, it's not uncommon to see a 800-1,000 person team. I was tired of working on those big teams, and I just wanted to do something smaller. Much like Apple, Unity definitely has a very rigorous testing process, because we make software for millions of people and that makes testing your code essential. I think I brought back some knowledge I had of the actual workings of how the chips on those devices work, which helps us to figure out what the fastest path is. That's huge when figuring out the best path to deliver graphics to phones. Being at Apple helped a lot in understanding how mobile devices in general work, so that broadened my skill set for sure. PC development is also definitely changing with the times. I think we're starting to see more people embracing low-level graphics APIs like Metal 11 and Vulkan 12 . Pretty much all the console devs are telling them, \"Welcome to the party, we've been doing this for years!\" On PS2 and on PS3, we were working with very specialized low-level API's that gave you access to the hardware. Finally now that people in the desktop space want Vulkan and Metal, we can tell them why that's important. Also, we're trying to use ECS 13 systems and data-oriented design at Unity, which I think is also something that we had to do on the console side to get performance. You had to have your data laid out efficiently to save memory and get performance. Now you're starting to see that be more of a focus in general code, which I think is a good thing. A lot of the other features we make at Unity are driven by the artists; if there's something they need to be able to do but don't have a solution, we create a solution out of necessity. As a graphics programmer, your number-one client is the art team. We're just making sure that they have all the tools they need to actually use the system. Working with artists and developers as part of the Spotlight team is kind of a mix of things. Most of our work is either implementing features for teams and then those features get rolled back into the engine itself, or we implement a feature for a team that just makes their game look cool. From there we can write a blog post about the cool feature that we made. Sometimes we come in near the end of development or in the middle, so it's kind of hard to change processes of how the team's working because, at that point, they're just interested in getting the game shipped. For that reason, we find it better to work with teams at the beginning of development so we can run them through how to make a Unity game really efficient, and from there they should be good to go. We usually work with a couple teams from the beginning stages on long engagements that will probably take a year or more. At the same time, we also will help small teams with one-off things that take only a month or two. With these different teams, we provide a variety of assistance.","title":"The Winding Road of an Engine Developer"},{"location":"interviews/RaymondGraham-interview/#advice-to-kickstart-a-career","text":"The changes in development from early 3D to now are hard to describe. I think I saw the advent of the programmable shader pipeline 15 , which I think completely changed everything. Then we saw the advent of compute shaders 16 , which has made getting into 3D graphics way harder. Back in the day it was much easier, because all your work just consisted of polygons and lights, and that was it. Now there's all these features and techniques that people are using for specific things, and all these different render paths 14 ; it's gotten a lot more complex these days. Keeping up with all of these new additions and bringing them into my day-to-day work has been my biggest challenge. I've been thinking a lot about how to help make things accessible for new graphics programmers. Early on, I think every programmer I knew had made a ray tracer 17 , and now that's all the rage again\u2014everyone's making ray tracers. I think that's a really good starting point, because it's just understanding the fundamentals of how light transport, reflections, refractions, and other essentials work. That's what helps graphics programmers build a solid foundation, and then they can build on that with more advanced skills. On top of that, definitely read every paper that's coming out and the latest things people are doing in the field. Quite frankly, there's just too much stuff to know. I think every good graphics programmer out there has to be able to communicate with their artists. As a graphics engineer, you're responsible for getting them the tools they need to make sure they fit within performance budgets and memory budgets. You need to be willing to take criticism and understand their goals. One of the main pointers I can give is that when people just come up to you and ask for a new feature, oftentimes younger programmers will go off and immediately get to work on that feature. When they bring it back, though, they've made something that is only kind of like what the artist asked for. When the artist sees it, they ask for something different that will meet their goals, and start piling more stuff onto the programmer. So before you do anything else, it's good to have an understanding of what the problem your artists are trying to solve is. From there, you can get the requirements of what is needed to solve that problem, and work with them on how to present the feature to them. A lot of times programmers will make a feature and then put some \"programmer UI\" on it and say it's done, but it's completely unusable thanks to that UI. So figure out how to make it usable for people who are not you\u2014that's also another key thing. Further down your career when you might be managing, balancing that with development is a common problem, one that I still have to this day. I try to manage it by keeping the team small. When I was at EA, the team was made up of three to four other graphics engineers, so I was still able to do my usual work while managing the team. I don't think I was a terribly good manager at that time, but I was still able to keep a 50/50 balance. I think all my management jobs have been like that, where I try to keep the teams small and still be able to work while managing. However, if I know there's a task that's going to take months of my time or requires me to sit down and really concentrate on something, I'm not gonna have time. I have to pass something like that on to somebody else who can focus 100% on that. One task like that specifically was all of the vision modes in Splinter Cell: Blacklist , like infrared. While it's a cool task, it also requires working really closely with artists, and then the design of it is gonna change constantly. Because I didn't have the time, I handed it off to another guy on the team so I don't even have to think or worry about it.","title":"Advice to Kickstart a Career"},{"location":"interviews/RaymondGraham-interview/#a-lost-art_1","text":"Engine programming is a lost art. It's important there are students and developers out there trying something like making a game engine, because it's just not being taught anymore.Then thinking about it from the perspective of wanting to make a game, there's the question of if one should spend two to three years making an engine for the game, or if they should just use Unity or Unreal and call it a day. While there are some real monetary advantages to not writing your own engine, there's also different advantages to writing your own engine. You have to balance the pros and cons of the engine development process. There's some people online on Twitter who will scream that you have to write your own engine and you have to know it yourself, but I don't think that's the solution for everybody. At the same time, I think understanding how engines work and the low-level stuff is incredibly important for all programmers. I think we'll get to a point where only a few people know how to make engines really well; we might already be at that point.** These days, I think hiring a graphics or engine programmer is close to impossible. ** It's too hard to find people that know this stuff. Interview conducted October 15, 2018. Disk I/O includes read or write operations involving a physical disk. In general, to load an asset from the disk, the system will need to read it from the hard disk, write it into the memory (and possibly cache), which takes a lot of time. \u21a9 A Synergistic Processing Unit (SPU) is the processing unit associated with the Cell architecture of the PS3. The PS3 had seven as part of its hardware, only six of which were usable by game developers. \u21a9 Vector unit architecture (VU) is the architecture for the Emotion Engine that was used in the Playstation 2 console. The two processing units were focused for 3D math and predecessor for the vertex shader pipelines. \u21a9 A draw call is a command from CPU to GPU that contains all the information encapsulated by CPU about textures, states, shaders, rendering objects, buffers, etc. \u21a9 Encapsulating a draw call is expensive, and the GPU can render fairly fast, so batching draw calls up is a good technique to speed up. \u21a9 Direct memory access (DMA) is a technique of computer systems that allows certain hardware subsystems to access main system memory without taking up the CPU cycles. \u21a9 Double buffer is the use of two buffers to hold data. By switching the buffers, the reader can see the complete version of data instead of a partially written one. \u21a9 Unified memory architecture use a portion of a computer's RAM rather than dedicated graphics memory. It is a single memory address space accessible from any processor in a system. \u21a9 Virtual memory is a memory management technique that abstracts uniformed memory space from different kind of storage device. \u21a9 The Unreal Material Editor is a node-based graph interface that enables you to create shaders. For more see the Unreal Documentation . \u21a9 Metal is a low-level, low-overhead hardware-accelerated 3D graphic and compute shader application programming interface (API) developed by Apple Inc. \u21a9 Vulkan is a low-overhead, cross-platform 3D graphics and compute API targeting high-performance realtime 3D graphics applications such as video games and interactive media across all platforms. \u21a9 Entity-Component-System (ECS) is an architectural pattern that follows composition over inheritance principle and is mostly used in games. \u21a9 Render paths are programs to affect the shading/rendering of lighting and shadow fidelity, along with other graphic details, with different performance characteristics. \u21a9 Programmable shader pipeline allows the developer to customize some phases in the render pipeline (mostly the vertex processing phase and the fragment shader phase). It was introduced by OpenGL 3.2 in 2009. \u21a9 A Compute Shader is a shader stage that is used entirely for computing arbitrary information. While it can do rendering, it is generally used for tasks not directly related to drawing triangles and pixels. \u21a9 In computer graphics,** ray tracing** is a rendering technique for generating an image by tracing the path of light as pixels in an image plane and simulating the effects of its encounters with virtual objects. \u21a9","title":"A Lost Art"},{"location":"interviews/RaymondGraham-video/","text":"The Havoc of Havok \u00b6","title":"Video Story"},{"location":"interviews/RaymondGraham-video/#the-havoc-of-havok","text":"","title":"The Havoc of Havok"},{"location":"interviews/ShaneeNishry-interview/","text":"Going Beyond the Books \u00b6 Shanee Nishry is a VR Software Engineer at Google working on the Daydream team. She is concurrently developing her own engine for a fantasy RTS game. Shanee has also programmed for game engines at companies including Side-kick and Moon Active. In her spare time, Shanee practices swordfighting and European martial arts. (The following is the edited transcription of a conversation we had with Shanee Nishry.) Who is Shanee? \u00b6 My school background is kind of boring. Other than high school, I'm self-taught, so I have no degree or any formal studies in the field. I started a Biotechnology degree, but I didn't finish it because I got into a game studio in the meantime. Maybe when I was about 18 or 19, while I was in my Chemistry and Biotechnology courses, I started studying C++ and I wanted to make a video game. I started teaching myself DirectX, so I used a bunch of books like Programming Role Playing Games with DirectX by Jim Adams and Game Coding Complete by Mike McShaffry. Both are really awesome books that I'd definitely recommend. I started making my first engine by following these books because I knew absolutely nothing, but these two were brilliant and gave really good example code. By the end of following them, I went from having no idea what I was doing to kind of having an idea what I was doing! Because I started programming game engines quite a while ago, I can't think of what was specifically the most confusing aspect. Truth be told, back then I literally knew nothing about game engine programming; I didn't even know how to code properly! I remember one of the first books that I used for C++ was called Beginning C++ Through Game Programming . I just followed the books and adapted its code to do what I needed, so the process was pretty simple. Keeping it Minimal, Decoupled & Data-Oriented \u00b6 There are a few things that have changed since I started in the games industry. One new paradigm that is very common in the industry right now is data-oriented design. Before that was introduced, everything was classes and hierarchies. Another recent question is \"how do you make the systems in your game interact with one another?\" So if you have input and you have your graphics and animations, you have to figure out how they will interact in a meaningful way. You also have to determine what gets access to the data. There are so many different paradigms to handle those questions. Some people will tell you to send an event whenever your position is changed, but then when you do that and learn about data-oriented design, you figure out everything is going in your cache and sending an event for every entity. It's just a mess. When working on Super Sam Adventures , we had to figure out how to access the transformation for an entity from different systems. We were originally using a map to access entities and then get the component out of them, and we noticed that on Android and iOS at the time, that map was so expensive because every lookup resulted in cache misses and we were doing too many lookups in the map. It just ruined the frame rate, so we had to change that. All of these questions about access, interaction between systems, and what a system actually needs to do were probably my biggest questions after I had a little bit of experience in game development. When you know nothing, you just follow some book or tutorial, and it all makes sense because you're doing what the book is telling you to do. When you start doing your own thing, though, things don't work as naturally. So maybe the biggest question I've had since then, and in some ways I (and other people I know) am still trying to answer it today, is about the interaction between systems; what is the responsibility of a system and how much should it be engineered? I feel like the general consensus these days\u2014and it might change a few years from now\u2014is that a system should be as small as it can be and do the minimum that it should do. The interaction between systems is a very dependent subject, but my preference is that it be as direct and as minimal as possible. By minimal I mean getting all the data that you'd need in one function, if possible. So for example, if I have a transformation system and a rendering system then my render system needs to get the transformation for its entities from the transformation system. Instead of doing it for every entity each time, if you can just make a synchronization point in your engine that says \"now I'm going to transfer all the data for the render system\", then that's probably better. Having specific synchronization points like that allows you to heavily specialize or multi-thread your systems. At the same time, I'm recruiting all of my threads to just fill up these few arrays of transformations, which will then be filled in the layout that the render system wants them to be. The render system then just takes the data and does whatever it does, and the rest of the engine is free to do whatever. That way, if you have any kind of multi-threading but you only have the synchronization points, there's no risk of a collision between threads because all of your data has been copied at a very specific point in time. That's the main idea: Make the systems as small as you can and interact as direct (but as little) as you can. One problem that you could find from this is having different implementations of an API and dealing with changes to that API, but it's a different problem which you might be able to avoid entirely. Developing for an iOS Engine \u00b6 The Super Sam team was very iOS focused. I felt like I was the only Android person in the group. Most of the people did not put considerations into cross-platform development, but I did and I feel like it paid off, because the game was eventually ported to Android and didn't take too much work. At the end of the day, I feel like as long as you keep the very specific interaction with a platform abstracted from the rest of the engine, then it shouldn't take too long to port it to a similar platform. I think that the most difficulty comes when you have a PC game and then you want to port it to mobile or console, because that brings two big issues to the table. One of them is the performance and the memory requirements that are often different, and the other problem is just the input\u2014moving from keyboard to controller or from touch input to keyboard and mouse. One of the special considerations we had to have when developing for mobile was binary size 1 . And that's partially why we developed our own physics system. We didn't want to link with big external libraries because we wanted to keep the binary and final app size extremely low, and people simply didn't install heavy games. The expectation is that a person would install a game using mobile network which could be slow and limited. Additionally, the Google Store and AppStore would show a warning when trying to install a game over a certain size and most people would simply not install when the warning showed up. One of the reasons not to use Unity back then was because Unity on its own added quite a few megabytes, and that was a big no-no. Developing for mobile was definitely an interesting experience especially back at the time between the iPhone 3GS and the iPhone 4. For the upgrade to iPhone 4, Apple pretty much quadrupled the number of pixels and doubled the resolution, but the GPU wasn't much better. Suddenly you render the game on an iPhone 4, which is supposed to be better, but the frame rate is just shit compared to an iPhone 3GS. The other fun part of working on mobile was memory, and this is something that is often shared by people who worked on older consoles where memory was so limited. In much the same way, mobile was also extremely limited; you had to make sure that you knew exactly how much memory is used by your app and GPU at any given time. Caching in on Memory \u00b6 I feel like the biggest concerns with developing solely for iOS were memory management and performance. We really wanted to hit 60 frames per second and have it very smooth with no frame drops whatsoever. We also had problems keeping memory low so the app doesn't get killed randomly by the OS, which was really annoying on mobile platforms. We had to make some kind of a list of least-used memory so if a block of memory was not being used then you could just discard it. We also had to limit and deny allocations above a certain amount of memory. It involves making sure that everything fits into a very specific and tight memory requirement, and it means that we have to immediately discard anything that was not used when you had to have something allocated. On the other hand, we would not just immediately discard a texture if it doesn't have references because it might have a reference in five frames, and loading it again would be silly. If you have it in memory and then a new memory allocation needs to be made and that texture is occupying the least used memory block, then you can discard it. That's LRU cache 2 basically. Getting the scene and managing it with all of the assets and managing the level so they fit into that very tight requirement was a very large amount of work, especially because Super Sam was an infinite scroller; you'd basically fall down in this specific type of world environment, and then at some point it might change to a different one. We had to make sure that these change areas were as seamless as possible and without any kind of frame spikes. When it comes to dealing with memory management issues, you should determine how much memory you're actually allocating and whether or not that matches your diagnostic tool. Then you can see how much memory is in graphics, how much of that is textures, and how much is meshes, and so on. Not to mention how much memory is going to your entity component system data. It's a really big question of how much memory you actually need and how strict you need to be in memory management. The game I'm working on right now is for PC, not mobile, so instead of being very strict on memory, I'm just focusing on the way that memory and systems are being used. Because of this, I don't need to track it all in a centralized place. On mobile, where memory was way tighter, it was definitely needed. Engine API Design is a Thing \u00b6 I've taken the approaches of both overriding the new and delete functions and constructing my own APIs for memory management. Even so, I can't say which one I prefer. I guess overwriting it made it easy to ensure that everyone is using it, but it was very implicit and sometimes implicit is not very good. I often find if you're managing memory, then you probably want to know exactly what memory allocator you're using anyway. So just overwriting a global function is not necessarily the best idea. For example, if I want to do a temporary allocation that's nearly immediately released, then I know I need to access the linear memory allocator (one that is reset every frame) and I probably also know the size, so I can instantiate a specific memory allocator. But if I override the global new allocator and then I have a general memory management class that tries to manage all kinds of use cases, it's probably not going to do the best job. It might do a good job, but the best is when the user actually knows what they're trying to achieve and they can access a specialized function or API to do exactly that. Also, the platform memory allocators have improved so much in recent years that if you're just replacing the new allocator, there might not be a lot of advantage to it, but if you have a specific use case that needs to be optimized then you should use a memory allocator there. I'll give a quick example of this: When making systems, you often need to allocate memory blocks for your entity components (i.e. this is my transformation, this is my physics component, etc.) You know exactly what the page size is going to be, so you can just use a custom allocator for that system. You also know roughly how many entities you're going to have, and even if you didn't anticipate and you had to increase the size of the memory block, then you can easily just acquire another memory block and use it for the memory allocator later. Then you can, again, just allocate and release pages from that allocator for the specific use case. That's where I find memory allocators to be more useful\u2014when I know how they're going to be used, and what they're going to do exactly. In that way, the user can specify the memory size they want and where it will go. Following a similar principle outside of memory API, no matter what engine I'm working on, I try to make an API as small and easy to use as possible. To do that, I do some stuff that other developers would probably kill me for. There is different advice out there on the use of singletons versus dependency injections 3 , but generally the consensus is that you should have some kind of a context for whatever stuff that you're using. But at the end of the day, if I am a game developer using an engine, all I really want to know is where on the screen my mouse is! So I literally just have a namespace Input , and GetMousePosition is a global function in that namespace. Having a namespace housing functions is something that can be controversial, because the function doesn't have context as to which window it is listening to. However, I find it very easy to use because I can easily tell someone to go into the codebase and do Input::GetMousePosition , Input::GetMouseButton , etc., and structuring it like that really helped my productivity. So I try to figure out the APIs in terms of what is going to be useful and how easy it will be use it. What does the developer need to do, and how will the API accommodate that? The trouble often comes when your system tries to do too much. That's why I advocate making small, specialized systems as opposed to big, beastly systems. For example, if you have a rendering system and inside that you have an AddWater and AddBox and AddSphere , then it can probably be split into a few different systems, each specializing in their own specific things. Good API design can also help port your game to a new platform. At the end of the day, if you expose all of your engine's functionality then you can figure out a way around it later. So if you have access to the keyboard and mouse input and also to touch inputs as part of the engine, then you don't need to specifically map touch input to a mouse button or a mouse input. You can map it that way, but you don't always want to do it. Often if you do it that way, people will say \"hey, I need a way to recognize gesture and multi-touch\" and so on, and it will just come and bite you back if you've done premature abstraction. That's another lesson, not to do too much abstraction. As long as you have your own engine layer communicating with the OS and then give in the data with the least abstraction as possible and just hide what OS it is and hide what functionality is, then that will be good enough. Editing with a Level Editor \u00b6 For developing a level editor, the best advice I can give is to figure out what is actually needed out of it. Determine who is going to use it and the first functionality or two that they are going to need, and then work from there. It doesn't need to be perfect, it just needs to have the bare minimum of functionality supported. For example, in my editor, I have three main functionalities that I need. One of them is the elevation editing for the heightmap 4 , another is the texture editing for the terrain, and the last one is the placement of entities into the world. Once you identify those minimum features, you should be able to implement the editor pretty quickly. The biggest mistake I learned from that experience is over-engineering; do not make features for the sake of making features, ever . This is especially the case when you're working on an engine, where you should always have a use case in mind. It's ideal if you are able to make a small game while you're making the engine, even if it's as simple as Pong or Breakout . If you can make something like that with your engine easily, then you're doing well! When I made my very first level editor, it was very tightly related to the game engine itself. That was the first mistake, because it was basically living in the same application and codebase, and so it affected everything else a lot. When making the Super Sam editor, it was an entirely different application, and it didn't even share any of the code base; all it needed to know was the level format and how to output that. We didn't even use the engine to render the levels. The biggest question that we had back then was how to make the levels be supported across multiple games and versions. The level format was used for Bitter Sam , Super Sam , Super Sam Adventures , and possibly other games in the future. Figuring out how to differentiate between different games in your level format when using the same editor was one question, and the other was figuring out versioning if you added a new component or creature. We also needed to be able to work around the different versions of the app that may be installed on smartphones. There could potentially be a mismatch between the version of the game on the phone and the level being downloaded from the server, which is completely unrelated to the update mechanism on the phone. If the level has something like a new monster in it but its texture isn't in the application, then things don't work. For those reasons, compatibility support was really important back then. The level editor of Super Sam was using a different framework to render the things in the editor as compared to the game. Personally, I want to match my level editor renderer with my engine renderer as closely as I can. I love it when you have \"what you see is what you get\", and also when you can play inside the editor. However, that's possibly over-engineering at times and brings in too many additional features. For my current level editor, I am using my game engine, but I'm making it in an entirely new application and only linking to whatever code that I need to. For example, right now I'm not linking to any of the gameplay stuff, but only to the rendering frameworks and the input and so on. Using the engine's render in the editor you get the same look and feel, you get the same kind of performance, but it's not the only option. If you look at games like Warcraft and Starcraft , they have a level editor which is not using their engine's UI; it's using just generic OS UI. But they have a viewport 5 that draws the game window into it, and even in that viewport they added functionality for things like zooming out further in the game. That's just an example of you having all the UI in your game engine, or mix and match; you can use OS functionality to make UI easier to do, while also importing the entity rendering and terrain rendering from your engine to do that. After-Hours Game Engine \u00b6 For my own engine architecture, I find that the biggest challenge is rendering large levels efficiently. My solution to that is chunking the world into different segments, which will let you immediately discard thousands of objects because they are off-screen. Another challenge is AI. My RTS is atypical because you can't control every unit; each unit has its own priorities and goals it wants to achieve in life (like making a video game!). I'm using a goal-oriented action planner 6 . So in the beginning, when having only a hundred entities, there's not much of a problem. But then, as you get to thousands of entities, you have to manage to keep some level of detail for the AI. You can't just make the AI stop functioning because they are off-screen; they are still alive and progressing. That's an interesting challenge that's so far removed from rendering, which I feel like is my main expertise. I can't just say \"If you don't see it, it doesn't exist.\" When a tree falls in the forest and nobody is around, it still exists! When developing a system I try to keep it as minimal and specialized as I can. For example, I'll do something like a quadtree 7 division with rendering, and that way it's really easy to discard entities not present on the screen. Similarly, using this quadtree division, I can put it into the AI and just update them at a different level of detail. That's the main consideration that I've had with my engine. Other than that, rendering things that scale terrain is a lot of fun. Just using CDLOD 8 for the giant terrain. If I remember correctly, the way I handled serialization in Super Sam was to just give an entity minimal transformation stuff like position and rotation, and then a tag of what entity it is. With my RTS, I take a similar approach where I have a blueprint that all entities can be instantiated from. In my case, I would have a footman or a werewolf blueprint, and then I can just say that an entity is referring to this blueprint while it holds its own transformation in the world. An interesting thing which I still haven't decided what to do if I need to have a unique entity for a level. The current idea I have is to add an additional blueprint definition that is level-specific, and then when instantiating that special entity it will still use the same system saying \"this is my position and this is the blueprint header file and that will have all of the data.\" I'm still not sure about this approach, because what if I went to add, say, a wounded soldier somewhere? Do I need to make a special blueprint for him? That's a problem that I just haven't reached yet, and there are different ideas on how to solve it. At the moment, I'm using JSON 9 to serialize stuff. I find that it's really useful if you do not have a binary format when you're just starting with things. If you're just getting to serialization, find a format that is easy to use and is human-readable, and only when you need to publish a product make it into a binary format that is small and specialized. Being Your Own Product Manager \u00b6 If you're doing a personal project at the same time you're working a full-time job, you basically have to be your own product manager. You have to set goals both for your time at work and your time at home. You have to say, \"okay, I'm going to work and I need to get to these milestones. Today I'm going to shape this feature.\" And then you focus on that specific feature. But then when you head back home, it's really easy to just lie down in bed, watch Netflix, and neglect your project. The only way to prevent it is not by gathering motivation but by being persistent. You have to be determined to spend at least five or ten minutes a day on your personal work. Figuring out where you left off can be hard, though, so be sure to keep notes so you can remember what is most important to be working on at the moment. From there, determine the minimum tasks you need to do and then just sit down and do them, and the motivation will come as you do it. Specialized Engines Aren't Going Away \u00b6 Many companies are making very good engines and improving their existing engines. I think we can look at Unity as an example here, because they have a very easy to use and commonly-used game engine, but even now they are making huge modifications to it. They have the Scriptable Render Pipeline 10 , and they are actively developing a new entity component system. With all that, we can see that even the big engines have to change. I think that's a sign of why specialized engines still exist; it's really difficult to make a generic engine that has everything. On the one hand, it's awesome because they're really making game development more accessible\u2014I know so many people that are not going to make a game engine but can just jump into Unity or Unreal and publish games. On the other hand, with big engines you're going to run into problems, like what happens when I need to have a million entities on the screen and the engine doesn't give me instancing 11 solutions. We still see a lot of specialized engines, but the big engines are likely to improve and get better in performance. The best part about them is that they have those generalized tools that everyone likes and can use. People can also overwrite and optimize systems in the engine where they need. So I think that what we're going to see a shift in big game engines that allow the user to access more low-level data, but also allow them to do stuff at a high-level if they have no idea what to do at the low-level or want to get things done quickly. Smaller game engines will still exist, there will always be control freaks like me who like to do their own thing. Interview conducted November 11, 2018. Binary size refers to the size of the binary files built from source code and other assets. \u21a9 A Least Recently Used cache scheme is a strategy for evicting data from a memory cache based on how recently the data has been accessed. \u21a9 Dependency injection is a technique where one object supplies the dependencies of another object. A dependency is something that one object needs to run correctly, and injection is the process of passing one object to another. \u21a9 In computer graphics and games, a heightmap is a texture (rasterized image) where pixels have different meaning rather than representing color. One common usage of heightmap is to store surface elevation data. \u21a9 In the context of games, a viewport is a region of a 2D rectangle that's used to project the 3D scene to a virtual camera and thus provide a way to view the 3D virtual world. \u21a9 Goal-oriented action planner (GOAP) is an artificial intelligence system for agents that allows them to plan a sequence of actions to satisfy a particular goal. For a detailed explanation, visit http://alumni.media.mit.edu/~jorkin/goap.html \u21a9 Quadtree is a special type of tree data structure used in spatial partitioning. It recursively divides the whole space into four quads of the same size, and keeps doing it until each leaf quad contains a certain amount of actual spatial units (like polygons when used for rendering, and colliders when used for collision detection). If you are interested in learning more, refer to the Spatial Partitioning chapter in Game Programming Patterns. \u21a9 CDLOD is short for the paper titled Continuous Distance-Dependent Level of Detail Rendering Heightmaps. It describes a technique for GPU-based rendering of heightmap terrains. \u21a9 JSON (JavaScript Object Notation) is a lightweight data-interchange format that can be used for a database. It features a set of syntax that's both easy for human to understand and for machine to parse. \u21a9 In Unity, the Scriptable Render Pipeline (SRP) is an alternative to the built-in pipeline. With the SRP, developers can control and tailor rendering via C# scripts. This way, they can either slightly modify or completely build and customize the render pipeline to their needs. \u21a9 From Wikipedia: Geometry instancing is the practice of rendering multiple copies of the same mesh in a scene at once. This technique is primarily used for objects such as trees, grass, or buildings which can be represented as repeated geometry without appearing unduly repetitive. \u21a9","title":"Interview"},{"location":"interviews/ShaneeNishry-interview/#going-beyond-the-books","text":"Shanee Nishry is a VR Software Engineer at Google working on the Daydream team. She is concurrently developing her own engine for a fantasy RTS game. Shanee has also programmed for game engines at companies including Side-kick and Moon Active. In her spare time, Shanee practices swordfighting and European martial arts. (The following is the edited transcription of a conversation we had with Shanee Nishry.)","title":"Going Beyond the Books"},{"location":"interviews/ShaneeNishry-interview/#who-is-shanee","text":"My school background is kind of boring. Other than high school, I'm self-taught, so I have no degree or any formal studies in the field. I started a Biotechnology degree, but I didn't finish it because I got into a game studio in the meantime. Maybe when I was about 18 or 19, while I was in my Chemistry and Biotechnology courses, I started studying C++ and I wanted to make a video game. I started teaching myself DirectX, so I used a bunch of books like Programming Role Playing Games with DirectX by Jim Adams and Game Coding Complete by Mike McShaffry. Both are really awesome books that I'd definitely recommend. I started making my first engine by following these books because I knew absolutely nothing, but these two were brilliant and gave really good example code. By the end of following them, I went from having no idea what I was doing to kind of having an idea what I was doing! Because I started programming game engines quite a while ago, I can't think of what was specifically the most confusing aspect. Truth be told, back then I literally knew nothing about game engine programming; I didn't even know how to code properly! I remember one of the first books that I used for C++ was called Beginning C++ Through Game Programming . I just followed the books and adapted its code to do what I needed, so the process was pretty simple.","title":"Who is Shanee?"},{"location":"interviews/ShaneeNishry-interview/#keeping-it-minimal-decoupled-data-oriented","text":"There are a few things that have changed since I started in the games industry. One new paradigm that is very common in the industry right now is data-oriented design. Before that was introduced, everything was classes and hierarchies. Another recent question is \"how do you make the systems in your game interact with one another?\" So if you have input and you have your graphics and animations, you have to figure out how they will interact in a meaningful way. You also have to determine what gets access to the data. There are so many different paradigms to handle those questions. Some people will tell you to send an event whenever your position is changed, but then when you do that and learn about data-oriented design, you figure out everything is going in your cache and sending an event for every entity. It's just a mess. When working on Super Sam Adventures , we had to figure out how to access the transformation for an entity from different systems. We were originally using a map to access entities and then get the component out of them, and we noticed that on Android and iOS at the time, that map was so expensive because every lookup resulted in cache misses and we were doing too many lookups in the map. It just ruined the frame rate, so we had to change that. All of these questions about access, interaction between systems, and what a system actually needs to do were probably my biggest questions after I had a little bit of experience in game development. When you know nothing, you just follow some book or tutorial, and it all makes sense because you're doing what the book is telling you to do. When you start doing your own thing, though, things don't work as naturally. So maybe the biggest question I've had since then, and in some ways I (and other people I know) am still trying to answer it today, is about the interaction between systems; what is the responsibility of a system and how much should it be engineered? I feel like the general consensus these days\u2014and it might change a few years from now\u2014is that a system should be as small as it can be and do the minimum that it should do. The interaction between systems is a very dependent subject, but my preference is that it be as direct and as minimal as possible. By minimal I mean getting all the data that you'd need in one function, if possible. So for example, if I have a transformation system and a rendering system then my render system needs to get the transformation for its entities from the transformation system. Instead of doing it for every entity each time, if you can just make a synchronization point in your engine that says \"now I'm going to transfer all the data for the render system\", then that's probably better. Having specific synchronization points like that allows you to heavily specialize or multi-thread your systems. At the same time, I'm recruiting all of my threads to just fill up these few arrays of transformations, which will then be filled in the layout that the render system wants them to be. The render system then just takes the data and does whatever it does, and the rest of the engine is free to do whatever. That way, if you have any kind of multi-threading but you only have the synchronization points, there's no risk of a collision between threads because all of your data has been copied at a very specific point in time. That's the main idea: Make the systems as small as you can and interact as direct (but as little) as you can. One problem that you could find from this is having different implementations of an API and dealing with changes to that API, but it's a different problem which you might be able to avoid entirely.","title":"Keeping it Minimal, Decoupled &amp; Data-Oriented"},{"location":"interviews/ShaneeNishry-interview/#developing-for-an-ios-engine","text":"The Super Sam team was very iOS focused. I felt like I was the only Android person in the group. Most of the people did not put considerations into cross-platform development, but I did and I feel like it paid off, because the game was eventually ported to Android and didn't take too much work. At the end of the day, I feel like as long as you keep the very specific interaction with a platform abstracted from the rest of the engine, then it shouldn't take too long to port it to a similar platform. I think that the most difficulty comes when you have a PC game and then you want to port it to mobile or console, because that brings two big issues to the table. One of them is the performance and the memory requirements that are often different, and the other problem is just the input\u2014moving from keyboard to controller or from touch input to keyboard and mouse. One of the special considerations we had to have when developing for mobile was binary size 1 . And that's partially why we developed our own physics system. We didn't want to link with big external libraries because we wanted to keep the binary and final app size extremely low, and people simply didn't install heavy games. The expectation is that a person would install a game using mobile network which could be slow and limited. Additionally, the Google Store and AppStore would show a warning when trying to install a game over a certain size and most people would simply not install when the warning showed up. One of the reasons not to use Unity back then was because Unity on its own added quite a few megabytes, and that was a big no-no. Developing for mobile was definitely an interesting experience especially back at the time between the iPhone 3GS and the iPhone 4. For the upgrade to iPhone 4, Apple pretty much quadrupled the number of pixels and doubled the resolution, but the GPU wasn't much better. Suddenly you render the game on an iPhone 4, which is supposed to be better, but the frame rate is just shit compared to an iPhone 3GS. The other fun part of working on mobile was memory, and this is something that is often shared by people who worked on older consoles where memory was so limited. In much the same way, mobile was also extremely limited; you had to make sure that you knew exactly how much memory is used by your app and GPU at any given time.","title":"Developing for an iOS Engine"},{"location":"interviews/ShaneeNishry-interview/#caching-in-on-memory","text":"I feel like the biggest concerns with developing solely for iOS were memory management and performance. We really wanted to hit 60 frames per second and have it very smooth with no frame drops whatsoever. We also had problems keeping memory low so the app doesn't get killed randomly by the OS, which was really annoying on mobile platforms. We had to make some kind of a list of least-used memory so if a block of memory was not being used then you could just discard it. We also had to limit and deny allocations above a certain amount of memory. It involves making sure that everything fits into a very specific and tight memory requirement, and it means that we have to immediately discard anything that was not used when you had to have something allocated. On the other hand, we would not just immediately discard a texture if it doesn't have references because it might have a reference in five frames, and loading it again would be silly. If you have it in memory and then a new memory allocation needs to be made and that texture is occupying the least used memory block, then you can discard it. That's LRU cache 2 basically. Getting the scene and managing it with all of the assets and managing the level so they fit into that very tight requirement was a very large amount of work, especially because Super Sam was an infinite scroller; you'd basically fall down in this specific type of world environment, and then at some point it might change to a different one. We had to make sure that these change areas were as seamless as possible and without any kind of frame spikes. When it comes to dealing with memory management issues, you should determine how much memory you're actually allocating and whether or not that matches your diagnostic tool. Then you can see how much memory is in graphics, how much of that is textures, and how much is meshes, and so on. Not to mention how much memory is going to your entity component system data. It's a really big question of how much memory you actually need and how strict you need to be in memory management. The game I'm working on right now is for PC, not mobile, so instead of being very strict on memory, I'm just focusing on the way that memory and systems are being used. Because of this, I don't need to track it all in a centralized place. On mobile, where memory was way tighter, it was definitely needed.","title":"Caching in on Memory"},{"location":"interviews/ShaneeNishry-interview/#engine-api-design-is-a-thing","text":"I've taken the approaches of both overriding the new and delete functions and constructing my own APIs for memory management. Even so, I can't say which one I prefer. I guess overwriting it made it easy to ensure that everyone is using it, but it was very implicit and sometimes implicit is not very good. I often find if you're managing memory, then you probably want to know exactly what memory allocator you're using anyway. So just overwriting a global function is not necessarily the best idea. For example, if I want to do a temporary allocation that's nearly immediately released, then I know I need to access the linear memory allocator (one that is reset every frame) and I probably also know the size, so I can instantiate a specific memory allocator. But if I override the global new allocator and then I have a general memory management class that tries to manage all kinds of use cases, it's probably not going to do the best job. It might do a good job, but the best is when the user actually knows what they're trying to achieve and they can access a specialized function or API to do exactly that. Also, the platform memory allocators have improved so much in recent years that if you're just replacing the new allocator, there might not be a lot of advantage to it, but if you have a specific use case that needs to be optimized then you should use a memory allocator there. I'll give a quick example of this: When making systems, you often need to allocate memory blocks for your entity components (i.e. this is my transformation, this is my physics component, etc.) You know exactly what the page size is going to be, so you can just use a custom allocator for that system. You also know roughly how many entities you're going to have, and even if you didn't anticipate and you had to increase the size of the memory block, then you can easily just acquire another memory block and use it for the memory allocator later. Then you can, again, just allocate and release pages from that allocator for the specific use case. That's where I find memory allocators to be more useful\u2014when I know how they're going to be used, and what they're going to do exactly. In that way, the user can specify the memory size they want and where it will go. Following a similar principle outside of memory API, no matter what engine I'm working on, I try to make an API as small and easy to use as possible. To do that, I do some stuff that other developers would probably kill me for. There is different advice out there on the use of singletons versus dependency injections 3 , but generally the consensus is that you should have some kind of a context for whatever stuff that you're using. But at the end of the day, if I am a game developer using an engine, all I really want to know is where on the screen my mouse is! So I literally just have a namespace Input , and GetMousePosition is a global function in that namespace. Having a namespace housing functions is something that can be controversial, because the function doesn't have context as to which window it is listening to. However, I find it very easy to use because I can easily tell someone to go into the codebase and do Input::GetMousePosition , Input::GetMouseButton , etc., and structuring it like that really helped my productivity. So I try to figure out the APIs in terms of what is going to be useful and how easy it will be use it. What does the developer need to do, and how will the API accommodate that? The trouble often comes when your system tries to do too much. That's why I advocate making small, specialized systems as opposed to big, beastly systems. For example, if you have a rendering system and inside that you have an AddWater and AddBox and AddSphere , then it can probably be split into a few different systems, each specializing in their own specific things. Good API design can also help port your game to a new platform. At the end of the day, if you expose all of your engine's functionality then you can figure out a way around it later. So if you have access to the keyboard and mouse input and also to touch inputs as part of the engine, then you don't need to specifically map touch input to a mouse button or a mouse input. You can map it that way, but you don't always want to do it. Often if you do it that way, people will say \"hey, I need a way to recognize gesture and multi-touch\" and so on, and it will just come and bite you back if you've done premature abstraction. That's another lesson, not to do too much abstraction. As long as you have your own engine layer communicating with the OS and then give in the data with the least abstraction as possible and just hide what OS it is and hide what functionality is, then that will be good enough.","title":"Engine API Design is a Thing"},{"location":"interviews/ShaneeNishry-interview/#editing-with-a-level-editor","text":"For developing a level editor, the best advice I can give is to figure out what is actually needed out of it. Determine who is going to use it and the first functionality or two that they are going to need, and then work from there. It doesn't need to be perfect, it just needs to have the bare minimum of functionality supported. For example, in my editor, I have three main functionalities that I need. One of them is the elevation editing for the heightmap 4 , another is the texture editing for the terrain, and the last one is the placement of entities into the world. Once you identify those minimum features, you should be able to implement the editor pretty quickly. The biggest mistake I learned from that experience is over-engineering; do not make features for the sake of making features, ever . This is especially the case when you're working on an engine, where you should always have a use case in mind. It's ideal if you are able to make a small game while you're making the engine, even if it's as simple as Pong or Breakout . If you can make something like that with your engine easily, then you're doing well! When I made my very first level editor, it was very tightly related to the game engine itself. That was the first mistake, because it was basically living in the same application and codebase, and so it affected everything else a lot. When making the Super Sam editor, it was an entirely different application, and it didn't even share any of the code base; all it needed to know was the level format and how to output that. We didn't even use the engine to render the levels. The biggest question that we had back then was how to make the levels be supported across multiple games and versions. The level format was used for Bitter Sam , Super Sam , Super Sam Adventures , and possibly other games in the future. Figuring out how to differentiate between different games in your level format when using the same editor was one question, and the other was figuring out versioning if you added a new component or creature. We also needed to be able to work around the different versions of the app that may be installed on smartphones. There could potentially be a mismatch between the version of the game on the phone and the level being downloaded from the server, which is completely unrelated to the update mechanism on the phone. If the level has something like a new monster in it but its texture isn't in the application, then things don't work. For those reasons, compatibility support was really important back then. The level editor of Super Sam was using a different framework to render the things in the editor as compared to the game. Personally, I want to match my level editor renderer with my engine renderer as closely as I can. I love it when you have \"what you see is what you get\", and also when you can play inside the editor. However, that's possibly over-engineering at times and brings in too many additional features. For my current level editor, I am using my game engine, but I'm making it in an entirely new application and only linking to whatever code that I need to. For example, right now I'm not linking to any of the gameplay stuff, but only to the rendering frameworks and the input and so on. Using the engine's render in the editor you get the same look and feel, you get the same kind of performance, but it's not the only option. If you look at games like Warcraft and Starcraft , they have a level editor which is not using their engine's UI; it's using just generic OS UI. But they have a viewport 5 that draws the game window into it, and even in that viewport they added functionality for things like zooming out further in the game. That's just an example of you having all the UI in your game engine, or mix and match; you can use OS functionality to make UI easier to do, while also importing the entity rendering and terrain rendering from your engine to do that.","title":"Editing with a Level Editor"},{"location":"interviews/ShaneeNishry-interview/#after-hours-game-engine","text":"For my own engine architecture, I find that the biggest challenge is rendering large levels efficiently. My solution to that is chunking the world into different segments, which will let you immediately discard thousands of objects because they are off-screen. Another challenge is AI. My RTS is atypical because you can't control every unit; each unit has its own priorities and goals it wants to achieve in life (like making a video game!). I'm using a goal-oriented action planner 6 . So in the beginning, when having only a hundred entities, there's not much of a problem. But then, as you get to thousands of entities, you have to manage to keep some level of detail for the AI. You can't just make the AI stop functioning because they are off-screen; they are still alive and progressing. That's an interesting challenge that's so far removed from rendering, which I feel like is my main expertise. I can't just say \"If you don't see it, it doesn't exist.\" When a tree falls in the forest and nobody is around, it still exists! When developing a system I try to keep it as minimal and specialized as I can. For example, I'll do something like a quadtree 7 division with rendering, and that way it's really easy to discard entities not present on the screen. Similarly, using this quadtree division, I can put it into the AI and just update them at a different level of detail. That's the main consideration that I've had with my engine. Other than that, rendering things that scale terrain is a lot of fun. Just using CDLOD 8 for the giant terrain. If I remember correctly, the way I handled serialization in Super Sam was to just give an entity minimal transformation stuff like position and rotation, and then a tag of what entity it is. With my RTS, I take a similar approach where I have a blueprint that all entities can be instantiated from. In my case, I would have a footman or a werewolf blueprint, and then I can just say that an entity is referring to this blueprint while it holds its own transformation in the world. An interesting thing which I still haven't decided what to do if I need to have a unique entity for a level. The current idea I have is to add an additional blueprint definition that is level-specific, and then when instantiating that special entity it will still use the same system saying \"this is my position and this is the blueprint header file and that will have all of the data.\" I'm still not sure about this approach, because what if I went to add, say, a wounded soldier somewhere? Do I need to make a special blueprint for him? That's a problem that I just haven't reached yet, and there are different ideas on how to solve it. At the moment, I'm using JSON 9 to serialize stuff. I find that it's really useful if you do not have a binary format when you're just starting with things. If you're just getting to serialization, find a format that is easy to use and is human-readable, and only when you need to publish a product make it into a binary format that is small and specialized.","title":"After-Hours Game Engine"},{"location":"interviews/ShaneeNishry-interview/#being-your-own-product-manager","text":"If you're doing a personal project at the same time you're working a full-time job, you basically have to be your own product manager. You have to set goals both for your time at work and your time at home. You have to say, \"okay, I'm going to work and I need to get to these milestones. Today I'm going to shape this feature.\" And then you focus on that specific feature. But then when you head back home, it's really easy to just lie down in bed, watch Netflix, and neglect your project. The only way to prevent it is not by gathering motivation but by being persistent. You have to be determined to spend at least five or ten minutes a day on your personal work. Figuring out where you left off can be hard, though, so be sure to keep notes so you can remember what is most important to be working on at the moment. From there, determine the minimum tasks you need to do and then just sit down and do them, and the motivation will come as you do it.","title":"Being Your Own Product Manager"},{"location":"interviews/ShaneeNishry-interview/#specialized-engines-arent-going-away","text":"Many companies are making very good engines and improving their existing engines. I think we can look at Unity as an example here, because they have a very easy to use and commonly-used game engine, but even now they are making huge modifications to it. They have the Scriptable Render Pipeline 10 , and they are actively developing a new entity component system. With all that, we can see that even the big engines have to change. I think that's a sign of why specialized engines still exist; it's really difficult to make a generic engine that has everything. On the one hand, it's awesome because they're really making game development more accessible\u2014I know so many people that are not going to make a game engine but can just jump into Unity or Unreal and publish games. On the other hand, with big engines you're going to run into problems, like what happens when I need to have a million entities on the screen and the engine doesn't give me instancing 11 solutions. We still see a lot of specialized engines, but the big engines are likely to improve and get better in performance. The best part about them is that they have those generalized tools that everyone likes and can use. People can also overwrite and optimize systems in the engine where they need. So I think that what we're going to see a shift in big game engines that allow the user to access more low-level data, but also allow them to do stuff at a high-level if they have no idea what to do at the low-level or want to get things done quickly. Smaller game engines will still exist, there will always be control freaks like me who like to do their own thing. Interview conducted November 11, 2018. Binary size refers to the size of the binary files built from source code and other assets. \u21a9 A Least Recently Used cache scheme is a strategy for evicting data from a memory cache based on how recently the data has been accessed. \u21a9 Dependency injection is a technique where one object supplies the dependencies of another object. A dependency is something that one object needs to run correctly, and injection is the process of passing one object to another. \u21a9 In computer graphics and games, a heightmap is a texture (rasterized image) where pixels have different meaning rather than representing color. One common usage of heightmap is to store surface elevation data. \u21a9 In the context of games, a viewport is a region of a 2D rectangle that's used to project the 3D scene to a virtual camera and thus provide a way to view the 3D virtual world. \u21a9 Goal-oriented action planner (GOAP) is an artificial intelligence system for agents that allows them to plan a sequence of actions to satisfy a particular goal. For a detailed explanation, visit http://alumni.media.mit.edu/~jorkin/goap.html \u21a9 Quadtree is a special type of tree data structure used in spatial partitioning. It recursively divides the whole space into four quads of the same size, and keeps doing it until each leaf quad contains a certain amount of actual spatial units (like polygons when used for rendering, and colliders when used for collision detection). If you are interested in learning more, refer to the Spatial Partitioning chapter in Game Programming Patterns. \u21a9 CDLOD is short for the paper titled Continuous Distance-Dependent Level of Detail Rendering Heightmaps. It describes a technique for GPU-based rendering of heightmap terrains. \u21a9 JSON (JavaScript Object Notation) is a lightweight data-interchange format that can be used for a database. It features a set of syntax that's both easy for human to understand and for machine to parse. \u21a9 In Unity, the Scriptable Render Pipeline (SRP) is an alternative to the built-in pipeline. With the SRP, developers can control and tailor rendering via C# scripts. This way, they can either slightly modify or completely build and customize the render pipeline to their needs. \u21a9 From Wikipedia: Geometry instancing is the practice of rendering multiple copies of the same mesh in a scene at once. This technique is primarily used for objects such as trees, grass, or buildings which can be represented as repeated geometry without appearing unduly repetitive. \u21a9","title":"Specialized Engines Aren't Going Away"},{"location":"interviews/ShaneeNishry-video/","text":"The Havoc of Havok \u00b6","title":"Video Story"},{"location":"interviews/ShaneeNishry-video/#the-havoc-of-havok","text":"","title":"The Havoc of Havok"},{"location":"interviews/TommyRefenes-interview/","text":"The Engine Sandwich: Made with Super Meat \u00b6 Tommy Refenes is the programmer for the million-selling and award-winning Super Meat Boy and the lead programmer on the upcoming Super Meat Boy Forever . He also appeared in Indie Game: The Movie. (The following is the edited transcription of a conversation we had with Tommy Refenes.) Compartmentalizing is Key \u00b6 Regarding my first attempt at a game engine, I experimented with making a game engine right before I started working in the games industry. I got really confused while working on it, but it was really helpful going through the process. It prepared me more for working on the 2x engine 1 and then porting that engine to the Xbox 360 and working on the 360. By the time I left that first company, I learned the benefits of engine abstraction 2 and trying to keep platform-dependent 3 code separate from game code. This is important because in the first engines I would write, everything was all just one project. It was just a quick and dirty way to make something that worked. These are the places where you have to compartmentalize the different parts of development. For instance, the way my current engine is structured, there is no concept of Xbox, Switch, or Playstation in the code of Super Meat Boy or Super Meat Boy Forever . There's very little concept of platform even in the engine code, which is the piece that actually talks to the platforms. The biggest point of confusion when I was starting out was actually learning how to properly abstract and compartmentalize and implant, because when you're at the beginning of making an engine, where the hell do you start? That's a huge challenge; even getting something simple like a triangle drawing on the screen takes 100 lines of OpenGL 4 code or 200 lines of DirectX 5 code. From that point, even when you have a triangle on-screen, where do you go from there? I think learning that I needed to compartmentalize,having: my asset loader, my game, my controller code, my audio code, having all those things... I think that was the biggest point of confusion for me. Being able to compartmentalize those systems actually allowed me to have that starting point. Basic Principles of Abstraction \u00b6 Right before PAX last year, Forever was only running on Switch but two or three days before the convention, I realized I wanted to be able to show it in 1080p because I only had the handheld Switches to show at our booth. At first, I thought I should just buy some computers, but then I looked over on my desk and saw my two Xbox dev kits. So I ported to Xbox in about two days, and then PlayStation in a day and a half. I attribute this quick turnaround time to how the engine is structured. If it weren't for abstraction, porting games wouldn't have been nearly as easy for me. The engine has a core layer which consists of memory management, threading, anything specific to system startup for consoles, file reading, and all the very low-level base systems that will change per platform. The core layer is on the very bottom. On top of the core layer is the engine layer, which sits between the core and the game (which is at the very very top). The engine layer handles stuff like asset management, including the SWF 6 reader and Spine 7 . It also contains gameplay information, such as levels and the base level of your characters and players. You build upwards from the engine layer to make your game, but it has all the tools for making a level, like placing tiles from assets. If you imagine the engine layer is sitting there as a sandwich between core layer on the bottom and game layer on top, out to the side you have graphics, audio, input, etc. All of those are abstracted out, so if I want to load an asset I go into Tommunism Graphics (the namespace and the name of my engine), and then access Texture and from there Create Texture. All of those exist in-engine, but that function itself\u2014the low-level Create Texture function\u2014exists in the graphics layer that talks directly to the API for that particular platform. On the PC, for instance, the graphics layer is DirectX 11, and DirectX 11 has all of these functions that are defined like: Present, Draw, Load Texture, Load Vertex Buffer, and Load Shader. The engine layer can call those, so when I compile the static library of DirectX 11, it links to the engine. The engine layer only cares that Draw, Present\u2014all these things that I need\u2014are defined in this lower level of the graphics API. It was easy because I wasn't porting the engine or the entire game; I was simply porting graphics and input. If you abstract to the point where the engine can load in textures and vertex buffers and shaders, that's pretty much all you need. So porting those libraries, those static libraries of inputs and graphics, audio and whatever else, plus core which is the lower level stuff with file reading and everything, is all that is needed. None of those libraries talk to each other; they all talk through the engine layer, and the gameplay layer talks to the engine layer, which means that when I ported the Xbox version, I only had to port about three libraries. Once I ported those three libraries, as soon as I boot the game up, it works. Sometimes a couple little weird graphical things will come up because DirectX 11 works differently on Xbox than it does on PC or PlayStation 4, so I have to switch some shader stuff. But every time I port something, I get something on the screen immediately because everything is abstracted and the engine doesn't care as long as functions are defined. Using 3 rd Party Libraries \u00b6 I like understanding what's happening in my engine at all layers. All of the of the render pipelines stuff is custom, the layers that talk to the Xbox One DirectX 11 API/PlayStation/Switch and any other platform's graphics API are all written by me and then put under my abstraction layer for graphics. One of the big 3 rd party libraries I used was Box2D 8 . When I looked at the Box2D code, I thought it was good and I understood how it worked, but didn't want to write it because I didn't want to go through the process of writing a physics engine that can do exactly what Box2D does. That would take iterations of velocity and position solving, which I'm capable of doing but wouldn't want to waste time on. What I did was modified Box2D to work with the physics in my engine by doing the physics solving, acting as a solver 9 . I think the biggest part of game development is the asset pipeline. With Flash, a lot of the asset pipeline is just taken care of. In addition to Box2D, there are some other third party libraries I use. I integrated FMOD 10 for audio, because I originally made my own audio engine for the first Meat Boy but didn't want to do that again moving forward. I also used Bink 11 by Rad Game Tools , because cutscenes were getting a little too crazy. Forever's black-and-white intro cutscene is rendered as vector, but everything else is actually movies now because it was just easier for me to edit the movies and add effects than it was to put them in Flash. That made life way easier. The way Bink just plays videos makes things so easy, because all I have to do is integrate a few things and then movies run on every platform. Overall, it's a much better use of my time and money. Customizing toward Flash \u00b6 The way my engine is structured, there is only one concept of Flash and that is the SWF file loader. Everything else goes to a higher concept of animation and instance, so while I have support for Flash, I also built-in support for Spine and .obj files. The engine doesn't care; for instance, some of the things in Super Meat Boy and Forever I know are going to be Flash animations because that's where all of our assets are. For Meat Boy's character animations, I have a CharacterAnimation_Flash class that is part of character animation, but the Flash would have special little things. This lets me use the best parts of different file types and the engine is able to render them regardless. That way it's not pigeon-holed or anything, it's just a thing that the engine supports. This versatility gives me a lot of options for future projects. Down the road, if I want to make a 3D Meat Boy , I can just make a 3D Meat Boy with the engine. Or if we were going to do hand-drawn pixel art, which Flash is terrible for, I can do that. Tools for Your Teammates \u00b6 My philosophy is whatever program my artists or designers are comfortable working in, I'm going to try to make it so that they can continue their work with that program. I know that on my own, I am not going to make something that is better than what they're used to. If you think of this with regards to programming, I'm very used to C++, and people always ask me if I'm going to use Jonathan Blow 's JAI 12 . And I tell them no, because I'm comfortable with C++. I'm not looking to change it, I don't think C++ really needs to be better because I've worked around the limitations of it for so long that at this point they're not limitations anymore. When making Super Meat Boy , Edmund 13 was the most comfortable with Flash, just like I was, since we both started out on Newgrounds 14 and everything. It's what he drew in and animated in. So instead of making something new or trying to shoehorn him into some other process, I took the burden on myself. I made a tool that would let us export all the images, timelines and animations from Flash. The tool arranged all the assets on the stage, and then moved and sorted them so they were packed into the smallest texture possible. Finally, it would export texture coordinates and go through the timeline, which would be used by my custom animation index and PNG files that run all of Super Meat Boy . And that made it easy for Edmund to just work the way he needed to work. After Super Meat Boy was done, I thought it was time to cut out the middleman of PNG files. I wanted to make my engine just render SWF files, which I figured has to be possible because Flash renders them. What I did was download the SWF file specs and read through the full documentation. I found out exactly what's in an SWF file and I used that to make the new engine. The new engine can just load in an SWF. I also tied in some native code that I can put into actual FLA 15 files, and the artists can export those to SWF. Now I can add C++ code to animations, which is great when working with Flash artists (such as Edmund, Temmie , Paul , Sandra , and Ivan ) because everything is in Flash. This goes back to making it easy for the people you are working with. For example, Paul was making all of the animations for the enemies and they're all vector graphics, so we can scale those to any size and they work in-engine. He doesn't have to jump through any hoops. The only thing he and the other animators can't use is the Flash's filter stuff, but I've supported a lot of the filters that are in Flash now because it just makes sense. The engine renders them fast and it's a nice little asset package that you can zip; it does a lot to help out the team. Pixel to Vector Art \u00b6 A lot of Forever is vector graphics, but a lot of it is still rasterized 16 images that are read and compressed into the SWFs. The only weird challenge that came with switching over to vector from raster graphics is the fact that graphics cards are made to render polygons, and they're actually pretty garbage at rendering textures. That's why you have games that look like the new Spider-Man that runs at a steady framerate, but a game that has a bunch of particle effects that are 2D and has high-resolution art can run like garbage. It has to do with the fill rate, and the way you get around the fill rate of the hardware is by carefully choosing which pixels to render. You really can't do that with full-screen PNGs because you have to encode alpha and optimize to make it actually look right. That kind of optimization is not as trivial as doing a backface cull 18 , where if I was rendering a character model, I would only render half of it because you don't see the other side. With the vector stuff, I actually needed to render a little differently. This meant I got to use more advanced rendering techniques like stencil buffers 19 and depth culling 20 , which actually makes the vector graphics render way faster than any of the PNGs. It wasn't so much a challenge, as it was a different way of thinking. Everything in Super Meat Boy was just layered PNGs; you draw your background, you draw your foreground, you go from bottom up. With vectors, though, you tend to go from top down and mixing the two can be ridiculously inconvenient. Stealth Loading in Super Meat Boy \u00b6 Super Meat Boy and Forever levels consist of the palette and level information, which then need to be loaded by the engine. Information in Super Meat Boy was essentially run-length encoded 21 \u2014all the tiles, object positions, and object properties. Those are all very small files that can be read immediately in the memory and then have direct access through memory. When you have direct memory access instead of actual file loading, you don't have to worry about cache misses or swapping memory around to be able to load everything. That's just the level data; the palette data is your textures and your animations\u2014everything that has to do with the visuals of the game. For example, The Forest 22 is a palette, so every level in the Forest uses the forest palette. In Super Meat Boy, when you would go into the chapter, it would start playing a little cutscene where we'd introduce the chapter, and at the same time it would start loading in the palette on a different thread. During all of this the game would pause, but you wouldn't notice it because at the end of the cutscenes it has the black and white screen that says \"The Forest\" that goes \"dun dun dunnnn!\" 23 Even if you skip the cutscene during that time, it's continuing to load the palette. Because the palettes aren't very big anyway, it's only about four seconds so that jingle gives the game enough time to load everything I have. That's then cached so when you're in the level the only thing it is loading new is new level data which is anywhere between 5 KB and 50 KB. Even the craziest levels are only about 100 KB. The textures were already loaded from the palette, then the game would continue from there. I utilized a lot with the presentation of the game to actually background load assets such as palettes. I think the boss battle cutscenes loaded either during the chapter intro or when you were on the overworld map getting ready to hit the button to go into the boss level. Again, the loads were small because the bosses were just a couple assets, and the levels and palette were already loaded. So for the Lil' Slugger 24 fight, all that is loaded is Lil' Slugger's animations, which are just him walking. The reason everything seems to load so fast is that all the heavy lifting is done when the player doesn't notice it. \"Garbage Physics\" \u00b6 The garbage physics came about all from iteration 25 . For Forever , I wanted to keep the exact same feel of Meat Boy with the new game, so a lot of the physics are exactly the same. The player doesn't have control over the direction, so physics from Super Meat Boy like air friction for when you're turning around in the air, the amount of time when you're going in one direction and then switch to another direction, those don't exist in Forever because you don't have any control over direction. But how his jump works and the new mechanics, like his punch and dive, were also developed from what I call \"garbage physics.\" There's different obstacles, like fans, that had to be adjusted for this game, which also required iteration. Nothing in the game is physically accurate to real physics, not a single thing. All of Meat Boy's weird controls are done in-game, not in-engine. Interview conducted September 13, 2018. Refers to Unreal Engine 2.x, which was originally debuted in 2002 with America's Army . The Unreal Engine is a source-available game engine developed by Epic Games \u21a9 Engine abstraction is the part of the engine code which depends on the hardware/software platform that the engine runs on and will be different on each platform. For example, the code that talks to the operating system on macOS will be different from that on Windows. Engine developers usually tackle this problem by having an abstraction layer on top of operating system code. So the code above that layer still looks the same when you swap out the underlying operating system. \u21a9 Platform-dependent code refers to application code that is dependent on one operating system, and typically won't run on multiple. \u21a9 OpenGL is short for Open Graphics Library - a cross-language, cross-platform application programming interface (API) for rendering 2D and 3D vector graphics. The API is typically used to interact with a graphics processing unit (GPU), to achieve hardware-accelerated rendering. It's the underlying rendering library for many modern game engines. \u21a9 Microsoft DirectX is a collection of application programming interfaces (APIs) for handling tasks related to multimedia, especially game programming, on Microsoft platforms, like Windows and Xbox. It is most known for Direct3D which is the graphics API used for creating windows and rendering, and serves similar purposes as OpenGL. \u21a9 SWF , short for Small Web Format, is an Adobe Flash file format used for multimedia, vector graphics and ActionScript. SWF files can contain animations or applets of varying degrees of interactivity and function. \u21a9 Spine is a 2D skeletal animation software for video games by Esoteric Software. \u21a9 Box2D is an open source C++ engine for simulating rigid bodies in 2D. Box2D is developed by Erin Catto and has the zlib license. \u21a9 Physics engine for games usually consists of two parts: collision detection and collision resolution, and solver refers to the resolution part. Collision detection detects what objects collide with each other first, and then the solver determines their correct physical response, like position, rotation, velocity, etc. \u21a9 FMOD is a cross-platform audio engine and authoring tool used throughout the game industry. It was used by over 2,000 games in the last 15 years. \u21a9 Bink is the defacto video codec for games created by Rad Tools. \u21a9 JAI is a language being developed by Jonathan Blow and his at Thekla to address some of the issues game developers have with the current industry standard, C++. \u21a9 Edmund McMillen worked on Super Meat Boy together with Tommy as an artist and designer. He is also famous for making The Binding of Isaac and its remake. \u21a9 Newgrounds is an American online entertainment and social media website and company. The site hosts user-generated content such as games, movies, audio, and artwork in four respective site \"portals\". \u21a9 FLA is the file format for projects created by Adobe Animate, and can contain graphics, video, text, audio, and more. They are often saved as SWF files to be used on the web. \u21a9 A rasterized image is one which is represented as a grid of pixels with RGBA color. \u21a9 Fill rate refers to the number of pixels a video card can read/write to the screen per second. \u21a9 Backface culling is the technique of performing visibility checks on a mesh to not render the back face (face not facing the camera). \u21a9 Stencil buffer is an additional depth buffer to the depth and color buffers. \u21a9 Depth culling is the process of deciding which elements to render based on the distance from the camera and if it is being hidden by another element. \u21a9 Run-length encoding is a form of lossless data compression where data is stored as a single data value and count, for more . \u21a9 The Forest is the first chapter of Super Meat Boy . \u21a9 For reference of what Tommy is talking about, here is a clip . \u21a9 Lil' Slugger is the first boss in the Super Meat Boy game. \u21a9 Tommy talked more about the creation of the Meat Boy physics in an interview with Casey Muratori at HandmadeCon 2015 . \u21a9","title":"Interview"},{"location":"interviews/TommyRefenes-interview/#the-engine-sandwich-made-with-super-meat","text":"Tommy Refenes is the programmer for the million-selling and award-winning Super Meat Boy and the lead programmer on the upcoming Super Meat Boy Forever . He also appeared in Indie Game: The Movie. (The following is the edited transcription of a conversation we had with Tommy Refenes.)","title":"The Engine Sandwich: Made with Super Meat"},{"location":"interviews/TommyRefenes-interview/#compartmentalizing-is-key","text":"Regarding my first attempt at a game engine, I experimented with making a game engine right before I started working in the games industry. I got really confused while working on it, but it was really helpful going through the process. It prepared me more for working on the 2x engine 1 and then porting that engine to the Xbox 360 and working on the 360. By the time I left that first company, I learned the benefits of engine abstraction 2 and trying to keep platform-dependent 3 code separate from game code. This is important because in the first engines I would write, everything was all just one project. It was just a quick and dirty way to make something that worked. These are the places where you have to compartmentalize the different parts of development. For instance, the way my current engine is structured, there is no concept of Xbox, Switch, or Playstation in the code of Super Meat Boy or Super Meat Boy Forever . There's very little concept of platform even in the engine code, which is the piece that actually talks to the platforms. The biggest point of confusion when I was starting out was actually learning how to properly abstract and compartmentalize and implant, because when you're at the beginning of making an engine, where the hell do you start? That's a huge challenge; even getting something simple like a triangle drawing on the screen takes 100 lines of OpenGL 4 code or 200 lines of DirectX 5 code. From that point, even when you have a triangle on-screen, where do you go from there? I think learning that I needed to compartmentalize,having: my asset loader, my game, my controller code, my audio code, having all those things... I think that was the biggest point of confusion for me. Being able to compartmentalize those systems actually allowed me to have that starting point.","title":"Compartmentalizing is Key"},{"location":"interviews/TommyRefenes-interview/#basic-principles-of-abstraction","text":"Right before PAX last year, Forever was only running on Switch but two or three days before the convention, I realized I wanted to be able to show it in 1080p because I only had the handheld Switches to show at our booth. At first, I thought I should just buy some computers, but then I looked over on my desk and saw my two Xbox dev kits. So I ported to Xbox in about two days, and then PlayStation in a day and a half. I attribute this quick turnaround time to how the engine is structured. If it weren't for abstraction, porting games wouldn't have been nearly as easy for me. The engine has a core layer which consists of memory management, threading, anything specific to system startup for consoles, file reading, and all the very low-level base systems that will change per platform. The core layer is on the very bottom. On top of the core layer is the engine layer, which sits between the core and the game (which is at the very very top). The engine layer handles stuff like asset management, including the SWF 6 reader and Spine 7 . It also contains gameplay information, such as levels and the base level of your characters and players. You build upwards from the engine layer to make your game, but it has all the tools for making a level, like placing tiles from assets. If you imagine the engine layer is sitting there as a sandwich between core layer on the bottom and game layer on top, out to the side you have graphics, audio, input, etc. All of those are abstracted out, so if I want to load an asset I go into Tommunism Graphics (the namespace and the name of my engine), and then access Texture and from there Create Texture. All of those exist in-engine, but that function itself\u2014the low-level Create Texture function\u2014exists in the graphics layer that talks directly to the API for that particular platform. On the PC, for instance, the graphics layer is DirectX 11, and DirectX 11 has all of these functions that are defined like: Present, Draw, Load Texture, Load Vertex Buffer, and Load Shader. The engine layer can call those, so when I compile the static library of DirectX 11, it links to the engine. The engine layer only cares that Draw, Present\u2014all these things that I need\u2014are defined in this lower level of the graphics API. It was easy because I wasn't porting the engine or the entire game; I was simply porting graphics and input. If you abstract to the point where the engine can load in textures and vertex buffers and shaders, that's pretty much all you need. So porting those libraries, those static libraries of inputs and graphics, audio and whatever else, plus core which is the lower level stuff with file reading and everything, is all that is needed. None of those libraries talk to each other; they all talk through the engine layer, and the gameplay layer talks to the engine layer, which means that when I ported the Xbox version, I only had to port about three libraries. Once I ported those three libraries, as soon as I boot the game up, it works. Sometimes a couple little weird graphical things will come up because DirectX 11 works differently on Xbox than it does on PC or PlayStation 4, so I have to switch some shader stuff. But every time I port something, I get something on the screen immediately because everything is abstracted and the engine doesn't care as long as functions are defined.","title":"Basic Principles of Abstraction"},{"location":"interviews/TommyRefenes-interview/#using-3rd-party-libraries","text":"I like understanding what's happening in my engine at all layers. All of the of the render pipelines stuff is custom, the layers that talk to the Xbox One DirectX 11 API/PlayStation/Switch and any other platform's graphics API are all written by me and then put under my abstraction layer for graphics. One of the big 3 rd party libraries I used was Box2D 8 . When I looked at the Box2D code, I thought it was good and I understood how it worked, but didn't want to write it because I didn't want to go through the process of writing a physics engine that can do exactly what Box2D does. That would take iterations of velocity and position solving, which I'm capable of doing but wouldn't want to waste time on. What I did was modified Box2D to work with the physics in my engine by doing the physics solving, acting as a solver 9 . I think the biggest part of game development is the asset pipeline. With Flash, a lot of the asset pipeline is just taken care of. In addition to Box2D, there are some other third party libraries I use. I integrated FMOD 10 for audio, because I originally made my own audio engine for the first Meat Boy but didn't want to do that again moving forward. I also used Bink 11 by Rad Game Tools , because cutscenes were getting a little too crazy. Forever's black-and-white intro cutscene is rendered as vector, but everything else is actually movies now because it was just easier for me to edit the movies and add effects than it was to put them in Flash. That made life way easier. The way Bink just plays videos makes things so easy, because all I have to do is integrate a few things and then movies run on every platform. Overall, it's a much better use of my time and money.","title":"Using 3rd Party Libraries"},{"location":"interviews/TommyRefenes-interview/#customizing-toward-flash","text":"The way my engine is structured, there is only one concept of Flash and that is the SWF file loader. Everything else goes to a higher concept of animation and instance, so while I have support for Flash, I also built-in support for Spine and .obj files. The engine doesn't care; for instance, some of the things in Super Meat Boy and Forever I know are going to be Flash animations because that's where all of our assets are. For Meat Boy's character animations, I have a CharacterAnimation_Flash class that is part of character animation, but the Flash would have special little things. This lets me use the best parts of different file types and the engine is able to render them regardless. That way it's not pigeon-holed or anything, it's just a thing that the engine supports. This versatility gives me a lot of options for future projects. Down the road, if I want to make a 3D Meat Boy , I can just make a 3D Meat Boy with the engine. Or if we were going to do hand-drawn pixel art, which Flash is terrible for, I can do that.","title":"Customizing toward Flash"},{"location":"interviews/TommyRefenes-interview/#tools-for-your-teammates","text":"My philosophy is whatever program my artists or designers are comfortable working in, I'm going to try to make it so that they can continue their work with that program. I know that on my own, I am not going to make something that is better than what they're used to. If you think of this with regards to programming, I'm very used to C++, and people always ask me if I'm going to use Jonathan Blow 's JAI 12 . And I tell them no, because I'm comfortable with C++. I'm not looking to change it, I don't think C++ really needs to be better because I've worked around the limitations of it for so long that at this point they're not limitations anymore. When making Super Meat Boy , Edmund 13 was the most comfortable with Flash, just like I was, since we both started out on Newgrounds 14 and everything. It's what he drew in and animated in. So instead of making something new or trying to shoehorn him into some other process, I took the burden on myself. I made a tool that would let us export all the images, timelines and animations from Flash. The tool arranged all the assets on the stage, and then moved and sorted them so they were packed into the smallest texture possible. Finally, it would export texture coordinates and go through the timeline, which would be used by my custom animation index and PNG files that run all of Super Meat Boy . And that made it easy for Edmund to just work the way he needed to work. After Super Meat Boy was done, I thought it was time to cut out the middleman of PNG files. I wanted to make my engine just render SWF files, which I figured has to be possible because Flash renders them. What I did was download the SWF file specs and read through the full documentation. I found out exactly what's in an SWF file and I used that to make the new engine. The new engine can just load in an SWF. I also tied in some native code that I can put into actual FLA 15 files, and the artists can export those to SWF. Now I can add C++ code to animations, which is great when working with Flash artists (such as Edmund, Temmie , Paul , Sandra , and Ivan ) because everything is in Flash. This goes back to making it easy for the people you are working with. For example, Paul was making all of the animations for the enemies and they're all vector graphics, so we can scale those to any size and they work in-engine. He doesn't have to jump through any hoops. The only thing he and the other animators can't use is the Flash's filter stuff, but I've supported a lot of the filters that are in Flash now because it just makes sense. The engine renders them fast and it's a nice little asset package that you can zip; it does a lot to help out the team.","title":"Tools for Your Teammates"},{"location":"interviews/TommyRefenes-interview/#pixel-to-vector-art","text":"A lot of Forever is vector graphics, but a lot of it is still rasterized 16 images that are read and compressed into the SWFs. The only weird challenge that came with switching over to vector from raster graphics is the fact that graphics cards are made to render polygons, and they're actually pretty garbage at rendering textures. That's why you have games that look like the new Spider-Man that runs at a steady framerate, but a game that has a bunch of particle effects that are 2D and has high-resolution art can run like garbage. It has to do with the fill rate, and the way you get around the fill rate of the hardware is by carefully choosing which pixels to render. You really can't do that with full-screen PNGs because you have to encode alpha and optimize to make it actually look right. That kind of optimization is not as trivial as doing a backface cull 18 , where if I was rendering a character model, I would only render half of it because you don't see the other side. With the vector stuff, I actually needed to render a little differently. This meant I got to use more advanced rendering techniques like stencil buffers 19 and depth culling 20 , which actually makes the vector graphics render way faster than any of the PNGs. It wasn't so much a challenge, as it was a different way of thinking. Everything in Super Meat Boy was just layered PNGs; you draw your background, you draw your foreground, you go from bottom up. With vectors, though, you tend to go from top down and mixing the two can be ridiculously inconvenient.","title":"Pixel to Vector Art"},{"location":"interviews/TommyRefenes-interview/#stealth-loading-in-super-meat-boy","text":"Super Meat Boy and Forever levels consist of the palette and level information, which then need to be loaded by the engine. Information in Super Meat Boy was essentially run-length encoded 21 \u2014all the tiles, object positions, and object properties. Those are all very small files that can be read immediately in the memory and then have direct access through memory. When you have direct memory access instead of actual file loading, you don't have to worry about cache misses or swapping memory around to be able to load everything. That's just the level data; the palette data is your textures and your animations\u2014everything that has to do with the visuals of the game. For example, The Forest 22 is a palette, so every level in the Forest uses the forest palette. In Super Meat Boy, when you would go into the chapter, it would start playing a little cutscene where we'd introduce the chapter, and at the same time it would start loading in the palette on a different thread. During all of this the game would pause, but you wouldn't notice it because at the end of the cutscenes it has the black and white screen that says \"The Forest\" that goes \"dun dun dunnnn!\" 23 Even if you skip the cutscene during that time, it's continuing to load the palette. Because the palettes aren't very big anyway, it's only about four seconds so that jingle gives the game enough time to load everything I have. That's then cached so when you're in the level the only thing it is loading new is new level data which is anywhere between 5 KB and 50 KB. Even the craziest levels are only about 100 KB. The textures were already loaded from the palette, then the game would continue from there. I utilized a lot with the presentation of the game to actually background load assets such as palettes. I think the boss battle cutscenes loaded either during the chapter intro or when you were on the overworld map getting ready to hit the button to go into the boss level. Again, the loads were small because the bosses were just a couple assets, and the levels and palette were already loaded. So for the Lil' Slugger 24 fight, all that is loaded is Lil' Slugger's animations, which are just him walking. The reason everything seems to load so fast is that all the heavy lifting is done when the player doesn't notice it.","title":"Stealth Loading in Super Meat Boy"},{"location":"interviews/TommyRefenes-interview/#garbage-physics","text":"The garbage physics came about all from iteration 25 . For Forever , I wanted to keep the exact same feel of Meat Boy with the new game, so a lot of the physics are exactly the same. The player doesn't have control over the direction, so physics from Super Meat Boy like air friction for when you're turning around in the air, the amount of time when you're going in one direction and then switch to another direction, those don't exist in Forever because you don't have any control over direction. But how his jump works and the new mechanics, like his punch and dive, were also developed from what I call \"garbage physics.\" There's different obstacles, like fans, that had to be adjusted for this game, which also required iteration. Nothing in the game is physically accurate to real physics, not a single thing. All of Meat Boy's weird controls are done in-game, not in-engine. Interview conducted September 13, 2018. Refers to Unreal Engine 2.x, which was originally debuted in 2002 with America's Army . The Unreal Engine is a source-available game engine developed by Epic Games \u21a9 Engine abstraction is the part of the engine code which depends on the hardware/software platform that the engine runs on and will be different on each platform. For example, the code that talks to the operating system on macOS will be different from that on Windows. Engine developers usually tackle this problem by having an abstraction layer on top of operating system code. So the code above that layer still looks the same when you swap out the underlying operating system. \u21a9 Platform-dependent code refers to application code that is dependent on one operating system, and typically won't run on multiple. \u21a9 OpenGL is short for Open Graphics Library - a cross-language, cross-platform application programming interface (API) for rendering 2D and 3D vector graphics. The API is typically used to interact with a graphics processing unit (GPU), to achieve hardware-accelerated rendering. It's the underlying rendering library for many modern game engines. \u21a9 Microsoft DirectX is a collection of application programming interfaces (APIs) for handling tasks related to multimedia, especially game programming, on Microsoft platforms, like Windows and Xbox. It is most known for Direct3D which is the graphics API used for creating windows and rendering, and serves similar purposes as OpenGL. \u21a9 SWF , short for Small Web Format, is an Adobe Flash file format used for multimedia, vector graphics and ActionScript. SWF files can contain animations or applets of varying degrees of interactivity and function. \u21a9 Spine is a 2D skeletal animation software for video games by Esoteric Software. \u21a9 Box2D is an open source C++ engine for simulating rigid bodies in 2D. Box2D is developed by Erin Catto and has the zlib license. \u21a9 Physics engine for games usually consists of two parts: collision detection and collision resolution, and solver refers to the resolution part. Collision detection detects what objects collide with each other first, and then the solver determines their correct physical response, like position, rotation, velocity, etc. \u21a9 FMOD is a cross-platform audio engine and authoring tool used throughout the game industry. It was used by over 2,000 games in the last 15 years. \u21a9 Bink is the defacto video codec for games created by Rad Tools. \u21a9 JAI is a language being developed by Jonathan Blow and his at Thekla to address some of the issues game developers have with the current industry standard, C++. \u21a9 Edmund McMillen worked on Super Meat Boy together with Tommy as an artist and designer. He is also famous for making The Binding of Isaac and its remake. \u21a9 Newgrounds is an American online entertainment and social media website and company. The site hosts user-generated content such as games, movies, audio, and artwork in four respective site \"portals\". \u21a9 FLA is the file format for projects created by Adobe Animate, and can contain graphics, video, text, audio, and more. They are often saved as SWF files to be used on the web. \u21a9 A rasterized image is one which is represented as a grid of pixels with RGBA color. \u21a9 Fill rate refers to the number of pixels a video card can read/write to the screen per second. \u21a9 Backface culling is the technique of performing visibility checks on a mesh to not render the back face (face not facing the camera). \u21a9 Stencil buffer is an additional depth buffer to the depth and color buffers. \u21a9 Depth culling is the process of deciding which elements to render based on the distance from the camera and if it is being hidden by another element. \u21a9 Run-length encoding is a form of lossless data compression where data is stored as a single data value and count, for more . \u21a9 The Forest is the first chapter of Super Meat Boy . \u21a9 For reference of what Tommy is talking about, here is a clip . \u21a9 Lil' Slugger is the first boss in the Super Meat Boy game. \u21a9 Tommy talked more about the creation of the Meat Boy physics in an interview with Casey Muratori at HandmadeCon 2015 . \u21a9","title":"\"Garbage Physics\""},{"location":"interviews/TommyRefenes-video/","text":"Connecting with Fans at PAX \u00b6","title":"Video Story"},{"location":"interviews/TommyRefenes-video/#connecting-with-fans-at-pax","text":"","title":"Connecting with Fans at PAX"},{"location":"interviews/WaltDestler-advice/","text":"Walt is an independent game/entertainment programmer and designer currently working in the San Francisco Bay area. He was a programmer and designer on WAY , the IGF 2012 Best Student Game Winner, and is now independently working on his new game Cosmoteer . We approached Walt because he also went to the ETC for graduate school. He was working on a side project to build a game engine for more than three years and is now making a game with his own engine. We did a Skype call with him. Advice ( not verbatim): \u00b6 Engine design When making your game, deliberately separate engine code from game code. Draw a clear wall between them right from the start and make design decisions to facilitate that. Build a game to prove that your engine works. Try to make engine code robust and reusable. Experience Walt's first engine originated from the game he was making in college, and he kept the engine code to be used for future games. He found he was adding features to the engine frequently when making his second game. He started with Unity when making his current game, and later switched to his own engine for the following reasons: Limitations of Unity's sprite animation system Hard to determine which functions get called before others (back when Script Execution Order setting was unavailable in Unity) 2D physics capability was limited, so he was forced to use only 2 dimensions of the 3D physics engine which was more complicated and expensive Components Two games and one game engine are huge amounts of work, meaning we will need to decide which components to implement and which to import: Custom engines don't necessarily have a ton of tools, like a level editor or a UI editor You can still use graphics API to draw UI for developer yourself There a handful of 3 rd party libraries that simplify development, like Iron Python for scripting and Farseer for physics (if physics is less interested and physX is too much) Networks for strategy games are easier to implement UDP should be used for latency-sensitive games like FPS and action games, but a layer needs to be built on top of UDP for reliability TCP is fine for things like strategy games Physics(collisions) are more useful for engines than netcode, but can still lead to glaring omissions. It's hard to graft physics component onto an existing game engine, but easier to graft networking.","title":"Walt Destler"},{"location":"interviews/WaltDestler-advice/#advice-not-verbatim","text":"Engine design When making your game, deliberately separate engine code from game code. Draw a clear wall between them right from the start and make design decisions to facilitate that. Build a game to prove that your engine works. Try to make engine code robust and reusable. Experience Walt's first engine originated from the game he was making in college, and he kept the engine code to be used for future games. He found he was adding features to the engine frequently when making his second game. He started with Unity when making his current game, and later switched to his own engine for the following reasons: Limitations of Unity's sprite animation system Hard to determine which functions get called before others (back when Script Execution Order setting was unavailable in Unity) 2D physics capability was limited, so he was forced to use only 2 dimensions of the 3D physics engine which was more complicated and expensive Components Two games and one game engine are huge amounts of work, meaning we will need to decide which components to implement and which to import: Custom engines don't necessarily have a ton of tools, like a level editor or a UI editor You can still use graphics API to draw UI for developer yourself There a handful of 3 rd party libraries that simplify development, like Iron Python for scripting and Farseer for physics (if physics is less interested and physX is too much) Networks for strategy games are easier to implement UDP should be used for latency-sensitive games like FPS and action games, but a layer needs to be built on top of UDP for reliability TCP is fine for things like strategy games Physics(collisions) are more useful for engines than netcode, but can still lead to glaring omissions. It's hard to graft physics component onto an existing game engine, but easier to graft networking.","title":"Advice (not verbatim):"},{"location":"interviews/coming-soon/","text":"Upcoming Interviews \u00b6 We are currently in talks with other game engine programmers about interviews, and we will update this page when we've got interviews scheduled. Subscribe to our mailing list Get notifications about the upcoming blogs and interviews!","title":"Upcoming Interviews"},{"location":"interviews/coming-soon/#upcoming-interviews","text":"We are currently in talks with other game engine programmers about interviews, and we will update this page when we've got interviews scheduled.","title":"Upcoming Interviews"},{"location":"interviews/person-video/","text":"Fake Video \u00b6","title":"Fake Video"},{"location":"interviews/person-video/#fake-video","text":"","title":"Fake Video"}]}